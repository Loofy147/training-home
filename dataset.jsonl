{"text": "<p align=\"center\"><img src=\"images/devops_exercises.png\"/></p> :information_source: &nbsp;This repo contains questions and exercises on various technical topics, sometimes related to DevOps and SRE :bar_chart: &nbsp;There are currently **2624** exercises and questions :warning: &nbsp;You can use these for preparing for an interview but most of the questions and exercises don't represent an actual interview. Please read [FAQ page](faq.md) for more details :stop_sign: &nbsp;If you are interested in pursuing a career as DevOps engineer, learning some of the concepts mentioned here would be useful, but you should know it's not about learning all the topics and technologies mentioned in this repository :pencil: &nbsp;You can add more exercises by submitting pull requests :) Read about contribution guidelines [here](CONTRIBUTING.md) **** <!-- ALL-TOPICS-LIST:START --> <!-- prettier-ignore-start --> <!-- markdownlint-disable --> <center> <table> <tr> <td align=\"center\"><a", "metadata": {"source_file": "learning-materials/README.md", "section": "Introduction", "language": "en", "created_at": "2025-07-19T19:22:01.992817"}}
{"text": "href=\"topics/devops/README.md\"><img src=\"images/devops.png\" width=\"75px;\" height=\"75px;\" alt=\"DevOps\" /><br /><b>DevOps</b></a></td> <td align=\"center\"><a href=\"topics/git/README.md\"><img src=\"images/git.png\" width=\"75px;\" height=\"75px;\" alt=\"Git\"/><br /><b>Git</b></a></td> <td align=\"center\"><a href=\"#network\"><img src=\"images/network.png\" width=\"75px;\" height=\"75px;\" alt=\"Network\"/><br /><b>Network</b></a></td> <td align=\"center\"><a href=\"#hardware\"><img src=\"images/hardware.png\" width=\"75px;\" height=\"75px;\" alt=\"Hardware\"/><br /><b>Hardware</b></a></td> <td align=\"center\"><a href=\"topics/kubernetes/README.md\"><img src=\"images/kubernetes.png\" width=\"75px;\" height=\"75px;\" alt=\"kubernetes\"/><br /><b>Kubernetes</b></a></td> </tr> <tr> <td align=\"center\"><a href=\"topics/software_development/README.md\"><img src=\"images/programming.png\" width=\"75px;\" height=\"75px;\" alt=\"programming\"/><br /><b>Software Development</b></a></td> <td align=\"center\"><a", "metadata": {"source_file": "learning-materials/README.md", "section": "Introduction", "language": "en", "created_at": "2025-07-19T19:22:01.992877"}}
{"text": "href=\"https://github.com/bregman-arie/python-exercises\"><img src=\"images/python.png\" width=\"75px;\" height=\"75px;\" alt=\"Python\"/><br /><b>Python</b></a></td> <td align=\"center\"><a href=\"https://github.com/bregman-arie/go-exercises\"><img src=\"images/Go.png\" width=\"75px;\" height=\"75px;\" alt=\"go\"/><br /><b>Go</b></a></td> <td align=\"center\"><a href=\"topics/perl/README.md\"><img src=\"images/perl.png\" width=\"75px;\" height=\"75px;\" alt=\"perl\"/><br /><b>Perl</b></a></td> <td align=\"center\"><a href=\"#regex\"><img src=\"images/regex.png\" width=\"75px;\" height=\"75px;\" alt=\"RegEx\"/><br /><b>Regex</b></a></td> </tr> <tr> <td align=\"center\"><a href=\"topics/cloud/README.md\"><img src=\"images/cloud.png\" width=\"75px;\" height=\"75px;\" alt=\"Cloud\"/><br /><b>Cloud</b></a></td> <td align=\"center\"><a href=\"topics/aws/README.md\"><img src=\"images/aws.png\" width=\"100px;\" height=\"75px;\" alt=\"aws\"/><br /><b>AWS</b></a></td> <td align=\"center\"><a href=\"topics/azure/README.md\"><img src=\"images/azure.png\" width=\"75px;\"", "metadata": {"source_file": "learning-materials/README.md", "section": "Introduction", "language": "en", "created_at": "2025-07-19T19:22:01.992906"}}
{"text": "height=\"75px;\" alt=\"azure\"/><br /><b>Azure</b></a></td> <td align=\"center\"><a href=\"topics/gcp/README.md\"><img src=\"images/googlecloud.png\" width=\"70px;\" height=\"70px;\" alt=\"Google Cloud Platform\"/><br /><b>Google Cloud Platform</b></a></td> <td align=\"center\"><a href=\"#openstack/README.md\"><img src=\"images/openstack.png\" width=\"75px;\" height=\"75px;\" alt=\"openstack\"/><br /><b>OpenStack</b></a></td> </tr> <tr> <td align=\"center\"><a href=\"#operating-system\"><img src=\"images/os.png\" width=\"75px;\" height=\"75px;\" alt=\"Operating System\"/><br /><b>Operating System</b></a></td> <td align=\"center\"><a href=\"topics/linux/README.md\"><img src=\"images/logos/linux.png\" width=\"75px;\" height=\"75px;\" alt=\"Linux\"/><br /><b>Linux</b></a></td> <td align=\"center\"><a href=\"#virtualization\"><img src=\"images/virtualization.png\" width=\"75px;\" height=\"75px;\" alt=\"Virtualization\"/><br /><b>Virtualization</b></a></td> <td align=\"center\"><a href=\"topics/dns/README.md\"><img src=\"images/dns.png\" width=\"75px;\"", "metadata": {"source_file": "learning-materials/README.md", "section": "Introduction", "language": "en", "created_at": "2025-07-19T19:22:01.992927"}}
{"text": "height=\"75px;\" alt=\"DNS\"/><br /><b>DNS</b></a></td> <td align=\"center\"><a href=\"topics/shell/README.md\"><img src=\"images/bash.png\" width=\"75px;\" height=\"75px;\" alt=\"Bash\"/><br /><b>Shell Scripting</b></a></td> </tr> <tr> <td align=\"center\"><a href=\"topics/databases/README.md\"><img src=\"images/databases.png\" width=\"75px;\" height=\"75px;\" alt=\"Databases\"/><br /><b>Databases</b></a></td> <td align=\"center\"><a href=\"#sql\"><img src=\"images/sql.png\" width=\"75px;\" height=\"75px;\" alt=\"sql\"/><br /><b>SQL</b></a></td> <td align=\"center\"><a href=\"#mongo\"><img src=\"images/mongo.png\" width=\"75px;\" height=\"75px;\" alt=\"Mongo\"/><br /><b>Mongo</b></a></td> <td align=\"center\"><a href=\"#testing\"><img src=\"images/testing.png\" width=\"75px;\" height=\"75px;\" alt=\"Testing\"/><br /><b>Testing</b></a></td> <td align=\"center\"><a href=\"#big-data\"><img src=\"images/big-data.png\" width=\"75px;\" height=\"75px;\" alt=\"Big Data\"/><br /><b>Big Data</b></a></td> </tr> <tr> <td align=\"center\"><a", "metadata": {"source_file": "learning-materials/README.md", "section": "Introduction", "language": "en", "created_at": "2025-07-19T19:22:01.992946"}}
{"text": "href=\"topics/cicd/README.md\"><img src=\"images/cicd.png\" width=\"75px;\" height=\"75px;\" alt=\"cicd\"/><br /><b>CI/CD</b></a></td> <td align=\"center\"><a href=\"#certificates\"><img src=\"images/certificates.png\" width=\"75px;\" height=\"75px;\" alt=\"Certificates\"/><br /><b>Certificates</b></a></td> <td align=\"center\"><a href=\"topics/containers/README.md\"><img src=\"images/containers.png\" width=\"75px;\" height=\"75px;\" alt=\"Containers\"/><br /><b>Containers</b></a></td> <td align=\"center\"><a href=\"topics/openshift/README.md\"><img src=\"images/openshift.png\" width=\"75px;\" height=\"75px;\" alt=\"OpenShift\"/><br /><b>OpenShift</b></a></td> <td align=\"center\"><a href=\"#storage\"><img src=\"images/storage.png\" width=\"75px;\" height=\"75px;\" alt=\"Storage\"/><br /><b>Storage</b></a></td> </tr> <tr> <td align=\"center\"><a href=\"topics/terraform/README.md\"><img src=\"images/terraform.png\" width=\"75px;\" height=\"75px;\" alt=\"Terraform\"/><br /><b>Terraform</b></a></td> <td align=\"center\"><a href=\"#puppet\"><img", "metadata": {"source_file": "learning-materials/README.md", "section": "Introduction", "language": "en", "created_at": "2025-07-19T19:22:01.992964"}}
{"text": "src=\"images/puppet.png\" width=\"75px;\" height=\"75px;\" alt=\"puppet\"/><br /><b>Puppet</b></a></td> <td align=\"center\"><a href=\"#distributed\"><img src=\"images/distributed.png\" width=\"75px;\" height=\"75px;\" alt=\"Distributed\"/><br /><b>Distributed</b></a></td> <td align=\"center\"><a href=\"#questions-you-ask\"><img src=\"images/you.png\" width=\"75px;\" height=\"75px;\" alt=\"you\"/><br /><b>Questions you can ask</b></a></td> <td align=\"center\"><a href=\"topics/ansible/README.md\"><img src=\"images/ansible.png\" width=\"75px;\" height=\"75px;\" alt=\"ansible\"/><br /><b>Ansible</b></a></td> </tr> <tr> <td align=\"center\"><a href=\"topics/observability/README.md\"><img src=\"images/observability.png\" width=\"75px;\" height=\"75px;\" alt=\"observability\"/><br /><b>Observability</b></a></td> <td align=\"center\"><a href=\"#prometheus\"><img src=\"images/prometheus.png\" width=\"75px;\" height=\"75px;\" alt=\"Prometheus\"/><br /><b>Prometheus</b></a></td> <td align=\"center\"><a href=\"topics/circleci/README.md\"><img", "metadata": {"source_file": "learning-materials/README.md", "section": "Introduction", "language": "en", "created_at": "2025-07-19T19:22:01.992980"}}
{"text": "src=\"images/logos/circleci.png\" width=\"70px;\" height=\"70px;\" alt=\"Circle CI\"/><br /><b>Circle CI</b></a></td> <td align=\"center\"><a href=\"topics/datadog/README.md\"><img src=\"images/logos/datadog.png\" width=\"80px;\" height=\"80px;\" alt=\"DataDog\"/><br /><b></b></a></td> <td align=\"center\"><a href=\"topics/grafana/README.md\"><img src=\"images/logos/grafana.png\" width=\"80px;\" height=\"80px;\" alt=\"Grafana\"/><br /><b>Grafana</b></a></td> </tr> <tr> <td align=\"center\"><a href=\"topics/argo/README.md\"><img src=\"images/logos/argo.png\" width=\"80px;\" height=\"80px;\" alt=\"Argo\"/><br /><b>Argo</b></a></td> <td align=\"center\"><a href=\"topics/soft_skills/README.md\"><img src=\"images/HR.png\" width=\"75px;\" height=\"75px;\" alt=\"HR\"/><br /><b>Soft Skills</b></a></td> <td align=\"center\"><a href=\"topics/security/README.md\"><img src=\"images/security.png\" width=\"75px;\" height=\"75px;\" alt=\"security\"/><br /><b>Security</b></a></td> <td align=\"center\"><a href=\"#system-design\"><img src=\"images/design.png\" width=\"75px;\"", "metadata": {"source_file": "learning-materials/README.md", "section": "Introduction", "language": "en", "created_at": "2025-07-19T19:22:01.993052"}}
{"text": "height=\"75px;\" alt=\"Design\"/><br /><b>System Design</b></a></td> </tr> <tr> <td align=\"center\"><a href=\"topics/chaos_engineering/README.md\"><img src=\"images/logos/chaos_engineering.png\" width=\"75px;\" height=\"75px;\" alt=\"Chaos Engineering\"/><br /><b>Chaos Engineering</b></a></td> <td align=\"center\"><a href=\"#Misc\"><img src=\"images/general.png\" width=\"75px;\" height=\"75px;\" alt=\"Misc\"/><br /><b>Misc</b></a></td> <td align=\"center\"><a href=\"#elastic\"><img src=\"images/elastic.png\" width=\"75px;\" height=\"75px;\" alt=\"Elastic\"/><br /><b>Elastic</b></a></td> <td align=\"center\"><a href=\"topics/kafka/README.md\"><img src=\"images/logos/kafka.png\" width=\"85px;\" height=\"80px;\" alt=\"Kafka\"/><br /><b>Kafka</b></a></td> <td align=\"center\"><a href=\"topics/node/node_questions_basic.md\"><img src=\"images/nodejs.png\" width=\"85px;\" height=\"80px;\" alt=\"NodeJs\"/><br /><b>NodeJs</b></a></td> </tr> </table> </center> <!-- markdownlint-enable --> <!-- prettier-ignore-end --> <!-- ALL-TOPICS-LIST:END -->", "metadata": {"source_file": "learning-materials/README.md", "section": "Introduction", "language": "en", "created_at": "2025-07-19T19:22:01.993072"}}
{"text": "<table> <tr> <td align=\"center\"><a href=\"https://play.google.com/store/apps/details?id=com.codingshell.kubeprep\"><img src=\"images/apps/kubeprep.png\" width=\"200px;\" height=\"300px;\" alt=\"KubePrep\"/><br /><b>KubePrep</b></a></td> <td align=\"center\"><a href=\"https://play.google.com/store/apps/details?id=com.codingshell.linuxmaster\"><img src=\"images/apps/linux_master.png\" width=\"200px;\" height=\"300px;\" alt=\"Linux Master\"/><br /><b>Linux Master</b></a></td> <td align=\"center\"><a href=\"https://play.google.com/store/apps/details?id=com.codingshell.system_design_hero\"><img src=\"images/apps/system_design_hero.png\" width=\"200px;\" height=\"300px;\" alt=\"Sytem Design Hero\"/><br /><b>System Design Hero</b></a></td> </tr> </table>", "metadata": {"source_file": "learning-materials/README.md", "section": "DevOps Applications", "language": "en", "created_at": "2025-07-19T19:22:01.993112"}}
{"text": "<details> <summary>In general, what do you need in order to communicate?</summary><br><b> - A common language (for the two ends to understand) - A way to address who you want to communicate with - A Connection (so the content of the communication can reach the recipients) </b></details> <details> <summary>What is TCP/IP?</summary><br><b> A set of protocols that define how two or more devices can communicate with each other. To learn more about TCP/IP, read [here](http://www.penguintutor.com/linux/basic-network-reference) </b></details> <details> <summary>What is Ethernet?</summary><br><b> Ethernet simply refers to the most common type of Local Area Network (LAN) used today. A LAN—in contrast to a WAN (Wide Area Network), which spans a larger geographical area—is a connected network of computers in a small area, like your office, college campus, or even home. </b></details> <details> <summary>What is a MAC address? What is it used for?</summary><br><b> A MAC address is a unique", "metadata": {"source_file": "learning-materials/README.md", "section": "Network", "language": "en", "created_at": "2025-07-19T19:22:01.999454"}}
{"text": "identification number or code used to identify individual devices on the network. Packets that are sent on the ethernet are always coming from a MAC address and sent to a MAC address. If a network adapter is receiving a packet, it is comparing the packet’s destination MAC address to the adapter’s own MAC address. </b></details> <details> <summary>When is this MAC address used?: ff:ff:ff:ff:ff:ff</summary><br><b> When a device sends a packet to the broadcast MAC address (FF:FF:FF:FF:FF:FF​), it is delivered to all stations on the local network. Ethernet broadcasts are used to resolve IP addresses to MAC addresses (by ARP) at the data link layer. </b></details> <details> <summary>What is an IP address?</summary><br><b> An Internet Protocol address (IP address) is a numerical label assigned to each device connected to a computer network that uses the Internet Protocol for communication.An IP address serves two main functions: host or network interface identification and location", "metadata": {"source_file": "learning-materials/README.md", "section": "Network", "language": "en", "created_at": "2025-07-19T19:22:01.999561"}}
{"text": "addressing. </b></details> <details> <summary>Explain the subnet mask and give an example</summary><br><b> A Subnet mask is a 32-bit number that masks an IP address and divides the IP addresses into network addresses and host addresses. Subnet Mask is made by setting network bits to all \"1\"s and setting host bits to all \"0\"s. Within a given network, out of the total usable host addresses, two are always reserved for specific purposes and cannot be allocated to any host. These are the first address, which is reserved as a network address (a.k.a network ID), and the last address used for network broadcast. [Example](https://github.com/philemonnwanne/projects/tree/main/exercises/exe-09) </b></details> <details> <summary>What is a private IP address? In which scenarios/system designs, one should use it?</summary><br><b> Private IP addresses are assigned to the hosts in the same network to communicate with one another. As the name \"private\" suggests, the devices having the private IP", "metadata": {"source_file": "learning-materials/README.md", "section": "Network", "language": "en", "created_at": "2025-07-19T19:22:01.999593"}}
{"text": "addresses assigned can't be reached by the devices from any external network. For example, if I am living in a hostel and I want my hostel mates to join the game server I have hosted, I will ask them to join via my server's private IP address, since the network is local to the hostel. </b></details> <details> <summary>What is a public IP address? In which scenarios/system designs, one should use it?</summary><br><b> A public IP address is a public-facing IP address. In the event that you were hosting a game server that you want your friends to join, you will give your friends your public IP address to allow their computers to identify and locate your network and server in order for the connection to take place. One time that you would not need to use a public-facing IP address is in the event that you were playing with friends who were connected to the same network as you, in that case, you would use a private IP address. In order for someone to be able to connect to your server that", "metadata": {"source_file": "learning-materials/README.md", "section": "Network", "language": "en", "created_at": "2025-07-19T19:22:01.999616"}}
{"text": "is located internally, you will have to set up a port forward to tell your router to allow traffic from the public domain into your network and vice versa. </b></details> <details> <summary>Explain the OSI model. What layers there are? What each layer is responsible for?</summary><br><b> - Application: user end (HTTP is here) - Presentation: establishes context between application-layer entities (Encryption is here) - Session: establishes, manages, and terminates the connections - Transport: transfers variable-length data sequences from a source to a destination host (TCP & UDP are here) - Network: transfers datagrams from one network to another (IP is here) - Data link: provides a link between two directly connected nodes (MAC is here) - Physical: the electrical and physical spec of the data connection (Bits are here) You can read more about the OSI model in [penguintutor.com](http://www.penguintutor.com/linux/basic-network-reference) </b></details> <details> <summary>For each of the", "metadata": {"source_file": "learning-materials/README.md", "section": "Network", "language": "en", "created_at": "2025-07-19T19:22:01.999636"}}
{"text": "following determines to which OSI layer it belongs: * Error correction * Packets routing * Cables and electrical signals * MAC address * IP address * Terminate connections * 3 way handshake</summary><br><b> * Error correction - Data link * Packets routing - Network * Cables and electrical signals - Physical * MAC address - Data link * IP address - Network * Terminate connections - Session * 3-way handshake - Transport </b></details> <details> <summary>What delivery schemes are you familiar with?</summary><br><b> Unicast: One-to-one communication where there is one sender and one receiver. Broadcast: Sending a message to everyone in the network. The address ff:ff:ff:ff:ff:ff is used for broadcasting. Two common protocols which use broadcast are ARP and DHCP. Multicast: Sending a message to a group of subscribers. It can be one-to-many or many-to-many. </b></details> <details> <summary>What is CSMA/CD? Is it used in modern ethernet networks?</summary><br><b> CSMA/CD stands for Carrier", "metadata": {"source_file": "learning-materials/README.md", "section": "Network", "language": "en", "created_at": "2025-07-19T19:22:01.999696"}}
{"text": "Sense Multiple Access / Collision Detection. Its primary focus is to manage access to a shared medium/bus where only one host can transmit at a given point in time. CSMA/CD algorithm: 1. Before sending a frame, it checks whether another host is already transmitting a frame. 2. If no one is transmitting, it starts transmitting the frame. 3. If two hosts transmit at the same time, we have a collision. 4. Both hosts stop sending the frame and they send everyone a 'jam signal' notifying everyone that a collision occurred 5. They are waiting for a random time before sending it again 6. Once each host waited for a random time, they try to send the frame again and so the cycle starts again </b></details> <details> <summary>Describe the following network devices and the difference between them: * router * switch * hub</summary><br><b> A router, switch, and hub are all network devices used to connect devices in a local area network (LAN). However, each device operates differently and has its", "metadata": {"source_file": "learning-materials/README.md", "section": "Network", "language": "en", "created_at": "2025-07-19T19:22:01.999749"}}
{"text": "specific use cases. Here is a brief description of each device and the differences between them: 1. Router: a network device that connects multiple network segments together. It operates at the network layer (Layer 3) of the OSI model and uses routing protocols to direct data between networks. Routers use IP addresses to identify devices and route data packets to the correct destination. 2. Switch: a network device that connects multiple devices on a LAN. It operates at the data link layer (Layer 2) of the OSI model and uses MAC addresses to identify devices and direct data packets to the correct destination. Switches allow devices on the same network to communicate with each other more efficiently and can prevent data collisions that can occur when multiple devices send data simultaneously. 3. Hub: a network device that connects multiple devices through a single cable and is used to connect multiple devices without segmenting a network. However, unlike a switch, it operates at the", "metadata": {"source_file": "learning-materials/README.md", "section": "Network", "language": "en", "created_at": "2025-07-19T19:22:01.999775"}}
{"text": "physical layer (Layer 1) of the OSI model and simply broadcasts data packets to all devices connected to it, regardless of whether the device is the intended recipient or not. This means that data collisions can occur, and the network's efficiency can suffer as a result. Hubs are generally not used in modern network setups, as switches are more efficient and provide better network performance. </b></details> <details> <summary>What is a \"Collision Domain\"?</summary><br><b> A collision domain is a network segment in which devices can potentially interfere with each other by attempting to transmit data at the same time. When two devices transmit data at the same time, it can cause a collision, resulting in lost or corrupted data. In a collision domain, all devices share the same bandwidth, and any device can potentially interfere with the transmission of data by other devices. </b></details> <details> <summary>What is a \"Broadcast Domain\"?</summary><br><b> A broadcast domain is a network", "metadata": {"source_file": "learning-materials/README.md", "section": "Network", "language": "en", "created_at": "2025-07-19T19:22:01.999797"}}
{"text": "segment in which all devices can communicate with each other by sending broadcast messages. A broadcast message is a message that is sent to all devices in a network rather than a specific device. In a broadcast domain, all devices can receive and process broadcast messages, regardless of whether the message was intended for them or not. </b></details> <details> <summary>three computers connected to a switch. How many collision domains are there? How many broadcast domains?</summary><br><b> Three collision domains and one broadcast domain </b></details> <details> <summary>How does a router work?</summary><br><b> A router is a physical or virtual appliance that passes information between two or more packet-switched computer networks. A router inspects a given data packet's destination Internet Protocol address (IP address), calculates the best way for it to reach its destination, and then forwards it accordingly. </b></details> <details> <summary>What is NAT?</summary><br><b> Network", "metadata": {"source_file": "learning-materials/README.md", "section": "Network", "language": "en", "created_at": "2025-07-19T19:22:01.999817"}}
{"text": "Address Translation (NAT) is a process in which one or more local IP addresses are translated into one or more Global IP address and vice versa in order to provide Internet access to the local hosts. </b></details> <details> <summary>What is a proxy? How does it work? What do we need it for?</summary><br><b> A proxy server acts as a gateway between you and the internet. It’s an intermediary server separating end users from the websites they browse. If you’re using a proxy server, internet traffic flows through the proxy server on its way to the address you requested. The request then comes back through that same proxy server (there are exceptions to this rule), and then the proxy server forwards the data received from the website to you. Proxy servers provide varying levels of functionality, security, and privacy depending on your use case, needs, or company policy. </b></details> <details> <summary>What is TCP? How does it work? What is the 3-way handshake?</summary><br><b> TCP 3-way", "metadata": {"source_file": "learning-materials/README.md", "section": "Network", "language": "en", "created_at": "2025-07-19T19:22:01.999836"}}
{"text": "handshake or three-way handshake is a process that is used in a TCP/IP network to make a connection between server and client. A three-way handshake is primarily used to create a TCP socket connection. It works when: - A client node sends an SYN data packet over an IP network to a server on the same or an external network. The objective of this packet is to ask/infer if the server is open for new connections. - The target server must have open ports that can accept and initiate new connections. When the server receives the SYN packet from the client node, it responds and returns a confirmation receipt – the ACK packet or SYN/ACK packet. - The client node receives the SYN/ACK from the server and responds with an ACK packet. </b></details> <details> <summary>What is round-trip delay or round-trip time?</summary><br><b> From [wikipedia](https://en.wikipedia.org/wiki/Round-trip_delay): \"the length of time it takes for a signal to be sent plus the length of time it takes for an", "metadata": {"source_file": "learning-materials/README.md", "section": "Network", "language": "en", "created_at": "2025-07-19T19:22:01.999858"}}
{"text": "acknowledgment of that signal to be received\" Bonus question: what is the RTT of LAN? </b></details> <details> <summary>How does an SSL handshake work?</summary><br><b> SSL handshake is a process that establishes a secure connection between a client and a server. 1. The client sends a Client Hello message to the server, which includes the client's version of the SSL/TLS protocol, a list of the cryptographic algorithms supported by the client, and a random value. 2. The server responds with a Server Hello message, which includes the server's version of the SSL/TLS protocol, a random value, and a session ID. 3. The server sends a Certificate message, which contains the server's certificate. 4. The server sends a Server Hello Done message, which indicates that the server is done sending messages for the Server Hello phase. 5. The client sends a Client Key Exchange message, which contains the client's public key. 6. The client sends a Change Cipher Spec message, which notifies the server", "metadata": {"source_file": "learning-materials/README.md", "section": "Network", "language": "en", "created_at": "2025-07-19T19:22:01.999879"}}
{"text": "that the client is about to send a message encrypted with the new cipher spec. 7. The client sends an Encrypted Handshake Message, which contains the pre-master secret encrypted with the server's public key. 8. The server sends a Change Cipher Spec message, which notifies the client that the server is about to send a message encrypted with the new cipher spec. 9. The server sends an Encrypted Handshake Message, which contains the pre-master secret encrypted with the client's public key. 10. The client and server can now exchange application data. </b></details> <details> <summary>What is the difference between TCP and UDP?</summary><br><b> TCP establishes a connection between the client and the server to guarantee the order of the packages, on the other hand, UDP does not establish a connection between the client and server and doesn't handle package orders. This makes UDP more lightweight than TCP and a perfect candidate for services like streaming.", "metadata": {"source_file": "learning-materials/README.md", "section": "Network", "language": "en", "created_at": "2025-07-19T19:22:01.999913"}}
{"text": "[Penguintutor.com](http://www.penguintutor.com/linux/basic-network-reference) provides a good explanation. </b></details> <details> <summary>What TCP/IP protocols are you familiar with?</summary><br><b> </b></details> <details> <summary>Explain the \"default gateway\"</summary><br><b> A default gateway serves as an access point or IP router that a networked computer uses to send information to a computer in another network or the internet. </b></details> <details> <summary>What is ARP? How does it work?</summary><br><b> ARP stands for Address Resolution Protocol. When you try to ping an IP address on your local network, say 192.168.1.1, your system has to turn the IP address 192.168.1.1 into a MAC address. This involves using ARP to resolve the address, hence its name. Systems keep an ARP look-up table where they store information about what IP addresses are associated with what MAC addresses. When trying to send a packet to an IP address, the system will first consult this table to see", "metadata": {"source_file": "learning-materials/README.md", "section": "Network", "language": "en", "created_at": "2025-07-19T19:22:01.999933"}}
{"text": "if it already knows the MAC address. If there is a value cached, ARP is not used. </b></details> <details> <summary>What is TTL? What does it help to prevent?</summary><br><b> - TTL (Time to Live) is a value in an IP (Internet Protocol) packet that determines how many hops or routers a packet can travel before it is discarded. Each time a packet is forwarded by a router, the TTL value is decreased by one. When the TTL value reaches zero, the packet is dropped, and an ICMP (Internet Control Message Protocol) message is sent back to the sender indicating that the packet has expired. - TTL is used to prevent packets from circulating indefinitely in the network, which can cause congestion and degrade network performance. - It also helps to prevent packets from being trapped in routing loops, where packets continuously travel between the same set of routers without ever reaching their destination. - In addition, TTL can be used to help detect and prevent IP spoofing attacks, where an", "metadata": {"source_file": "learning-materials/README.md", "section": "Network", "language": "en", "created_at": "2025-07-19T19:22:01.999951"}}
{"text": "attacker attempts to impersonate another device on the network by using a false or fake IP address. By limiting the number of hops that a packet can travel, TTL can help prevent packets from being routed to destinations that are not legitimate. </b></details> <details> <summary>What is DHCP? How does it work?</summary><br><b> It stands for Dynamic Host Configuration Protocol and allocates IP addresses, subnet masks, and gateways to hosts. This is how it works: * A host upon entering a network broadcasts a message in search of a DHCP server (DHCP DISCOVER) * An offer message is sent back by the DHCP server as a packet containing lease time, subnet mask, IP addresses, etc (DHCP OFFER) * Depending on which offer is accepted, the client sends back a reply broadcast letting all DHCP servers know (DHCP REQUEST) * The server sends an acknowledgment (DHCP ACK) Read more [here](https://linuxjourney.com/lesson/dhcp-overview) </b></details> <details> <summary>Can you have two DHCP servers on the", "metadata": {"source_file": "learning-materials/README.md", "section": "Network", "language": "en", "created_at": "2025-07-19T19:22:01.999970"}}
{"text": "same network? How does it work?</summary><br><b> It is possible to have two DHCP servers on the same network, however, it is not recommended, and it is important to configure them carefully to prevent conflicts and configuration problems. - When two DHCP servers are configured on the same network, there is a risk that both servers will assign IP addresses and other network configuration settings to the same device, which can cause conflicts and connectivity issues. Additionally, if the DHCP servers are configured with different network settings or options, devices on the network may receive conflicting or inconsistent configuration settings. - However, in some cases, it may be necessary to have two DHCP servers on the same network, such as in large networks where one DHCP server may not be able to handle all the requests. In such cases, DHCP servers can be configured to serve different IP address ranges or different subnets, so they do not interfere with each other. </b></details>", "metadata": {"source_file": "learning-materials/README.md", "section": "Network", "language": "en", "created_at": "2025-07-19T19:22:01.999990"}}
{"text": "<details> <summary>What is SSL tunneling? How does it work?</summary><br><b> - SSL (Secure Sockets Layer) tunneling is a technique used to establish a secure, encrypted connection between two endpoints over an insecure network, such as the Internet. The SSL tunnel is created by encapsulating the traffic within an SSL connection, which provides confidentiality, integrity, and authentication. Here's how SSL tunneling works: 1. A client initiates an SSL connection to a server, which involves a handshake process to establish the SSL session. 2. Once the SSL session is established, the client and server negotiate encryption parameters, such as the encryption algorithm and key length, then exchange digital certificates to authenticate each other. 3. The client then sends traffic through the SSL tunnel to the server, which decrypts the traffic and forwards it to its destination. 4. The server sends traffic back through the SSL tunnel to the client, which decrypts the traffic and forwards it", "metadata": {"source_file": "learning-materials/README.md", "section": "Network", "language": "en", "created_at": "2025-07-19T19:22:02.000008"}}
{"text": "to the application. </b></details> <details> <summary>What is a socket? Where can you see the list of sockets in your system?</summary><br><b> - A socket is a software endpoint that enables two-way communication between processes over a network. Sockets provide a standardized interface for network communication, allowing applications to send and receive data across a network. To view the list of open sockets on a Linux system: ***netstat -an*** - This command displays a list of all open sockets, along with their protocol, local address, foreign address, and state. </b></details> <details> <summary>What is IPv6? Why should we consider using it if we have IPv4?</summary><br><b> - IPv6 (Internet Protocol version 6) is the latest version of the Internet Protocol (IP), which is used to identify and communicate with devices on a network. IPv6 addresses are 128-bit addresses and are expressed in hexadecimal notation, such as 2001:0db8:85a3:0000:0000:8a2e:0370:7334. There are several reasons", "metadata": {"source_file": "learning-materials/README.md", "section": "Network", "language": "en", "created_at": "2025-07-19T19:22:02.000026"}}
{"text": "why we should consider using IPv6 over IPv4: 1. Address space: IPv4 has a limited address space, which has been exhausted in many parts of the world. IPv6 provides a much larger address space, allowing for trillions of unique IP addresses. 2. Security: IPv6 includes built-in support for IPsec, which provides end-to-end encryption and authentication for network traffic. 3. Performance: IPv6 includes features that can help to improve network performance, such as multicast routing, which allows a single packet to be sent to multiple destinations simultaneously. 4. Simplified network configuration: IPv6 includes features that can simplify network configuration, such as stateless autoconfiguration, which allows devices to automatically configure their own IPv6 addresses without the need for a DHCP server. 5. Better mobility support: IPv6 includes features that can improve mobility support, such as Mobile IPv6, which allows devices to maintain their IPv6 addresses as they move between", "metadata": {"source_file": "learning-materials/README.md", "section": "Network", "language": "en", "created_at": "2025-07-19T19:22:02.000045"}}
{"text": "different networks. </b></details> <details> <summary>What is VLAN?</summary><br><b> - A VLAN (Virtual Local Area Network) is a logical network that groups together a set of devices on a physical network, regardless of their physical location. VLANs are created by configuring network switches to assign a specific VLAN ID to frames sent by devices connected to a specific port or group of ports on the switch. </b></details> <details> <summary>What is MTU?</summary><br><b> MTU stands for Maximum Transmission Unit. It's the size of the largest PDU (protocol Data Unit) that can be sent in a single transaction. </b></details> <details> <summary>What happens if you send a packet that is bigger than the MTU?</summary><br><b> With the IPv4 protocol, the router can fragment the PDU and then send all the fragmented PDU through the transaction. With IPv6 protocol, it issues an error to the user's computer. </b></details> <details> <summary>True or False? Ping is using UDP because it doesn't care", "metadata": {"source_file": "learning-materials/README.md", "section": "Network", "language": "en", "created_at": "2025-07-19T19:22:02.000073"}}
{"text": "about reliable connection</summary><br><b> False. Ping is actually using ICMP (Internet Control Message Protocol) which is a network protocol used to send diagnostic messages and control messages related to network communication. </b></details> <details> <summary>What is SDN?</summary><br><b> - SDN stands for Software-Defined Networking. It is an approach to network management that emphasizes the centralization of network control, enabling administrators to manage network behavior through a software abstraction. - In a traditional network, network devices such as routers, switches, and firewalls are configured and managed individually, using specialized software or command-line interfaces. In contrast, SDN separates the network control plane from the data plane, allowing administrators to manage network behavior through a centralized software controller. </b></details> <details> <summary>What is ICMP? What is it used for?</summary><br><b> - ICMP stands for Internet Control Message", "metadata": {"source_file": "learning-materials/README.md", "section": "Network", "language": "en", "created_at": "2025-07-19T19:22:02.000093"}}
{"text": "Protocol. It is a protocol used for diagnostic and control purposes in IP networks. It is a part of the Internet Protocol suite, operating at the network layer. ICMP messages are used for a variety of purposes, including: 1. Error reporting: ICMP messages are used to report errors that occur in the network, such as a packet that could not be delivered to its destination. 2. Ping: ICMP is used to send ping messages, which are used to test whether a host or network is reachable and to measure the round-trip time for packets. 3. Path MTU discovery: ICMP is used to discover the Maximum Transmission Unit (MTU) of a path, which is the largest packet size that can be transmitted without fragmentation. 4. Traceroute: ICMP is used by the traceroute utility to trace the path that packets take through the network. 5. Router discovery: ICMP is used to discover the routers in a network. </b></details> <details> <summary>What is NAT? How does it work?</summary><br><b> NAT stands for Network Address", "metadata": {"source_file": "learning-materials/README.md", "section": "Network", "language": "en", "created_at": "2025-07-19T19:22:02.000111"}}
{"text": "Translation. It’s a way to map multiple local private addresses to a public one before transferring the information. Organizations that want multiple devices to employ a single IP address use NAT, as do most home routers. For example, your computer's private IP could be 192.168.1.100, but your router maps the traffic to its public IP (e.g. 1.1.1.1). Any device on the internet would see the traffic coming from your public IP (1.1.1.1) instead of your private IP (192.168.1.100). </b></details> <details> <summary>Which port number is used in each of the following protocols?: * SSH * SMTP * HTTP * DNS * HTTPS * FTP * SFTP </summary><br><b> * SSH - 22 * SMTP - 25 * HTTP - 80 * DNS - 53 * HTTPS - 443 * FTP - 21 * SFTP - 22 </b></details> <details> <summary>Which factors affect network performance?</summary><br><b> Several factors can affect network performance, including: 1. Bandwidth: The available bandwidth of a network connection can significantly impact its performance. Networks with", "metadata": {"source_file": "learning-materials/README.md", "section": "Network", "language": "en", "created_at": "2025-07-19T19:22:02.000130"}}
{"text": "limited bandwidth can experience slow data transfer rates, high latency, and poor responsiveness. 2. Latency: Latency refers to the delay that occurs when data is transmitted from one point in a network to another. High latency can result in slow network performance, especially for real-time applications like video conferencing and online gaming. 3. Network congestion: When too many devices are using a network at the same time, network congestion can occur, leading to slow data transfer rates and poor network performance. 4. Packet loss: Packet loss occurs when packets of data are dropped during transmission. This can result in slower network speeds and lower overall network performance. 5. Network topology: The physical layout of a network, including the placement of switches, routers, and other network devices, can impact network performance. 6. Network protocol: Different network protocols have different performance characteristics, which can impact network performance. For example,", "metadata": {"source_file": "learning-materials/README.md", "section": "Network", "language": "en", "created_at": "2025-07-19T19:22:02.000151"}}
{"text": "TCP is a reliable protocol that can guarantee the delivery of data, but it can also result in slower performance due to the overhead required for error checking and retransmission. 7. Network security: Security measures such as firewalls and encryption can impact network performance, especially if they require significant processing power or introduce additional latency. 8. Distance: The physical distance between devices on a network can impact network performance, especially for wireless networks where signal strength and interference can affect connectivity and data transfer rates. </b></details> <details> <summary>What is APIPA?</summary><br><b> APIPA is a set of IP addresses that devices are allocated when the main DHCP server is not reachable </b></details> <details> <summary>What IP range does APIPA use?</summary><br><b> APIPA uses the IP range: 169.254.0.1 - 169.254.255.254. </b></details>", "metadata": {"source_file": "learning-materials/README.md", "section": "Network", "language": "en", "created_at": "2025-07-19T19:22:02.000169"}}
{"text": "<details> <summary>What does \"control plane\" refer to?</summary><br><b> The control plane is a part of the network that decides how to route and forward packets to a different location. </b></details> <details> <summary>What does \"data plane\" refer to?</summary><br><b> The data plane is a part of the network that actually forwards the data/packets. </b></details> <details> <summary>What does \"management plane\" refer to?</summary><br><b> It refers to monitoring and management functions. </b></details> <details> <summary>To which plane (data, control, ...) does creating routing tables belong to?</summary><br><b> Control Plane. </b></details> <details> <summary>Explain Spanning Tree Protocol (STP).</summary><br><b> </b></details> <details> <summary>What is link aggregation? Why is it used?</summary><br><b> </b></details> <details> <summary>What is Asymmetric Routing? How to deal with it?</summary><br><b> </b></details> <details> <summary>What overlay (tunnel) protocols are you familiar", "metadata": {"source_file": "learning-materials/README.md", "section": "Control Plane and Data Plane", "language": "en", "created_at": "2025-07-19T19:22:02.001362"}}
{"text": "with?</summary><br><b> </b></details> <details> <summary>What is GRE? How does it work?</summary><br><b> </b></details> <details> <summary>What is VXLAN? How does it work?</summary><br><b> </b></details> <details> <summary>What is SNAT?</summary><br><b> </b></details> <details> <summary>Explain OSPF.</summary><br><b> OSPF (Open Shortest Path First) is a routing protocol that can be implemented on various types of routers. In general, OSPF is supported on most modern routers, including those from vendors such as Cisco, Juniper, and Huawei. The protocol is designed to work with IP-based networks, including both IPv4 and IPv6. Also, it uses a hierarchical network design, where routers are grouped into areas, with each area having its own topology map and routing table. This design helps to reduce the amount of routing information that needs to be exchanged between routers and improve network scalability. The OSPF 4 Types of routers are: * Internal Router * Area Border Routers * Autonomous", "metadata": {"source_file": "learning-materials/README.md", "section": "Control Plane and Data Plane", "language": "en", "created_at": "2025-07-19T19:22:02.001394"}}
{"text": "Systems Boundary Routers * Backbone Routers Learn more about OSPF router types: https://www.educba.com/ospf-router-types/ </b></details> <details> <summary>What is latency?</summary><br><b> Latency is the time taken for information to reach its destination from the source. </b></details> <details> <summary>What is bandwidth?</summary><br><b> Bandwidth is the capacity of a communication channel to measure how much data the latter can handle over a specific time period. More bandwidth would imply more traffic handling and thus more data transfer. </b></details> <details> <summary>What is throughput?</summary><br><b> Throughput refers to the measurement of the real amount of data transferred over a certain period of time across any transmission channel. </b></details> <details> <summary>When performing a search query, what is more important, latency or throughput? And how to ensure that we manage global infrastructure? </summary><br><b> Latency. To have good latency, a search query should", "metadata": {"source_file": "learning-materials/README.md", "section": "Control Plane and Data Plane", "language": "en", "created_at": "2025-07-19T19:22:02.001436"}}
{"text": "be forwarded to the closest data center. </b></details> <details> <summary>When uploading a video, what is more important, latency or throughput? And how to assure that?</summary><br><b> Throughput. To have good throughput, the upload stream should be routed to an underutilized link. </b></details> <details> <summary>What other considerations (except latency and throughput) are there when forwarding requests?</summary><br><b> * Keep caches updated (which means the request could be forwarded not to the closest data center) </b></details> <details> <summary>Explain Spine & Leaf</summary><br><b> \"Spine & Leaf\" is a networking topology commonly used in data center environments to connect multiple switches and manage network traffic efficiently. It is also known as \"spine-leaf\" architecture or \"leaf-spine\" topology. This design provides high bandwidth, low latency, and scalability, making it ideal for modern data centers handling large volumes of data and traffic. Within a Spine & Leaf", "metadata": {"source_file": "learning-materials/README.md", "section": "Control Plane and Data Plane", "language": "en", "created_at": "2025-07-19T19:22:02.001456"}}
{"text": "network there are two main tipology of switches: * Spine Switches: Spine switches are high-performance switches arranged in a spine layer. These switches act as the core of the network and are typically interconnected with each leaf switch. Each spine switch is connected to all the leaf switches in the data center. * Leaf Switches: Leaf switches are connected to end devices like servers, storage arrays, and other networking equipment. Each leaf switch is connected to every spine switch in the data center. This creates a non-blocking, full-mesh connectivity between leaf and spine switches, ensuring any leaf switch can communicate with any other leaf switch with maximum throughput. The Spine & Leaf architecture has become increasingly popular in data centers due to its ability to handle the demands of modern cloud computing, virtualization, and big data applications, providing a scalable, high-performance, and reliable network infrastructure </b></details> <details> <summary>What is", "metadata": {"source_file": "learning-materials/README.md", "section": "Control Plane and Data Plane", "language": "en", "created_at": "2025-07-19T19:22:02.001481"}}
{"text": "Network Congestion? What can cause it?</summary><br><b> Network congestion occurs when there is too much data to transmit on a network and it doesn't have enough capacity to handle the demand. </br> This can lead to increased latency and packet loss. The causes can be multiple, such as high network usage, large file transfers, malware, hardware issues, or network design problems. </br> To prevent network congestion, it's important to monitor your network usage and implement strategies to limit or manage the demand. </b></details> <details> <summary>What can you tell me about the UDP packet format? What about the TCP packet format? How is it different?</summary><br><b> </b></details> <details> <summary>What is the exponential backoff algorithm? Where is it used?</summary><br><b> </b></details> <details> <summary>Using Hamming code, what would be the code word for the following data word 100111010001101?</summary><br><b> 00110011110100011101 </b></details> <details> <summary>Give", "metadata": {"source_file": "learning-materials/README.md", "section": "Control Plane and Data Plane", "language": "en", "created_at": "2025-07-19T19:22:02.001501"}}
{"text": "examples of protocols found in the application layer</summary><br><b> * Hypertext Transfer Protocol (HTTP) - used for the webpages on the internet * Simple Mail Transfer Protocol (SMTP) - email transmission * Telecommunications Network - (TELNET) - terminal emulation to allow a client access to a telnet server * File Transfer Protocol (FTP) - facilitates the transfer of files between any two machines * Domain Name System (DNS) - domain name translation * Dynamic Host Configuration Protocol (DHCP) - allocates IP addresses, subnet masks, and gateways to hosts * Simple Network Management Protocol (SNMP) - gathers data on devices on the network </b></details> <details> <summary>Give examples of protocols found in the Network Layer</summary><br><b> * Internet Protocol (IP) - assists in routing packets from one machine to another * Internet Control Message Protocol (ICMP) - lets one know what is going such as error messages and debugging information </b></details> <details> <summary>What is", "metadata": {"source_file": "learning-materials/README.md", "section": "Control Plane and Data Plane", "language": "en", "created_at": "2025-07-19T19:22:02.001519"}}
{"text": "HSTS?</summary><br><b> HTTP Strict Transport Security is a web server directive that informs user agents and web browsers how to handle its connection through a response header sent at the very beginning and back to the browser. This forces connections over HTTPS encryption, disregarding any script's call to load any resource in that domain over HTTP. Read more [here](https://www.globalsign.com/en/blog/what-is-hsts-and-how-do-i-use-it#:~:text=HTTP%20Strict%20Transport%20Security%20(HSTS,and%20back%20to%20the%20browser.) </b></details>", "metadata": {"source_file": "learning-materials/README.md", "section": "Control Plane and Data Plane", "language": "en", "created_at": "2025-07-19T19:22:02.001537"}}
{"text": "<details> <summary>What is the Internet? Is it the same as the World Wide Web?</summary><br><b> The internet refers to a network of networks, transferring huge amounts of data around the globe.<br> The World Wide Web is an application running on millions of servers, on top of the internet, accessed through what is known as the web browser </b></details> <details> <summary>What is the ISP?</summary><br><b> ISP (Internet Service Provider) is the local internet company provider. </b></details>", "metadata": {"source_file": "learning-materials/README.md", "section": "Network - Misc", "language": "en", "created_at": "2025-07-19T19:22:02.001607"}}
{"text": "|Name|Topic|Objective & Instructions|Solution|Comments| |--------|--------|------|----|----| |Fork 101|Fork|[Link](topics/os/fork_101.md)|[Link](topics/os/solutions/fork_101_solution.md) |Fork 102|Fork|[Link](topics/os/fork_102.md)|[Link](topics/os/solutions/fork_102_solution.md)", "metadata": {"source_file": "learning-materials/README.md", "section": "Operating System Exercises", "language": "en", "created_at": "2025-07-19T19:22:02.001628"}}
{"text": "<details> <summary>What is an operating system?</summary><br><b> From the book \"Operating Systems: Three Easy Pieces\": \"responsible for making it easy to run programs (even allowing you to seemingly run many at the same time), allowing programs to share memory, enabling programs to interact with devices, and other fun stuff like that\". </b></details>", "metadata": {"source_file": "learning-materials/README.md", "section": "Operating System - Self Assessment", "language": "en", "created_at": "2025-07-19T19:22:02.001671"}}
{"text": "<details> <summary>Can you explain what is a process?</summary><br><b> A process is a running program. A program is one or more instructions and the program (or process) is executed by the operating system. </b></details> <details> <summary>If you had to design an API for processes in an operating system, what would this API look like?</summary><br><b> It would support the following: * Create - allow to create new processes * Delete - allow to remove/destroy processes * State - allow to check the state of the process, whether it's running, stopped, waiting, etc. * Stop - allow to stop a running process </b></details> <details> <summary>How a process is created?</summary><br><b> * The OS is reading program's code and any additional relevant data * Program's code is loaded into the memory or more specifically, into the address space of the process. * Memory is allocated for program's stack (aka run-time stack). The stack also initialized by the OS with data like argv, argc and parameters", "metadata": {"source_file": "learning-materials/README.md", "section": "Operating System - Process", "language": "en", "created_at": "2025-07-19T19:22:02.002334"}}
{"text": "to main() * Memory is allocated for program's heap which is required for dynamically allocated data like the data structures linked lists and hash tables * I/O initialization tasks are performed, like in Unix/Linux based systems, where each process has 3 file descriptors (input, output and error) * OS is running the program, starting from main() </b></details> <details> <summary>True or False? The loading of the program into the memory is done eagerly (all at once)</summary><br><b> False. It was true in the past but today's operating systems perform lazy loading, which means only the relevant pieces required for the process to run are loaded first. </b></details> <details> <summary>What are different states of a process?</summary><br><b> * Running - it's executing instructions * Ready - it's ready to run, but for different reasons it's on hold * Blocked - it's waiting for some operation to complete, for example I/O disk request </b></details> <details> <summary>What are some reasons", "metadata": {"source_file": "learning-materials/README.md", "section": "Operating System - Process", "language": "en", "created_at": "2025-07-19T19:22:02.002372"}}
{"text": "for a process to become blocked?</summary><br><b> - I/O operations (e.g. Reading from a disk) - Waiting for a packet from a network </b></details> <details> <summary>What is Inter Process Communication (IPC)?</summary><br><b> Inter-process communication (IPC) refers to the mechanisms provided by an operating system that allow processes to manage shared data. </b></details> <details> <summary>What is \"time sharing\"?</summary><br><b> Even when using a system with one physical CPU, it's possible to allow multiple users to work on it and run programs. This is possible with time sharing, where computing resources are shared in a way it seems to the user, the system has multiple CPUs, but in fact it's simply one CPU shared by applying multiprogramming and multi-tasking. </b></details> <details> <summary>What is \"space sharing\"?</summary><br><b> Somewhat the opposite of time sharing. While in time sharing a resource is used for a while by one entity and then the same resource can be used by", "metadata": {"source_file": "learning-materials/README.md", "section": "Operating System - Process", "language": "en", "created_at": "2025-07-19T19:22:02.002393"}}
{"text": "another resource, in space sharing the space is shared by multiple entities but in a way where it's not being transferred between them.<br> It's used by one entity, until this entity decides to get rid of it. Take for example storage. In storage, a file is yours, until you decide to delete it. </b></details> <details> <summary>What component determines which process runs at a given moment in time?</summary><br><b> CPU scheduler </b></details>", "metadata": {"source_file": "learning-materials/README.md", "section": "Operating System - Process", "language": "en", "created_at": "2025-07-19T19:22:02.002413"}}
{"text": "<details> <summary>What is \"virtual memory\" and what purpose does serve?</summary><br><b> Virtual memory combines your computer's RAM with temporary space on your hard disk. When RAM runs low, virtual memory helps to move data from RAM to a space called a paging file. Moving data to paging file can free up the RAM, so your computer can complete its work. In general, the more RAM your computer has, the faster the programs run. https://www.minitool.com/lib/virtual-memory.html </b></details> <details> <summary>What is demand paging?</summary><br><b> Demand paging is a memory management technique where pages are loaded into physical memory only when accessed by a process. It optimizes memory usage by loading pages on demand, reducing startup latency and space overhead. However, it introduces some latency when accessing pages for the first time. Overall, it’s a cost-effective approach for managing memory resources in operating systems. </b></details> <details> <summary>What is", "metadata": {"source_file": "learning-materials/README.md", "section": "Operating System - Memory", "language": "en", "created_at": "2025-07-19T19:22:02.003123"}}
{"text": "copy-on-write?</summary><br><b> Copy-on-write (COW) is a resource management concept, with the goal to reduce unnecessary copying of information. It is a concept, which is implemented for instance within the POSIX fork syscall, which creates a duplicate process of the calling process. The idea: 1. If resources are shared between 2 or more entities (for example shared memory segments between 2 processes), the resources don't need to be copied for every entity, but rather every entity has a READ operation access permission on the shared resource. (the shared segments are marked as read-only) (Think of every entity having a pointer to the location of the shared resource, which can be dereferenced to read its value) 2. If one entity would perform a WRITE operation on a shared resource, a problem would arise, since the resource also would be permanently changed for ALL other entities sharing it. (Think of a process modifying some variables on the stack, or allocatingy some data dynamically", "metadata": {"source_file": "learning-materials/README.md", "section": "Operating System - Memory", "language": "en", "created_at": "2025-07-19T19:22:02.003165"}}
{"text": "on the heap, these changes to the shared resource would also apply for ALL other processes, this is definitely an undesirable behaviour) 3. As a solution only, if a WRITE operation is about to be performed on a shared resource, this resource gets COPIED first and then the changes are applied. </b></details> <details> <summary>What is a kernel, and what does it do?</summary><br><b> The kernel is part of the operating system and is responsible for tasks like: * Allocating memory * Schedule processes * Control CPU </b></details> <details> <summary>True or False? Some pieces of the code in the kernel are loaded into protected areas of the memory so applications can't overwrite them.</summary><br><b> True </b></details> <details> <summary>What is POSIX?</summary><br><b> POSIX (Portable Operating System Interface) is a set of standards that define the interface between a Unix-like operating system and application programs. </b></details> <details> <summary>Explain what is Semaphore and what", "metadata": {"source_file": "learning-materials/README.md", "section": "Operating System - Memory", "language": "en", "created_at": "2025-07-19T19:22:02.003187"}}
{"text": "its role in operating systems.</summary><br><b> A semaphore is a synchronization primitive used in operating systems and concurrent programming to control access to shared resources. It's a variable or abstract data type that acts as a counter or a signaling mechanism for managing access to resources by multiple processes or threads. </b></details> <details> <summary>What is cache? What is buffer?</summary><br><b> Cache: Cache is usually used when processes are reading and writing to the disk to make the process faster, by making similar data used by different programs easily accessible. Buffer: Reserved place in RAM, which is used to hold data for temporary purposes. </b></details>", "metadata": {"source_file": "learning-materials/README.md", "section": "Operating System - Memory", "language": "en", "created_at": "2025-07-19T19:22:02.003207"}}
{"text": "<details> <summary>What is Virtualization?</summary><br><b> Virtualization uses software to create an abstraction layer over computer hardware, that allows the hardware elements of a single computer - processors, memory, storage and more - to be divided into multiple virtual computers, commonly called virtual machines (VMs). </b></details> <details> <summary>What is a hypervisor?</summary><br><b> Red Hat: \"A hypervisor is software that creates and runs virtual machines (VMs). A hypervisor, sometimes called a virtual machine monitor (VMM), isolates the hypervisor operating system and resources from the virtual machines and enables the creation and management of those VMs.\" Read more [here](https://www.redhat.com/en/topics/virtualization/what-is-a-hypervisor) </b></details> <details> <summary>What types of hypervisors are there?</summary><br><b> Hosted hypervisors and bare-metal hypervisors. </b></details> <details> <summary>What are the advantages and disadvantges of bare-metal", "metadata": {"source_file": "learning-materials/README.md", "section": "Virtualization", "language": "en", "created_at": "2025-07-19T19:22:02.003587"}}
{"text": "hypervisor over a hosted hypervisor?</summary><br><b> Due to having its own drivers and a direct access to hardware components, a baremetal hypervisor will often have better performances along with stability and scalability. On the other hand, there will probably be some limitation regarding loading (any) drivers so a hosted hypervisor will usually benefit from having a better hardware compatibility. </b></details> <details> <summary>What types of virtualization are there?</summary><br><b> Operating system virtualization Network functions virtualization Desktop virtualization </b></details> <details> <summary>Is containerization is a type of Virtualization?</summary><br><b> Yes, it's a operating-system-level virtualization, where the kernel is shared and allows to use multiple isolated user-spaces instances. </b></details> <details> <summary>How the introduction of virtual machines changed the industry and the way applications were deployed?</summary><br><b> The introduction of virtual", "metadata": {"source_file": "learning-materials/README.md", "section": "Virtualization", "language": "en", "created_at": "2025-07-19T19:22:02.003639"}}
{"text": "machines allowed companies to deploy multiple business applications on the same hardware, while each application is separated from each other in secured way, where each is running on its own separate operating system. </b></details>", "metadata": {"source_file": "learning-materials/README.md", "section": "Virtualization", "language": "en", "created_at": "2025-07-19T19:22:02.003678"}}
{"text": "<details> <summary>Do we need virtual machines in the age of containers? Are they still relevant?</summary><br><b> Yes, virtual machines are still relevant even in the age of containers. While containers provide a lightweight and portable alternative to virtual machines, they do have certain limitations. Virtual machines still matter because they offer isolation and security, can run different operating systems, and are good for legacy apps. Containers limitations for example are sharing the host kernel. </b></details>", "metadata": {"source_file": "learning-materials/README.md", "section": "Virtual Machines", "language": "en", "created_at": "2025-07-19T19:22:02.003782"}}
{"text": "<details> <summary>What is Prometheus? What are some of Prometheus's main features?</summary><br><b> Prometheus is a popular open-source systems monitoring and alerting toolkit, originally developed at SoundCloud. It is designed to collect and store time-series data, and to allow for querying and analysis of that data using a powerful query language called PromQL. Prometheus is frequently used to monitor cloud-native applications, microservices, and other modern infrastructure. Some of the main features of Prometheus include: 1. Data model: Prometheus uses a flexible data model that allows users to organize and label their time-series data in a way that makes sense for their particular use case. Labels are used to identify different dimensions of the data, such as the source of the data or the environment in which it was collected. 2. Pull-based architecture: Prometheus uses a pull-based model to collect data from targets, meaning that the Prometheus server actively queries its targets", "metadata": {"source_file": "learning-materials/README.md", "section": "Prometheus", "language": "en", "created_at": "2025-07-19T19:22:02.006416"}}
{"text": "for metrics data at regular intervals. This architecture is more scalable and reliable than a push-based model, which would require every target to push data to the server. 3. Time-series database: Prometheus stores all of its data in a time-series database, which allows users to perform queries over time ranges and to aggregate and analyze their data in various ways. The database is optimized for write-heavy workloads, and can handle a high volume of data with low latency. 4. Alerting: Prometheus includes a powerful alerting system that allows users to define rules based on their metrics data and to send alerts when certain conditions are met. Alerts can be sent via email, chat, or other channels, and can be customized to include specific details about the problem. 5. Visualization: Prometheus has a built-in graphing and visualization tool, called PromDash, which allows users to create custom dashboards to monitor their systems and applications. PromDash supports a variety of graph", "metadata": {"source_file": "learning-materials/README.md", "section": "Prometheus", "language": "en", "created_at": "2025-07-19T19:22:02.006451"}}
{"text": "types and visualization options, and can be customized using CSS and JavaScript. Overall, Prometheus is a powerful and flexible tool for monitoring and analyzing systems and applications, and is widely used in the industry for cloud-native monitoring and observability. </b></details> <details> <summary>In what scenarios it might be better to NOT use Prometheus?</summary><br><b> From Prometheus documentation: \"if you need 100% accuracy, such as for per-request billing\". </b></details> <details> <summary>Describe Prometheus architecture and components</summary><br><b> The Prometheus architecture consists of four major components: 1. Prometheus Server: The Prometheus server is responsible for collecting and storing metrics data. It has a simple built-in storage layer that allows it to store time-series data in a time-ordered database. 2. Client Libraries: Prometheus provides a range of client libraries that enable applications to expose their metrics data in a format that can be ingested", "metadata": {"source_file": "learning-materials/README.md", "section": "Prometheus", "language": "en", "created_at": "2025-07-19T19:22:02.006471"}}
{"text": "by the Prometheus server. These libraries are available for a range of programming languages, including Java, Python, and Go. 3. Exporters: Exporters are software components that expose existing metrics from third-party systems and make them available for ingestion by the Prometheus server. Prometheus provides exporters for a range of popular technologies, including MySQL, PostgreSQL, and Apache. 4. Alertmanager: The Alertmanager component is responsible for processing alerts generated by the Prometheus server. It can handle alerts from multiple sources and provides a range of features for deduplicating, grouping, and routing alerts to appropriate channels. Overall, the Prometheus architecture is designed to be highly scalable and resilient. The server and client libraries can be deployed in a distributed fashion to support monitoring across large-scale, highly dynamic environments </b></details> <details> <summary>Can you compare Prometheus to other solutions like InfluxDB for", "metadata": {"source_file": "learning-materials/README.md", "section": "Prometheus", "language": "en", "created_at": "2025-07-19T19:22:02.006491"}}
{"text": "example?</summary><br><b> Compared to other monitoring solutions, such as InfluxDB, Prometheus is known for its high performance and scalability. It can handle large volumes of data and can easily be integrated with other tools in the monitoring ecosystem. InfluxDB, on the other hand, is known for its ease of use and simplicity. It has a user-friendly interface and provides easy-to-use APIs for collecting and querying data. Another popular solution, Nagios, is a more traditional monitoring system that relies on a push-based model for collecting data. Nagios has been around for a long time and is known for its stability and reliability. However, compared to Prometheus, Nagios lacks some of the more advanced features, such as multi-dimensional data model and powerful query language. Overall, the choice of a monitoring solution depends on the specific needs and requirements of the organization. While Prometheus is a great choice for large-scale monitoring and alerting, InfluxDB may be a", "metadata": {"source_file": "learning-materials/README.md", "section": "Prometheus", "language": "en", "created_at": "2025-07-19T19:22:02.006510"}}
{"text": "better fit for smaller environments that require ease of use and simplicity. Nagios remains a solid choice for organizations that prioritize stability and reliability over advanced features. </b></details> <details> <summary>What is an Alert?</summary><br><b> In Prometheus, an alert is a notification triggered when a specific condition or threshold is met. Alerts can be configured to trigger when certain metrics cross a certain threshold or when specific events occur. Once an alert is triggered, it can be routed to various channels, such as email, pager, or chat, to notify relevant teams or individuals to take appropriate action. Alerts are a critical component of any monitoring system, as they allow teams to proactively detect and respond to issues before they impact users or cause system downtime. </b></details> <details> <summary>What is an Instance? What is a Job?</summary><br><b> In Prometheus, an instance refers to a single target that is being monitored. For example, a single", "metadata": {"source_file": "learning-materials/README.md", "section": "Prometheus", "language": "en", "created_at": "2025-07-19T19:22:02.006561"}}
{"text": "server or service. A job is a set of instances that perform the same function, such as a set of web servers serving the same application. Jobs allow you to define and manage a group of targets together. In essence, an instance is an individual target that Prometheus collects metrics from, while a job is a collection of similar instances that can be managed as a group. </b></details> <details> <summary>What core metrics types Prometheus supports?</summary><br><b> Prometheus supports several types of metrics, including: 1. Counter: A monotonically increasing value used for tracking counts of events or samples. Examples include the number of requests processed or the total number of errors encountered. 2. Gauge: A value that can go up or down, such as CPU usage or memory usage. Unlike counters, gauge values can be arbitrary, meaning they can go up and down based on changes in the system being monitored. 3. Histogram: A set of observations or events that are divided into buckets based on", "metadata": {"source_file": "learning-materials/README.md", "section": "Prometheus", "language": "en", "created_at": "2025-07-19T19:22:02.006582"}}
{"text": "their value. Histograms help in analyzing the distribution of a metric, such as request latencies or response sizes. 4. Summary: A summary is similar to a histogram, but instead of buckets, it provides a set of quantiles for the observed values. Summaries are useful for monitoring the distribution of request latencies or response sizes over time. Prometheus also supports various functions and operators for aggregating and manipulating metrics, such as sum, max, min, and rate. These features make it a powerful tool for monitoring and alerting on system metrics. </b></details> <details> <summary>What is an exporter? What is it used for?</summary><br><b> The exporter serves as a bridge between the third-party system or application and Prometheus, making it possible for Prometheus to monitor and collect data from that system or application. The exporter acts as a server, listening on a specific network port for requests from Prometheus to scrape metrics. It collects metrics from the", "metadata": {"source_file": "learning-materials/README.md", "section": "Prometheus", "language": "en", "created_at": "2025-07-19T19:22:02.006601"}}
{"text": "third-party system or application and transforms them into a format that can be understood by Prometheus. The exporter then exposes these metrics to Prometheus via an HTTP endpoint, making them available for collection and analysis. Exporters are commonly used to monitor various types of infrastructure components such as databases, web servers, and storage systems. For example, there are exporters available for monitoring popular databases such as MySQL and PostgreSQL, as well as web servers like Apache and Nginx. Overall, exporters are a critical component of the Prometheus ecosystem, allowing for the monitoring of a wide range of systems and applications, and providing a high degree of flexibility and extensibility to the platform. </b></details> <details> <summary>Which Prometheus best practices?</summary><br><b> Here are three of them: 1. Label carefully: Careful and consistent labeling of metrics is crucial for effective querying and alerting. Labels should be clear, concise, and", "metadata": {"source_file": "learning-materials/README.md", "section": "Prometheus", "language": "en", "created_at": "2025-07-19T19:22:02.006620"}}
{"text": "include all relevant information about the metric. 2. Keep metrics simple: The metrics exposed by exporters should be simple and focus on a single aspect of the system being monitored. This helps avoid confusion and ensures that the metrics are easily understandable by all members of the team. 3. Use alerting sparingly: While alerting is a powerful feature of Prometheus, it should be used sparingly and only for the most critical issues. Setting up too many alerts can lead to alert fatigue and result in important alerts being ignored. It is recommended to set up only the most important alerts and adjust the thresholds over time based on the actual frequency of alerts. </b></details> <details> <summary>How to get total requests in a given period of time?</summary><br><b> To get the total requests in a given period of time using Prometheus, you can use the *sum* function along with the *rate* function. Here is an example query that will give you the total number of requests in the last", "metadata": {"source_file": "learning-materials/README.md", "section": "Prometheus", "language": "en", "created_at": "2025-07-19T19:22:02.006638"}}
{"text": "hour: ``` sum(rate(http_requests_total[1h])) ``` In this query, *http_requests_total* is the name of the metric that tracks the total number of HTTP requests, and the *rate* function calculates the per-second rate of requests over the last hour. The *sum* function then adds up all of the requests to give you the total number of requests in the last hour. You can adjust the time range by changing the duration in the *rate* function. For example, if you wanted to get the total number of requests in the last day, you could change the function to *rate(http_requests_total[1d])*. </b></details> <details> <summary>What HA in Prometheus means?</summary><br><b> HA stands for High Availability. This means that the system is designed to be highly reliable and always available, even in the face of failures or other issues. In practice, this typically involves setting up multiple instances of Prometheus and ensuring that they are all synchronized and able to work together seamlessly. This can be", "metadata": {"source_file": "learning-materials/README.md", "section": "Prometheus", "language": "en", "created_at": "2025-07-19T19:22:02.006657"}}
{"text": "achieved through a variety of techniques, such as load balancing, replication, and failover mechanisms. By implementing HA in Prometheus, users can ensure that their monitoring data is always available and up-to-date, even in the face of hardware or software failures, network issues, or other problems that might otherwise cause downtime or data loss. </b></details> <details> <summary>How do you join two metrics?</summary><br><b> In Prometheus, joining two metrics can be achieved using the *join()* function. The *join()* function combines two or more time series based on their label values. It takes two mandatory arguments: *on* and *table*. The on argument specifies the labels to join *on* and the *table* argument specifies the time series to join. Here's an example of how to join two metrics using the *join()* function: ``` sum_series( join( on(service, instance) request_count_total, on(service, instance) error_count_total, ) ) ``` In this example, the *join()* function combines the", "metadata": {"source_file": "learning-materials/README.md", "section": "Prometheus", "language": "en", "created_at": "2025-07-19T19:22:02.006675"}}
{"text": "*request_count_total* and *error_count_total* time series based on their *service* and *instance* label values. The *sum_series()* function then calculates the sum of the resulting time series </b></details> <details> <summary>How to write a query that returns the value of a label?</summary><br><b> To write a query that returns the value of a label in Prometheus, you can use the *label_values* function. The *label_values* function takes two arguments: the name of the label and the name of the metric. For example, if you have a metric called *http_requests_total* with a label called *method*, and you want to return all the values of the *method* label, you can use the following query: ``` label_values(http_requests_total, method) ``` This will return a list of all the values for the *method* label in the *http_requests_total* metric. You can then use this list in further queries or to filter your data. </b></details> <details> <summary>How do you convert cpu_user_seconds to cpu usage in", "metadata": {"source_file": "learning-materials/README.md", "section": "Prometheus", "language": "en", "created_at": "2025-07-19T19:22:02.006693"}}
{"text": "percentage?</summary><br><b> To convert *cpu_user_seconds* to CPU usage in percentage, you need to divide it by the total elapsed time and the number of CPU cores, and then multiply by 100. The formula is as follows: ``` 100 * sum(rate(process_cpu_user_seconds_total{job=\"<job-name>\"}[<time-period>])) by (instance) / (<time-period> * <num-cpu-cores>) ``` Here, *<job-name>* is the name of the job you want to query, *<time-period>* is the time range you want to query (e.g. *5m*, *1h*), and *<num-cpu-cores>* is the number of CPU cores on the machine you are querying. For example, to get the CPU usage in percentage for the last 5 minutes for a job named *my-job* running on a machine with 4 CPU cores, you can use the following query: ``` 100 * sum(rate(process_cpu_user_seconds_total{job=\"my-job\"}[5m])) by (instance) / (5m * 4) ``` </b></details>", "metadata": {"source_file": "learning-materials/README.md", "section": "Prometheus", "language": "en", "created_at": "2025-07-19T19:22:02.006759"}}
{"text": "<details> <summary>What are some characteristics of the Go programming language?</summary><br><b> * Strong and static typing - the type of the variables can't be changed over time and they have to be defined at compile time * Simplicity * Fast compile times * Built-in concurrency * Garbage collected * Platform independent * Compile to standalone binary - anything you need to run your app will be compiled into one binary. Very useful for version management in run-time. Go also has good community. </b></details> <details> <summary>What is the difference between <code>var x int = 2</code> and <code>x := 2</code>?</summary><br><b> The result is the same, a variable with the value 2. With <code>var x int = 2</code> we are setting the variable type to integer while with <code>x := 2</code> we are letting Go figure out by itself the type. </b></details> <details> <summary>True or False? In Go we can redeclare variables and once declared we must use it.</summary> False. We can't redeclare", "metadata": {"source_file": "learning-materials/README.md", "section": "Go", "language": "en", "created_at": "2025-07-19T19:22:02.007984"}}
{"text": "variables but yes, we must used declared variables. </b></details> <details> <summary>What libraries of Go have you used?</summary><br><b> This should be answered based on your usage but some examples are: * fmt - formatted I/O </b></details> <details> <summary>What is the problem with the following block of code? How to fix it? ``` func main() { var x float32 = 13.5 var y int y = x } ``` </summary><br><b> </b></details> <details> <summary>The following block of code tries to convert the integer 101 to a string but instead we get \"e\". Why is that? How to fix it? ```go package main import \"fmt\" func main() { var x int = 101 var y string y = string(x) fmt.Println(y) } ``` </summary><br><b> It looks what unicode value is set at 101 and uses it for converting the integer to a string. If you want to get \"101\" you should use the package \"strconv\" and replace <code>y = string(x)</code> with <code>y = strconv.Itoa(x)</code> </b></details> <details> <summary>What is wrong with the following", "metadata": {"source_file": "learning-materials/README.md", "section": "Go", "language": "en", "created_at": "2025-07-19T19:22:02.008017"}}
{"text": "code?: ``` package main func main() { var x = 2 var y = 3 const someConst = x + y } ``` </summary><br><b> Constants in Go can only be declared using constant expressions. But `x`, `y` and their sum is variable. <br> <code>const initializer x + y is not a constant</code> </b></details> <details> <summary>What will be the output of the following block of code?: ```go package main import \"fmt\" const ( x = iota y = iota ) const z = iota func main() { fmt.Printf(\"%v\\n\", x) fmt.Printf(\"%v\\n\", y) fmt.Printf(\"%v\\n\", z) } ``` </summary><br><b> Go's iota identifier is used in const declarations to simplify definitions of incrementing numbers. Because it can be used in expressions, it provides a generality beyond that of simple enumerations. <br> `x` and `y` in the first iota group, `z` in the second. <br> [Iota page in Go Wiki](https://github.com/golang/go/wiki/Iota) </b></details> <details> <summary>What _ is used for in Go?</summary><br><b> It avoids having to declare all the variables for the", "metadata": {"source_file": "learning-materials/README.md", "section": "Go", "language": "en", "created_at": "2025-07-19T19:22:02.008050"}}
{"text": "returns values. It is called the [blank identifier](https://golang.org/doc/effective_go.html#blank). <br> [answer in SO](https://stackoverflow.com/questions/27764421/what-is-underscore-comma-in-a-go-declaration#answer-27764432) </b></details> <details> <summary>What will be the output of the following block of code?: ```go package main import \"fmt\" const ( _ = iota + 3 x ) func main() { fmt.Printf(\"%v\\n\", x) } ``` </summary><br><b> Since the first iota is declared with the value `3` (` + 3`), the next one has the value `4` </b></details> <details> <summary>What will be the output of the following block of code?: ```go package main import ( \"fmt\" \"sync\" \"time\" ) func main() { var wg sync.WaitGroup wg.Add(1) go func() { time.Sleep(time.Second * 2) fmt.Println(\"1\") wg.Done() }() go func() { fmt.Println(\"2\") }() wg.Wait() fmt.Println(\"3\") } ``` </summary><br><b> Output: 2 1 3 [Aritcle about sync/waitgroup](https://tutorialedge.net/golang/go-waitgroup-tutorial/) [Golang package", "metadata": {"source_file": "learning-materials/README.md", "section": "Go", "language": "en", "created_at": "2025-07-19T19:22:02.008070"}}
{"text": "sync](https://golang.org/pkg/sync/) </b></details> <details> <summary>What will be the output of the following block of code?: ```go package main import ( \"fmt\" ) func mod1(a []int) { for i := range a { a[i] = 5 } fmt.Println(\"1:\", a) } func mod2(a []int) { a = append(a, 125) // ! for i := range a { a[i] = 5 } fmt.Println(\"2:\", a) } func main() { s1 := []int{1, 2, 3, 4} mod1(s1) fmt.Println(\"1:\", s1) s2 := []int{1, 2, 3, 4} mod2(s2) fmt.Println(\"2:\", s2) } ``` </summary><br><b> Output: <code><br> 1 [5 5 5 5]<br> 1 [5 5 5 5]<br> 2 [5 5 5 5 5]<br> 2 [1 2 3 4]<br> </code> In `mod1` a is link, and when we're using `a[i]`, we're changing `s1` value to. But in `mod2`, `append` creates new slice, and we're changing only `a` value, not `s2`. [Aritcle about arrays](https://golangbot.com/arrays-and-slices/), [Blog post about `append`](https://blog.golang.org/slices) </b></details> <details> <summary>What will be the output of the following block of code?: ```go package main import (", "metadata": {"source_file": "learning-materials/README.md", "section": "Go", "language": "en", "created_at": "2025-07-19T19:22:02.008089"}}
{"text": "\"container/heap\" \"fmt\" ) // An IntHeap is a min-heap of ints. type IntHeap []int func (h IntHeap) Len() int { return len(h) } func (h IntHeap) Less(i, j int) bool { return h[i] < h[j] } func (h IntHeap) Swap(i, j int) { h[i], h[j] = h[j], h[i] } func (h *IntHeap) Push(x interface{}) { // Push and Pop use pointer receivers because they modify the slice's length, // not just its contents. *h = append(*h, x.(int)) } func (h *IntHeap) Pop() interface{} { old := *h n := len(old) x := old[n-1] *h = old[0 : n-1] return x } func main() { h := &IntHeap{4, 8, 3, 6} heap.Init(h) heap.Push(h, 7) fmt.Println((*h)[0]) } ``` </summary><br><b> Output: 3 [Golang container/heap package](https://golang.org/pkg/container/heap/) </b></details>", "metadata": {"source_file": "learning-materials/README.md", "section": "Go", "language": "en", "created_at": "2025-07-19T19:22:02.008108"}}
{"text": "<details> <summary>What are the advantages of MongoDB? Or in other words, why choosing MongoDB and not other implementation of NoSQL?</summary><br><b> MongoDB advantages are as following: - Schemaless - Easy to scale-out - No complex joins - Structure of a single object is clear </b></details> <details> <summary>What is the difference between SQL and NoSQL?</summary><br><b> The main difference is that SQL databases are structured (data is stored in the form of tables with rows and columns - like an excel spreadsheet table) while NoSQL is unstructured, and the data storage can vary depending on how the NoSQL DB is set up, such as key-value pair, document-oriented, etc. </b></details> <details> <summary>In what scenarios would you prefer to use NoSQL/Mongo over SQL?</summary><br><b> * Heterogeneous data which changes often * Data consistency and integrity is not top priority * Best if the database needs to scale rapidly </b></details> <details> <summary>What is a document? What is a", "metadata": {"source_file": "learning-materials/README.md", "section": "Mongo", "language": "en", "created_at": "2025-07-19T19:22:02.008487"}}
{"text": "collection?</summary><br><b> * A document is a record in MongoDB, which is stored in BSON (Binary JSON) format and is the basic unit of data in MongoDB. * A collection is a group of related documents stored in a single database in MongoDB. </b></details> <details> <summary>What is an aggregator?</summary><br><b> * An aggregator is a framework in MongoDB that performs operations on a set of data to return a single computed result. </b></details> <details> <summary>What is better? Embedded documents or referenced?</summary><br><b> * There is no definitive answer to which is better, it depends on the specific use case and requirements. Some explanations : Embedded documents provide atomic updates, while referenced documents allow for better normalization. </b></details> <details> <summary>Have you performed data retrieval optimizations in Mongo? If not, can you think about ways to optimize a slow data retrieval?</summary><br><b> * Some ways to optimize data retrieval in MongoDB are:", "metadata": {"source_file": "learning-materials/README.md", "section": "Mongo", "language": "en", "created_at": "2025-07-19T19:22:02.008545"}}
{"text": "indexing, proper schema design, query optimization and database load balancing. </b></details>", "metadata": {"source_file": "learning-materials/README.md", "section": "Mongo", "language": "en", "created_at": "2025-07-19T19:22:02.008567"}}
{"text": "<details> <summary>Explain this query: <code>db.books.find({\"name\": /abc/})</code></summary><br><b> </b></details> <details> <summary>Explain this query: <code>db.books.find().sort({x:1})</code></summary><br><b> </b></details> <details> <summary>What is the difference between find() and find_one()?</code></summary><br><b> * `find()` returns all documents that match the query conditions. * find_one() returns only one document that matches the query conditions (or null if no match is found). </b></details> <details> <summary>How can you export data from Mongo DB?</code></summary><br><b> * mongoexport * programming languages </b></details>", "metadata": {"source_file": "learning-materials/README.md", "section": "Queries", "language": "en", "created_at": "2025-07-19T19:22:02.008627"}}
{"text": "|Name|Topic|Objective & Instructions|Solution|Comments| |--------|--------|------|----|----| | Functions vs. Comparisons | Query Improvements | [Exercise](topics/sql/improve_query.md) | [Solution](topics/sql/solutions/improve_query.md)", "metadata": {"source_file": "learning-materials/README.md", "section": "SQL Exercises", "language": "en", "created_at": "2025-07-19T19:22:02.008651"}}
{"text": "<details> <summary>What is SQL?</summary><br><b> SQL (Structured Query Language) is a standard language for relational databases (like MySQL, MariaDB, ...).<br> It's used for reading, updating, removing and creating data in a relational database. </b></details> <details> <summary>How is SQL Different from NoSQL</summary><br><b> The main difference is that SQL databases are structured (data is stored in the form of tables with rows and columns - like an excel spreadsheet table) while NoSQL is unstructured, and the data storage can vary depending on how the NoSQL DB is set up, such as key-value pair, document-oriented, etc. </b></details> <details> <summary>When is it best to use SQL? NoSQL?</summary><br><b> SQL - Best used when data integrity is crucial. SQL is typically implemented with many businesses and areas within the finance field due to it's ACID compliance. NoSQL - Great if you need to scale things quickly. NoSQL was designed with web applications in mind, so it works great if", "metadata": {"source_file": "learning-materials/README.md", "section": "SQL Self Assessment", "language": "en", "created_at": "2025-07-19T19:22:02.008899"}}
{"text": "you need to quickly spread the same information around to multiple servers Additionally, since NoSQL does not adhere to the strict table with columns and rows structure that Relational Databases require, you can store different data types together. </b></details>", "metadata": {"source_file": "learning-materials/README.md", "section": "SQL Self Assessment", "language": "en", "created_at": "2025-07-19T19:22:02.008926"}}
{"text": "For these questions, we will be using the Customers and Orders tables shown below: **Customers** Customer_ID | Customer_Name | Items_in_cart | Cash_spent_to_Date ------------ | ------------- | ------------- | ------------- 100204 | John Smith | 0 | 20.00 100205 | Jane Smith | 3 | 40.00 100206 | Bobby Frank | 1 | 100.20 **ORDERS** Customer_ID | Order_ID | Item | Price | Date_sold ------------ | ------------- | ------------- | ------------- | ------------- 100206 | A123 | Rubber Ducky | 2.20 | 2019-09-18 100206 | A123 | Bubble Bath | 8.00 | 2019-09-18 100206 | Q987 | 80-Pack TP | 90.00 | 2019-09-20 100205 | Z001 | Cat Food - Tuna Fish | 10.00 | 2019-08-05 100205 | Z001 | Cat Food - Chicken | 10.00 | 2019-08-05 100205 | Z001 | Cat Food - Beef | 10.00 | 2019-08-05 100205 | Z001 | Cat Food - Kitty quesadilla | 10.00 | 2019-08-05 100204 | X202 | Coffee | 20.00 | 2019-04-29 <details> <summary>How would I select all fields from this table?</summary><br><b> Select * <br> From Customers;", "metadata": {"source_file": "learning-materials/README.md", "section": "Practical SQL - Basics", "language": "en", "created_at": "2025-07-19T19:22:02.009633"}}
{"text": "</b></details> <details> <summary>How many items are in John's cart?</summary><br><b> Select Items_in_cart <br> From Customers <br> Where Customer_Name = \"John Smith\"; </b></details> <details> <summary>What is the sum of all the cash spent across all customers?</summary><br><b> Select SUM(Cash_spent_to_Date) as SUM_CASH <br> From Customers; </b></details> <details> <summary>How many people have items in their cart?</summary><br><b> Select count(1) as Number_of_People_w_items <br> From Customers <br> where Items_in_cart > 0; </b></details> <details> <summary>How would you join the customer table to the order table?</summary><br><b> You would join them on the unique key. In this case, the unique key is Customer_ID in both the Customers table and Orders table </b></details> <details> <summary>How would you show which customer ordered which items?</summary><br><b> Select c.Customer_Name, o.Item <br> From Customers c <br> Left Join Orders o <br> On c.Customer_ID = o.Customer_ID;", "metadata": {"source_file": "learning-materials/README.md", "section": "Practical SQL - Basics", "language": "en", "created_at": "2025-07-19T19:22:02.009657"}}
{"text": "</b></details> <details> <summary>Using a with statement, how would you show who ordered cat food, and the total amount of money spent?</summary><br><b> with cat_food as ( <br> Select Customer_ID, SUM(Price) as TOTAL_PRICE <br> From Orders <br> Where Item like \"%Cat Food%\" <br> Group by Customer_ID <br> ) <br> Select Customer_name, TOTAL_PRICE <br> From Customers c <br> Inner JOIN cat_food f <br> ON c.Customer_ID = f.Customer_ID <br> where c.Customer_ID in (Select Customer_ID from cat_food); Although this was a simple statement, the \"with\" clause really shines when a complex query needs to be run on a table before joining to another. With statements are nice, because you create a pseudo temp when running your query, instead of creating a whole new table. The Sum of all the purchases of cat food weren't readily available, so we used a with statement to create the pseudo table to retrieve the sum of the prices spent by each customer, then join the table normally. </b></details> <details>", "metadata": {"source_file": "learning-materials/README.md", "section": "Practical SQL - Basics", "language": "en", "created_at": "2025-07-19T19:22:02.009677"}}
{"text": "<summary>Which of the following queries would you use? ``` SELECT count(*) SELECT count(*) FROM shawarma_purchases FROM shawarma_purchases WHERE vs. WHERE YEAR(purchased_at) == '2017' purchased_at >= '2017-01-01' AND purchased_at <= '2017-31-12' ``` </summary><br><b> ``` SELECT count(*) FROM shawarma_purchases WHERE purchased_at >= '2017-01-01' AND purchased_at <= '2017-31-12' ``` When you use a function (`YEAR(purchased_at)`) it has to scan the whole database as opposed to using indexes and basically the column as it is, in its natural state. </b></details>", "metadata": {"source_file": "learning-materials/README.md", "section": "Practical SQL - Basics", "language": "en", "created_at": "2025-07-19T19:22:02.009697"}}
{"text": "<details> <summary>What components/projects of OpenStack are you familiar with?</summary><br><b> I’m most familiar with several core OpenStack components: - Nova for compute resource provisioning, including VM lifecycle management. - Neutron for networking, focusing on creating and managing networks, subnets, and routers. - Cinder for block storage, used to attach and manage storage volumes. - Keystone for identity services, handling authentication and authorization. I’ve implemented these in past projects, configuring them for scalability and security to support multi-tenant environments. </b></details> <details> <summary>Can you tell me what each of the following services/projects is responsible for?: - Nova - Neutron - Cinder - Glance - Keystone</summary><br><b> * Nova - Manage virtual instances * Neutron - Manage networking by providing Network as a service (NaaS) * Cinder - Block Storage * Glance - Manage images for virtual machines and containers (search, get and register) *", "metadata": {"source_file": "learning-materials/README.md", "section": "OpenStack", "language": "en", "created_at": "2025-07-19T19:22:02.010309"}}
{"text": "Keystone - Authentication service across the cloud </b></details> <details> <summary>Identify the service/project used for each of the following: * Copy or snapshot instances * GUI for viewing and modifying resources * Block Storage * Manage virtual instances </summary><br><b> * Glance - Images Service. Also used for copying or snapshot instances * Horizon - GUI for viewing and modifying resources * Cinder - Block Storage * Nova - Manage virtual instances </b></details> <details> <summary>What is a tenant/project?</summary><br><b> </b></details> <details> <summary>Determine true or false: * OpenStack is free to use * The service responsible for networking is Glance * The purpose of tenant/project is to share resources between different projects and users of OpenStack</summary><br><b> </b></details> <details> <summary>Describe in detail how you bring up an instance with a floating IP</summary><br><b> </b></details> <details> <summary>You get a call from a customer saying: \"I can ping my", "metadata": {"source_file": "learning-materials/README.md", "section": "OpenStack", "language": "en", "created_at": "2025-07-19T19:22:02.010340"}}
{"text": "instance but can't connect (ssh) it\". What might be the problem?</summary><br><b> </b></details> <details> <summary>What types of networks OpenStack supports?</summary><br><b> </b></details> <details> <summary>How do you debug OpenStack storage issues? (tools, logs, ...)</summary><br><b> </b></details> <details> <summary>How do you debug OpenStack compute issues? (tools, logs, ...)</summary><br><b> </b></details>", "metadata": {"source_file": "learning-materials/README.md", "section": "OpenStack", "language": "en", "created_at": "2025-07-19T19:22:02.010367"}}
{"text": "<details> <summary>Have you deployed OpenStack in the past? If yes, can you describe how you did it?</summary><br><b> </b></details> <details> <summary>Are you familiar with TripleO? How is it different from Devstack or Packstack?</summary><br><b> You can read about TripleO right [here](https://docs.openstack.org/tripleo-docs/latest) </b></details>", "metadata": {"source_file": "learning-materials/README.md", "section": "OpenStack Deployment & TripleO", "language": "en", "created_at": "2025-07-19T19:22:02.010405"}}
{"text": "<details> <summary>Can you describe Nova in detail?</summary><br><b> * Used to provision and manage virtual instances * It supports Multi-Tenancy in different levels - logging, end-user control, auditing, etc. * Highly scalable * Authentication can be done using internal system or LDAP * Supports multiple types of block storage * Tries to be hardware and hypervisor agnostice </b></details> <details> <summary>What do you know about Nova architecture and components?</summary><br><b> * nova-api - the server which serves metadata and compute APIs * the different Nova components communicate by using a queue (Rabbitmq usually) and a database * a request for creating an instance is inspected by nova-scheduler which determines where the instance will be created and running * nova-compute is the component responsible for communicating with the hypervisor for creating the instance and manage its lifecycle </b></details>", "metadata": {"source_file": "learning-materials/README.md", "section": "OpenStack Compute", "language": "en", "created_at": "2025-07-19T19:22:02.010563"}}
{"text": "<details> <summary>Explain Neutron in detail</summary><br><b> * One of the core component of OpenStack and a standalone project * Neutron focused on delivering networking as a service * With Neutron, users can set up networks in the cloud and configure and manage a variety of network services * Neutron interacts with: * Keystone - authorize API calls * Nova - nova communicates with neutron to plug NICs into a network * Horizon - supports networking entities in the dashboard and also provides topology view which includes networking details </b></details> <details> <summary>Explain each of the following components: - neutron-dhcp-agent - neutron-l3-agent - neutron-metering-agent - neutron-*-agtent - neutron-server</summary><br><b> * neutron-l3-agent - L3/NAT forwarding (provides external network access for VMs for example) * neutron-dhcp-agent - DHCP services * neutron-metering-agent - L3 traffic metering * neutron-*-agtent - manages local vSwitch configuration on each compute (based on", "metadata": {"source_file": "learning-materials/README.md", "section": "OpenStack Networking (Neutron)", "language": "en", "created_at": "2025-07-19T19:22:02.011043"}}
{"text": "chosen plugin) * neutron-server - exposes networking API and passes requests to other plugins if required </b></details> <details> <summary>Explain these network types: - Management Network - Guest Network - API Network - External Network</summary><br><b> * Management Network - used for internal communication between OpenStack components. Any IP address in this network is accessible only within the datacetner * Guest Network - used for communication between instances/VMs * API Network - used for services API communication. Any IP address in this network is publicly accessible * External Network - used for public communication. Any IP address in this network is accessible by anyone on the internet </b></details> <details> <summary>In which order should you remove the following entities: * Network * Port * Router * Subnet</summary><br><b> - Port - Subnet - Router - Network There are many reasons for that. One for example: you can't remove router if there are active ports assigned to it.", "metadata": {"source_file": "learning-materials/README.md", "section": "OpenStack Networking (Neutron)", "language": "en", "created_at": "2025-07-19T19:22:02.011071"}}
{"text": "</b></details> <details> <summary>What is a provider network?</summary><br><b> </b></details> <details> <summary>What components and services exist for L2 and L3?</summary><br><b> </b></details> <details> <summary>What is the ML2 plug-in? Explain its architecture</summary><br><b> </b></details> <details> <summary>What is the L2 agent? How does it works and what is it responsible for?</summary><br><b> </b></details> <details> <summary>What is the L3 agent? How does it works and what is it responsible for?</summary><br><b> </b></details> <details> <summary>Explain what the Metadata agent is responsible for</summary><br><b> </b></details> <details> <summary>What networking entities Neutron supports?</summary><br><b> </b></details> <details> <summary>How do you debug OpenStack networking issues? (tools, logs, ...)</summary><br><b> </b></details>", "metadata": {"source_file": "learning-materials/README.md", "section": "OpenStack Networking (Neutron)", "language": "en", "created_at": "2025-07-19T19:22:02.011091"}}
{"text": "<details> <summary>Explain Glance in detail</summary><br><b> * Glance is the OpenStack image service * It handles requests related to instances disks and images * Glance also used for creating snapshots for quick instances backups * Users can use Glance to create new images or upload existing ones </b></details> <details> <summary>Describe Glance architecture</summary><br><b> * glance-api - responsible for handling image API calls such as retrieval and storage. It consists of two APIs: 1. registry-api - responsible for internal requests 2. user API - can be accessed publicly * glance-registry - responsible for handling image metadata requests (e.g. size, type, etc). This component is private which means it's not available publicly * metadata definition service - API for custom metadata * database - for storing images metadata * image repository - for storing images. This can be a filesystem, swift object storage, HTTP, etc. </b></details>", "metadata": {"source_file": "learning-materials/README.md", "section": "OpenStack - Glance", "language": "en", "created_at": "2025-07-19T19:22:02.011268"}}
{"text": "<details> <summary>Explain Swift in detail</summary><br><b> * Swift is Object Store service and is an highly available, distributed and consistent store designed for storing a lot of data * Swift is distributing data across multiple servers while writing it to multiple disks * One can choose to add additional servers to scale the cluster. All while swift maintaining integrity of the information and data replications. </b></details> <details> <summary>Can users store by default an object of 100GB in size?</summary><br><b> Not by default. Object Storage API limits the maximum to 5GB per object but it can be adjusted. </b></details> <details> <summary>Explain the following in regards to Swift: * Container * Account * Object </summary><br><b> - Container - Defines a namespace for objects. - Account - Defines a namespace for containers - Object - Data content (e.g. image, document, ...) </b></details> <details> <summary>True or False? there can be two objects with the same name in the same", "metadata": {"source_file": "learning-materials/README.md", "section": "OpenStack - Swift", "language": "en", "created_at": "2025-07-19T19:22:02.011575"}}
{"text": "container but not in two different containers</summary><br><b> False. Two objects can have the same name if they are in different containers. </b></details>", "metadata": {"source_file": "learning-materials/README.md", "section": "OpenStack - Swift", "language": "en", "created_at": "2025-07-19T19:22:02.011604"}}
{"text": "<details> <summary>Explain Cinder in detail</summary><br><b> * Cinder is OpenStack Block Storage service * It basically provides used with storage resources they can consume with other services such as Nova * One of the most used implementations of storage supported by Cinder is LVM * From user perspective this is transparent which means the user doesn't know where, behind the scenes, the storage is located or what type of storage is used </b></details> <details> <summary>Describe Cinder's components</summary><br><b> * cinder-api - receives API requests * cinder-volume - manages attached block devices * cinder-scheduler - responsible for storing volumes </b></details>", "metadata": {"source_file": "learning-materials/README.md", "section": "OpenStack - Cinder", "language": "en", "created_at": "2025-07-19T19:22:02.011703"}}
{"text": "<details> <summary>Can you describe the following concepts in regards to Keystone? - Role - Tenant/Project - Service - Endpoint - Token </summary><br><b> - Role - A list of rights and privileges determining what a user or a project can perform - Tenant/Project - Logical representation of a group of resources isolated from other groups of resources. It can be an account, organization, ... - Service - An endpoint which the user can use for accessing different resources - Endpoint - a network address which can be used to access a certain OpenStack service - Token - Used for access resources while describing which resources can be accessed by using a scope </b></details> <details> <summary>What are the properties of a service? In other words, how a service is identified?</summary><br><b> Using: - Name - ID number - Type - Description </b></details> <details> <summary>Explain the following: - PublicURL - InternalURL - AdminURL</summary><br><b> - PublicURL - Publicly accessible through", "metadata": {"source_file": "learning-materials/README.md", "section": "OpenStack - Keystone", "language": "en", "created_at": "2025-07-19T19:22:02.011958"}}
{"text": "public internet - InternalURL - Used for communication between services - AdminURL - Used for administrative management </b></details> <details> <summary>What is a service catalog?</summary><br><b> A list of services and their endpoints </b></details>", "metadata": {"source_file": "learning-materials/README.md", "section": "OpenStack - Keystone", "language": "en", "created_at": "2025-07-19T19:22:02.011981"}}
{"text": "<details> <summary>Describe each of the following services * Swift * Sahara * Ironic * Trove * Aodh * Ceilometer </summary><br><b> * Swift - highly available, distributed, eventually consistent object/blob store * Sahara - Manage Hadoop Clusters * Ironic - Bare Metal Provisioning * Trove - Database as a service that runs on OpenStack * Aodh - Alarms Service * Ceilometer - Track and monitor usage </b></details> <details> <summary>Identify the service/project used for each of the following: * Database as a service which runs on OpenStack * Bare Metal Provisioning * Track and monitor usage * Alarms Service * Manage Hadoop Clusters * highly available, distributed, eventually consistent object/blob store </summary><br><b> * Database as a service which runs on OpenStack - Trove * Bare Metal Provisioning - Ironic * Track and monitor usage - Ceilometer * Alarms Service - Aodh * Manage Hadoop Clusters * Manage Hadoop Clusters - Sahara * highly available, distributed, eventually consistent", "metadata": {"source_file": "learning-materials/README.md", "section": "OpenStack Advanced - Services", "language": "en", "created_at": "2025-07-19T19:22:02.012191"}}
{"text": "object/blob store - Swift </b></details>", "metadata": {"source_file": "learning-materials/README.md", "section": "OpenStack Advanced - Services", "language": "en", "created_at": "2025-07-19T19:22:02.012211"}}
{"text": "<details> <summary>Can you describe Keystone service in detail?</summary><br><b> * You can't have OpenStack deployed without Keystone * It Provides identity, policy and token services * The authentication provided is for both users and services * The authorization supported is token-based and user-based. * There is a policy defined based on RBAC stored in a JSON file and each line in that file defines the level of access to apply </b></details> <details> <summary>Describe Keystone architecture</summary><br><b> * There is a service API and admin API through which Keystone gets requests * Keystone has four backends: * Token Backend - Temporary Tokens for users and services * Policy Backend - Rules management and authorization * Identity Backend - users and groups (either standalone DB, LDAP, ...) * Catalog Backend - Endpoints * It has pluggable environment where you can integrate with: * LDAP * KVS (Key Value Store) * SQL * PAM * Memcached </b></details> <details> <summary>Describe the", "metadata": {"source_file": "learning-materials/README.md", "section": "OpenStack Advanced - Keystone", "language": "en", "created_at": "2025-07-19T19:22:02.012452"}}
{"text": "Keystone authentication process</summary><br><b> * Keystone gets a call/request and checks whether it's from an authorized user, using username, password and authURL * Once confirmed, Keystone provides a token. * A token contains a list of user's projects so there is no to authenticate every time and a token can submitted instead </b></details>", "metadata": {"source_file": "learning-materials/README.md", "section": "OpenStack Advanced - Keystone", "language": "en", "created_at": "2025-07-19T19:22:02.012473"}}
{"text": "<details> <summary>What each of the following does?: * nova-api * nova-compuate * nova-conductor * nova-cert * nova-consoleauth * nova-scheduler </summary><br><b> * nova-api - responsible for managing requests/calls * nova-compute - responsible for managing instance lifecycle * nova-conductor - Mediates between nova-compute and the database so nova-compute doesn't access it directly </b></details> <details> <summary>What types of Nova proxies are you familiar with?</summary><br><b> * Nova-novncproxy - Access through VNC connections * Nova-spicehtml5proxy - Access through SPICE * Nova-xvpvncproxy - Access through a VNC connection </b></details>", "metadata": {"source_file": "learning-materials/README.md", "section": "OpenStack Advanced - Compute (Nova)", "language": "en", "created_at": "2025-07-19T19:22:02.012552"}}
{"text": "<details> <summary>Explain BGP dynamic routing</summary><br><b> </b></details> <details> <summary>What is the role of network namespaces in OpenStack?</summary><br><b> </b></details>", "metadata": {"source_file": "learning-materials/README.md", "section": "OpenStack Advanced - Networking (Neutron)", "language": "en", "created_at": "2025-07-19T19:22:02.012577"}}
{"text": "<details> <summary>Can you describe Horizon in detail?</summary><br><b> * Django-based project focusing on providing an OpenStack dashboard and the ability to create additional customized dashboards * You can use it to access the different OpenStack services resources - instances, images, networks, ... * By accessing the dashboard, users can use it to list, create, remove and modify the different resources * It's also highly customizable and you can modify or add to it based on your needs </b></details> <details> <summary>What can you tell about Horizon architecture?</summary><br><b> * API is backward compatible * There are three type of dashboards: user, system and settings * It provides core support for all OpenStack core projects such as Neutron, Nova, etc. (out of the box, no need to install extra packages or plugins) * Anyone can extend the dashboards and add new components * Horizon provides templates and core classes from which one can build its own dashboard </b></details>", "metadata": {"source_file": "learning-materials/README.md", "section": "OpenStack Advanced - Horizon", "language": "en", "created_at": "2025-07-19T19:22:02.012837"}}
{"text": "<details> <summary>What is Puppet? How does it works?</summary><br><b> * Puppet is a configuration management tool ensuring that all systems are configured to a desired and predictable state. </b></details> <details> <summary>Explain Puppet architecture</summary><br><b> * Puppet has a primary-secondary node architecture. The clients are distributed across the network and communicate with the primary-secondary environment where Puppet modules are present. The client agent sends a certificate with its ID to the server; the server then signs that certificate and sends it back to the client. This authentication allows for secure and verifiable communication between the client and the master. </b></details> <details> <summary>Can you compare Puppet to other configuration management tools? Why did you chose to use Puppet?</summary><br><b> * Puppet is often compared to other configuration management tools like Chef, Ansible, SaltStack, and cfengine. The choice to use Puppet often depends on", "metadata": {"source_file": "learning-materials/README.md", "section": "Puppet", "language": "en", "created_at": "2025-07-19T19:22:02.013245"}}
{"text": "an organization's needs, such as ease of use, scalability, and community support. </b></details> <details> <summary>Explain the following: * Module * Manifest * Node </summary><br><b> * Modules - are a collection of manifests, templates, and files * Manifests - are the actual codes for configuring the clients * Node - allows you to assign specific configurations to specific nodes </b></details> <details> <summary>Explain Facter</summary><br><b> * Facter is a standalone tool in Puppet that collects information about a system and its configuration, such as the operating system, IP addresses, memory, and network interfaces. This information can be used in Puppet manifests to make decisions about how resources should be managed, and to customize the behavior of Puppet based on the characteristics of the system. Facter is integrated into Puppet, and its facts can be used within Puppet manifests to make decisions about resource management. </b></details> <details> <summary>What is", "metadata": {"source_file": "learning-materials/README.md", "section": "Puppet", "language": "en", "created_at": "2025-07-19T19:22:02.013267"}}
{"text": "MCollective?</summary><br><b> * MCollective is a middleware system that integrates with Puppet to provide orchestration, remote execution, and parallel job execution capabilities. </b></details> <details> <summary>Do you have experience with writing modules? Which module have you created and for what?</summary><br><b> </b></details> <details> <summary>Explain what is Hiera</summary><br><b> * Hiera is a hierarchical data store in Puppet that is used to separate data from code, allowing data to be more easily separated, managed, and reused. </b></details>", "metadata": {"source_file": "learning-materials/README.md", "section": "Puppet", "language": "en", "created_at": "2025-07-19T19:22:02.013287"}}
{"text": "<details> <summary>What is the Elastic Stack?</summary><br><b> The Elastic Stack consists of: * Elasticsearch * Kibana * Logstash * Beats * Elastic Hadoop * APM Server Elasticsearch, Logstash and Kibana are also known as the ELK stack. </b></details> <details> <summary>Explain what is Elasticsearch</summary><br><b> From the official [docs](https://www.elastic.co/guide/en/elasticsearch/reference/current/documents-indices.html): \"Elasticsearch is a distributed document store. Instead of storing information as rows of columnar data, Elasticsearch stores complex data structures that have been serialized as JSON documents\" </b></details> <details> <summary>What is Logstash?</summary><br><b> From the [blog](https://logit.io/blog/post/the-top-50-elk-stack-and-elasticsearch-interview-questions): \"Logstash is a powerful, flexible pipeline that collects, enriches and transports data. It works as an extract, transform & load (ETL) tool for collecting log messages.\" </b></details> <details>", "metadata": {"source_file": "learning-materials/README.md", "section": "Elastic", "language": "en", "created_at": "2025-07-19T19:22:02.013641"}}
{"text": "<summary>Explain what beats are</summary><br><b> Beats are lightweight data shippers. These data shippers installed on the client where the data resides. Examples of beats: Filebeat, Metricbeat, Auditbeat. There are much more.<br> </b></details> <details> <summary>What is Kibana?</summary><br><b> From the official docs: \"Kibana is an open source analytics and visualization platform designed to work with Elasticsearch. You use Kibana to search, view, and interact with data stored in Elasticsearch indices. You can easily perform advanced data analysis and visualize your data in a variety of charts, tables, and maps.\" </b></details> <details> <summary>Describe what happens from the moment an app logged some information until it's displayed to the user in a dashboard when the Elastic stack is used</summary><br><b> The process may vary based on the chosen architecture and the processing you may want to apply to the logs. One possible workflow is: 1. The data logged by the application is", "metadata": {"source_file": "learning-materials/README.md", "section": "Elastic", "language": "en", "created_at": "2025-07-19T19:22:02.013661"}}
{"text": "picked by filebeat and sent to logstash 2. Logstash process the log based on the defined filters. Once done, the output is sent to Elasticsearch 2. Elasticsearch stores the document it got and the document is indexed for quick future access 4. The user creates visualizations in Kibana which based on the indexed data 5. The user creates a dashboard which composed out of the visualization created in the previous step </b></details>", "metadata": {"source_file": "learning-materials/README.md", "section": "Elastic", "language": "en", "created_at": "2025-07-19T19:22:02.013681"}}
{"text": "<details> <summary>What is a data node?</summary><br><b> This is where data is stored and also where different processing takes place (e.g. when you search for a data). </b></details> <details> <summary>What is a master node?</summary><br><b> Part of a master node responsibilities: * Track the status of all the nodes in the cluster * Verify replicas are working and the data is available from every data node. * No hot nodes (no data node that works much harder than other nodes) While there can be multiple master nodes in reality only of them is the elected master node. </b></details> <details> <summary>What is an ingest node?</summary><br><b> A node which responsible for processing the data according to ingest pipeline. In case you don't need to use logstash then this node can receive data from beats and process it, similarly to how it can be processed in Logstash. </b></details> <details> <summary>What is Coordinating only node?</summary><br><b> From the official docs: Coordinating", "metadata": {"source_file": "learning-materials/README.md", "section": "Elasticsearch", "language": "en", "created_at": "2025-07-19T19:22:02.015016"}}
{"text": "only nodes can benefit large clusters by offloading the coordinating node role from data and master-eligible nodes. They join the cluster and receive the full cluster state, like every other node, and they use the cluster state to route requests directly to the appropriate place(s). </b></details> <details> <summary>How data is stored in Elasticsearch?</summary><br><b> * Data is stored in an index * The index is spread across the cluster using shards </b></details> <details> <summary>What is an Index?</summary><br><b> Index in Elasticsearch is in most cases compared to a whole database from the SQL/NoSQL world.<br> You can choose to have one index to hold all the data of your app or have multiple indices where each index holds different type of your app (e.g. index for each service your app is running). The official docs also offer a great explanation (in general, it's really good documentation, as every project should have): \"An index can be thought of as an optimized collection of", "metadata": {"source_file": "learning-materials/README.md", "section": "Elasticsearch", "language": "en", "created_at": "2025-07-19T19:22:02.015078"}}
{"text": "documents and each document is a collection of fields, which are the key-value pairs that contain your data\" </b></details> <details> <summary>Explain Shards</summary><br><b> An index is split into shards and documents are hashed to a particular shard. Each shard may be on a different node in a cluster and each one of the shards is a self contained index.<br> This allows Elasticsearch to scale to an entire cluster of servers. </b></details> <details> <summary>What is an Inverted Index?</summary><br><b> From the official docs: \"An inverted index lists every unique word that appears in any document and identifies all of the documents each word occurs in.\" </b></details> <details> <summary>What is a Document?</summary><br><b> Continuing with the comparison to SQL/NoSQL a Document in Elasticsearch is a row in table in the case of SQL or a document in a collection in the case of NoSQL. As in NoSQL a document is a JSON object which holds data on a unit in your app. What is this unit depends", "metadata": {"source_file": "learning-materials/README.md", "section": "Elasticsearch", "language": "en", "created_at": "2025-07-19T19:22:02.015109"}}
{"text": "on the your app. If your app related to book then each document describes a book. If you are app is about shirts then each document is a shirt. </b></details> <details> <summary>You check the health of your elasticsearch cluster and it's red. What does it mean? What can cause the status to be yellow instead of green?</summary><br><b> Red means some data is unavailable in your cluster. Some shards of your indices are unassigned. There are some other states for the cluster. Yellow means that you have unassigned shards in the cluster. You can be in this state if you have single node and your indices have replicas. Green means that all shards in the cluster are assigned to nodes and your cluster is healthy. </b></details> <details> <summary>True or False? Elasticsearch indexes all data in every field and each indexed field has the same data structure for unified and quick query ability</summary><br><b> False. From the official docs: \"Each indexed field has a dedicated, optimized data", "metadata": {"source_file": "learning-materials/README.md", "section": "Elasticsearch", "language": "en", "created_at": "2025-07-19T19:22:02.015131"}}
{"text": "structure. For example, text fields are stored in inverted indices, and numeric and geo fields are stored in BKD trees.\" </b></details> <details> <summary>What reserved fields a document has?</summary><br><b> * _index * _id * _type </b></details> <details> <summary>Explain Mapping</summary><br><b> </b></details> <details> <summary>What are the advantages of defining your own mapping? (or: when would you use your own mapping?)</summary><br><b> * You can optimize fields for partial matching * You can define custom formats of known fields (e.g. date) * You can perform language-specific analysis </b></details> <details> <summary>Explain Replicas</summary><br><b> In a network/cloud environment where failures can be expected any time, it is very useful and highly recommended to have a failover mechanism in case a shard/node somehow goes offline or disappears for whatever reason. To this end, Elasticsearch allows you to make one or more copies of your index’s shards into what are called", "metadata": {"source_file": "learning-materials/README.md", "section": "Elasticsearch", "language": "en", "created_at": "2025-07-19T19:22:02.015152"}}
{"text": "replica shards, or replicas for short. </b></details> <details> <summary>Can you explain Term Frequency & Document Frequency?</summary><br><b> Term Frequency is how often a term appears in a given document and Document Frequency is how often a term appears in all documents. They both are used for determining the relevance of a term by calculating Term Frequency / Document Frequency. </b></details> <details> <summary>You check \"Current Phase\" under \"Index lifecycle management\" and you see it's set to \"hot\". What does it mean?</summary><br><b> \"The index is actively being written to\". More about the phases [here](https://www.elastic.co/guide/en/elasticsearch/reference/7.6/ilm-policy-definition.html) </b></details> <details> <summary>What this command does? <code>curl -X PUT \"localhost:9200/customer/_doc/1?pretty\" -H 'Content-Type: application/json' -d'{ \"name\": \"John Doe\" }'</code></summary><br><b> It creates customer index if it doesn't exists and adds a new document with the field name", "metadata": {"source_file": "learning-materials/README.md", "section": "Elasticsearch", "language": "en", "created_at": "2025-07-19T19:22:02.015174"}}
{"text": "which is set to \"John Dow\". Also, if it's the first document it will get the ID 1. </b></details> <details> <summary>What will happen if you run the previous command twice? What about running it 100 times?</code></summary><br><b> 1. If name value was different then it would update \"name\" to the new value 2. In any case, it bumps version field by one </b></details> <details> <summary>What is the Bulk API? What would you use it for?</code></summary><br><b> Bulk API is used when you need to index multiple documents. For high number of documents it would be significantly faster to use rather than individual requests since there are less network roundtrips. </b></details>", "metadata": {"source_file": "learning-materials/README.md", "section": "Elasticsearch", "language": "en", "created_at": "2025-07-19T19:22:02.015193"}}
{"text": "<details> <summary>Explain Elasticsearch query syntax (Booleans, Fields, Ranges)</summary><br><b> </b></details> <details> <summary>Explain what is Relevance Score</summary><br><b> </b></details> <details> <summary>Explain Query Context and Filter Context</summary><br><b> From the official docs: \"In the query context, a query clause answers the question “How well does this document match this query clause?” Besides deciding whether or not the document matches, the query clause also calculates a relevance score in the _score meta-field.\" \"In a filter context, a query clause answers the question “Does this document match this query clause?” The answer is a simple Yes or No — no scores are calculated. Filter context is mostly used for filtering structured data\" </b></details> <details> <summary>Describe how would an architecture of production environment with large amounts of data would be different from a small-scale environment</summary><br><b> There are several possible answers for", "metadata": {"source_file": "learning-materials/README.md", "section": "Query DSL", "language": "en", "created_at": "2025-07-19T19:22:02.015599"}}
{"text": "this question. One of them is as follows: A small-scale architecture of elastic will consist of the elastic stack as it is. This means we will have beats, logstash, elastcsearch and kibana.<br> A production environment with large amounts of data can include some kind of buffering component (e.g. Reddis or RabbitMQ) and also security component such as Nginx. </b></details>", "metadata": {"source_file": "learning-materials/README.md", "section": "Query DSL", "language": "en", "created_at": "2025-07-19T19:22:02.015631"}}
{"text": "<details> <summary>What are Logstash plugins? What plugins types are there?</summary><br><b> * Input Plugins - how to collect data from different sources * Filter Plugins - processing data * Output Plugins - push data to different outputs/services/platforms </b></details> <details> <summary>What is grok?</summary><br><b> A logstash plugin which modifies information in one format and immerse it in another. </b></details> <details> <summary>How grok works?</summary><br><b> </b></details> <details> <summary>What grok patterns are you familiar with?</summary><br><b> </b></details> <details> <summary>What is `_grokparsefailure?`</summary><br><b> </b></details> <details> <summary>How do you test or debug grok patterns?</summary><br><b> </b></details> <details> <summary>What are Logstash Codecs? What codecs are there?</summary><br><b> </b></details>", "metadata": {"source_file": "learning-materials/README.md", "section": "Logstash", "language": "en", "created_at": "2025-07-19T19:22:02.015799"}}
{"text": "<details> <summary>What can you find under \"Discover\" in Kibana?</summary><br><b> The raw data as it is stored in the index. You can search and filter it. </b></details> <details> <summary>You see in Kibana, after clicking on Discover, \"561 hits\". What does it mean?</summary><br><b> Total number of documents matching the search results. If not query used then simply the total number of documents. </b></details> <details> <summary>What can you find under \"Visualize\"?</summary><br><b> \"Visualize\" is where you can create visual representations for your data (pie charts, graphs, ...) </b></details> <details> <summary>What visualization types are supported/included in Kibana?</summary><br><b> </b></details> <details> <summary>What visualization type would you use for statistical outliers</summary><br><b> </b></details> <details> <summary>Describe in detail how do you create a dashboard in Kibana</summary><br><b> </b></details>", "metadata": {"source_file": "learning-materials/README.md", "section": "Kibana", "language": "en", "created_at": "2025-07-19T19:22:02.015945"}}
{"text": "<details> <summary>What is Filebeat?</summary><br><b> Filebeat is used to monitor the logging directories inside of VMs or mounted as a sidecar if exporting logs from containers, and then forward these logs onward for further processing, usually to logstash. </b></details> <details> <summary>If one is using ELK, is it a must to also use filebeat? In what scenarios it's useful to use filebeat?</summary><br><b> Filebeat is a typical component of the ELK stack, since it was developed by Elastic to work with the other products (Logstash and Kibana). It's possible to send logs directly to logstash, though this often requires coding changes for the application. Particularly for legacy applications with little test coverage, it might be a better option to use filebeat, since you don't need to make any changes to the application code. </b></details> <details> <summary>What is a harvester?</summary><br><b> Read", "metadata": {"source_file": "learning-materials/README.md", "section": "Filebeat", "language": "en", "created_at": "2025-07-19T19:22:02.016166"}}
{"text": "[here](https://www.elastic.co/guide/en/beats/filebeat/current/how-filebeat-works.html#harvester) </b></details> <details> <summary>True or False? a single harvester harvest multiple files, according to the limits set in filebeat.yml</summary><br><b> False. One harvester harvests one file. </b></details> <details> <summary>What are filebeat modules?</summary><br><b> These are pre-configured modules for specific types of logging locations (eg, Traefik, Fargate, HAProxy) to make it easy to configure forwarding logs using filebeat. They have different configurations based on where you're collecting logs from. </b></details>", "metadata": {"source_file": "learning-materials/README.md", "section": "Filebeat", "language": "en", "created_at": "2025-07-19T19:22:02.016186"}}
{"text": "<details> <summary>How do you secure an Elastic Stack?</summary><br><b> You can generate certificates with the provided elastic utils and change configuration to enable security using certificates model. </b></details>", "metadata": {"source_file": "learning-materials/README.md", "section": "Elastic Stack", "language": "en", "created_at": "2025-07-19T19:22:02.016214"}}
{"text": "<details> <summary>Explain Distributed Computing (or Distributed System)</summary><br><b> According to Martin Kleppmann: \"Many processes running on many machines...only message-passing via an unreliable network with variable delays, and the system may suffer from partial failures, unreliable clocks, and process pauses.\" Another definition: \"Systems that are physically separated, but logically connected\" </b></details> <details> <summary>What can cause a system to fail?</summary><br><b> * Network * CPU * Memory * Disk </b></details> <details> <summary>Do you know what is \"CAP theorem\"? (aka as Brewer's theorem)</summary><br><b> According to the CAP theorem, it's not possible for a distributed data store to provide more than two of the following at the same time: * Availability: Every request receives a response (it doesn't has to be the most recent data) * Consistency: Every request receives a response with the latest/most recent data * Partition tolerance: Even if some the data is", "metadata": {"source_file": "learning-materials/README.md", "section": "Distributed", "language": "en", "created_at": "2025-07-19T19:22:02.016555"}}
{"text": "lost/dropped, the system keeps running </b></details> <details> <summary>What are the problems with the following design? How to improve it?<br> <img src=\"images/distributed/distributed_design_standby.png\" width=\"500x;\" height=\"350px;\"/> </summary><br><b> 1. The transition can take time. In other words, noticeable downtime. 2. Standby server is a waste of resources - if first application server is running then the standby does nothing </b></details> <details> <summary>What are the problems with the following design? How to improve it?<br> <img src=\"images/distributed/distributed_design_lb.png\" width=\"700x;\" height=\"350px;\"/> </summary><br><b> Issues: If load balancer dies , we lose the ability to communicate with the application. Ways to improve: * Add another load balancer * Use DNS A record for both load balancers * Use message queue </b></details> <details> <summary>What is \"Shared-Nothing\" architecture?</summary><br><b> It's an architecture in which data is and retrieved from a", "metadata": {"source_file": "learning-materials/README.md", "section": "Distributed", "language": "en", "created_at": "2025-07-19T19:22:02.016576"}}
{"text": "single, non-shared, source usually exclusively connected to one node as opposed to architectures where the request can get to one of many nodes and the data will be retrieved from one shared location (storage, memory, ...). </b></details> <details> <summary>Explain the Sidecar Pattern (Or sidecar proxy)</summary><br><b> </b></details>", "metadata": {"source_file": "learning-materials/README.md", "section": "Distributed", "language": "en", "created_at": "2025-07-19T19:22:02.016595"}}
{"text": "|Name|Topic|Objective & Instructions|Solution|Comments| |--------|--------|------|----|----| | Highly Available \"Hello World\" | [Exercise](topics/devops/ha_hello_world.md) | [Solution](topics/devops/solutions/ha_hello_world.md) <details> <summary>What happens when you type in a URL in an address bar in a browser?</summary><br><b> 1. The browser searches for the record of the domain name IP address in the DNS in the following order: * Browser cache * Operating system cache * The DNS server configured on the user's system (can be ISP DNS, public DNS, ...) 2. If it couldn't find a DNS record locally, a full DNS resolution is started. 3. It connects to the server using the TCP protocol 4. The browser sends an HTTP request to the server 5. The server sends an HTTP response back to the browser 6. The browser renders the response (e.g. HTML) 7. The browser then sends subsequent requests as needed to the server to get the embedded links, javascript, images in the HTML and then steps 3 to 5 are", "metadata": {"source_file": "learning-materials/README.md", "section": "Misc", "language": "en", "created_at": "2025-07-19T19:22:02.016828"}}
{"text": "repeated. TODO: add more details! </b></details>", "metadata": {"source_file": "learning-materials/README.md", "section": "Misc", "language": "en", "created_at": "2025-07-19T19:22:02.016875"}}
{"text": "<details> <summary>Explain what is an API</summary><br><b> I like this definition from [blog.christianposta.com](https://blog.christianposta.com/microservices/api-gateways-are-going-through-an-identity-crisis): \"An explicitly and purposefully defined interface designed to be invoked over a network that enables software developers to get programmatic access to data and functionality within an organization in a controlled and comfortable way.\" </b></details> <details> <summary>What is an API specification?</summary><br><b> From [swagger.io](https://swagger.io/resources/articles/difference-between-api-documentation-specification): \"An API specification provides a broad understanding of how an API behaves and how the API links with other APIs. It explains how the API functions and the results to expect when using the API\" </b></details> <details> <summary>True or False? API Definition is the same as API Specification</summary><br><b> False. From", "metadata": {"source_file": "learning-materials/README.md", "section": "API", "language": "en", "created_at": "2025-07-19T19:22:02.017515"}}
{"text": "[swagger.io](https://swagger.io/resources/articles/difference-between-api-documentation-specification): \"An API definition is similar to an API specification in that it provides an understanding of how an API is organized and how the API functions. But the API definition is aimed at machine consumption instead of human consumption of APIs.\" </b></details> <details> <summary>What is an API gateway?</summary><br><b> An API gateway is like the gatekeeper that controls how different parts talk to each other and how information is exchanged between them. The API gateway provides a single point of entry for all clients, and it can perform several tasks, including routing requests to the appropriate backend service, load balancing, security and authentication, rate limiting, caching, and monitoring. By using an API gateway, organizations can simplify the management of their APIs, ensure consistent security and governance, and improve the performance and scalability of their backend services.", "metadata": {"source_file": "learning-materials/README.md", "section": "API", "language": "en", "created_at": "2025-07-19T19:22:02.017537"}}
{"text": "They are also commonly used in microservices architectures, where there are many small, independent services that need to be accessed by different clients. </b></details> <details> <summary>What are the advantages of using/implementing an API gateway?</summary><br><b> Advantages: - Simplifies API management: Provides a single entry point for all requests, which simplifies the management and monitoring of multiple APIs. - Improves security: Able to implement security features like authentication, authorization, and encryption to protect the backend services from unauthorized access. - Enhances scalability: Can handle traffic spikes and distribute requests to backend services in a way that maximizes resource utilization and improves overall system performance. - Enables service composition: Can combine different backend services into a single API, providing more granular control over the services that clients can access. - Facilitates integration with external systems: Can be used to", "metadata": {"source_file": "learning-materials/README.md", "section": "API", "language": "en", "created_at": "2025-07-19T19:22:02.017556"}}
{"text": "expose internal services to external partners or customers, making it easier to integrate with external systems and enabling new business models. </b></details> <details> <summary>What is a Payload in API?</summary><br><b> </b></details> <details> <summary>What is Automation? How it's related or different from Orchestration?</summary><br><b> Automation is the act of automating tasks to reduce human intervention or interaction in regards to IT technology and systems.<br> While automation focuses on a task level, Orchestration is the process of automating processes and/or workflows which consists of multiple tasks that usually across multiple systems. </b></details> <details> <summary>Tell me about interesting bugs you've found and also fixed</summary><br><b> </b></details> <details> <summary>What is a Debugger and how it works?</summary><br><b> </b></details> <details> <summary>What services an application might have?</summary><br><b> * Authorization * Logging * Authentication *", "metadata": {"source_file": "learning-materials/README.md", "section": "API", "language": "en", "created_at": "2025-07-19T19:22:02.017575"}}
{"text": "Ordering * Front-end * Back-end ... </b></details> <details> <summary>What is Metadata?</summary><br><b> Data about data. Basically, it describes the type of information that an underlying data will hold. </b></details> <details> <summary>You can use one of the following formats: JSON, YAML, XML. Which one would you use? Why?</summary><br><b> I can't answer this for you :) </b></details> <details> <summary>What's KPI?</summary><br><b> </b></details> <details> <summary>What's OKR?</summary><br><b> </b></details> <details> <summary>What's DSL (Domain Specific Language)?</summary><br><b> Domain Specific Language (DSLs) are used to create a customised language that represents the domain such that domain experts can easily interpret it. </b></details> <details> <summary>What's the difference between KPI and OKR?</summary><br><b> </b></details>", "metadata": {"source_file": "learning-materials/README.md", "section": "API", "language": "en", "created_at": "2025-07-19T19:22:02.017593"}}
{"text": "<details> <summary>What is YAML?</summary><br><b> Data serialization language used by many technologies today like Kubernetes, Ansible, etc. </b></details> <details> <summary>True or False? Any valid JSON file is also a valid YAML file</summary><br><b> True. Because YAML is superset of JSON. </b></details> <details> <summary>What is the format of the following data? ``` { applications: [ { name: \"my_app\", language: \"python\", version: 20.17 } ] } ``` </summary><br><b> JSON </b></details> <details> <summary>What is the format of the following data? ``` applications: - app: \"my_app\" language: \"python\" version: 20.17 ``` </summary><br><b> YAML </b></details> <details> <summary>How to write a multi-line string with YAML? What use cases is it good for?</summary><br><b> ``` someMultiLineString: | look mama I can write a multi-line string I love YAML ``` It's good for use cases like writing a shell script where each line of the script is a different command. </b></details> <details>", "metadata": {"source_file": "learning-materials/README.md", "section": "YAML", "language": "en", "created_at": "2025-07-19T19:22:02.017877"}}
{"text": "<summary>What is the difference between <code>someMultiLineString: |</code> to <code>someMultiLineString: ></code>?</summary><br><b> using `>` will make the multi-line string to fold into a single line ``` someMultiLineString: > This is actually a single line do not let appearances fool you ``` </b></details> <details> <summary>What are placeholders in YAML?</summary><br><b> They allow you reference values instead of directly writing them and it is used like this: ``` username: {{ my.user_name }} ``` </b></details> <details> <summary>How can you define multiple YAML components in one file?</summary><br><b> Using this: `---` For Examples: ``` document_number: 1 --- document_number: 2 ``` </b></details>", "metadata": {"source_file": "learning-materials/README.md", "section": "YAML", "language": "en", "created_at": "2025-07-19T19:22:02.017903"}}
{"text": "<details> <summary>Explain what is a firmware</summary><br><b> [Wikipedia](https://en.wikipedia.org/wiki/Firmware): \"In computing, firmware is a specific class of computer software that provides the low-level control for a device's specific hardware. Firmware, such as the BIOS of a personal computer, may contain basic functions of a device, and may provide hardware abstraction services to higher-level software such as operating systems.\" </b></details>", "metadata": {"source_file": "learning-materials/README.md", "section": "Firmware", "language": "en", "created_at": "2025-07-19T19:22:02.017958"}}
{"text": "<details> <summary>When running a cassandra cluster, how often do you need to run nodetool repair in order to keep the cluster consistent? * Within the columnFamily GC-grace Once a week * Less than the compacted partition minimum bytes * Depended on the compaction strategy </summary><br><b> </b></details>", "metadata": {"source_file": "learning-materials/README.md", "section": "Cassandra", "language": "en", "created_at": "2025-07-19T19:22:02.018035"}}
{"text": "<details> <summary>What is HTTP?</summary><br><b> [Avinetworks](https://avinetworks.com/glossary/layer-7/): HTTP stands for Hypertext Transfer Protocol. HTTP uses TCP port 80 to enable internet communication. It is part of the Application Layer (L7) in OSI Model. </b></details> <details> <summary>Describe HTTP request lifecycle</summary><br><b> * Resolve host by request to DNS resolver * Client SYN * Server SYN+ACK * Client SYN * HTTP request * HTTP response </b></details> <details> <summary>True or False? HTTP is stateful</summary><br><b> False. It doesn't maintain state for incoming request. </b></details> <details> <summary>How HTTP request looks like?</summary><br><b> It consists of: * Request line - request type * Headers - content info like length, encoding, etc. * Body (not always included) </b></details> <details> <summary>What HTTP method types are there?</summary><br><b> * GET * POST * HEAD * PUT * DELETE * CONNECT * OPTIONS * TRACE </b></details> <details> <summary>What HTTP", "metadata": {"source_file": "learning-materials/README.md", "section": "HTTP", "language": "en", "created_at": "2025-07-19T19:22:02.018701"}}
{"text": "response codes are there?</summary><br><b> * 1xx - informational * 2xx - Success * 3xx - Redirect * 4xx - Error, client fault * 5xx - Error, server fault </b></details> <details> <summary>What is HTTPS?</summary><br><b> HTTPS is a secure version of the HTTP protocol used to transfer data between a web browser and a web server. It encrypts the communication using SSL/TLS encryption to ensure that the data is private and secure. Learn more: https://www.cloudflare.com/learning/ssl/why-is-http-not-secure/ </b></details> <details> <summary>Explain HTTP Cookies</summary><br><b> HTTP is stateless. To share state, we can use Cookies. TODO: explain what is actually a Cookie </b></details> <details> <summary>What is HTTP Pipelining?</summary><br><b> </b></details> <details> <summary>You get \"504 Gateway Timeout\" error from an HTTP server. What does it mean?</summary><br><b> The server didn't receive a response from another server it communicates with in a timely manner. </b></details> <details>", "metadata": {"source_file": "learning-materials/README.md", "section": "HTTP", "language": "en", "created_at": "2025-07-19T19:22:02.018759"}}
{"text": "<summary>What is a proxy?</summary><br><b> A proxy is a server that acts as a middleman between a client device and a destination server. It can help improve privacy, security, and performance by hiding the client's IP address, filtering content, and caching frequently accessed data. - Proxies can be used for load balancing, distributing traffic across multiple servers to help prevent server overload and improve website or application performance. They can also be used for data analysis, as they can log requests and traffic, providing useful insights into user behavior and preferences. </b></details> <details> <summary>What is a reverse proxy?</summary><br><b> A reverse proxy is a type of proxy server that sits between a client and a server, but it is used to manage traffic going in the opposite direction of a traditional forward proxy. In a forward proxy, the client sends requests to the proxy server, which then forwards them to the destination server. However, in a reverse proxy, the", "metadata": {"source_file": "learning-materials/README.md", "section": "HTTP", "language": "en", "created_at": "2025-07-19T19:22:02.018783"}}
{"text": "client sends requests to the destination server, but the requests are intercepted by the reverse proxy before they reach the server. - They're commonly used to improve web server performance, provide high availability and fault tolerance, and enhance security by preventing direct access to the back-end server. They are often used in large-scale web applications and high-traffic websites to manage and distribute requests to multiple servers, resulting in improved scalability and reliability. </b></details> <details> <summary>When you publish a project, you usually publish it with a license. What types of licenses are you familiar with and which one do you prefer to use?</summary><br><b> </b></details> <details> <summary>Explain what is \"X-Forwarded-For\"</summary><br><b> [Wikipedia](https://en.wikipedia.org/wiki/X-Forwarded-For): \"The X-Forwarded-For (XFF) HTTP header field is a common method for identifying the originating IP address of a client connecting to a web server through an", "metadata": {"source_file": "learning-materials/README.md", "section": "HTTP", "language": "en", "created_at": "2025-07-19T19:22:02.018802"}}
{"text": "HTTP proxy or load balancer.\" </b></details>", "metadata": {"source_file": "learning-materials/README.md", "section": "HTTP", "language": "en", "created_at": "2025-07-19T19:22:02.018821"}}
{"text": "<details> <summary>What is a load balancer?</summary><br><b> A load balancer accepts (or denies) incoming network traffic from a client, and based on some criteria (application related, network, etc.) it distributes those communications out to servers (at least one). </b></details> <details> <summary>Why to used a load balancer?</summary><br><b> * Scalability - using a load balancer, you can possibly add more servers in the backend to handle more requests/traffic from the clients, as opposed to using one server. * Redundancy - if one server in the backend dies, the load balancer will keep forwarding the traffic/requests to the second server so users won't even notice one of the servers in the backend is down. </b></details> <details> <summary>What load balancer techniques/algorithms are you familiar with?</summary><br><b> * Round Robin * Weighted Round Robin * Least Connection * Weighted Least Connection * Resource Based * Fixed Weighting * Weighted Response Time * Source IP Hash * URL", "metadata": {"source_file": "learning-materials/README.md", "section": "Load Balancers", "language": "en", "created_at": "2025-07-19T19:22:02.019287"}}
{"text": "Hash </b></details> <details> <summary>What are the drawbacks of round robin algorithm in load balancing?</summary><br><b> * A simple round robin algorithm knows nothing about the load and the spec of each server it forwards the requests to. It is possible, that multiple heavy workloads requests will get to the same server while other servers will got only lightweight requests which will result in one server doing most of the work, maybe even crashing at some point because it unable to handle all the heavy workloads requests by its own. * Each request from the client creates a whole new session. This might be a problem for certain scenarios where you would like to perform multiple operations where the server has to know about the result of operation so basically, being sort of aware of the history it has with the client. In round robin, first request might hit server X, while second request might hit server Y and ask to continue processing the data that was processed on server X", "metadata": {"source_file": "learning-materials/README.md", "section": "Load Balancers", "language": "en", "created_at": "2025-07-19T19:22:02.019309"}}
{"text": "already. </b></details> <details> <summary>What is an Application Load Balancer?</summary><br><b> </b></details> <details> <summary>In which scenarios would you use ALB?</summary><br><b> </b></details> <details> <summary>At what layers a load balancer can operate?</summary><br><b> L4 and L7 </b></details> <details> <summary>Can you perform load balancing without using a dedicated load balancer instance?</summary><br><b> Yes, you can use DNS for performing load balancing. </b></details> <details> <summary>What is DNS load balancing? What its advantages? When would you use it?</summary><br><b> </b></details>", "metadata": {"source_file": "learning-materials/README.md", "section": "Load Balancers", "language": "en", "created_at": "2025-07-19T19:22:02.019328"}}
{"text": "<details> <summary>What are sticky sessions? What are their pros and cons?</summary><br><b> Recommended read: * [Red Hat Article](https://access.redhat.com/solutions/900933) Cons: * Can cause uneven load on instance (since requests routed to the same instances) Pros: * Ensures in-proc sessions are not lost when a new request is created </b></details> <details> <summary>Name one use case for using sticky sessions</summary><br><b> You would like to make sure the user doesn't lose the current session data. </b></details> <details> <summary>What sticky sessions use for enabling the \"stickiness\"?</summary><br><b> Cookies. There are application based cookies and duration based cookies. </b></details> <details> <summary>Explain application-based cookies</summary><br><b> * Generated by the application and/or the load balancer * Usually allows to include custom data </b></details> <details> <summary>Explain duration-based cookies</summary><br><b> * Generated by the load balancer * Session is", "metadata": {"source_file": "learning-materials/README.md", "section": "Load Balancers - Sticky Sessions", "language": "en", "created_at": "2025-07-19T19:22:02.019567"}}
{"text": "not sticky anymore once the duration elapsed </b></details>", "metadata": {"source_file": "learning-materials/README.md", "section": "Load Balancers - Sticky Sessions", "language": "en", "created_at": "2025-07-19T19:22:02.019597"}}
{"text": "<details> <summary>Explain each of the following load balancing techniques * Round Robin * Weighted Round Robin * Least Connection * Weighted Least Connection * Resource Based * Fixed Weighting * Weighted Response Time * Source IP Hash * URL Hash </summary><br><b> </b></details> <details> <summary>Explain use case for connection draining?</summary><br><b> To ensure that a Classic Load Balancer stops sending requests to instances that are de-registering or unhealthy, while keeping the existing connections open, use connection draining. This enables the load balancer to complete in-flight requests made to instances that are de-registering or unhealthy. The maximum timeout value can be set between 1 and 3,600 seconds on both GCP and AWS. </b></details>", "metadata": {"source_file": "learning-materials/README.md", "section": "Load Balancers - Load Balancing Algorithms", "language": "en", "created_at": "2025-07-19T19:22:02.019739"}}
{"text": "<details> <summary>Are you familiar with \"Creative Commons\"? What do you know about it?</summary><br><b> The Creative Commons license is a set of copyright licenses that allow creators to share their work with the public while retaining some control over how it can be used. The license was developed as a response to the restrictive standards of traditional copyright laws, which limited access of creative works. Its creators to choose the terms under which their works can be shared, distributed, and used by others. They're six main types of Creative Commons licenses, each with different levels of restrictions and permissions, the six licenses are: * Attribution (CC BY): Allows others to distribute, remix, and build upon the work, even commercially, as long as they credit the original creator. * Attribution-ShareAlike (CC BY-SA): Allows others to remix and build upon the work, even commercially, as long as they credit the original creator and release any new creations under the same", "metadata": {"source_file": "learning-materials/README.md", "section": "Licenses", "language": "en", "created_at": "2025-07-19T19:22:02.020195"}}
{"text": "license. * Attribution-NoDerivs (CC BY-ND): Allows others to distribute the work, even commercially, but they cannot remix or change it in any way and must credit the original creator. * Attribution-NonCommercial (CC BY-NC): Allows others to remix and build upon the work, but they cannot use it commercially and must credit the original creator. * Attribution-NonCommercial-ShareAlike (CC BY-NC-SA): Allows others to remix and build upon the work, but they cannot use it commercially, must credit the original creator, and must release any new creations under the same license. * Attribution-NonCommercial-NoDerivs (CC BY-NC-ND): Allows others to download and share the work, but they cannot use it commercially, remix or change it in any way, and must credit the original creator. Simply stated, the Creative Commons licenses are a way for creators to share their work with the public while retaining some control over how it can be used. The licenses promote creativity, innovation, and", "metadata": {"source_file": "learning-materials/README.md", "section": "Licenses", "language": "en", "created_at": "2025-07-19T19:22:02.020217"}}
{"text": "collaboration, while also respecting the rights of creators while still encouraging the responsible use of creative works. More information: https://creativecommons.org/licenses/ </b></details> <details> <summary>Explain the differences between copyleft and permissive licenses</summary><br><b> In Copyleft, any derivative work must use the same licensing while in permissive licensing there are no such condition. GPL-3 is an example of copyleft license while BSD is an example of permissive license. </b></details>", "metadata": {"source_file": "learning-materials/README.md", "section": "Licenses", "language": "en", "created_at": "2025-07-19T19:22:02.020236"}}
{"text": "<details> <summary>How a search engine works?</summary><br><b> </b></details> <details> <summary>How auto completion works?</summary><br><b> </b></details> <details> <summary>What is faster than RAM?</summary><br><b> CPU cache. [Source](https://www.enterprisestorageforum.com/hardware/cache-memory/) </b></details> <details> <summary>What is a memory leak?</summary><br><b> A memory leak is a programming error that occurs when a program fails to release memory that is no longer needed, causing the program to consume increasing amounts of memory over time. The leaks can lead to a variety of problems, including system crashes, performance degradation, and instability. Usually occurring after failed maintenance on older systems and compatibility with new components over time. </b></details> <details> <summary>What is your favorite protocol?</summary><br><b> SSH HTTP DHCP DNS ... </b></details> <details> <summary>What is Cache API?</summary><br><b> </b></details> <details> <summary>What is", "metadata": {"source_file": "learning-materials/README.md", "section": "Random", "language": "en", "created_at": "2025-07-19T19:22:02.020376"}}
{"text": "the C10K problem? Is it relevant today?</summary><br><b> https://idiallo.com/blog/c10k-2016 </b></details>", "metadata": {"source_file": "learning-materials/README.md", "section": "Random", "language": "en", "created_at": "2025-07-19T19:22:02.020396"}}
{"text": "<details> <summary>What types of storage are there?</summary><br><b> * File * Block * Object </b></details> <details> <summary>Explain Object Storage</summary><br><b> - Data is divided to self-contained objects - Objects can contain metadata </b></details> <details> <summary>What are the pros and cons of object storage?</summary><br><b> Pros: - Usually with object storage, you pay for what you use as opposed to other storage types where you pay for the storage space you allocate - Scalable storage: Object storage mostly based on a model where what you use, is what you get and you can add storage as need Cons: - Usually performs slower than other types of storage - No granular modification: to change an object, you have re-create it </b></details> <details> <summary>What are some use cases for using object storage?</summary><br><b> </b></details> <details> <summary>Explain File Storage</summary><br><b> - File Storage used for storing data in files, in a hierarchical structure - Some of", "metadata": {"source_file": "learning-materials/README.md", "section": "Storage", "language": "en", "created_at": "2025-07-19T19:22:02.020788"}}
{"text": "the devices for file storage: hard drive, flash drive, cloud-based file storage - Files usually organized in directories </b></details> <details> <summary>What are the pros and cons of File Storage?</summary><br><b> Pros: - Users have full control of their own files and can run variety of operations on the files: delete, read, write and move. - Security mechanism allows for users to have a better control at things such as file locking </b></details> <details> <summary>What are some examples of file storage?</summary><br><b> Local filesystem Dropbox Google Drive </b></details> <details> <summary>What types of storage devices are there?</summary><br><b> </b></details> <details> <summary>Explain IOPS</summary><br><b> </b></details> <details> <summary>Explain storage throughput</summary><br><b> </b></details> <details> <summary>What is a filesystem?</summary><br><b> A file system is a way for computers and other electronic devices to organize and store data files. It provides a structure", "metadata": {"source_file": "learning-materials/README.md", "section": "Storage", "language": "en", "created_at": "2025-07-19T19:22:02.020815"}}
{"text": "that helps to organize data into files and directories, making it easier to find and manage information. A file system is crucial for providing a way to store and manage data in an organized manner. Commonly used filed systems: Windows: * NTFS * exFAT Mac OS: * HFS+ *APFS </b></details> <details> <summary>Explain Dark Data</summary><br><b> </b></details> <details> <summary>Explain MBR</summary><br><b> </b></details> <a name=\"questions-you-ask\"></a>", "metadata": {"source_file": "learning-materials/README.md", "section": "Storage", "language": "en", "created_at": "2025-07-19T19:22:02.020866"}}
{"text": "A list of questions you as a candidate can ask the interviewer during or after the interview. These are only a suggestion, use them carefully. Not every interviewer will be able to answer these (or happy to) which should be perhaps a red flag warning for your regarding working in such place but that's really up to you. <details> <summary>What do you like about working here?</summary><br><b> </b></details> <details> <summary>How does the company promote personal growth?</summary><br><b> </b></details> <details> <summary>What is the current level of technical debt you are dealing with?</summary><br><b> Be careful when asking this question - all companies, regardless of size, have some level of tech debt. Phrase the question in the light that all companies have the deal with this, but you want to see the current pain points they are dealing with <br> This is a great way to figure how managers deal with unplanned work, and how good they are at setting expectations with projects.", "metadata": {"source_file": "learning-materials/README.md", "section": "Questions you CAN ask", "language": "en", "created_at": "2025-07-19T19:22:02.021322"}}
{"text": "</b></details> <details> <summary>Why I should NOT join you? (or 'what you don't like about working here?')</summary><br><b> </b></details> <details> <summary>What was your favorite project you've worked on?</summary><br><b> This can give you insights in some of the cool projects a company is working on, and if you would enjoy working on projects like these. This is also a good way to see if the managers are allowing employees to learn and grow with projects outside of the normal work you'd do. </b></details> <details> <summary>If you could change one thing about your day to day, what would it be?</summary><br><b> Similar to the tech debt question, this helps you identify any pain points with the company. Additionally, it can be a great way to show how you'd be an asset to the team.<br> For Example, if they mention they have problem X, and you've solved that in the past, you can show how you'd be able to mitigate that problem. </b></details> <details> <summary>Let's say that we agree", "metadata": {"source_file": "learning-materials/README.md", "section": "Questions you CAN ask", "language": "en", "created_at": "2025-07-19T19:22:02.021353"}}
{"text": "and you hire me to this position, after X months, what do you expect that I have achieved?</summary><br><b> Not only this will tell you what is expected from you, it will also provide big hint on the type of work you are going to do in the first months of your job. </b></details>", "metadata": {"source_file": "learning-materials/README.md", "section": "Questions you CAN ask", "language": "en", "created_at": "2025-07-19T19:22:02.021374"}}
{"text": "<details> <summary>Explain white-box testing</summary><br><b> </b></details> <details> <summary>Explain black-box testing</summary><br><b> </b></details> <details> <summary>What are unit tests?</summary><br><b> Unit test are a software testing technique that involves systimatically breaking down a system and testing each individual part of the assembly. These tests are automated and can be run repeatedly to allow developers to catch edge case scenarios or bugs quickly while developing. The main objective of unit tests are to verify each function is producing proper outputs given a set of inputs. </b></details> <details> <summary>What types of tests would you run to test a web application?</summary><br><b> </b></details> <details> <summary>Explain test harness?</summary><br><b> </b></details> <details> <summary>What is A/B testing?</summary><br><b> </b></details> <details> <summary>What is network simulation and how do you perform it?</summary><br><b> </b></details> <details>", "metadata": {"source_file": "learning-materials/README.md", "section": "Testing", "language": "en", "created_at": "2025-07-19T19:22:02.021528"}}
{"text": "<summary>What types of performances tests are you familiar with?</summary><br><b> </b></details> <details> <summary>Explain the following types of tests: * Load Testing * Stress Testing * Capacity Testing * Volume Testing * Endurance Testing </summary><br><b> </b></details>", "metadata": {"source_file": "learning-materials/README.md", "section": "Testing", "language": "en", "created_at": "2025-07-19T19:22:02.021547"}}
{"text": "Given a text file, perform the following exercises", "metadata": {"source_file": "learning-materials/README.md", "section": "Regex", "language": "en", "created_at": "2025-07-19T19:22:02.021563"}}
{"text": "<details> <summary>Extract all the numbers</summary><br><b> - \"\\d+\" </b></details> <details> <summary>Extract the first word of each line</summary><br><b> - \"^\\w+\" Bonus: extract the last word of each line - \"\\w+(?=\\W*$)\" (in most cases, depends on line formatting) </b></details> <details> <summary>Extract all the IP addresses</summary><br><b> - \"\\b(?:\\d{1,3}\\ .){3}\\d{1,3}\\b\" IPV4:(This format looks for 1 to 3 digit sequence 3 times) </b></details> <details> <summary>Extract dates in the format of yyyy-mm-dd or yyyy-dd-mm</summary><br><b> </b></details> <details> <summary>Extract email addresses</summary><br><b> - \"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\ .[A-Za-z]{2,}\\b\" </b></details>", "metadata": {"source_file": "learning-materials/README.md", "section": "Extract", "language": "en", "created_at": "2025-07-19T19:22:02.021631"}}
{"text": "<details> <summary>Replace tabs with four spaces</summary><br><b> </b></details> <details> <summary>Replace 'red' with 'green'</summary><br><b> </b></details>", "metadata": {"source_file": "learning-materials/README.md", "section": "Replace", "language": "en", "created_at": "2025-07-19T19:22:02.021653"}}
{"text": "<details> <summary>Explain what a \"single point of failure\" is. </summary><br><b> A \"single point of failure\", in a system or organization, if it were to fail would cause the entire system to fail or significantly disrupt it's operation. In other words, it is a vulnerability where there is no backup in place to compensate for the failure. </b></details> <details> <summary>What is CDN?</summary><br><b> CDN (Content Delivery Network) responsible for distributing content geographically. Part of it, is what is known as edge locations, aka cache proxies, that allows users to get their content quickly due to cache features and geographical distribution. </b></details> <details> <summary>Explain Multi-CDN</summary><br><b> In single CDN, the whole content is originated from content delivery network.<br> In multi-CDN, content is distributed across multiple different CDNs, each might be on a completely different provider/cloud. </b></details> <details> <summary>What are the benefits of Multi-CDN", "metadata": {"source_file": "learning-materials/README.md", "section": "System Design", "language": "en", "created_at": "2025-07-19T19:22:02.022354"}}
{"text": "over a single CDN?</summary><br><b> * Resiliency: Relying on one CDN means no redundancy. With multiple CDNs you don't need to worry about your CDN being down * Flexibility in Costs: Using one CDN enforces you to specific rates of that CDN. With multiple CDNs you can take into consideration using less expensive CDNs to deliver the content. * Performance: With Multi-CDN there is bigger potential in choosing better locations which more close to the client asking the content * Scale: With multiple CDNs, you can scale services to support more extreme conditions </b></details> <details> <summary>Explain \"3-Tier Architecture\" (including pros and cons)</summary><br><b> A \"3-Tier Architecture\" is a pattern used in software development for designing and structuring applications. It divides the application into 3 interconnected layers: Presentation, Business logic and Data storage. PROS: * Scalability * Security * Reusability CONS: * Complexity * Performance overhead * Cost and development time", "metadata": {"source_file": "learning-materials/README.md", "section": "System Design", "language": "en", "created_at": "2025-07-19T19:22:02.022382"}}
{"text": "</b></details> <details> <summary>Explain Mono-repo vs. Multi-repo.What are the cons and pros of each approach?</summary><br><b> In a Mono-repo, all the code for an organization is stored in a single,centralized repository. PROS (Mono-repo): * Unified tooling * Code Sharing CONS (Mono-repo): * Increased complexity * Slower cloning In a Multi-repo setup, each component is stored in it's own separate repository. Each repository has it's own version control history. PROS (Multi-repo): * Simpler to manage * Different teams and developers can work on different parts of the project independently, making parallel development easier. CONS (Multi-repo): * Code duplication * Integration challenges </b></details> <details> <summary>What are the drawbacks of monolithic architecture?</summary><br><b> * Not suitable for frequent code changes and the ability to deploy new features * Not designed for today's infrastructure (like public clouds) * Scaling a team to work monolithic architecture is more", "metadata": {"source_file": "learning-materials/README.md", "section": "System Design", "language": "en", "created_at": "2025-07-19T19:22:02.022452"}}
{"text": "challenging * If a single component in this architecture fails, then the entire application fails. </b></details> <details> <summary>What are the advantages of microservices architecture over a monolithic architecture?</summary><br><b> * Each of the services individually fail without escalating into an application-wide outage. * Each service can be developed and maintained by a separate team and this team can choose its own tools and coding language </b></details> <details> <summary>What's a service mesh?</summary><br><b> It is a layer that facilitates communication management and control between microservices in a containerized application. It handles tasks such as load balancing, encryption, and monitoring. </b></details> <details> <summary>Explain \"Loose Coupling\"</summary><br><b> In \"Loose Coupling\", components of a system communicate with each other with a little understanding of each other's internal workings. This improves scalability and ease of modification in complex systems.", "metadata": {"source_file": "learning-materials/README.md", "section": "System Design", "language": "en", "created_at": "2025-07-19T19:22:02.022475"}}
{"text": "</b></details> <details> <summary>What is a message queue? When is it used?</summary><br><b> It is a communication mechanism used in distributed systems to enable asynchronous communication between different components. It is generally used when the systems use a microservices approach. </b></details>", "metadata": {"source_file": "learning-materials/README.md", "section": "System Design", "language": "en", "created_at": "2025-07-19T19:22:02.022493"}}
{"text": "<details> <summary>Explain Scalability</summary><br><b> The ability easily grow in size and capacity based on demand and usage. </b></details> <details> <summary>Explain Elasticity</summary><br><b> The ability to grow but also to reduce based on what is required </b></details> <details> <summary>Explain Disaster Recovery</summary><br><b> Disaster recovery is the process of restoring critical business systems and data after a disruptive event. The goal is to minimize the impact and resume normal business activities quickly. This involves creating a plan, testing it, backing up critical data, and storing it in safe locations. In case of a disaster, the plan is then executed, backups are restored, and systems are hopefully brought back online. The recovery process may take hours or days depending on the damages of infrastructure. This makes business planning important, as a well-designed and tested disaster recovery plan can minimize the impact of a disaster and keep operations going.", "metadata": {"source_file": "learning-materials/README.md", "section": "Scalability", "language": "en", "created_at": "2025-07-19T19:22:02.023207"}}
{"text": "</b></details> <details> <summary>Explain Fault Tolerance and High Availability</summary><br><b> Fault Tolerance - The ability to self-heal and return to normal capacity. Also the ability to withstand a failure and remain functional. High Availability - Being able to access a resource (in some use cases, using different platforms) </b></details> <details> <summary>What is the difference between high availability and Disaster Recovery?</summary><br><b> [wintellect.com](https://www.wintellect.com/high-availability-vs-disaster-recovery): \"High availability, simply put, is eliminating single points of failure and disaster recovery is the process of getting a system back to an operational state when a system is rendered inoperative. In essence, disaster recovery picks up when high availability fails, so HA first.\" </b></details> <details> <summary>Explain Vertical Scaling</summary><br><b> Vertical Scaling is the process of adding resources to increase power of existing servers. For example,", "metadata": {"source_file": "learning-materials/README.md", "section": "Scalability", "language": "en", "created_at": "2025-07-19T19:22:02.023236"}}
{"text": "adding more CPUs, adding more RAM, etc. </b></details> <details> <summary>What are the disadvantages of Vertical Scaling?</summary><br><b> With vertical scaling alone, the component still remains a single point of failure. In addition, it has hardware limit where if you don't have more resources, you might not be able to scale vertically. </b></details> <details> <summary>Which type of cloud services usually support vertical scaling?</summary><br><b> Databases, cache. It's common mostly for non-distributed systems. </b></details> <details> <summary>Explain Horizontal Scaling</summary><br><b> Horizontal Scaling is the process of adding more resources that will be able handle requests as one unit </b></details> <details> <summary>What is the disadvantage of Horizontal Scaling? What is often required in order to perform Horizontal Scaling?</summary><br><b> A load balancer. You can add more resources, but if you would like them to be part of the process, you have to serve them the", "metadata": {"source_file": "learning-materials/README.md", "section": "Scalability", "language": "en", "created_at": "2025-07-19T19:22:02.023256"}}
{"text": "requests/responses. Also, data inconsistency is a concern with horizontal scaling. </b></details> <details> <summary>Explain in which use cases will you use vertical scaling and in which use cases you will use horizontal scaling</summary><br><b> </b></details> <details> <summary>Explain Resiliency and what ways are there to make a system more resilient</summary><br><b> </b></details> <details> <summary>Explain \"Consistent Hashing\"</summary><br><b> </b></details> <details> <summary>How would you update each of the services in the following drawing without having app (foo.com) downtime?<br> <img src=\"images/design/cdn-no-downtime.png\" width=\"300x;\" height=\"400px;\"/> </summary><br><b> </b></details> <details> <summary>What is the problem with the following architecture and how would you fix it?<br> <img src=\"images/design/producers_consumers_issue.png\" width=\"400x;\" height=\"300px;\"/> </summary><br><b> The load on the producers or consumers may be high which will then cause them to hang or", "metadata": {"source_file": "learning-materials/README.md", "section": "Scalability", "language": "en", "created_at": "2025-07-19T19:22:02.023275"}}
{"text": "crash.<br> Instead of working in \"push mode\", the consumers can pull tasks only when they are ready to handle them. It can be fixed by using a streaming platform like Kafka, Kinesis, etc. This platform will make sure to handle the high load/traffic and pass tasks/messages to consumers only when the ready to get them. <img src=\"images/design/producers_consumers_fix.png\" width=\"300x;\" height=\"200px;\"/> </b></details> <details> <summary>Users report that there is huge spike in process time when adding little bit more data to process as an input. What might be the problem?<br> <img src=\"images/design/input-process-output.png\" width=\"300x;\" height=\"200px;\"/> </summary><br><b> </b></details> <details> <summary>How would you scale the architecture from the previous question to hundreds of users?</summary><br><b> </b></details>", "metadata": {"source_file": "learning-materials/README.md", "section": "Scalability", "language": "en", "created_at": "2025-07-19T19:22:02.023293"}}
{"text": "<details> <summary>What is \"cache\"? In which cases would you use it?</summary><br><b> </b></details> <details> <summary>What is \"distributed cache\"?</summary><br><b> </b></details> <details> <summary>What is a \"cache replacement policy\"?</summary><br><b> Take a look [here](https://en.wikipedia.org/wiki/Cache_replacement_policies) </b></details> <details> <summary>Which cache replacement policies are you familiar with?</summary><br><b> You can find a list [here](https://en.wikipedia.org/wiki/Cache_replacement_policies) </b></details> <details> <summary>Explain the following cache policies: * FIFO * LIFO * LRU</summary><br><b> Read about it [here](https://en.wikipedia.org/wiki/Cache_replacement_policies) </b></details> <details> <summary>Why not writing everything to cache instead of a database/datastore?</summary><br><b> Caching and databases serve different purposes and are optimized for different use cases. Caching is used to speed up read operations by storing frequently accessed", "metadata": {"source_file": "learning-materials/README.md", "section": "Cache", "language": "en", "created_at": "2025-07-19T19:22:02.023485"}}
{"text": "data in memory or on a fast storage medium. By keeping data close to the application, caching reduces the latency and overhead of accessing data from a slower, more distant storage system such as a database or disk. On the other hand, databases are optimized for storing and managing persistent data. Databases are designed to handle concurrent read and write operations, enforce consistency and integrity constraints, and provide features such as indexing and querying. </b></details>", "metadata": {"source_file": "learning-materials/README.md", "section": "Cache", "language": "en", "created_at": "2025-07-19T19:22:02.023594"}}
{"text": "<details> <summary>How you prepare for a migration? (or plan a migration)</summary><br><b> You can mention: roll-back & roll-forward cut over dress rehearsals DNS redirection </b></details> <details> <summary>Explain \"Branch by Abstraction\" technique</summary><br><b> </b></details>", "metadata": {"source_file": "learning-materials/README.md", "section": "Migrations", "language": "en", "created_at": "2025-07-19T19:22:02.023660"}}
{"text": "<details> <summary>Can you design a video streaming website?</summary><br><b> </b></details> <details> <summary>Can you design a photo upload website?</summary><br><b> </b></details> <details> <summary>How would you build a URL shortener?</summary><br><b> </b></details>", "metadata": {"source_file": "learning-materials/README.md", "section": "Design a system", "language": "en", "created_at": "2025-07-19T19:22:02.023740"}}
{"text": "Additional exercises can be found in [system-design-notebook repository](https://github.com/bregman-arie/system-design-notebook). <p align=\"center\"><a href=\"https://github.com/bregman-arie/system-design-notebook\"><img src=\"images/system_design_notebook.png\"/></a></p>", "metadata": {"source_file": "learning-materials/README.md", "section": "More System Design Questions", "language": "en", "created_at": "2025-07-19T19:22:02.023770"}}
{"text": "<details> <summary>What is a CPU?</summary><br><b> A central processing unit (CPU) performs basic arithmetic, logic, controlling, and input/output (I/O) operations specified by the instructions in the program. This contrasts with external components such as main memory and I/O circuitry, and specialized processors such as graphics processing units (GPUs). </b></details> <details> <summary>What is RAM?</summary><br><b> RAM (Random Access Memory) is the hardware in a computing device where the operating system (OS), application programs and data in current use are kept so they can be quickly reached by the device's processor. RAM is the main memory in a computer. It is much faster to read from and write to than other kinds of storage, such as a hard disk drive (HDD), solid-state drive (SSD) or optical drive. </b></details> <details> <summary>What is a GPU?</summary><br><b> A GPU, or Graphics Processing Unit, is a specialized electronic circuit designed to expedite image and video", "metadata": {"source_file": "learning-materials/README.md", "section": "Hardware", "language": "en", "created_at": "2025-07-19T19:22:02.025820"}}
{"text": "processing for display on a computer screen. </b></details> <details> <summary>What is an embedded system?</summary><br><b> An embedded system is a computer system - a combination of a computer processor, computer memory, and input/output peripheral devices—that has a dedicated function within a larger mechanical or electronic system. It is embedded as part of a complete device often including electrical or electronic hardware and mechanical parts. </b></details> <details> <summary>Can you give an example of an embedded system?</summary><br><b> A common example of an embedded system is a microwave oven's digital control panel, which is managed by a microcontroller. When committed to a certain goal, Raspberry Pi can serve as an embedded system. </b></details> <details> <summary>What types of storage are there?</summary><br><b> There are several types of storage, including hard disk drives (HDDs), solid-state drives (SSDs), and optical drives (CD/DVD/Blu-ray). Other types of storage", "metadata": {"source_file": "learning-materials/README.md", "section": "Hardware", "language": "en", "created_at": "2025-07-19T19:22:02.025854"}}
{"text": "include USB flash drives, memory cards, and network-attached storage (NAS). </b></details> <details> <summary>What are some considerations DevOps teams should keep in mind when selecting hardware for their job?</summary><br> Choosing the right DevOps hardware is essential for ensuring streamlined CI/CD pipelines, timely feedback loops, and consistent service availability. Here's a distilled guide on what DevOps teams should consider: 1. **Understanding Workloads**: - **CPU**: Consider the need for multi-core or high-frequency CPUs based on your tasks. - **RAM**: Enough memory is vital for activities like large-scale coding or intensive automation. - **Storage**: Evaluate storage speed and capacity. SSDs might be preferable for swift operations. 2. **Expandability**: - **Horizontal Growth**: Check if you can boost capacity by adding more devices. - **Vertical Growth**: Determine if upgrades (like RAM, CPU) to individual machines are feasible. 3. **Connectivity Considerations**: - **Data", "metadata": {"source_file": "learning-materials/README.md", "section": "Hardware", "language": "en", "created_at": "2025-07-19T19:22:02.025879"}}
{"text": "Transfer**: Ensure high-speed network connections for activities like code retrieval and data transfers. - **Speed**: Aim for low-latency networks, particularly important for distributed tasks. - **Backup Routes**: Think about having backup network routes to avoid downtimes. 4. **Consistent Uptime**: - Plan for hardware backups like RAID configurations, backup power sources, or alternate network connections to ensure continuous service. 5. **System Compatibility**: - Make sure your hardware aligns with your software, operating system, and intended platforms. 6. **Power Efficiency**: - Hardware that uses energy efficiently can reduce costs in long-term, especially in large setups. 7. **Safety Measures**: - Explore hardware-level security features, such as TPM, to enhance protection. 8. **Overseeing & Control**: - Tools like ILOM can be beneficial for remote handling. - Make sure the hardware can be seamlessly monitored for health and performance. 9. **Budgeting**: - Consider both", "metadata": {"source_file": "learning-materials/README.md", "section": "Hardware", "language": "en", "created_at": "2025-07-19T19:22:02.025899"}}
{"text": "initial expenses and long-term costs when budgeting. 10. **Support & Community**: - Choose hardware from reputable vendors known for reliable support. - Check for available drivers, updates, and community discussions around the hardware. 11. **Planning Ahead**: - Opt for hardware that can cater to both present and upcoming requirements. 12. **Operational Environment**: - **Temperature Control**: Ensure cooling systems to manage heat from high-performance units. - **Space Management**: Assess hardware size considering available rack space. - **Reliable Power**: Factor in consistent and backup power sources. 13. **Cloud Coordination**: - If you're leaning towards a hybrid cloud setup, focus on how local hardware will mesh with cloud resources. 14. **Life Span of Hardware**: - Be aware of the hardware's expected duration and when you might need replacements or upgrades. 15. **Optimized for Virtualization**: - If utilizing virtual machines or containers, ensure the hardware is compatible", "metadata": {"source_file": "learning-materials/README.md", "section": "Hardware", "language": "en", "created_at": "2025-07-19T19:22:02.025918"}}
{"text": "and optimized for such workloads. 16. **Adaptability**: - Modular hardware allows individual component replacements, offering more flexibility. 17. **Avoiding Single Vendor Dependency**: - Try to prevent reliance on a single vendor unless there are clear advantages. 18. **Eco-Friendly Choices**: - Prioritize sustainably produced hardware that's energy-efficient and environmentally responsible. In essence, DevOps teams should choose hardware that is compatible with their tasks, versatile, gives good performance, and stays within their budget. Furthermore, long-term considerations such as maintenance, potential upgrades, and compatibility with impending technological shifts must be prioritized. </details> <details> <summary>What is the role of hardware in disaster recovery planning and implementation?</summary><br> Hardware is critical in disaster recovery (DR) solutions. While the broader scope of DR includes things like standard procedures, norms, and human roles, it's the hardware", "metadata": {"source_file": "learning-materials/README.md", "section": "Hardware", "language": "en", "created_at": "2025-07-19T19:22:02.025975"}}
{"text": "that keeps business processes running smoothly. Here's an outline of how hardware works with DR: 1. **Storing Data and Ensuring Its Duplication**: - **Backup Equipment**: Devices like tape storage, backup servers, and external HDDs keep essential data stored safely at a different location. - **Disk Arrays**: Systems such as RAID offer a safety net. If one disk crashes, the others compensate. 2. **Alternate Systems for Recovery**: - **Backup Servers**: These step in when the main servers falter, maintaining service flow. - **Traffic Distributors**: Devices like load balancers share traffic across servers. If a server crashes, they reroute users to operational ones. 3. **Alternate Operation Hubs**: - **Ready-to-use Centers**: Locations equipped and primed to take charge immediately when the main center fails. - **Basic Facilities**: Locations with necessary equipment but lacking recent data, taking longer to activate. - **Semi-prepped Facilities**: Locations somewhat prepared with select", "metadata": {"source_file": "learning-materials/README.md", "section": "Hardware", "language": "en", "created_at": "2025-07-19T19:22:02.025996"}}
{"text": "systems and data, taking a moderate duration to activate. 4. **Power Backup Mechanisms**: - **Instant Power Backup**: Devices like UPS offer power during brief outages, ensuring no abrupt shutdowns. - **Long-term Power Solutions**: Generators keep vital systems operational during extended power losses. 5. **Networking Equipment**: - **Backup Internet Connections**: Having alternatives ensures connectivity even if one provider faces issues. - **Secure Connection Tools**: Devices ensuring safe remote access, especially crucial during DR situations. 6. **On-site Physical Setup**: - **Organized Housing**: Structures like racks to neatly store and manage hardware. - **Emergency Temperature Control**: Backup cooling mechanisms to counter server overheating in HVAC malfunctions. 7. **Alternate Communication Channels**: - **Orbit-based Phones**: Handy when regular communication methods falter. - **Direct Communication Devices**: Devices like radios useful when primary systems are down. 8.", "metadata": {"source_file": "learning-materials/README.md", "section": "Hardware", "language": "en", "created_at": "2025-07-19T19:22:02.026020"}}
{"text": "**Protection Mechanisms**: - **Electronic Barriers & Alert Systems**: Devices like firewalls and intrusion detection keep DR systems safeguarded. - **Physical Entry Control**: Systems controlling entry and monitoring, ensuring only cleared personnel have access. 9. **Uniformity and Compatibility in Hardware**: - It's simpler to manage and replace equipment in emergencies if hardware configurations are consistent and compatible. 10. **Equipment for Trials and Upkeep**: - DR drills might use specific equipment to ensure the primary systems remain unaffected. This verifies the equipment's readiness and capacity to manage real crises. In summary, while software and human interventions are important in disaster recovery operations, it is the hardware that provides the underlying support. It is critical for efficient disaster recovery plans to keep this hardware resilient, duplicated, and routinely assessed. </details> <details> <summary>What is a RAID?</summary><br> <b> RAID is an acronym", "metadata": {"source_file": "learning-materials/README.md", "section": "Hardware", "language": "en", "created_at": "2025-07-19T19:22:02.026039"}}
{"text": "that stands for \"Redundant Array of Independent Disks.\" It is a technique that combines numerous hard drives into a single device known as an array in order to improve performance, expand storage capacity, and/or offer redundancy to prevent data loss. RAID levels (for example, RAID 0, RAID 1, and RAID 5) provide varied benefits in terms of performance, redundancy, and storage efficiency. </b></details> <details> <summary>What is a microcontroller?</summary><br> <b> A microcontroller is a small integrated circuit that controls certain tasks in an embedded system. It typically includes a CPU, memory, and input/output peripherals. </b></details> <details> <summary>What is a Network Interface Controller or NIC?</summary><br><b> A Network Interface Controller (NIC) is a piece of hardware that connects a computer to a network and allows it to communicate with other devices. </b></details> <details> <summary>What is a DMA?</summary><br><b> Direct memory access (DMA) is a feature of computer", "metadata": {"source_file": "learning-materials/README.md", "section": "Hardware", "language": "en", "created_at": "2025-07-19T19:22:02.026061"}}
{"text": "systems that allows certain hardware subsystems to access main system memory independently of the central processing unit (CPU).DMA enables devices to share and receive data from the main memory in a computer. It does this while still allowing the CPU to perform other tasks. </b></details> <details> <summary>What is a Real-Time Operating Systems?</summary><br><b> A real-time operating system (RTOS) is an operating system (OS) for real-time computing applications that processes data and events that have critically defined time constraints. An RTOS is distinct from a time-sharing operating system, such as Unix, which manages the sharing of system resources with a scheduler, data buffers, or fixed task prioritization in a multitasking or multiprogramming environment. Processing time requirements need to be fully understood and bound rather than just kept as a minimum. All processing must occur within the defined constraints. Real-time operating systems are event-driven and preemptive,", "metadata": {"source_file": "learning-materials/README.md", "section": "Hardware", "language": "en", "created_at": "2025-07-19T19:22:02.026079"}}
{"text": "meaning the OS can monitor the relevant priority of competing tasks, and make changes to the task priority. Event-driven systems switch between tasks based on their priorities, while time-sharing systems switch the task based on clock interrupts. </b></details> <details> <summary>List of interrupt types</summary><br><b> There are six classes of interrupts possible: * External * Machine check * I/O * Program * Restart * Supervisor call (SVC) </b></details>", "metadata": {"source_file": "learning-materials/README.md", "section": "Hardware", "language": "en", "created_at": "2025-07-19T19:22:02.026097"}}
{"text": "<details> <summary>Explain what is exactly Big Data</summary><br><b> As defined by Doug Laney: * Volume: Extremely large volumes of data * Velocity: Real time, batch, streams of data * Variety: Various forms of data, structured, semi-structured and unstructured * Veracity or Variability: Inconsistent, sometimes inaccurate, varying data </b></details> <details> <summary>What is DataOps? How is it related to DevOps?</summary><br><b> DataOps seeks to reduce the end-to-end cycle time of data analytics, from the origin of ideas to the literal creation of charts, graphs and models that create value. DataOps combines Agile development, DevOps and statistical process controls and applies them to data analytics. </b></details> <details> <summary>What is Data Architecture?</summary><br><b> An answer from [talend.com](https://www.talend.com/resources/what-is-data-architecture): \"Data architecture is the process of standardizing how organizations collect, store, transform, distribute, and use", "metadata": {"source_file": "learning-materials/README.md", "section": "Big Data", "language": "en", "created_at": "2025-07-19T19:22:02.026376"}}
{"text": "data. The goal is to deliver relevant data to people who need it, when they need it, and help them make sense of it.\" </b></details> <details> <summary>Explain the different formats of data</summary><br><b> * Structured - data that has defined format and length (e.g. numbers, words) * Semi-structured - Doesn't conform to a specific format but is self-describing (e.g. XML, SWIFT) * Unstructured - does not follow a specific format (e.g. images, test messages) </b></details> <details> <summary>What is a Data Warehouse?</summary><br><b> [Wikipedia's explanation on Data Warehouse](https://en.wikipedia.org/wiki/Data_warehouse) [Amazon's explanation on Data Warehouse](https://aws.amazon.com/data-warehouse) </b></details> <details> <summary>What is Data Lake?</summary><br><b> [Data Lake - Wikipedia](https://en.wikipedia.org/wiki/Data_lake) </b></details> <details> <summary>Can you explain the difference between a data lake and a data warehouse?</summary><br><b> </b></details> <details>", "metadata": {"source_file": "learning-materials/README.md", "section": "Big Data", "language": "en", "created_at": "2025-07-19T19:22:02.026409"}}
{"text": "<summary>What is \"Data Versioning\"? What models of \"Data Versioning\" are there?</summary><br><b> </b></details> <details> <summary>What is ETL?</summary><br><b> </b></details>", "metadata": {"source_file": "learning-materials/README.md", "section": "Big Data", "language": "en", "created_at": "2025-07-19T19:22:02.026429"}}
{"text": "<details> <summary>Explain what is Hadoop</summary><br><b> [Apache Hadoop - Wikipedia](https://en.wikipedia.org/wiki/Apache_Hadoop) </b></details> <details> <summary>Explain Hadoop YARN</summary><br><b> Responsible for managing the compute resources in clusters and scheduling users' applications </b></details> <details> <summary>Explain Hadoop MapReduce</summary><br><b> A programming model for large-scale data processing </b></details> <details> <summary>Explain Hadoop Distributed File Systems (HDFS)</summary><br><b> * Distributed file system providing high aggregate bandwidth across the cluster. * For a user it looks like a regular file system structure but behind the scenes it's distributed across multiple machines in a cluster * Typical file size is TB and it can scale and supports millions of files * It's fault tolerant which means it provides automatic recovery from faults * It's best suited for running long batch operations rather than live analysis </b></details> <details>", "metadata": {"source_file": "learning-materials/README.md", "section": "Apache Hadoop", "language": "en", "created_at": "2025-07-19T19:22:02.026591"}}
{"text": "<summary>What do you know about HDFS architecture?</summary><br><b> [HDFS Architecture](http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html) * Master-slave architecture * Namenode - master, Datanodes - slaves * Files split into blocks * Blocks stored on datanodes * Namenode controls all metadata </b></details>", "metadata": {"source_file": "learning-materials/README.md", "section": "Apache Hadoop", "language": "en", "created_at": "2025-07-19T19:22:02.026611"}}
{"text": "<details> <summary>Explain what is Ceph</summary><br><b> Ceph is an Open-Source Distributed Storage System designed to provide excellent performance, reliability, and scalability. It's often used in cloud computing environments and Data Centers. </b></details> <details> <summary>True or False? Ceph favor consistency and correctness over performances</summary><br><b> True </b></details> <details> <summary>Which services or types of storage Ceph supports?</summary><br><b> * Object (RGW) * Block (RBD) * File (CephFS) </b></details> <details> <summary>What is RADOS?</summary><br><b> * Reliable Autonomic Distributed Object Storage * Provides low-level data object storage service * Strong Consistency * Simplifies design and implementation of higher layers (block, file, object) </b></details> <details> <summary>Describe RADOS software components</summary><br><b> * Monitor * Central authority for authentication, data placement, policy * Coordination point for all other cluster components *", "metadata": {"source_file": "learning-materials/README.md", "section": "Ceph", "language": "en", "created_at": "2025-07-19T19:22:02.027164"}}
{"text": "Protect critical cluster state with Paxos * Manager * Aggregates real-time metrics (throughput, disk usage, etc.) * Host for pluggable management functions * 1 active, 1+ standby per cluster * OSD (Object Storage Daemon) * Stores data on an HDD or SSD * Services client IO requests </b></details> <details> <summary>What is the workflow of retrieving data from Ceph?</summary><br><b> The work flow is as follows: 1. The client sends a request to the ceph cluster to retrieve data: > **Client could be any of the following** >> * Ceph Block Device >> * Ceph Object Gateway >> * Any third party ceph client 2. The client retrieves the latest cluster map from the Ceph Monitor 3. The client uses the CRUSH algorithm to map the object to a placement group. The placement group is then assigned to a OSD. 4. Once the placement group and the OSD Daemon are determined, the client can retrieve the data from the appropriate OSD </b></details> <details> <summary>What is the workflow of writing data to", "metadata": {"source_file": "learning-materials/README.md", "section": "Ceph", "language": "en", "created_at": "2025-07-19T19:22:02.027192"}}
{"text": "Ceph?</summary><br><b> The work flow is as follows: 1. The client sends a request to the ceph cluster to retrieve data 2. The client retrieves the latest cluster map from the Ceph Monitor 3. The client uses the CRUSH algorithm to map the object to a placement group. The placement group is then assigned to a Ceph OSD Daemon dynamically. 4. The client sends the data to the primary OSD of the determined placement group. If the data is stored in an erasure-coded pool, the primary OSD is responsible for encoding the object into data chunks and coding chunks, and distributing them to the other OSDs. </b></details> <details> <summary>What are \"Placement Groups\"?</summary><br><b> </b></details> <details> <summary>Describe in the detail the following: Objects -> Pool -> Placement Groups -> OSDs</summary><br><b> </b></details> <details> <summary>What is OMAP?</summary><br><b> </b></details> <details> <summary>What is a metadata server? How it works?</summary><br><b> </b></details>", "metadata": {"source_file": "learning-materials/README.md", "section": "Ceph", "language": "en", "created_at": "2025-07-19T19:22:02.027212"}}
{"text": "<details> <summary>What is Packer? What is it used for?</summary><br><b> In general, Packer automates machine images creation. It allows you to focus on configuration prior to deployment while making the images. This allows you start the instances much faster in most cases. </b></details> <details> <summary>Packer follows a \"configuration->deployment\" model or \"deployment->configuration\"?</summary><br><b> A configuration->deployment which has some advantages like: 1. Deployment Speed - you configure once prior to deployment instead of configuring every time you deploy. This allows you to start instances/services much quicker. 2. More immutable infrastructure - with configuration->deployment it's not likely to have very different deployments since most of the configuration is done prior to the deployment. Issues like dependencies errors are handled/discovered prior to deployment in this model. </b></details>", "metadata": {"source_file": "learning-materials/README.md", "section": "Packer", "language": "en", "created_at": "2025-07-19T19:22:02.027363"}}
{"text": "<details> <summary>Explain Semantic Versioning</summary><br><b> [This](https://semver.org/) page explains it perfectly: ``` Given a version number MAJOR.MINOR.PATCH, increment the: MAJOR version when you make incompatible API changes MINOR version when you add functionality in a backwards compatible manner PATCH version when you make backwards compatible bug fixes Additional labels for pre-release and build metadata are available as extensions to the MAJOR.MINOR.PATCH format. ``` </b></details>", "metadata": {"source_file": "learning-materials/README.md", "section": "Release", "language": "en", "created_at": "2025-07-19T19:22:02.027425"}}
{"text": "If you are looking for a way to prepare for a certain exam this is the section for you. Here you'll find a list of certificates, each references to a separate file with focused questions that will help you to prepare to the exam. Good luck :)", "metadata": {"source_file": "learning-materials/README.md", "section": "Certificates", "language": "en", "created_at": "2025-07-19T19:22:02.027489"}}
{"text": "* [Cloud Practitioner](certificates/aws-cloud-practitioner.md) (Latest update: 2020) * [Solutions Architect Associate](certificates/aws-solutions-architect-associate.md) (Latest update: 2021) * [Cloud SysOps Administration Associate](certificates/aws-cloud-sysops-associate.md) (Latest update: Oct 2022)", "metadata": {"source_file": "learning-materials/README.md", "section": "AWS", "language": "en", "created_at": "2025-07-19T19:22:02.027568"}}
{"text": "* [AZ-900](certificates/azure-fundamentals-az-900.md) (Latest update: 2021)", "metadata": {"source_file": "learning-materials/README.md", "section": "Azure", "language": "en", "created_at": "2025-07-19T19:22:02.027587"}}
{"text": "* [Certified Kubernetes Administrator (CKA)](topics/kubernetes/CKA.md) (Latest update: 2022)", "metadata": {"source_file": "learning-materials/README.md", "section": "Kubernetes", "language": "en", "created_at": "2025-07-19T19:22:02.027602"}}
{"text": "<p align=\"center\"><a href=\"https://github.com/bregman-arie/sre-checklist\"><img width=\"500px\" src=\"images/sre_checklist.png\"/></a></p> <p align=\"center\"><a href=\"https://github.com/bregman-arie/howtheydevops\"><img src=\"images/how_they_devops.png\"/></a></p> <p align=\"center\"><a href=\"https://github.com/bregman-arie/devops-resources\"><img src=\"images/devops_resources.png\"/></a></p> <p align=\"center\"><a href=\"https://github.com/bregman-arie/infraverse\"><img src=\"images/infraverse.png\"/></a></p>", "metadata": {"source_file": "learning-materials/README.md", "section": "Additional DevOps and SRE Projects", "language": "en", "created_at": "2025-07-19T19:22:02.027622"}}
{"text": "Thanks to all of our amazing [contributors](https://github.com/bregman-arie/devops-exercises/graphs/contributors) who make it easy for everyone to learn new things :) Logos credits can be found [here](credits.md)", "metadata": {"source_file": "learning-materials/README.md", "section": "Credits", "language": "en", "created_at": "2025-07-19T19:22:02.027648"}}
{"text": "[![License: CC BY-NC-ND 3.0](https://img.shields.io/badge/License-CC%20BY--NC--ND%203.0-lightgrey.svg)](https://creativecommons.org/licenses/by-nc-nd/3.0/)", "metadata": {"source_file": "learning-materials/README.md", "section": "License", "language": "en", "created_at": "2025-07-19T19:22:02.027662"}}
{"text": "Jenkins logo created by <a href='https://twitter.com/ks_nenasheva'>Ksenia Nenasheva</a> and published through <a href=\"https://jenkins.io\">jenkins.io</a> is licensed under <a href=\"https://creativecommons.org/licenses/by-sa/3.0/\">cc by-sa 3.0</a><br> Git Logo by <a href=\"https://twitter.com/jasonlong\">Jason Long</a> is licensed under the <a href=\"https://creativecommons.org/licenses/by/3.0/\">Creative Commons Attribution 3.0 Unported License</a><br> Terraform logo created by <a href=\"https://www.hashicorp.com\">Hashicorp®</a><br> Docker logo created by <a href=\"https://www.docker.com\">Docker®</a><br> The Python logo is a trademark of the Python Software Foundation®<br> Puppet logo created by <a href=\"https://puppet.com\">Puppet®</a><br> Bash logo created by Prospect One<br> OpenStack logo created by and a trademark of The <a href=\"https://www.openstack.org\">OpenStack Foundation®</a><br> Linux, Kubernetes and Prometheus logos are trademarks of The Linux Foundation®<br> Mongo logo is a", "metadata": {"source_file": "learning-materials/credits.md", "section": "Credits", "language": "en", "created_at": "2025-07-19T19:22:02.028041"}}
{"text": "trademark of <a href=\"http://www.mongodb.com\">Mongo®</a><br> Distributed logo by <a href=\"https://www.iconfinder.com/Flatart\">Flatart</a><br> Challenge icon by Elizabeth Arostegui in Technology Mix \"Question you ask\" (man raising hand) and \"Database\" icons by [Webalys](https://www.iconfinder.com/webalys) Testing logo by [Flatart](https://www.iconfinder.com/Flatart)<br> Google Cloud Plataform Logo created by <a href=\"https://about.google/\">Google®</a><br> VirtualBox Logo created by <a href=\"http://www.iconarchive.com/artist/dakirby309.html\">dAKirby309</a>, under the <a href=\"https://creativecommons.org/licenses/by-nc/4.0/\">Creative Commons Attribution-Noncommercial 4.0 License</a>. Certificates logo by <a href=\"https://www.iconfinder.com/Flatart\">Flatart</a><br> Storage icon by <a href=\"https://www.iconfinder.com/iconic_hub\">Dinosoftlab</a><br> CI/CD icon made made by <a href=\"https://www.flaticon.com/authors/freepik\" title=\"Freepik\">Freepik</a> from <a href=\"https://www.flaticon.com/\"", "metadata": {"source_file": "learning-materials/credits.md", "section": "Credits", "language": "en", "created_at": "2025-07-19T19:22:02.028074"}}
{"text": "title=\"Flaticon\">www.flaticon.com</a></div> Chaos Engineering logo made by Arie Bregman", "metadata": {"source_file": "learning-materials/credits.md", "section": "Credits", "language": "en", "created_at": "2025-07-19T19:22:02.028096"}}
{"text": "<p align=\"center\"><img src=\"images/devops_exercises.png\"/></p> :information_source: &nbsp;此存储库包含有关各种技术主题的问题和练习，有时与 DevOps 和 SRE 相关 :bar_chart: &nbsp;当前有 **2624** 个问题 :warning: &nbsp;您可以使用这些来准备面试，但大多数问题和练习并不代表实际的面试。请阅读[常见问题](faq.md)了解更多详情 :page_facing_up: &nbsp;不同的面试官专注于不同的事情。 有些人将重点放在你的简历上，而另一些人可能将重点放在方案问题或特定的技术问题上。 在这个仓库，我尽力覆盖各种类型的 DevOps 问题，供你练习和测试你的知识 :pencil: &nbsp;你可以通过提交拉取请求来添加更多练习:) 在[此处](CONTRIBUTING.md)阅读贡献指南 **** <!-- ALL-TOPICS-LIST:START --> <!-- prettier-ignore-start --> <!-- markdownlint-disable --> <center> <table> <tr> <td align=\"center\"><a href=\"topics/devops/README.md\"><img src=\"images/devops.png\" width=\"75px;\" height=\"75px;\" alt=\"DevOps\" /><br /><b>DevOps</b></a></td> <td align=\"center\"><a href=\"topics/git/README.md\"><img src=\"images/git.png\" width=\"75px;\" height=\"75px;\" alt=\"Git\"/><br /><b>Git</b></a></td> <td align=\"center\"><a href=\"#network\"><img src=\"images/network.png\" width=\"75px;\" height=\"75px;\" alt=\"Network\"/><br /><b>Network</b></a></td> <td", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "Introduction", "language": "en", "created_at": "2025-07-19T19:22:02.030697"}}
{"text": "align=\"center\"><a href=\"#hardware\"><img src=\"images/hardware.png\" width=\"75px;\" height=\"75px;\" alt=\"Hardware\"/><br /><b>Hardware</b></a></td> <td align=\"center\"><a href=\"topics/kubernetes/README.md\"><img src=\"images/kubernetes.png\" width=\"75px;\" height=\"75px;\" alt=\"kubernetes\"/><br /><b>Kubernetes</b></a></td> </tr> <tr> <td align=\"center\"><a href=\"topics/software_development/README.md\"><img src=\"images/programming.png\" width=\"75px;\" height=\"75px;\" alt=\"programming\"/><br /><b>Software Development</b></a></td> <td align=\"center\"><a href=\"https://github.com/bregman-arie/python-exercises\"><img src=\"images/python.png\" width=\"75px;\" height=\"75px;\" alt=\"Python\"/><br /><b>Python</b></a></td> <td align=\"center\"><a href=\"https://github.com/bregman-arie/go-exercises\"><img src=\"images/Go.png\" width=\"75px;\" height=\"75px;\" alt=\"go\"/><br /><b>Go</b></a></td> <td align=\"center\"><a href=\"topics/perl/README.md\"><img src=\"images/perl.png\" width=\"75px;\" height=\"75px;\" alt=\"perl\"/><br", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "Introduction", "language": "en", "created_at": "2025-07-19T19:22:02.030774"}}
{"text": "/><b>Perl</b></a></td> <td align=\"center\"><a href=\"#regex\"><img src=\"images/regex.png\" width=\"75px;\" height=\"75px;\" alt=\"RegEx\"/><br /><b>Regex</b></a></td> </tr> <tr> <td align=\"center\"><a href=\"topics/cloud/README.md\"><img src=\"images/cloud.png\" width=\"75px;\" height=\"75px;\" alt=\"Cloud\"/><br /><b>Cloud</b></a></td> <td align=\"center\"><a href=\"topics/aws/README.md\"><img src=\"images/aws.png\" width=\"100px;\" height=\"75px;\" alt=\"aws\"/><br /><b>AWS</b></a></td> <td align=\"center\"><a href=\"topics/azure/README.md\"><img src=\"images/azure.png\" width=\"75px;\" height=\"75px;\" alt=\"azure\"/><br /><b>Azure</b></a></td> <td align=\"center\"><a href=\"topics/gcp/README.md\"><img src=\"images/googlecloud.png\" width=\"70px;\" height=\"70px;\" alt=\"Google Cloud Platform\"/><br /><b>Google Cloud Platform</b></a></td> <td align=\"center\"><a href=\"#openstack/README.md\"><img src=\"images/openstack.png\" width=\"75px;\" height=\"75px;\" alt=\"openstack\"/><br /><b>OpenStack</b></a></td> </tr> <tr> <td align=\"center\"><a", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "Introduction", "language": "en", "created_at": "2025-07-19T19:22:02.030800"}}
{"text": "href=\"#operating-system\"><img src=\"images/os.png\" width=\"75px;\" height=\"75px;\" alt=\"Operating System\"/><br /><b>Operating System</b></a></td> <td align=\"center\"><a href=\"topics/linux/README.md\"><img src=\"images/logos/linux.png\" width=\"75px;\" height=\"75px;\" alt=\"Linux\"/><br /><b>Linux</b></a></td> <td align=\"center\"><a href=\"#virtualization\"><img src=\"images/virtualization.png\" width=\"75px;\" height=\"75px;\" alt=\"Virtualization\"/><br /><b>Virtualization</b></a></td> <td align=\"center\"><a href=\"topics/dns/README.md\"><img src=\"images/dns.png\" width=\"75px;\" height=\"75px;\" alt=\"DNS\"/><br /><b>DNS</b></a></td> <td align=\"center\"><a href=\"topics/shell/README.md\"><img src=\"images/bash.png\" width=\"75px;\" height=\"75px;\" alt=\"Bash\"/><br /><b>Shell Scripting</b></a></td> </tr> <tr> <td align=\"center\"><a href=\"topics/databases/README.md\"><img src=\"images/databases.png\" width=\"75px;\" height=\"75px;\" alt=\"Databases\"/><br /><b>Databases</b></a></td> <td align=\"center\"><a href=\"#sql\"><img", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "Introduction", "language": "en", "created_at": "2025-07-19T19:22:02.030853"}}
{"text": "src=\"images/sql.png\" width=\"75px;\" height=\"75px;\" alt=\"sql\"/><br /><b>SQL</b></a></td> <td align=\"center\"><a href=\"#mongo\"><img src=\"images/mongo.png\" width=\"75px;\" height=\"75px;\" alt=\"Mongo\"/><br /><b>Mongo</b></a></td> <td align=\"center\"><a href=\"#testing\"><img src=\"images/testing.png\" width=\"75px;\" height=\"75px;\" alt=\"Testing\"/><br /><b>Testing</b></a></td> <td align=\"center\"><a href=\"#big-data\"><img src=\"images/big-data.png\" width=\"75px;\" height=\"75px;\" alt=\"Big Data\"/><br /><b>Big Data</b></a></td> </tr> <tr> <td align=\"center\"><a href=\"topics/cicd/README.md\"><img src=\"images/cicd.png\" width=\"75px;\" height=\"75px;\" alt=\"cicd\"/><br /><b>CI/CD</b></a></td> <td align=\"center\"><a href=\"#certificates\"><img src=\"images/certificates.png\" width=\"75px;\" height=\"75px;\" alt=\"Certificates\"/><br /><b>Certificates</b></a></td> <td align=\"center\"><a href=\"topics/containers/README.md\"><img src=\"images/containers.png\" width=\"75px;\" height=\"75px;\" alt=\"Containers\"/><br /><b>Containers</b></a></td>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "Introduction", "language": "en", "created_at": "2025-07-19T19:22:02.030873"}}
{"text": "<td align=\"center\"><a href=\"topics/openshift/README.md\"><img src=\"images/openshift.png\" width=\"75px;\" height=\"75px;\" alt=\"OpenShift\"/><br /><b>OpenShift</b></a></td> <td align=\"center\"><a href=\"#storage\"><img src=\"images/storage.png\" width=\"75px;\" height=\"75px;\" alt=\"Storage\"/><br /><b>Storage</b></a></td> </tr> <tr> <td align=\"center\"><a href=\"topics/terraform/README.md\"><img src=\"images/terraform.png\" width=\"75px;\" height=\"75px;\" alt=\"Terraform\"/><br /><b>Terraform</b></a></td> <td align=\"center\"><a href=\"#puppet\"><img src=\"images/puppet.png\" width=\"75px;\" height=\"75px;\" alt=\"puppet\"/><br /><b>Puppet</b></a></td> <td align=\"center\"><a href=\"#distributed\"><img src=\"images/distributed.png\" width=\"75px;\" height=\"75px;\" alt=\"Distributed\"/><br /><b>Distributed</b></a></td> <td align=\"center\"><a href=\"#questions-you-ask\"><img src=\"images/you.png\" width=\"75px;\" height=\"75px;\" alt=\"you\"/><br /><b>Questions you can ask</b></a></td> <td align=\"center\"><a href=\"topics/ansible/README.md\"><img", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "Introduction", "language": "en", "created_at": "2025-07-19T19:22:02.030890"}}
{"text": "src=\"images/ansible.png\" width=\"75px;\" height=\"75px;\" alt=\"ansible\"/><br /><b>Ansible</b></a></td> </tr> <tr> <td align=\"center\"><a href=\"topics/observability/README.md\"><img src=\"images/observability.png\" width=\"75px;\" height=\"75px;\" alt=\"observability\"/><br /><b>Observability</b></a></td> <td align=\"center\"><a href=\"#prometheus\"><img src=\"images/prometheus.png\" width=\"75px;\" height=\"75px;\" alt=\"Prometheus\"/><br /><b>Prometheus</b></a></td> <td align=\"center\"><a href=\"topics/circleci/README.md\"><img src=\"images/logos/circleci.png\" width=\"70px;\" height=\"70px;\" alt=\"Circle CI\"/><br /><b>Circle CI</b></a></td> <td align=\"center\"><a href=\"topics/datadog/README.md\"><img src=\"images/logos/datadog.png\" width=\"80px;\" height=\"80px;\" alt=\"DataDog\"/><br /><b></b></a></td> <td align=\"center\"><a href=\"topics/grafana/README.md\"><img src=\"images/logos/grafana.png\" width=\"80px;\" height=\"80px;\" alt=\"Grafana\"/><br /><b>Grafana</b></a></td> </tr> <tr> <td align=\"center\"><a", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "Introduction", "language": "en", "created_at": "2025-07-19T19:22:02.030907"}}
{"text": "href=\"topics/argo/README.md\"><img src=\"images/logos/argo.png\" width=\"80px;\" height=\"80px;\" alt=\"Argo\"/><br /><b>Argo</b></a></td> <td align=\"center\"><a href=\"topics/soft_skills/README.md\"><img src=\"images/HR.png\" width=\"75px;\" height=\"75px;\" alt=\"HR\"/><br /><b>Soft Skills</b></a></td> <td align=\"center\"><a href=\"topics/security/README.md\"><img src=\"images/security.png\" width=\"75px;\" height=\"75px;\" alt=\"security\"/><br /><b>Security</b></a></td> <td align=\"center\"><a href=\"#system-design\"><img src=\"images/design.png\" width=\"75px;\" height=\"75px;\" alt=\"Design\"/><br /><b>System Design</b></a></td> </tr> <tr> <td align=\"center\"><a href=\"topics/chaos_engineering/README.md\"><img src=\"images/logos/chaos_engineering.png\" width=\"75px;\" height=\"75px;\" alt=\"Chaos Engineering\"/><br /><b>Chaos Engineering</b></a></td> <td align=\"center\"><a href=\"#Misc\"><img src=\"images/general.png\" width=\"75px;\" height=\"75px;\" alt=\"Misc\"/><br /><b>Misc</b></a></td> <td align=\"center\"><a href=\"#elastic\"><img", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "Introduction", "language": "en", "created_at": "2025-07-19T19:22:02.030923"}}
{"text": "src=\"images/elastic.png\" width=\"75px;\" height=\"75px;\" alt=\"Elastic\"/><br /><b>Elastic</b></a></td> <td align=\"center\"><a href=\"topics/kafka/README.md\"><img src=\"images/logos/kafka.png\" width=\"85px;\" height=\"80px;\" alt=\"Kafka\"/><br /><b>Kafka</b></a></td> </tr> </table> </center> <!-- markdownlint-enable --> <!-- prettier-ignore-end --> <!-- ALL-TOPICS-LIST:END -->", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "Introduction", "language": "en", "created_at": "2025-07-19T19:22:02.030940"}}
{"text": "<details> <summary>一般来说，你需要什么才能进行交流？</summary><br><b> - 一种共同的语言（供两端理解） - 与你想要沟通的人交流的方法 - 一个连接（以便通信内容能够到达接收者） </b></details> <details> <summary>什么是 TCP/IP？</summary><br><b> 一组协议，定义了两个或多个设备如何相互通信。 了解更多关于TCP/IP, 阅读 [这里](http://www.penguintutor.com/linux/basic-network-reference) </b></details> <details> <summary>什么是以太网？</summary><br><b> 以太网简单地指的是当今最常见的局域网（LAN）类型。与跨越较大地理区域的广域网（WAN）相对，LAN是一个连接在小范围内的计算机网络，比如你的办公室、大学校园或者家庭。 </b></details> <details> <summary>什么是 MAC 地址？它有什么用途？</summary><br><b> MAC地址是用于识别网络上各个设备的唯一标识号码或代码。 通过以太网发送的数据包始终来自一个 MAC 地址并发送到一个 MAC 地址。如果网络适配器接收到一个数据包，它会将该数据包的目标 MAC 地址与适配器自身的 MAC 地址进行比较。 </b></details> <details> <summary>这个 MAC 地址是在什么时候使用的？: ff:ff:ff:ff:ff:ff</summary><br><b> 当设备向广播 MAC 地址（FF:FF:FF:FF:FF:FF）发送数据包时，它会传递给本地网络上的所有站点。以太网广播用于在数据链路层通过 ARP 解析 IP 地址到 MAC 地址。 </b></details> <details> <summary>什么是 IP 地址？</summary><br><b> 互联网协议地址（IP 地址）是分配给连接到使用互联网协议进行通信的计算机网络上的每个设备的数字标签。IP地址具有两个主要功能：主机或网络接口识别和位置寻址。 </b></details> <details> <summary>解释子网掩码并举例说明</summary><br><b>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "网络", "language": "en", "created_at": "2025-07-19T19:22:02.032220"}}
{"text": "子网掩码是一个32位的数字，用于屏蔽 IP 地址并将 IP 地址分为网络地址和主机地址。子网掩码通过将网络位设置为全部\"1\"，将主机位设置为全部\"0\"来生成。在给定的网络中，总可用主机地址中始终保留两个用于特定目的，并且不能分配给任何主机。这些是第一个地址，被保留作为网络地址（也称为网络 ID），以及最后一个用于网络广播的地址。 [例子](https://github.com/philemonnwanne/projects/tree/main/exercises/exe-09) </b></details> <details> <summary>私有 IP 地址是什么？在哪些场景/系统设计中应该使用它？</summary><br><b> 私有IP地址被分配给同一网络中的主机，以便彼此通信。正如“私有”这个名字所暗示的那样，拥有私有IP地址的设备无法被来自任何外部网络的设备访问到。例如，如果我住在一个宿舍，并且我希望我的室友们加入我托管的游戏服务器，我会要求他们通过我的服务器的私有IP地址加入，因为该网络是局域网。 </b></details> <details> <summary>什么是公共 IP 地址？在哪些场景/系统设计中，应该使用它？</summary><br><b> 公共IP地址是面向公众的 IP 地址。如果你正在托管一个游戏服务器，希望你的朋友加入，你会给他们提供你的公共IP地址，以便他们的计算机能够识别和定位到你的网络和服务器，从而进行连接。在与与您连接到同一网络的朋友玩耍时，并不需要使用面向公众的IP地址，在这种情况下，您将使用私有IP地址。为了使某人能够连接到内部位置的服务器上，您需要设置端口转发来告诉路由器允许来自公共域名和网络之间的流量通信。 </b></details> <details> <summary>解释 OSI 模型。有哪几层？每层负责什么？</summary><br><b> - 应用程序：用户端（ HTTP 在此）。 - 演示：建立应用层实体之间的上下文（加密在这里）。 - 会话：建立、管理和终止连接。 - 传输：将可变长度的数据序列从源主机传输到目标主机（ TCP 和 UDP 在此）。 - 网络：将数据报从一个网络传输到另一个网络（ IP 在此）。 - 数据链路：提供两个直接连接的节点之间的链接（MAC在此）。 -", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "网络", "language": "en", "created_at": "2025-07-19T19:22:02.032301"}}
{"text": "物理特性：数据连接的电气和物理规格（位数在此）。 您可以在 [penguintutor.com](http://www.penguintutor.com/linux/basic-network-reference) 阅读有关OSI模型的更多信息。 </b></details> <details> <summary>对于以下每个确定其属于哪个 OSI 层： * 错误更正 * 数据包路由 * 电缆和电信号 * MAC 地址 * IP 地址 * 终止连接 * 3 次握手</summary><br><b> * 错误纠正 - 数据链路 * 数据包路由 - 网络 * 电缆和电信号 - 物理 * MAC 地址 - 数据链路 * IP 地址 - 网络 * 终止连接 - 会话 * 3次握手 - 传输 </b></details> <details> <summary>你熟悉哪些交付计划？</summary><br><b> 单播：一对一的通信，其中有一个发送者和一个接收者。 广播：向网络中的所有人发送消息。地址 ff:ff:ff:ff:ff:ff 用于广播。 使用广播的两个常见协议是 ARP 和 DHCP。 多播：向一组订阅者发送消息。它可以是一对多或多对多的。 </b></details> <details> <summary>什么是 CSMA/CD？它在现代以太网网络中使用吗？</summary><br><b> CSMA/CD 代表载波侦听多路访问冲突检测。 其主要目标是管理对共享介质/总线的访问，每次只有一个主机可以传输。 CSMA/CD 算法： 1. 在发送帧之前，它会检查是否有另一个主机正在传输帧。 2. 如果没有人在传输，它就开始传输帧。 3. 如果两个主机同时传输，就会发生碰撞。 4. 两个主机都停止发送帧，并向所有人发送一个“干扰信号”，通知大家发生了碰撞。 5. 他们正在等待一个随机的时间再次发送它。 6. 一旦每个主机等待了随机时间，它们会再次尝试发送帧，从而重新开始循环。 </b></details> <details> <summary>描述以下网络设备及其之间的区别： * 路由器 * 交换机 * 集线器</summary><br><b>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "网络", "language": "en", "created_at": "2025-07-19T19:22:02.032329"}}
{"text": "路由器、交换机和集线器都是用于连接局域网（LAN）中的设备的网络设备。然而，每个设备的操作方式不同，并且具有其特定的使用情况。以下是对每个设备及其之间区别的简要描述： 1. 路由器：一种网络设备，用于连接多个网络段。它在OSI模型的网络层（第3层）上运行，并使用路由协议来指导网络之间的数据传输。路由器使用IP地址来识别设备并将数据包定向到正确的目标位置。 2. 交换机：一种网络设备，用于连接局域网上的多个设备。它在OSI模型的数据链路层（第二层）工作，并使用MAC地址来识别设备并将数据包定向到正确的目标。交换机可以使同一网络上的设备更高效地相互通信，并且可以防止多个设备同时发送数据时可能发生的数据碰撞。 3. 集线器：一种网络设备，通过单根电缆连接多个设备，并用于在不分割网络的情况下连接多个设备。然而，与交换机不同的是，它在OSI模型的物理层（第1层）上运行，并且只是将数据包广播到所有连接到它的设备，无论该设备是否为预期接收者。这意味着可能会发生数据碰撞，并且网络效率可能因此受到影响。由于交换机更高效并提供更好的网络性能，所以现代网络设置通常不使用集线器。 </b></details> <details> <summary>什么是“冲突域”？</summary><br><b> 冲突域是一个网络段，在这个网络段中，设备可能会因为试图同时传输数据而相互干扰。当两个设备同时传输数据时，可能会发生碰撞，导致数据丢失或损坏。在冲突域中，所有设备共享同样的带宽，并且任何设备都有可能干扰其他设备的数据传输。 </b></details> <details> <summary>什么是“广播域”？</summary><br><b> 广播域是一个网络段，其中所有设备可以通过发送广播消息相互通信。广播消息是一条发送给网络中所有设备而不是特定设备的消息。在广播域中，所有设备都可以接收和处理广播消息，无论该消息是否针对它们。 </b></details> <details> <summary>连接到一个交换机的三台计算机。有多少个冲突域？有多少个广播域？</summary><br><b> 三个冲突域和一个广播域 </b></details> <details> <summary>路由器是如何工作的？</summary><br><b>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "网络", "language": "en", "created_at": "2025-07-19T19:22:02.032363"}}
{"text": "路由器是一种物理或虚拟设备，用于在两个或多个分组交换的计算机网络之间传递信息。路由器检查给定数据包的目标互联网协议地址（IP地址），计算它到达目的地的最佳路径，然后相应地转发它。 </b></details> <details> <summary>什么是NAT？</summary><br><b> 网络地址转换（NAT）是一个过程，其中一个或多个本地IP地址被翻译成一个或多个全局IP地址，反之亦然，以便为本地主机提供互联网访问。 </b></details> <details> <summary>什么是代理？它是如何工作的？我们为什么需要它？</summary><br><b> 代理服务器充当您和互联网之间的网关。它是一个中介服务器，将最终用户与他们浏览的网站分离开来。 如果您使用代理服务器，互联网流量将通过代理服务器传输到您请求的地址。然后，该请求再次通过相同的代理服务器返回（有一些例外情况），然后代理服务器将从网站接收到的数据转发给您。 代理服务器根据您的使用情况、需求或公司政策提供不同级别的功能、安全性和隐私保护。 </b></details> <details> <summary>TCP 是什么？它如何工作？三次握手是什么？</summary><br><b> TCP 三次握手，又称为三向握手，在 TCP/IP 网络中用于建立服务器和客户端之间的连接的过程。 三次握手主要用于创建 TCP 套接字连接。它在以下情况下起作用： - 一个客户节点通过IP网络向同一网络或外部网络上的服务器发送SYN数据包。该数据包的目标是询问/推断服务器是否对新连接开放。 - 目标服务器必须具有可以接受和发起新连接的开放端口。当服务器从客户节点收到SYN数据包时，它会响应并返回确认收据 - ACK 数据包或 SYN/ACK 数据包。 - 客户端节点接收到来自服务器的 SYN/ACK，并用一个 ACK数据包作出响应。 </b></details> <details> <summary>什么是往返延迟或往返时间？</summary><br><b> 摘自 [维基百科](https://en.wikipedia.org/wiki/Round-trip_delay)：\"发送信号所需的时间加上收到信号确认所需的时间\"。 附加问题：局域网的 RTT 是多少？ </b></details>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "网络", "language": "en", "created_at": "2025-07-19T19:22:02.032385"}}
{"text": "<details> <summary>SSL 握手是如何进行的？</summary><br><b> SSL 握手是在客户端和服务器之间建立安全连接的过程。 1. 客户端向服务器发送一个Client Hello消息，其中包括客户端的SSL/TLS协议版本、客户端支持的加密算法列表和一个随机值。 2. 服务器响应一个Server Hello消息，其中包括服务器的SSL/TLS协议版本、一个随机值和会话ID。 3. 服务器发送一个证书消息，其中包含了服务器的证书。 4. 服务器发送 Server Hello Done 信息，表示服务器已完成服务器 Hello 阶段的信息发送。 5. 客户发送包含客户公钥的客户密钥交换信息。 6. 客户端发送 \"更改密码规格 \"报文，通知服务器客户端即将发送使用新密码规格加密的报文。 7. 客户端发送一个加密的握手消息，其中包含使用服务器的公钥加密的预主密钥。 8. 服务器发送 \"更改密码规格 \"信息，通知客户端服务器即将发送使用新密码规格加密的信息。 9. 服务器发送加密握手信息，其中包含用客户机公钥加密的预主密钥。 10. 客户端和服务器现在可以交换应用数据。 </b></details> <details> <summary>TCP 和 UDP 有什么区别？</summary><br><b> TCP 在客户端和服务器之间建立连接，以保证数据包的顺序，而 UDP 不在客户端和服务器之间建立连接，也不处理数据包顺序。这使得 UDP 比 TCP 更轻便，是流媒体等服务的理想选择。 [Penguintutor.com](http://www.penguintutor.com/linux/basic-network-reference) 提供了很好的解释。 </b></details> <details> <summary>您熟悉哪些 TCP/IP 协议？</summary><br><b> </b></details> <details> <summary>解释“默认网关”</summary><br><b> 默认网关是一个接入点或 IP 路由器，联网计算机利用它将信息发送到另一个网络或互联网上的计算机。 </b></details> <details> <summary>什么是 ARP？它是如何工作的？</summary><br><b> ARP", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "网络", "language": "en", "created_at": "2025-07-19T19:22:02.032420"}}
{"text": "是地址解析协议（Address Resolution Protocol）的缩写。当您尝试 ping 本地网络上的一个 IP 地址（如 192.168.1.1）时，您的系统必须将 IP 地址 192.168.1.1 转换为 MAC 地址。这就需要使用 ARP 来解析该地址，ARP 也因此而得名。 系统会保存一个 ARP 查找表，其中存储了哪些 IP 地址与哪些 MAC 地址相关联的信息。当试图向某个 IP 地址发送数据包时，系统会首先查询该表，看是否已经知道该 MAC 地址。如果有缓存值，则不使用 ARP。 </b></details> <details> <summary>什么是 TTL？它有助于防止什么？</summary><br><b> - TTL（生存时间）是IP（Internet Protocol，互联网协议）数据包中的一个值，它决定了在被丢弃之前数据包可以经过多少跳或路由器。每次通过路由器转发数据包时，TTL值会减少一。当TTL值达到零时，数据包将被丢弃，并向发送方发送ICMP（Internet Control Message Protocol，互联网控制消息协议）消息以指示该数据包已过期。 - TTL 用于防止数据包在网络中无限循环，否则会造成拥塞并降低网络性能。 - 它还有助于防止数据包陷入路由环路，即数据包在同一组路由器之间不断往返而永远无法到达目的地。 - 此外，TTL 还可用于帮助检测和防止 IP 欺骗攻击，在这种攻击中，攻击者试图通过使用虚假或伪造的 IP 地址来冒充网络上的其他设备。通过限制数据包的跳数，TTL 可以帮助防止数据包被路由到不合法的目的地。 </b></details> <details> <summary>什么是 DHCP？它是如何工作的？</summary><br><b> 它代表动态主机配置协议，为主机分配 IP 地址、子网掩码和网关。它是这样工作的： * 主机在进入网络时广播一条寻找 DHCP 服务器的信息（DHCP DISCOVER）。 * DHCP 服务器会以数据包的形式发回要约信息，其中包含租用时间、子网掩码、IP 地址等信息（DHCP OFFER）。 * 根据接受的提议，客户端会发送回复广播，让所有 DHCP 服务器都知道（DHCP 请求）。 * 服务器发送确认（DHCP ACK） 更多信息", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "网络", "language": "en", "created_at": "2025-07-19T19:22:02.032444"}}
{"text": "[此处](https://linuxjourney.com/lesson/dhcp-overview) </b></details> <details> <summary>同一个网络中可以有两个 DHCP 服务器吗？它是如何工作的？</summary><br><b> 可以在同一网络上安装两个 DHCP 服务器，但不建议这样做，而且必须仔细配置，以防止冲突和配置问题。 - 在同一网络上配置两个 DHCP 服务器时，两个服务器都有可能为同一设备分配 IP 地址和其他网络配置设置，从而导致冲突和连接问题。此外，如果 DHCP 服务器配置了不同的网络设置或选项，网络上的设备可能会收到冲突或不一致的配置设置。 - 不过，在某些情况下，可能有必要在同一网络中设置两个 DHCP 服务器，例如在大型网络中，一个 DHCP 服务器可能无法处理所有请求。在这种情况下，可以将 DHCP 服务器配置为不同的 IP 地址范围或不同的子网，这样它们就不会相互干扰。 </b></details> <details> <summary>什么是 SSL 隧道？它是如何工作的？</summary><br><b> - SSL（安全套接字层）隧道是一种技术，用于在互联网等不安全网络上的两个端点之间建立安全的加密连接。SSL 隧道是通过将流量封装在 SSL 连接中创建的，SSL 连接可提供保密性、完整性和身份验证。 下面介绍 SSL 隧道的工作原理： 1. 客户端启动与服务器的 SSL 连接，其中包括建立 SSL 会话的握手过程。 2. SSL 会话建立后，客户端和服务器会协商加密参数，如加密算法和密钥长度，然后交换数字证书，以验证彼此的身份。 3. 客户端随后通过 SSL 隧道将流量发送到服务器，服务器解密流量并将其转发到目标位置。 4. 服务器通过 SSL 隧道将流量发送回客户端，客户端对流量进行解密并将其转发给应用程序。 </b></details> <details> <summary>什么是套接字？在哪里可以看到系统中的套接字列表？</summary><br><b> - 套接字是一种软件端点，可使进程之间通过网络进行双向通信。套接字为网络通信提供了一个标准化接口，允许应用程序在网络上发送和接收数据。查看 Linux 系统上打开的套接字列表： ***netstat -an*** -", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "网络", "language": "en", "created_at": "2025-07-19T19:22:02.032465"}}
{"text": "该命令显示所有打开套接字的列表，以及它们的协议、本地地址、外来地址和状态。 </b></details> <details> <summary>什么是 IPv6？如果我们有 IPv4，为什么还要考虑使用它？</summary><br><b> - IPv6（互联网协议版本 6）是互联网协议（IP）的最新版本，用于识别网络上的设备并与之通信。IPv6 地址是 128 位地址，用十六进制表示，如 2001:0db8:85a3:0000:0000:8a2e:0370:7334。 我们应该考虑使用 IPv6 而不是 IPv4 有几个原因： 1. 地址空间：IPv4 的地址空间有限，在世界上许多地方已经耗尽。IPv6 提供了更大的地址空间，可提供数万亿个唯一的 IP 地址。 2. 安全性：IPv6 包含对 IPsec 的内置支持，为网络流量提供端到端加密和身份验证。 3. 性能：IPv6 包括一些有助于提高网络性能的功能，例如组播路由，它允许将一个数据包同时发送到多个目的地。 4. 简化网络配置：IPv6 包含可简化网络配置的功能，例如无状态自动配置，它允许设备自动配置自己的 IPv6 地址，而无需 DHCP 服务器。 5. 更好的移动性支持：IPv6 包含可改进移动性支持的功能，如移动 IPv6，它允许设备在不同网络之间移动时保持其 IPv6 地址。 </b></details> <details> <summary>什么是 VLAN？</summary><br><b> - VLAN（虚拟局域网）是一种逻辑网络，它将物理网络上的一组设备组合在一起，而不管它们的物理位置如何。创建 VLAN 的方法是配置网络交换机，为连接到交换机上特定端口或端口组的设备发送的帧分配特定的 VLAN ID。 </b></details> <details> <summary>什么是 MTU？</summary><br><b> MTU 是最大传输单元（Maximum Transmission Unit）的缩写。它是指单个事务中可发送的最大 PDU（协议数据单元）的大小。 </b></details> <details> <summary>如果发送的数据包大于 MTU，会发生什么情况？</summary><br><b> 在 IPv4 协议中，路由器可以对 PDU 进行分片，然后通过事务发送所有已分片的", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "网络", "language": "en", "created_at": "2025-07-19T19:22:02.032485"}}
{"text": "PDU。 使用 IPv6 协议时，它会向用户计算机发出错误信息。 </b></details> <details> <summary>真还是假？Ping 使用 UDP 是因为它不在乎连接是否可靠</summary><br><b> 错。Ping 实际上使用的是 ICMP（互联网控制报文协议），这是一种用于发送与网络通信有关的诊断信息和控制信息的网络协议。 </b></details> <details> <summary>什么是 SDN？</summary><br><b> - SDN 是软件定义网络（Software-Defined Networking）的缩写。它是一种网络管理方法，强调网络控制的集中化，使管理员能够通过软件抽象来管理网络行为。 - 在传统网络中，路由器、交换机和防火墙等网络设备需要使用专用软件或命令行界面进行单独配置和管理。相比之下，SDN 将网络控制平面与数据平面分开，允许管理员通过集中式软件控制器管理网络行为。 </b></details> <details> <summary>什么是 ICMP？它有什么用途？</summary><br><b> - ICMP 是 Internet Control Message Protocol 的缩写。它是 IP 网络中用于诊断和控制的协议。它是互联网协议套件的一部分，在网络层运行。 ICMP消息被用于各种目的，包括： 1. 错误报告：ICMP消息用于报告网络中发生的错误，例如无法将数据包传递到其目的地。 2. Ping：ICMP 用于发送 ping 信息，该信息用于测试主机或网络是否可连接，并测量数据包的往返时间。 3. 路径 MTU 发现：ICMP 用于发现路径的最大传输单元（MTU），即无需分片即可传输的最大数据包大小。 4. 跟踪路由跟踪路由实用程序使用 ICMP 跟踪数据包通过网络的路径。 5. 路由器发现ICMP 用于发现网络中的路由器。 </b></details> <details> <summary>什么是 NAT？它是如何工作的？</summary><br><b> NAT 是网络地址转换的缩写。它是一种在传输信息前将多个本地专用地址映射到一个公共地址的方法。希望多个设备使用一个 IP 地址的组织和大多数家用路由器一样，都会使用 NAT。 例如，你电脑的私有 IP 可能是", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "网络", "language": "en", "created_at": "2025-07-19T19:22:02.032518"}}
{"text": "192.168.1.100，但你的路由器会将流量映射到它的公共 IP（如 1.1.1.1）。互联网上的任何设备都会看到来自公共 IP（1.1.1.1）而不是私人 IP（192.168.1.100）的流量。 </b></details> <details> <summary>下列协议中使用的端口号分别是？ * SSH * SMTP * HTTP * DNS * HTTPS * FTP * SFTP </summary><br><b> * SSH - 22 * SMTP - 25 * HTTP - 80 * DNS - 53 * HTTPS - 443 * FTP - 21 * SFTP - 22 </b></details> <details> <summary>哪些因素会影响网络性能？</summary><br><b> 有几个因素会影响网络性能，包括： 1. 带宽：网络连接的可用带宽会极大地影响其性能。带宽有限的网络可能会出现数据传输速率慢、延迟高和响应速度差等问题。 2. 延迟：延迟是指数据从网络中的一个点传输到另一个点时发生的延迟。高延迟会导致网络性能缓慢，尤其是视频会议和在线游戏等实时应用。 3. 网络拥塞：当太多设备同时使用网络时，就会出现网络拥塞，导致数据传输速率缓慢和网络性能低下。 4. 数据包丢失：当数据包在传输过程中丢失时，就会出现丢包现象。这会导致网络速度变慢，整体网络性能降低。 5. 网络拓扑：网络的物理布局，包括交换机、路由器和其他网络设备的位置，都会影响网络性能。 6. 网络协议：不同的网络协议具有不同的性能特征，会影响网络性能。例如，TCP 是一种可靠的协议，可以保证数据的传输，但也会因错误检查和重传所需的开销而导致性能降低。 7. 网络安全：防火墙和加密等安全措施会影响网络性能，尤其是在需要大量处理能力或引入额外延迟的情况下。 8. 距离：网络设备之间的物理距离会影响网络性能，尤其是无线网络，信号强度和干扰会影响连接性和数据传输速率。 </b></details> <details> <summary>什么是 APIPA？</summary><br><b> APIPA 是分配给设备的一组 IP 地址 当主 DHCP 服务器无法访问时分配给设备的 IP 地址 </b></details> <details> <summary>APIPA", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "网络", "language": "en", "created_at": "2025-07-19T19:22:02.032540"}}
{"text": "使用哪个 IP 范围？</summary><br><b> APIPA 使用的 IP 范围是169.254.0.1 - 169.254.255.254. </b></details>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "网络", "language": "en", "created_at": "2025-07-19T19:22:02.032560"}}
{"text": "<details> <summary>\"控制平面\"是指什么？</summary><br><b> 控制平面是网络的一部分，它决定如何将数据包路由和转发到不同的位置。 </b></details> <details> <summary>数据平面 \"指的是什么？</summary><br><b> 数据平面是网络中实际转发数据/数据包的部分。 </b></details> <details> <summary>管理平面 \"指的是什么？</summary><br><b> 它指的是监测和管理功能。 </b></details> <details> <summary>创建路由表属于哪个平面（数据、控制......）？</summary><br><b> 控制平面。 </b></details> <details> <summary>解释生成树协议（STP）。</summary><br><b> </b></details> <details> <summary>什么是链路聚合？为什么要使用？</summary><br><b> </b></details> <details> <summary>什么是非对称路由？如何处理？</summary><br><b> </b></details> <details> <summary>您熟悉哪些覆盖（隧道）协议？</summary><br><b> </b></details> <details> <summary>什么是 GRE？它是如何运作的？</summary><br><b> </b></details> <details> <summary>什么是 VXLAN？它是如何工作的？</summary><br><b> </b></details> <details> <summary>什么是 SNAT？</summary><br><b> </b></details> <details> <summary>解释 OSPF。</summary><br><b> OSPF（开放式最短路径优先）是一种路由协议，可在各种类型的路由器上实施。一般来说，大多数现代路由器都支持 OSPF，包括思科、瞻博网络和华为等供应商的路由器。该协议设计用于基于 IP 的网络，包括 IPv4 和", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "控制平面和数据平面", "language": "en", "created_at": "2025-07-19T19:22:02.032847"}}
{"text": "IPv6。此外，它采用分层网络设计，将路由器分组为区域，每个区域都有自己的拓扑图和路由表。这种设计有助于减少路由器之间需要交换的路由信息量，提高网络的可扩展性。 OSPF 4 路由器类型有 * Internal Router * Area Border Routers * Autonomous Systems Boundary Routers * Backbone Routers 了解有关 OSPF 路由器类型的更多信息： https://www.educba.com/ospf-router-types </b></details> <details> <summary>什么是延迟？</summary><br><b> 延迟是指信息从信息源到达目的地所需的时间。 </b></details> <details> <summary>什么是带宽？</summary><br><b> 带宽是通信信道的容量，用于衡量后者在特定时间段内可处理的数据量。带宽越大，意味着处理的流量越多，数据传输量也就越大。 </b></details> <details> <summary>什么是吞吐量？</summary><br><b> 吞吐量是指在一定时间内通过任何传输通道传输的实际数据量。 </b></details> <details> <summary>在进行搜索查询时，延迟和吞吐量哪个更重要？如何确保我们对全球基础设施进行管理？ </summary><br><b> 延迟。要获得良好的延迟，搜索查询应转发到最近的数据中心。 </b></details> <details> <summary>上传视频时，延迟和吞吐量哪个更重要？如何确保这一点？</summary><br><b> 吞吐量。为获得良好的吞吐量，上传数据流应被路由到未充分利用的链路。 </b></details> <details> <summary>转发请求时还需要考虑哪些因素（除了延迟和吞吐量）？</summary><br><b> * 保持缓存更新（这意味着请求可能不会被转发到最近的数据中心） </b></details> <details> <summary>解释 Spine & Leaf</summary><br><b> </b></details> <details>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "控制平面和数据平面", "language": "en", "created_at": "2025-07-19T19:22:02.032877"}}
{"text": "<summary>什么是网络拥塞？什么原因会导致网络拥塞？</summary><br><b> 当网络上需要传输的数据过多，而网络容量不足以满足需求时，就会出现网络拥塞。 </br> 这会导致延迟和数据包丢失增加。原因可能是多方面的，如网络使用率高、文件传输量大、恶意软件、硬件问题或网络设计问题。</br> 为防止网络拥塞，必须监控网络使用情况，并实施策略来限制或管理需求。 </b></details> <details> <summary>关于 UDP 数据包格式，您能告诉我什么？TCP 数据包格式如何？有何不同？</summary><br><b> </b></details> <details> <summary>什么是指数后退算法？在哪里使用？</summary><br><b> </b></details> <details> <summary>使用汉明码，以下数据字 100111010001101 的码字是什么？</summary><br><b> 00110011110100011101 </b></details> <details> <summary>举例说明应用层中的协议</summary><br><b> * 超文本传输协议（HTTP）--用于互联网上的网页 * 简单邮件传输协议（SMTP）--用于电子邮件传输 * 电信网络（TELNET）--终端模拟，允许客户端访问 telnet 服务器 * 文件传输协议（FTP）--便于在任何两台机器之间传输文件 * 域名系统 (DNS) - 域名转换 * 动态主机配置协议（DHCP）--为主机分配 IP 地址、子网掩码和网关 * 简单网络管理协议（SNMP）--收集网络设备数据 </b></details> <details> <summary>举例说明网络层中的协议</summary><br><b> * 互联网协议 (IP) - 协助将数据包从一台机器路由到另一台机器 * 互联网控制消息协议（ICMP）--让人知道发生了什么，如错误信息和调试信息 </b></details> <details> <summary>什么是 HSTS？</summary><br><b> HTTP 严格传输安全（HTTP Strict Transport", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "控制平面和数据平面", "language": "en", "created_at": "2025-07-19T19:22:02.032916"}}
{"text": "Security）是一种网络服务器指令，它通过在开始时发送并返回给浏览器的响应标头，告知用户代理和网络浏览器如何处理其连接。这将强制通过 HTTPS 加密连接，忽略任何脚本通过 HTTP 加载该域中任何资源的调用。 阅读更多 [此处](https://www.globalsign.com/en/blog/what-is-hsts-and-how-do-i-use-it#:~:text=HTTP%20Strict%20Transport%20Security%20(HSTS,and%20back%20to%20the%20browser.) </b></details>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "控制平面和数据平面", "language": "en", "created_at": "2025-07-19T19:22:02.032938"}}
{"text": "<details> <summary>什么是互联网？它和万维网一样吗？</summary><br><b> 互联网是一个由网络组成的网络，在全球范围内传输大量数据。<br> 万维网是一个运行在数百万服务器上的应用程序，它位于互联网之上，可通过所谓的网络浏览器访问 </b></details> <details> <summary>什么是ISP?</summary><br><b> ISP（互联网服务提供商）是当地的互联网公司。 </b></details>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "网络 - 其他", "language": "en", "created_at": "2025-07-19T19:22:02.032960"}}
{"text": "<a name=\"devops-beginner\"></a>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "DevOps", "language": "en", "created_at": "2025-07-19T19:22:02.032976"}}
{"text": "<details> <summary>什么是 DevOps? DevOps 帮助我们完成什么?</summary><br><b> </b></details> <details> <summary>DevOps 的反模式是什么?</summary><br><b> </b></details> <details> <summary>什么是持续集成?</summary><br><b> 开发人员经常将代码集成到共享仓库中的一种开发实践。 它的范围可以从每天或每周进行几次更改，到大规模在一个小时内进行几次更改。 验证每段代码（更改/补丁），以使更改可以安全地合并。 如今，使用自动构建来确保代码可以集成的测试更改是一种常见的做法。 它可以是一个运行在不同级别（单元，功能等）的多个测试的构建，也可以是所有或某些必须通过以将更改合并到存储库中的多个单独的构建。 </b></details> <details> <summary>什么是持续部署?</summary><br><b> </b></details> <details> <summary>什么是持续交付?</summary><br><b> </b></details> <details> <summary>你认为CI / CD的最佳做法是什么?</summary><br><b> </b></details> <details> <summary>你将用于以下哪些系统和/或工具？： * CI/CD * 基础架构 * 配置管理 * 监控 & 报警 * 日志 * 代码审查 * 代码覆盖率 * 测试集</summary><br><b> * CI/CD - Jenkins, Circle CI, Travis * 基础架构 - Terraform, CloudFormation * 配置管理 - Ansible, Puppet, Chef * 监控 & 报警 - Prometheus, Nagios * 日志 - Logstash, Graylog, Fluentd * 代码审查 - Gerrit, Review Board * 代码覆盖率 - Cobertura, Clover, JaCoCo * 测试集 - Robot, Serenity, Gauge </b></details> <details>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "初级", "language": "en", "created_at": "2025-07-19T19:22:02.033273"}}
{"text": "<summary>你在选择工具/技术时是怎么考虑的?</summary><br><b> 你可以使用以下一项或全部： * 成熟与尖端 * 社区规模 * 体系结构方面-代理与无代理，主控与无主控等 </b></details> <details> <summary>解释可变基础架构与不变基础架构</summary><br><b> 在可变的基础架构原则中，更改将应用到现有基础架构之上并随着时间的推移而变化 基础架构建立了变化的历史。 Ansible，Puppet和Chef这些工具 遵循可变的基础架构原则。 在不变的基础架构原则中，每项更改实际上都是新的基础架构。 所以改变 到服务器将导致新服务器而不是更新服务器。 Terraform是 遵循不变的基础架构原则的一个例子。 </b></details> <details> <summary>你熟悉什么方式来交付软件?</summary><br><b> * 存档 - 将你所有的应用文件收集到一个存档中（例如tar），并将其交付给用户。 * 打包 - 取决于操作系统，你可以使用OS软件包格式（例如，在RHEL / Fefodra中为RPM）来交付软件，并使用标准打包程序命令来安装，卸载和更新它 * 映像 - VM或容器映像，其中包已包含在其中，以便成功运行。 </b></details> <details> <summary>什么是缓存? 缓存是怎么工作的? 为什么缓存很重要?</summary><br><b> </b></details> <details> <summary>解释一下无状态和有状态</summary><br><b> </b></details> <details> <summary>什么是HTTP及其工作方式?</summary><br><b> </b></details> <details> <summary>描述一下设置某些类型的Web服务器的工作流程 (Apache, IIS, Tomact, ...)</summary><br><b> </b></details> <details> <summary>解释一下监控. 它是什么? 为什么监控是重要的?</summary><br><b> </b></details> <details>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "初级", "language": "en", "created_at": "2025-07-19T19:22:02.033306"}}
{"text": "<summary>你熟悉那些监控方法?</summary><br><b> </b></details> <a name=\"devops-advanced\"></a>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "初级", "language": "en", "created_at": "2025-07-19T19:22:02.033328"}}
{"text": "<details> <summary>告诉我你是如何执行CI / CD资源的计划容量 (如服务器, 存储, 等等.)</summary><br><b> </b></details> <details> <summary>你将如何为依赖于其他多个应用程序的应用程序构建/实现CD?</summary><br><b> </b></details> <details> <summary>你如何衡量CI / CD的质量？ 有那些你正在使用的指标吗？</summary><br><b> </b></details> <details> <summary>什么是配置漂移？ 它引起什么问题？</summary><br><b> 当配置和软件完全相同的服务器环境中的某个服务器上发生配置漂移 或服务器正在应用其他服务器无法获得的更新或配置，并且随着时间的推移，这些服务器将变为 略有不同。 这种情形可能会导致难以识别和重现的错误。 </b></details> <details> <summary>怎样处理配置漂移?</summary><br><b> </b></details> <details> <summary>你是否有跨项目变更测试的经验？ (又名交叉依赖)</summary><br><b> 注意：交叉依赖是指你对单独的项目进行了两个或多个更改，并且你希望在相互构建中对其进行测试，而不是分别测试每个更改。 </b></details> <details> <summary>在哪种情况下，你希望使用SQL?</summary><br><b> * 同类数据，预计不会发生变化 * ACID合规性很重要 </b></details>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "高级", "language": "en", "created_at": "2025-07-19T19:22:02.033390"}}
{"text": "<a name=\"jenkins-beginner\"></a>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "Jenkins", "language": "en", "created_at": "2025-07-19T19:22:02.033434"}}
{"text": "<details> <summary>什么是 Jenkins? 你用它来做什么?</summary><br><b> </b></details> <details> <summary>相比其他的竞争者 jenkins 有什么优势? 你能把jenkins 和下面的系统做一个比较吗?: * Travis * Bamboo * Teamcity * CircleCI</summary><br><b> </b></details> <details> <summary>解释以下: * Job * Build * Plugin * Slave * Executor</summary><br><b> </b></details> <details> <summary> 你在 Jenkins 用过什么插件?</summary><br><b> </b></details> <details> <summary>解释一下 CI/CD 你在 Jenkins 是怎么实现他们的 </summary><br><b> </b></details> <details> <summary>有什么类型的工作？ 你使用了哪些类型，为什么？</summary><br><b> </b></details> <details> <summary>你如何向用户报告构建结果？ 你熟悉什么那些方式？</summary><br><b> </b></details> <details> <summary>每次有更改提交，你都需要运行单元测试。 详细描述管道的环境以及每个阶段将执行的操作</summary><br><b> </b></details> <details> <summary>怎样保护 Jenkins?</summary><br><b> </b></details> <details> <summary>你能描述一些 Jenkins 最佳实践吗?</summary><br><b> </b></details> <a name=\"jenkins-advanced\"></a>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "初级", "language": "en", "created_at": "2025-07-19T19:22:02.033548"}}
{"text": "<details> <summary>如何为一个特定的构建获取多个从属?</summary><br><b> </b></details> <details> <summary>你的组织中有四个团队。 如何优先考虑每个团队的建设？ 例如，x团队的工作将始终在y团队之前运行</summary><br><b> </b></details> <details> <summary>你有部署 Jenkins 插件的经验吗? 你能描述一下吗?</summary><br><b> </b></details> <details> <summary>如果你要管理许多工作，你可能使用Jenkins UI。 你如何每周/每月管理数百个作业的创建和删除？</summary><br><b> </b></details> <details> <summary>Jenkins 有那些限制?</summary><br><b> * 测试交叉依赖关系（来自多个项目的变更） * 从任何阶段开始构建（尽管cloudbees实现了称为检查点的东西） </b></details> <details> <summary>你是如何实施从某个阶段而不是从最开始构建的选项？</summary><br><b> </b></details> <details> <summary>你曾经写过 Jenkins 脚本吗? 如果有，有哪些? 分别是怎么样工作的？</summary><br><b> </b></details>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "高级", "language": "en", "created_at": "2025-07-19T19:22:02.033607"}}
{"text": "<a name=\"cloud-beginner\"></a>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "Cloud", "language": "en", "created_at": "2025-07-19T19:22:02.033626"}}
{"text": "<details> <summary>云计算的优势是什么？ 至少列出3个优势</summary><br><b> </b></details> <details> <summary>他们分别是那种类型的云计算?</summary><br><b> IAAS PAAS SAAS </b></details> <details> <summary>解释一下以下云计算部署： * Public * Hybrid * Private</summary><br><b> </b></details>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "初级", "language": "en", "created_at": "2025-07-19T19:22:02.033650"}}
{"text": "<a name=\"aws-beginner\"></a>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "AWS", "language": "en", "created_at": "2025-07-19T19:22:02.033666"}}
{"text": "<details> <summary>解释以下 * 可用区 * 区域 * 边缘位置</summary><br><b> </b> <b> AWS区域是遍布全球不同地理位置的数据中心，每个区域彼此完全独立。 在每个区域内，有多个隔离的位置，称为可用区。 多个可用区可确保其中之一发生故障时具有高可用性。 边缘位置基本上是内容传递网络，它缓存数据并确保较低的延迟和更快地传递给任何位置的用户。 他们位于世界主要城市。 </b> </details>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "全局基础设施", "language": "en", "created_at": "2025-07-19T19:22:02.033689"}}
{"text": "<details> <summary>解释一下什么是S3，以及它用来干嘛</summary><br> <b> S3代表3 S（Simple Storage Service）。 S3是一种对象存储服务，它是快速，可伸缩和持久的。 S3使客户能够上传，下载或存储最大5 TB的文件或对象。 同时每个文件的最大大小为5 GB（如果大小超过5 GB，则分段上传）。 </b> </details> <details> <summary>什么是存储桶?</summary><br><b> S3存储桶是一种资源，类似于文件系统中的文件夹，并且允许存储由数据及其元数据组成的对象。 </b></details> <details> <summary>对还是错? 存储桶必须全局唯一</summary><br><b> True </b></details> <details> <summary>S3 中 包含哪些对象 ? * 另一种问法: 在对象上下文中解释键，值，版本ID和元数据</summary><br><b> </b></details> <details> <summary>解释一下数据一致性</summary><br><b> </b></details> <details> <summary>你可以在s3上托管动态网站吗？ 静态网站呢?</summary><br><b> </b></details> <details> <summary>你在S3上下文中采取了哪些安全措施?</summary><br><b> </b></details>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "S3", "language": "en", "created_at": "2025-07-19T19:22:02.033780"}}
{"text": "<details> <summary>解释一下什么是CloudFront及其用途</summary><br><b> </b></details> <details> <summary>解释以下 * 域 * 边缘位置 * 分布</summary><br><b> </b></details> <details> <summary>CDN用户可以使用哪些交付方式?</summary><br><b> </b></details> <details> <summary>对还是错? 在TTL的生命周期内缓存对象</summary><br><b> </b></details>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "CloudFront", "language": "en", "created_at": "2025-07-19T19:22:02.033821"}}
{"text": "<details> <summary>你创建了哪种类型的实例?</summary><br><b> </b></details> <details> <summary>如何为给定的EC2实例增加RAM?</summary><br><b> 停止实例，使其实例类型与所需的RAM匹配，然后启动实例。 </b></details> <details> <summary>什么是 AMI?</summary><br><b> </b></details> <details> <summary>EC2实例有多少个存储选项?</summary><br><b> </b></details> <details> <summary>EC2实例停止或终止时会发生什么?</summary><br><b> </b></details> <details> <summary>什么是安全组?</summary><br><b> </b></details> <details> <summary>如何将实例迁移到另一个可用性区域?</summary><br><b> </b></details> <details> <summary>什么是安全组?</summary><br><b> </b></details> <details> <summary>什么是竞价型实例?</summary><br><b> </b></details>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "EC2", "language": "en", "created_at": "2025-07-19T19:22:02.033861"}}
{"text": "<a name=\"network-beginner\"></a>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "网络", "language": "en", "created_at": "2025-07-19T19:22:02.033879"}}
{"text": "<details> <summary>什么是以太网?</summary><br><b> </b></details> <details> <summary>什么是一个 MAC 地址? 它用来干嘛?</summary><br><b> </b></details> <details> <summary>什么时候这个 MAC 地址会被用来使用?: ff:ff:ff:ff:ff:ff</summary><br><b> </b></details> <details> <summary>什么是一个 IP 地址? 什么是子网?</summary><br><b> </b></details> <details> <summary>解释一下 OSI 模型. 有那些层? 每层负责什么?</summary><br><b> 应用层：用户端（HTTP在这一层） 表示层：在应用程序层实体之间建立上下文（加密在这一层） 会话层：建立，管理和终止连接 传输层：将可变长度的数据序列从源传输到目标主机（TCP和UDP在这一层） 网络层：将数据报从一个网络传输到另一个网络（IP 层在这里） 数据链接层：提供两个直接连接的节点之间的链接（MAC在这一层） 物理层：数据连接的电气和物理规格（比特在这一一层） </b></details> <details> <summary>你熟悉哪些传送方案?</summary><br><b> 单位广播：一对一通信，其中有一个发送方和一个接收方。 广播：向网络中的所有人发送消息。 地址ff：ff：ff：ff：ff：ff：ff用于广播。 使用广播的两个常见协议是ARP和DHCP。 组播：向一组订户发送消息。 它可以是一对多或多对多。 </b></details> <details> <summary>什么是 CSMA/CD? 在现代以太网中有使用吗?</summary><br><b> CSMA / CD代表载波侦听多路访问/冲突检测。 它的主要重点是管理对共享媒体/总线的访问，在该共享媒体/总线上，在给定的时间点只能传输一个主机。 CSMA / CD算法： 1. 在发送帧之前，它会检查其他主机是否已经在发送帧。 2. 如果没有人发送，它将开始发送帧。 3. 如果两个主机同时传输，则发生冲突。 4. 双方主机均停止发送帧，并向每个人发送“干扰信号”，通知每个人发生冲突 5.", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "初级", "language": "en", "created_at": "2025-07-19T19:22:02.034107"}}
{"text": "他们正在等待随机时间，然后再次发送 6. 一旦每个主机等待一段随机时间，他们就会尝试再次发送帧 </b></details> <details> <summary>描述以下网络设备及其之间的区别： * 路由器 * 交换机 * 集线器</summary><br><b> </b></details> <details> <summary>什么是 NAT?</summary><br><b> </b></details> <details> <summary>TCP 和 UDP 两者之间有那些区别?</summary><br><b> </b></details> <details> <summary>TCP 是怎样工作的? 什么是 3 次握手?</summary><br><b> </b></details> <details> <summary>什么是 ARP? 它是怎么工作的?</summary><br><b> </b></details> <details> <summary>什么是 TTL?</summary><br><b> </b></details> <details> <summary>什么是DHCP? 它是怎么工作的?</summary><br><b> </b></details> <details> <summary>什么是SSL 隧道? 它是怎么工作的?</summary><br><b> </b></details> <details> <summary>什么是套接字? 在哪里可以看到系统中的套接字列表?</summary><br><b> </b></details> <details> <summary>什么是IPv6? 如果我们拥有IPv4，为什么要考虑使用它?</summary><br><b> </b></details> <details> <summary>什么是VLAN?</summary><br><b> </b></details> <details> <summary>什么是MTU?</summary><br><b> </b></details> <details> <summary>什么是SDN?</summary><br><b> </b></details> <details> <summary>什么是ICMP?", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "初级", "language": "en", "created_at": "2025-07-19T19:22:02.034159"}}
{"text": "它有什么用途?</summary><br><b> </b></details> <details> <summary>什么是NAT? 它是怎么工作的?</summary><br><b> </b></details> <a name=\"network-advanced\"></a>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "初级", "language": "en", "created_at": "2025-07-19T19:22:02.034181"}}
{"text": "<details> <summary>解释一下生成树协议 (STP)</summary><br><b> </b></details> <details> <summary>什么是链路聚合? 为什么使用它?</summary><br><b> </b></details> <details> <summary>什么是非对称路由? 怎样处理它?</summary><br><b> </b></details> <details> <summary>你熟悉哪些叠加（隧道）协议?</summary><br><b> </b></details> <details> <summary>什么是GRE? 它是怎么工作的?</summary><br><b> </b></details> <details> <summary>什么是VXLAN? 它是怎么工作的?</summary><br><b> </b></details> <details> <summary>什么是SNAT?</summary><br><b> </b></details> <details> <summary>解释一下 OSPF</summary><br><b> </b></details> <details> <summary>解释一下 Spine & Leaf</summary><br><b> </b></details> <details> <summary>使用海明码, 100111010001101 会编码成什么码?</summary><br><b> 00110011110100011101 </b></details>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "高级", "language": "en", "created_at": "2025-07-19T19:22:02.034241"}}
{"text": "<a name=\"linux-beginner\"></a>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "Linux", "language": "en", "created_at": "2025-07-19T19:22:02.034260"}}
{"text": "<details> <summary>你有那些 Linux 经验? 当你可以在多个操作系统上设置应用程序时，你希望在哪个操作系统上进行设置以及为什么?</summary><br><b> </b></details> <details> <summary>解释以下每个命令的作用，并举例说明如何使用它 * ls * rm * rmdir (你能使用 <code>rm</code>完成同样的结果吗?) * grep * wc * curl * touch * man * nslookup or dig * df</summary><br><b> </b></details> <details> <summary>运行命令 <code>df</code> 你会得到 \"找不到命令\". 可能出现什么问题以及如何修复它?</summary><br><b> </b></details> <details> <summary>如何确保服务将在你选择的操作系统上启动?</summary><br><b> </b></details> <details> <summary>你如何定期安排任务?</summary><br><b> 你能使用命令 <code>cron</code> 和 <code>at</code>. 对于cron，使用以下格式安排任务： <minute> <hour> <day of month> <month> <day of week> <command to execute> 任务存储在cron文件中。 </b></details> <details> <summary>你过去是否安排了任务？ 什么样的任务？</summary><br><b> 通常，你将安排批处理作业。 </b></details>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "初级", "language": "en", "created_at": "2025-07-19T19:22:02.034374"}}
{"text": "<details> <summary>怎样改变一个文件的权限?</summary><br><b> 使用 `chmod` 命令. </b></details> <details> <summary>下面的权限意味着什么?: * 777 * 644 * 750</summary><br><b> 777 - 所有人有读和写和可执行权限（意味着你很懒） 644 - 拥有者有读和写的权限、其他人只有读权限 750 - 拥有者有所有权限, 组成员可以读和执行权限、其他人没有权限 </b></details> <details> <summary>解释一下什么是setgid, setuid 和 sticky bit</summary><br><b> </b></details> <details> <summary>如何在不向其提供登录系统功能的情况下将新用户添加到系统?</summary><br><b> * adduser user_name --shell=/bin/false --no-create-home </b></details> <details> <summary>在使用systemd的系统上，如何显示日志?</summary> * journalctl </b></details>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "权限", "language": "en", "created_at": "2025-07-19T19:22:02.034442"}}
{"text": "<details> <summary>你正在使用什么进行故障排除和调试 <b>网络</b> 问题?</summary><br><b> <code>dstat -t</code> 非常适合辨别网络和磁盘问题。 <code>netstat -tnlaup</code> 可用于查看哪些进程在哪些端口上运行。 <code>lsof -i -P</code> 可以用于与netstat相同的目的。 <code>ngrep -d any metafilter</code> 用于将正则表达式与数据包的载荷相匹配。 <code>tcpdump</code> 用于捕获数据包 <code>wireshark</code> 与tcpdump相同的概念，但带有GUI（可选）。 </b></details> <details> <summary>你正在使用什么进行故障排除和调试 <b>磁盘 & 文件系统</b> 问题?</summary><br><b> <code>dstat -t</code> 非常适合辨别网络和磁盘问题。 <code>opensnoop</code> 可以用来查看正在系统上打开哪些文件（实时）。 </b></details> <details> <summary>你正在使用什么进行故障排除和调试 <b>进程</b> 问题?</summary><br><b> <code>strace</code> 非常适合了解你的程序的功能。 它打印你的程序执行的每个系统调用。 </b></details> <details> <summary>你正在使用什么来调试CPU相关问题?</summary><br><b> <code>top</code> 显示每个进程消耗多少CPU占比 <code>perf</code> 是采样分析器的理想选择，通常来说，找出哪些CPU周期被“浪费”了 <code>flamegraphs</code> 非常适合CPU消耗可视化（http://www.brendangregg.com/flamegraphs.html） </b></details> <details> <summary>你收到一个电话，说“我的系统运行缓慢” - 你将如何处理?</summary><br><b> 1. 使用<code>top</code>检查是否有任何资源消耗你的CPU或RAM。", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "调试", "language": "en", "created_at": "2025-07-19T19:22:02.034776"}}
{"text": "2. 运行<code>dstat -t</code>来检查它是否与磁盘或网络有关。 3. 使用<code>iostat</code>检查 I/O 统计信息 </b></details> <details> <summary>什么是Linux内核模块以及如何加载新模块?</summary><br><b> </b></details> <details> <summary>什么是KVM?</summary><br><b> </b></details> <details> <summary>SSH和SSL之间的区别是什么?</summary><br><b> </b></details> <details> <summary>SSH端口转发是什么?</summary><br><b> </b></details> <details> <summary>解释重定向</summary><br><b> </b></details> <details> <summary>什么是通配符？ 你能举一个使用它们的例子吗？</summary><br><b> </b></details> <details> <summary>我们在以下每个命令中使用grep做什么？ * <code>grep '[0-9]\\{1,3\\}\\.[0-9]\\{1,3\\}\\.[0-9]\\{1,3\\}\\.[0-9]\\{1,3\\}' some_file</code> * <code>grep -E \"error|failure\" some_file</code> * <code>grep '[0-9]$' some_file</code> </summary><br><b> 1. 一个 IP 地址 2. 单词 \"error\" 或 \"failure\" 3. 以数字结尾的行 </b></details> <details> <summary>告诉我你了解所有有关Linux启动过程的知识</summary><br><b> </b></details> <details> <summary>什么是退出码? 你熟悉那些退出码?</summary><br><b> 退出码（或返回码）表示子进程返回其父进程的码。 0是退出码，表示成功，而大于1的码表示错误。 每个数字都有不同的含义，具体取决于应用程序的开发方式。", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "调试", "language": "en", "created_at": "2025-07-19T19:22:02.034811"}}
{"text": "我认为这是一篇可以了解更多的好博客：https://shapeshed.com/unix-exit-codes </b></details> <details> <summary>软链接和硬链接之间的区别是什么?</summary><br><b> 硬链接是使用相同inode的相同文件。 软链接是使用不同inode的另一个文件的快捷方式。 可以在不同的文件系统之间创建软链接，而硬链接只能在同一文件系统内创建。 </b></details> <details> <summary>什么是交换分区? 它用来做什么的?</summary><br><b> </b></details> <details> <summary>你试图创建一个新文件，但显示“文件系统已满”。 你使用df检查是否有可用空间，你看到还有20％的空间。 可能是什么问题?</summary><br><b> </b></details> <details> <summary>你对LVM有什么了解?</summary><br><b> </b></details> <details> <summary>解释以下关于LVM: * PV * VG * LV</summary><br><b> </b></details> <details> <summary>RAID用于什么用途？ 你能否解释RAID 0、1、5和10之间的区别？</summary><br><b> </b></details> <details> <summary>什么是懒卸载?</summary><br><b> </b></details> <details> <summary>修复以下命令： * sed \"s/1/2/g' /tmp/myFile * find . -iname \\*.yaml -exec sed -i \"s/1/2/g\" {} ;</summary><br><b> </b></details> <details> <summary>解释以下每个路径中存储的内容以及是否有一些独特之处</summary><br><b> * /tmp * /var/log * /bin * /proc * /usr/local </b></details> <details> <summary>你能在 /etc/services 找到什么", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "调试", "language": "en", "created_at": "2025-07-19T19:22:02.034856"}}
{"text": "</summary><br><b> </b></details> <details> <summary>什么是 chroot?</summary><br><b> </b></details>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "调试", "language": "en", "created_at": "2025-07-19T19:22:02.034878"}}
{"text": "<details> <summary>如何在后台运行进程以及为什么要优先运行?</summary><br><b> 你可以通过在命令末尾指定＆来实现。至于为什么，因为一些命令/过程会占用大量的时间来完成执行或永远运行 </b></details> <details> <summary>你如何查找特定进程占用的内存量?</summary><br><b> </b></details> <details> <summary>运行“ kill”时使用什么信号 <process id>'?</summary><br><b> 默认信号为SIGTERM（15）。 该信号可以优雅地终止进程，这意味着它可以保存当前状态配置。 </b></details> <details> <summary>你熟悉哪些信号?</summary><br><b> SIGTERM - 终止进程的默认信号 SIGHUP - 常用用法是重新加载配置 SIGKILL - 不能捕获或忽略的信号 运行 `kill -l` 查看所有可用的信号 </b></details> <details> <summary>什么是 trap?</summary><br><b> </b></details> <details> <summary>当你按下Ctrl + C会发生什么?</summary><br><b> </b></details> <details> <summary>什么是守护程序?</summary><br><b> </b></details> <details> <summary>Linux中进程的可能状态是什么?</summary><br><b> Running（运行态） Waiting (等待态)) Stopped（暂停态） Terminated（终止态） Zombie（假死态） </b></details> <details> <summary>什么是僵尸进程? 你是如何避免的?</summary> </b></details> <details> <summary>什么是初始进程?</summary><br><b> </b></details> <details> <summary>如何更改进程的优先级？ 你为什么想这么做?</summary><br><b> </b></details> <details>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "进程", "language": "en", "created_at": "2025-07-19T19:22:02.035116"}}
{"text": "<summary>你能解释一下网络进程/连接如何建立以及如何终止?</summary><br></b> </b></details> <details> <summary>什么是系统调用？ 你熟悉哪些系统调用?</summary><br><b> </b></details> <details> <summary><code>strace</code> 做什么的?</summary><br><b> </b></details> <details> <summary>查找所有以“ .yml”结尾的文件，并替换每个文件中的2分之一的数字</summary><br><b> ind /some_dir -iname \\*.yml -print0 | xargs -0 -r sed -i \"s/1/2/g\" </b></details> <details> <summary>如何查看系统有多少可用内存？ 如何检查每个进程的内存消耗?</summary><br><b> 你可以使用命令<code>top</code> 和 <code>free</code> </b></details> <details> <summary>你如何将一个50行的文件拆分为两个25行的文件?</summary><br><b> 你可以使用 <code>split</code> 命令就像这样<code>split -l 25 some_file</code> </b></details> <details> <summary>什么是文件描述符? 你熟悉那些文件描述符?</summary><br><b> Kerberos 文件描述符，也称为文件处理程序，是一个唯一的编号，用于标识操作系统中的打开文件。 在 Linux (和 Unix) 前三个描述符是: * 0 - 输入的默认数据流 * 1 - 输出的默认数据流 * 2 - 与错误相关的输出的默认数据流 这有一篇好的文章关于这个主题的: https://www.computerhope.com/jargon/f/file-descriptor.htm </b></details> <details> <summary>什么是 inode?</summary><br><b>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "进程", "language": "en", "created_at": "2025-07-19T19:22:02.035139"}}
{"text": "Linux中的每个文件（和目录）都有一个索引节点，即与文件相关的存储元数据信息的数据结构 ，例如文件的大小，所有者，权限等。 </b></details> <details> <summary>如何列出活动的网络连接?</summary><br><b> </b></details> <details> <summary>什么是NTP? 它是用来干什么的?</summary><br><b> </b></details> <details> <summary>什么是SELiunx?</summary><br><b> </b></details> <details> <summary>什么是Kerberos?</summary><br><b> </b></details> <details> <summary>什么是nftables?</summary><br><b> </b></details> <details> <summary>firewalld守护程序负责什么?</summary><br><b> </b></details>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "进程", "language": "en", "created_at": "2025-07-19T19:22:02.035159"}}
{"text": "<details> <summary>什么是网络名称空间? 它用来干什么的?</summary><br><b> </b></details> <details> <summary>你如何将Linux服务器变成路由器?</summary><br><b> </b></details> <details> <summary>什么是路由表? 你是怎样查看它的?</summary><br><b> </b></details> <details> <summary>什么是数据包嗅探器？ 你过去曾经使用过吗？ 如果是，你使用了哪些数据包嗅探器以及用于什么目的？</summary><br><b> </b></details>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "Network", "language": "en", "created_at": "2025-07-19T19:22:02.035187"}}
{"text": "<details> <summary>文件 <code>/etc/resolv.conf</code> 用来做什么的? 它包含那些内容?</summary><br><b> </b></details> <details> <summary>什么是 \"A record\"?</summary><br><b> </b></details> <details> <summary>什么是 PTR 记录?</summary><br><b> A记录将域名指向IP地址，而PTR记录则相反，并将IP地址解析为域名。 </b></details> <details> <summary>什么是 MX 记录?</summary><br><b> </b></details> <details> <summary>DNS是使用TCP还是UDP?</summary><br><b> </b></details>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "DNS", "language": "en", "created_at": "2025-07-19T19:22:02.035222"}}
{"text": "<details> <summary>你有打包经验吗? 你能解释一下它是怎么工作的</summary><br><b> </b></details> <details> <summary>RPM: 解释特定格式（应包括什么内容）</summary><br><b> </b></details> <details> <summary>你如何列出包内容?</summary><br><b> </b></details> <a name=\"linux-advanced\"></a>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "Packaging", "language": "en", "created_at": "2025-07-19T19:22:02.035247"}}
{"text": "<details> <summary>当你执行 <code>ls</code>发生了什么? 提供一个详细的答案</summary><br><b> </b></details> <details> <summary>你能描述流程的创建方式吗?</summary><br><b> </b></details> <details> <summary>以下块做什么?: ``` open(\"/my/file\") = 5 read(5, \"file content\") ``` </summary><br><b> 系统调用正在读 <code>/my/file</code>文件 以及 5 是文件描述符数字. </b></details> <details> <summary>进程和线程的区别是什么?</summary><br><b> </b></details>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "高级", "language": "en", "created_at": "2025-07-19T19:22:02.035284"}}
{"text": "<details> <summary>当你运行 <code>ip a</code> 你看到一个设备叫做 'lo'. 它是什么以及为什么我们需要它?</summary><br><b> </b></details> <details> <summary><code>traceroute</code> 命令做什么的? 它是怎么工作的?</summary><br><b> </b></details> <details> <summary>什么是网络绑定? 你熟悉什么类型?</summary><br><b> </b></details> <details> <summary>如何链接两个单独的网络名称空间，以便你可以从另一个命名空间ping一个命名空间上的接口?</summary><br><b> </b></details> <details> <summary>什么是cgroup？ 在什么情况下你会使用它们？</summary><br><b> </b></details> <details> <summary>如何创建一定大小的文件?</summary><br><b> 这有一些方式去做: * dd if=/dev/urandom of=new_file.txt bs=2MB count=1 * truncate -s 2M new_file.txt * fallocate -l 2097152 new_file.txt </b></details> <details> <summary>以下系统调用之间有什么区别?: exec(), fork(), vfork() and clone()?</summary><br><b> </b></details> <details> <summary>解释流程描述符和任务结构</summary><br><b> </b></details> <details> <summary>线程和进程之间有什么区别?</summary><br><b> </b></details> <details> <summary>解释内核线程</summary><br><b> </b></details> <details> <summary>使用套接字系统调用时会发生什么?</summary><br><b> 这有一篇好的文章关于这个主题的:", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "Network", "language": "en", "created_at": "2025-07-19T19:22:02.035419"}}
{"text": "https://ops.tips/blog/how-linux-creates-sockets </b></details>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "Network", "language": "en", "created_at": "2025-07-19T19:22:02.035442"}}
{"text": "<a name=\"ansible-beginner\"></a>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "Ansible", "language": "en", "created_at": "2025-07-19T19:22:02.035455"}}
{"text": "<details> <summary>在Ansible中描述以下每个组件，包括它们之间的关系： * Task * Module * Play * Playbook * Role</summary><br><b> 任务 – 调用特定的Ansible模块 模块 – Ansible在你自己的主机或远程主机上执行的实际代码单元。 模块按类别（数据库，文件，网络等）编制索引，也称为任务插件。 Play – 在给定主机上执行的一个或多个任务 Playbook – 一个或多个Play。 每个Play可以在相同或不同的主机上执行 角色 – Ansible角色使你可以基于某些功能/服务对资源进行分组，以便可以轻松地重用它们。 在角色中，你具有变量，默认值，文件，模板，处理程序，任务和元数据的目录。 然后，你只需在剧本中指定角色即可使用该角色。 </b></details> <details> <summary>你熟悉哪些Ansible最佳做法? 至少列出 3 条</summary><br><b> </b></details> <details> <summary>什么是清单文件以及如何定义一个?</summary><br><b> 清单文件定义了在其上执行Ansible任务的主机和/或主机组。 一个清单文件的例子 192.168.1.2 192.168.1.3 192.168.1.4 [web_servers] 190.40.2.20 190.40.2.21 190.40.2.22 </b></details> <details> <summary>什么是动态清单文件? 什么时候使用?</summary><br><br> 动态清单文件可跟踪来自一个或多个来源（例如云提供商和CMDB系统）的主机。 应该使用当使用外部源时，尤其是在环境中的主机正在自动启动和关闭，而无需跟踪这些源中的所有更改。 </b></details> <details> <summary>你只想在特定的次要操作系统上运行Ansible Play，你将如何实现?</summary><br><b> </b></details> <details> <summary>写任务创建目录 ‘/tmp/new_directory’</summary><br><b> ``` - name: Create a new", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "初级", "language": "en", "created_at": "2025-07-19T19:22:02.035928"}}
{"text": "directory file: path: \"/tmp/new_directory\" state: directory ``` </b></details> <details> <summary>接下来的Play会有什么结果?</summary><br><b> ``` --- - name: Print information about my host hosts: localhost gather_facts: 'no' tasks: - name: Print hostname debug: msg: \"It's me, {{ ansible_hostname }}\" ``` 提供完成的代码后，请始终进行彻底检查。 如果你的回答是“这将失败”，那么你是对的。 我们正在使用一个事实（ansible_hostname）， 这是我们正在运行的主机上收集到的信息。 但是在这种情况下，我们禁用了事实收集（gather_facts：no），因此该变量将是未定义的，这将导致失败。 </b></details> <details> <summary>如果系统上存在文件 \"/tmp/mario\"，则编写 playbook 以在所有主机上安装 \"zlib\" 和 \"vim\" .</summary><br><b> ``` --- - hosts: all vars: mario_file: /tmp/mario package_list: - 'zlib' - 'vim' tasks: - name: Check for mario file stat: path: \"{{ mario_file }}\" register: mario_f - name: Install zlib and vim if mario file exists become: \"yes\" package: name: \"{{ item }}\" state: present with_items: \"{{ package_list }}\" when: mario_f.stat.exists ``` </b></details> <details> <summary>编写一个 playbook ，将文件 \"/tmp/system_info\"", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "初级", "language": "en", "created_at": "2025-07-19T19:22:02.035961"}}
{"text": "部署到除控制器组之外的所有主机上，并具有以下内容：</summary><br><b> ``` 我是 <HOSTNAME> 我的操作系统是 <OS> ``` 替换 <HOSTNAME> 和 <OS> 以及正在运行的特定主机的实际数据 The playbook 部署system_info文件 ``` --- - name: Deploy /tmp/system_info file hosts: all:!controllers tasks: - name: Deploy /tmp/system_info template: src: system_info.j2 dest: /tmp/system_info ``` The content of the system_info.j2 template ```", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "初级", "language": "en", "created_at": "2025-07-19T19:22:02.035983"}}
{"text": "I'm {{ ansible_hostname }} and my operating system is {{ ansible_distribution }} ``` </b></details> <details> <summary>变量 \"whoami\" 在以下位置定义： * 角色默认设置 -> whoami: mario * 额外的变量（使用 -e 传递给Ansible CLI的变量）-> whoami: toad * 托管事实 -> whoami: luigi * 广告资源变量（与哪种类型无关）-> whoami: browser 根据可变优先级，将使用哪个？ </summary><br><b> 正确的答案是 ‘toad’。 变量优先级是关于变量在不同位置设置时如何相互覆盖的。 如果你到目前为止还没有体验过，我相信你会在某个时候确定的，这使它成为一个有用的话题。 在我们的问题上下文中，顺序将是额外的var（始终覆盖任何其他变量）-> 主机事实 -> 库存变量 -> 角色默认值（最弱）。 完整的列表可以在上面的链接中找到。 另外，请注意Ansible 1.x和2.x之间存在显着差异。 </b></details> <details> <summary>对于以下每个语句，确定对还是错: * 模块是任务的集合 * 最好使用shell或命令而不是特定的模块 * 主机事实会覆盖 play 变量 * 角色可能包括以下内容：var，meta 和 handler * 通过从外部来源提取信息来生成动态清单 * 最佳做法是使用2个空格而不是4个缩进 * 用来触发处理程序的“通知” * \"hosts：all：！controllers\"表示 \"仅在控制器组主机上运行 </summary><br><b> </b></details> <details> <summary>什么是ansible-pull? 与ansible-playbook相比有何不同?</summary><br><b> </b></details> <a name=\"ansible-advanced\"></a>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "{{ ansible_managed }}", "language": "en", "created_at": "2025-07-19T19:22:02.036140"}}
{"text": "<details> <summary>什么是过滤器？ 你有写过滤器的经验吗?</summary><br><b> </b></details> <details> <summary>编写过滤器来转化字符串大写</summary><br><b> <code> def cap(self, string): return string.capitalize() </code> </b></details> <details> <summary>你如何测试基于Ansible的项目?</summary><br><b> </b></details> <details> <summary>什么是回调插件？ 使用回调插件可以实现什么？</summary><br><b> </b></details>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "高级", "language": "en", "created_at": "2025-07-19T19:22:02.036189"}}
{"text": "<a name=\"terraform-beginner\"></a>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "Terraform", "language": "en", "created_at": "2025-07-19T19:22:02.036232"}}
{"text": "<details> <summary>你能解释一下什么是Terraform? 它是怎么工作的?</summary><br><b> 读 [这里](https://www.terraform.io/intro/index.html#what-is-terraform-) </b></details> <details> <summary>什么使基础架构代码受益?</summary><br><b> - 供应，修改和删除基础架构的全自动过程 - 基础结构的版本控制，可让你快速回滚到以前的版本 - 通过自动化测试和代码审查来验证基础架构的质量和稳定性 - 减少基础架构任务的重复性 </b></details> <details> <summary>为什么选择Terraform，而不选择其他技术？ （例如，Ansible，Puppet，CloufFormation)</summary><br><b> 常见的错误答案是说 Ansible 和 Puppet 是配置管理工具而 Terraform 是置备工具。 尽管从技术上讲是正确的，但这并不意味着 Ansible 和 Puppet 不能 用于配置基础结构。 另外，这根本没有解释为什么应该在 CloudFormation上 使用 Terraform。 Terraform与其他工具相比的优势： * 它遵循不变的基础架构方法，该方法具有避免配置随时间变化的优势 * Ansible和Puppet具有更多的过程性（你提到了每个步骤要执行的操作），而Terraform是声明性的，因为你描述的是总体所需的状态，而不是每个资源或任务的状态。 你可以举一个在每个工具中从1台服务器转到2台服务器的示例。 在terrform中，你指定2，在Ansible和puppet中，你仅需配置1个其他服务器，因此你需要明确确保仅配置另一台服务器。 </b></details> <details> <summary>解释什么是\"Terraform configuration\"</summary><br><b> </b></details> <details> <summary>解释以下每个: * Provider * Resource * Provisioner </summary> </b></details> <details>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "初级", "language": "en", "created_at": "2025-07-19T19:22:02.036447"}}
{"text": "<summary><code>terraform.tfstate</code> 文件用来做什么?</summary><br><b> 它跟踪创建的资源的ID，以便Terraform知道它正在管理什么。 </b></details> <details> <summary>解释以下命令的作用: * <code>terraform init</code> * <code>terraform plan</code> * <code>terraform validate</code> * <code>terraform apply</code> </summary><br><b> <code>terraform init</code> 扫描你的代码以查明你正在使用哪些提供程序并下载它们。 <code>terraform plan</code> 可以让你在实际执行操作之前先查看terraform即将执行的操作。 <code>terraform apply</code> 将提供指定的.tf文件资源。 </b></details> <details> <summary>如何记下一个由外部源或者通过 <code>terraform apply</code>改变的变量?</summary><br><b> 你用这种方式: <code>variable “my_var” {}</code> </b></details> <details> <summary>举例说明几种Terraform最佳实践</summary><br><b> </b></details> <details> <summary>解释一下隐式和显式依赖项在Terraform中如何工作</summary><br><b> </b></details> <details> <summary>什么是<code>local-exec</code> and <code>remote-exec</code> in the context of provisioners?</summary><br><b> </b></details> <details> <summary>什么是\"tainted 资源\"?</summary><br><b> 这是成功创建的资源，但在配置期间失败。 Terraform将失败，并将该资源标记为“tainted”。", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "初级", "language": "en", "created_at": "2025-07-19T19:22:02.036471"}}
{"text": "</b></details> <details> <summary><code>terraform taint</code> 做了什么?</summary><br><b> </b></details> <details> <summary>Terraform支持哪些类型的变量?</summary><br><b> Strimg Integer Map List </b></details> <details> <summary>什么是输出变量以及 <code>terraform output</code> 做了什么?</summary><br><b> </b></details> <details> <summary>解释 Modules</summary> </b></details> <details> <summary>什么是 Terraform Registry?</summary><br><b> </b></details> <a name=\"terraform-advanced\"></a>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "初级", "language": "en", "created_at": "2025-07-19T19:22:02.036491"}}
{"text": "<details> <summary>解释 \"Remote State\". 什么时候使用它以及如何使用它?</summary><br><b> </b></details> <details> <summary>解释 \"State Locking\"</summary><br><b> </b></details>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "高级", "language": "en", "created_at": "2025-07-19T19:22:02.036514"}}
{"text": "<a name=\"docker-beginner\"></a>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "Docker", "language": "en", "created_at": "2025-07-19T19:22:02.036529"}}
{"text": "<details> <summary>什么是Docker? 你用它做什么?</summary><br><b> </b></details> <details> <summary>容器与VM有何不同?</summary><br><b> 容器和虚拟机之间的主要区别是容器使你可以虚拟化 操作系统上有多个工作负载，而对于VM，则将硬件虚拟化为 在多台计算机上运行各自的操作系统。 </b></details> <details> <summary>在哪种情况下，你将使用容器，而在哪种情况下，则更喜欢使用虚拟机?</summary><br><b> 在以下情况下，你应该选择虚拟机： * 你需要运行一个需要操作系统所有资源和功能的应用程序 * 你需要完全隔离和安全 在以下情况下，你应该选择容器： * 你需要快速启动的轻量级解决方案 * 运行单个应用程序的多个版本或实例 </b></details> <details> <summary>解释一下 Docker 架构</summary><br><b> </b></details> <details> <summary>详细描述一下当运行`docker run hello-world`时背后发生了什么?</summary><br><b> Docker CLI 将你的请求传递给Docker守护程序。 Docker 守护程序从 Docker Hub 下载映像 Docker 守护程序使用下载的映像创建一个新容器 Docker 守护程序将输出从容器重定向到 Docker CLI，后者将其重定向到标准输出 </b></details> <details> <summary>你怎样运行容器?</summary><br><b> </b></details> <details> <summary>你熟悉那些与容器相关的最佳实践?</summary><br><b> </b></details> <details> <summary>`docker commit` 干什么的? 什么时候需要使用它?</summary><br><b> </b></details> <details> <summary>你如何将数据从一个容器转移到另一个容器?</summary><br><b> </b></details> <details>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "初级", "language": "en", "created_at": "2025-07-19T19:22:02.036646"}}
{"text": "<summary>容器存在时容器的数据会发生什么?</summary><br><b> </b></details> <details> <summary>解释以下每个命令的作用 * docker run * docker rm * docker ps * docker build * docker commit</summary><br><b> </b></details> <details> <summary>如何删除未运行的旧容器?</summary><br><b> </b></details>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "初级", "language": "en", "created_at": "2025-07-19T19:22:02.036667"}}
{"text": "<details> <summary>什么是 Dockerfile</summary><br><b> </b></details> <details> <summary>Dockerfile中 ADD 和 COPY 之间的区别是什么?</summary><br><b> </b></details> <details> <summary>Dockerfile中 CMD 和 RUN 之间的区别是什么?</summary><br><b> </b></details> <details> <summary>解释一下什么是 Docker compose 以及它用来做什么</summary><br><b> </b></details> <details> <summary>Docker compose，Docker swarm 和 Kubernetes 有什么区别?</summary><br><b> </b></details> <details> <summary>解释 Docker interlock</summary><br><b> </b></details> <details> <summary>Docker Hub 和 Docker Cloud 之间的区别是什么?</summary><br><b> Docker Hub是一个本地 Docker 注册表服务，可让你运行 pull 和 push 命令以从 Docker Hub 安装和部署 Docker映像。 Docker Cloud构建在Docker Hub之上，因此Docker Cloud提供了 与Docker Hub相比，你拥有更多的可选/功能。 一个例子是 群管理，这意味着你可以在Docker Cloud中创建新的群。 </b></details> <details> <summary>存储 Docker 镜像的位置在哪里?</summary><br><b> </b></details> <details> <summary>解释一下镜像层</summary><br><b> </b></details> <a name=\"docker-advanced\"></a>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "Dockerfile", "language": "en", "created_at": "2025-07-19T19:22:02.036830"}}
{"text": "<details> <summary>你如何在Docker中管理持久性存储?</summary><br><b> </b></details> <details> <summary>如何从容器内部连接到容器运行所在的主机的本地主机? </summary><br><b> </b></details> <details> <summary>如何将文件从Docker容器复制到主机，反之亦然?</summary><br><b> </b></details>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "高级", "language": "en", "created_at": "2025-07-19T19:22:02.036915"}}
{"text": "<a name=\"kubernetes-beginner\"></a>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "Kubernetes", "language": "en", "created_at": "2025-07-19T19:22:02.036935"}}
{"text": "<details> <summary>什么是Kubernetes?</summary><br><b> </b></details> <details> <summary>为什么Docker还不够？ 为什么我们需要Kubernetes?</summary><br><b> </b></details> <details> <summary>描述一下 Kuberenets 的架构</summary><br><b> </b></details> <details> <summary>你是怎样监控你的 Kuberenets?</summary><br><b> </b></details> <details> <summary>什么是kubectl? 你如何使用它?</summary><br><b> </b></details> <details> <summary>什么是kubconfig? 你用它来做什么?</summary><br><b> </b></details>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "初级", "language": "en", "created_at": "2025-07-19T19:22:02.036967"}}
{"text": "<details> <summary>你如何创建用户？ 用户信息的存储位置?</summary><br><b> </b></details> <details> <summary>你知道如何不使用 adduser/useradd 命令创建新用户吗?</summary><br><b> </b></details>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "Users", "language": "en", "created_at": "2025-07-19T19:22:02.036988"}}
{"text": "<a name=\"coding-beginner\"></a>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "Coding", "language": "en", "created_at": "2025-07-19T19:22:02.037002"}}
{"text": "<details> <summary>你更喜欢将哪种编程语言用于与DevOps相关的任务？ 为什么要专门这个?</summary><br><b> </b></details> <details> <summary>什么是面向对象编程? 它为什么如此重要?</summary><br><b> </b></details> <details> <summary>解释一下递归</summary><br><b> </b></details> <details> <summary>解释一下什么是设计模式，并详细描述其中的三个</summary><br><b> </b></details> <details> <summary>解释 big O 符号</summary><br><b> </b></details>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "初级", "language": "en", "created_at": "2025-07-19T19:22:02.037028"}}
{"text": "<details> <summary>用你想要的任何语言，编写一个函数来确定给定的字符串是否是回文串</summary><br><b> </b></details> <a name=\"coding-advanced\"></a>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "Strings", "language": "en", "created_at": "2025-07-19T19:22:02.037046"}}
{"text": "<details> <summary>给定3种设计模式。 你知道如何以你选择的任何语言实现（提供示例）这些设计模式?</summary><br><b> </b></details>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "高级", "language": "en", "created_at": "2025-07-19T19:22:02.037060"}}
{"text": "<a name=\"python-beginner\"></a>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "Python", "language": "en", "created_at": "2025-07-19T19:22:02.037073"}}
{"text": "<details> <summary>Python编程语言的一些特点是什么?</summary><br><b> ``` 1. 这是一种由 Guido Van Rosum 于1991年创建的高级通用编程语言。 2. 语言被解释为CPython（用C语言编写）最常用/维护的实现。 3. 它是强类型的。 类型系统是鸭子类型和渐进式的。 4. Python注重可读性，并使用空格/缩进代替括号{} 5. python 包管理器称为PIP“ pip install packages”，具有超过200.000可用的软件包。 6. Python 附带安装了pip和一个大的标准库，为程序员提供了许多预置的解决方案。 7. 在python中，“一切”都是一个对象。 还有许多其他特性，但这是每个python程序员都应该知道的主要特性。 ``` </b></details> <details> <summary>Python支持哪些数据类型，哪些是可变的？ 如何显示某个数据类型是可变的?</summary><br><b> 可变数据类型是: List Dictionary Set 不可变数据类型是: Numbers (int, float, ...) String Bool Tuple Frozenset 通常，你可以使用函数hash（）来检查对象的可变性，如果它是可哈希的，则是不可变的，尽管由于用户定义的对象可能是可变的且可哈希的，所以它并不总是按预期工作 </b></details> <details> <summary>什么是PEP8? 举例说明3种风格指南</summary><br><b> PEP8是Python的编码约定和样式指南的列表 5 种样式指南: 1. 将所有行限制为最多79个字符。 2. 用两个空行包围顶级函数和类定义。 3. 制作一个元素的元组时使用逗号 4. 使用空格（而不是制表符）进行缩进 5. 每个缩进级别使用4个空格 </b></details> <details> <summary>解释一下继承以及如何在Python中使用它</summary><br><b> ``` 根据定义，继承是一种机制，其中一个对象充当另一个对象的基础，并保留其所有对象属性。 因此，如果B类继承自A类，那么A类的每个特征也将在B类中提供。A类将是“基类”，B类将是“派生类”。", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "初级", "language": "en", "created_at": "2025-07-19T19:22:02.037833"}}
{"text": "当你有几个共享相同功能的类时，这很方便。 基本语法: class Base: pass class Derived(Base): pass A more forged example: class Animal: def __init__(self): print(\"and I'm alive!\") def eat(self, food): print(\"ñom ñom ñom\", food) class Human(Animal): def __init__(self, name): print('My name is ', name) super().__init__() def write_poem(self): print('Foo bar bar foo foo bar!') class Dog(Animal): def __init__(self, name): print('My name is', name) super().__init__() def bark(self): print('woof woof') michael = Human('Michael') michael.eat('Spam') michael.write_poem() bruno = Dog('Bruno') bruno.eat('bone') bruno.bark() >>> My name is Michael >>> and I'm alive! >>> ñom ñom ñom Spam >>> Foo bar bar foo foo bar! >>> My name is Bruno >>> and I'm alive! >>> ñom ñom ñom bone >>> woof woof 调用super（）会调用Base方法，因此，调用super().__init__() 就是调用 Animal__init__。 有一个称为 MetaClasses 的更高级的python功能，可帮助程序员直接控制类的创建。 ``` </b></details> <details> <summary> 什么是一个错误? 什么是一个异常? 你熟悉哪些异常类型?</summary><br><b> ``` ＃ 请注意，你通常不需要了解编译过程，而只需知道一切都来自哪里 ＃", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "初级", "language": "en", "created_at": "2025-07-19T19:22:02.037873"}}
{"text": "并给出完整的答案表明你真正知道你在说什么。 通常，每个编译过程都有两个步骤。 - 分析 - 产生代码. Analysis can be broken into: 1. 词法分析 (标记源代码) 2. 语法分析 (如果语法正确，请检查标记是否合法，tldr) for i in 'foo' ^ SyntaxError: invalid syntax We missed ':' 3. 语义分析 (上下文分析，合法语法仍然会触发错误，你是否尝试过除以0，哈希可变对象或使用未声明的函数?) 1/0 ZeroDivisionError: division by zero 这三个分析步骤负责错误处理。 第二步将负责错误，主要是语法错误，这是最常见的错误。 第三步将负责异常。 如我们所见，异常是语义错误，有许多内置的异常： ImportError ValueError KeyError FileNotFoundError IndentationError IndexError ... 你还可以具有用户定义的异常，这些异常必须直接或间接地从Exception类继承。 常见例子: class DividedBy2Error(Exception): def __init__(self, message): self.message = message def division(dividend,divisor): if divisor == 2: raise DividedBy2Error('I dont want you to divide by 2!') return dividend / divisor division(100, 2) >>> __main__.DividedBy2Error: I dont want you to divide by 2! ``` </b></details> <details> <summary>解释 异常处理以及如何在Python中使用它</summary><br><b> </b></details> <details> <summary>编写一个可以恢复字符串的程序（例如，pizza -> azzip）</summary><br><b> ``` 最简单的是 str[::-1] 但不是效率最高的. \"经典\" 方式: foo = '' for", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "初级", "language": "en", "created_at": "2025-07-19T19:22:02.037897"}}
{"text": "char in 'pizza': foo = char + foo >> 'azzip' ``` </b></details> <details> <summary>编写一个函数以返回一个或多个数字的和。 用户将决定要使用多少个数字</summary><br><b> 首先，你询问用户要使用的数字量。 使用while循环，每个循环将amount_of_numbers减1，直到amount_of_numbers变为0。 在while循环中，你想询问用户一个数字，该数字将在每次循环运行时添加一个变量。 ``` def return_sum(): amount_of_numbers = int(input(\"How many numbers? \")) total_sum = 0 while amount_of_numbers != 0: num = int(input(\"Input a number. \")) total_sum += num amount_of_numbers -= 1 return total_sum ``` </b></details> <details> <summary>如何将两个排序列表合并为一个排序列表?</summary><br><b> </b></details> <details> <summary> _ 在 Python 中用于什么?</summary><br><b> 1. i18n中的翻译查询 2. 将最后执行的表达式或语句的结果保存在交互式解释器中。 3. 作为通用“可丢弃”变量名。 例如：x，y，_ = get_data（）（使用了x和y，但是由于我们不关心第三个变量，因此我们将其“扔掉了”）。 </b></details>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "初级", "language": "en", "created_at": "2025-07-19T19:22:02.037944"}}
{"text": "<details> <summary>你可以在Python中实现“二分法搜索”吗?</summary><br><b> </b></details>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "Algorithms Implementation", "language": "en", "created_at": "2025-07-19T19:22:02.037970"}}
{"text": "<details> <summary>如何写文件?</summary><br><b> ``` with open('file.txt', 'w') as file: file.write(\"My insightful comment\") ``` </b></details> <details> <summary>如何反转文件?</summary><br><b> </b></details>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "Files", "language": "en", "created_at": "2025-07-19T19:22:02.037994"}}
{"text": "<details> <summary>如何在Python中执行与正则表达式相关的操作？ （匹配模式，替代字符串等）</summary><br><b> 使用 re 模式 </b></details> <details> <summary>如何用 \"blue\" 替换字符串 \"green\"?</summary><br><b> </b></details> <details> <summary>如何找到一个变量中的所有IP地址？ 如何在文件中找到它们?</summary><br><b> </b></details> <details> <summary>按每个嵌套列表的第二项对列表列表进行排序</summary><br><b> ``` li = [[1, 4], [2, 1], [3, 9], [4, 2], [4, 5]] sorted(x, key=lambda l: l[1]) ``` </b></details> <details> <summary>你可以编写一个函数来打印给定目录中的所有文件吗？ 包括子目录</summary><br><b> </b></details> <details> <summary>你有下面的列表: <code>[{'name': 'Mario', 'food': ['mushrooms', 'goombas']}, {'name': 'Luigi', 'food': ['mushrooms', 'turtles']}]</code> 获取所有的食物类型，最后输出: {'mushrooms', 'goombas', 'turtles'}</summary><br><b> ``` brothers_menu = \\ [{'name': 'Mario', 'food': ['mushrooms', 'goombas']}, {'name': 'Luigi', 'food': ['mushrooms', 'turtles']}]", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "Regex", "language": "en", "created_at": "2025-07-19T19:22:02.038119"}}
{"text": "def get_food(brothers_menu) -> set: temp = [] for brother in brothers_menu: for food in brother['food']: temp.append(food) return set(temp)", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "\"经典\" 方式", "language": "en", "created_at": "2025-07-19T19:22:02.038149"}}
{"text": "set([food for bro in x for food in bro['food']]) ``` </b></details> <details> <summary>什么是List 加强？ 它比典型的循环更好吗？ 为什么？ 你能示范如何使用它吗?</summary><br><b> </b></details> <details> <summary>怎样反转 string?</summary><br><b> 最简短的方式是: <code>my_string[::-1]</code> 但是这不是效率最高的. <br> 经典方式是: ``` def reverse_string(string): temp = \"\" for char in string: temp = char + temp return temp ``` </b></details> <details> <summary>如何按值对字典排序?</summary><br><b> </b></details> <details> <summary>如何按键对字典排序?</summary><br><b> </b></details> <details> <summary>解释数据序列化以及如何使用Python执行</summary><br><b> </b></details> <details> <summary>你如何在Python中处理参数解析?</summary><br><b> </b></details> <details> <summary>解释什么是GIL</summary><br><b> </b></details> <details> <summary>什么是迭代器? 为什么使用迭代器?</summary><br><b> </b></details> <details> <summary>解释以下方法的类型以及如何使用它们: * Static method * Class method * instance method</summary><br><b> </b></details> <details> <summary>怎样反转 list?</summary><br><b> </b></details> <details> <summary>空的", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "一直先行方式 (Using list comprehension)", "language": "en", "created_at": "2025-07-19T19:22:02.038298"}}
{"text": "<code>return</code> 返回什么?</summary><br><b> </b></details>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "一直先行方式 (Using list comprehension)", "language": "en", "created_at": "2025-07-19T19:22:02.038319"}}
{"text": "<details> <summary>描述操作的时间复杂度<code>access</code>, <code>search</code> <code>insert</code> and <code>remove</code> 下面的数据结构:</summary><br><b> * Stack * Queue * Linked List * Binary Search Tree </b></details> <details> <summary>以下算法的最好，最差和平均情况的复杂度是什么?: * Quicksort * Mergesort * Bucket Sort * Radix Sort </summary> </b></details> <a name=\"python-advanced\"></a>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "Time Complexity", "language": "en", "created_at": "2025-07-19T19:22:02.038371"}}
{"text": "<details> <summary>解释什么是装饰器</summary><br><b> </b></details> <details> <summary>你能展示如何编写和使用装饰器吗?</summary><br><b> </b></details> <details> <summary>编写脚本来确定给定端口上是否可以访问给定主机</summary><br><b> </b></details> <details> <summary>这个查询熟悉数据类吗？ 你能解释一下他们是干什么用的吗?</summary><br><b> </b></details> <details> <summary>解释一下上下文管理</summary><br><b> </b></details> <details> <summary>解释一下缓冲协议</summary><br><b> </b></details> <details> <summary>解释一下描述符</summary><br><b> </b></details> <details> <summary>你有抓取网络（爬虫）的经验吗？ 你能描述一下你用过什么以及用什么?</summary><br><b> </b></details> <details> <summary>你可以在Python中实现链接链表吗?</summary><br><b> </b></details> <details> <summary>你已经创建了一个网页，用户可以在其中上传文档。 但是，根据文档大小，读取上传文件的功能会运行很长时间，并且用户必须等待读取操作完成才能继续使用该网站。 你怎么能解决这个问题?</summary><br><b> </b></details>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "高级", "language": "en", "created_at": "2025-07-19T19:22:02.038421"}}
{"text": "<a name=\"prometheus-beginner\"></a>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "Prometheus", "language": "en", "created_at": "2025-07-19T19:22:02.038440"}}
{"text": "<details> <summary>什么是Prometheus? Prometheus的主要特点是什么?</summary><br><b> </b></details> <details> <summary>描述 Prometheus 架构和组件</summary><br><b> </b></details> <details> <summary>你能否将 Prometheus 与其他解决方案（例如InfluxDB）进行比较?</summary><br><b> </b></details> <details> <summary>什么是an Alert?</summary><br><b> </b></details> <details> <summary>描述以下Prometheus组件: * Prometheus server * Push Gateway * Alert Manager</summary><br><b> 负责抓取存储数据的Prometheus服务器推送网关用于短期作业警报管理负责警报 ;） </b></details> <details> <summary>什么是一个实例? 什么是一个作业?</summary><br><b> </b></details> <details> <summary>Prometheus支持哪些核心指标类型?</summary><br><b> </b></details> <details> <summary>什么是一个 exporter? 它用来做什么?</summary><br><b> </b></details> <details> <summary>你熟悉哪些Prometheus最佳做法？ 至少命名三个</summary><br><b> </b></details> <details> <summary>如何在给定时间内获得总请求?</summary><br><b> </b></details> <a name=\"prometheus-advanced\"></a>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "初级", "language": "en", "created_at": "2025-07-19T19:22:02.038518"}}
{"text": "<details> <summary>你如何加入两个指标?</summary><br><b> </b></details> <details> <summary>如何编写返回标签值的查询?</summary><br><b> </b></details> <details> <summary>如何将cpu_user_seconds转换为cpu使用率（百分比）?</summary><br><b> </b></details>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "高级", "language": "en", "created_at": "2025-07-19T19:22:02.038568"}}
{"text": "<a name=\"git-beginner\"></a>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "Git", "language": "en", "created_at": "2025-07-19T19:22:02.038585"}}
{"text": "<details> <summary><code>git pull</code> 和 <code>git fetch</code>的区别是什么?</summary><br><b> 简单来说, git pull = git fetch + git merge 当你运行git pull时，它会从远程或中央获取所有更改 存储库，并将其附加到本地存储库中的相应分支。 git fetch从远程存储库获取所有更改，将更改存储在 本地存储库中的单独分支 </b></details> <details> <summary>解释以下: <code>git 目录</code>, <code>工作目录</code> 和 <code>暂存区</code></summary><br><b> Git目录是Git存储项目的元数据和对象数据库的地方。 这是Git最重要的部分，当你从另一台计算机克隆存储库时，它就是复制的。 工作目录是项目一个版本的单个签出。 这些文件将从Git目录中的压缩数据库中拉出，并放置在磁盘上供你使用或修改。 暂存区是一个简单文件，通常包含在你的Git目录中，用于存储有关下一次提交的内容的信息。 有时称为索引，但将其称为暂存区已成为标准。 答案来自 [git-scm.com](https://git-scm.com/book/en/v1/Getting-Started-Git-Basics#_the_three_states) </b></details> <details> <summary>怎么解决 git merge 冲突?</summary><br><b> <p> 首先，打开有冲突的文件，然后确定有什么冲突。 接下来，根据你的公司或团队接受的是什么，你可以与自己的 同事解决冲突或自行解决 解决冲突后，使用 git add <file_name> 添加文件。 最后，运行`git rebase --continue`。 </p> </b></details> <details> <summary><code>git reset</code> 和 <code>git revert</code>区别是什么?</summary><br><b> <p> `git revert` 创建一个新的提交，撤消上一次提交的更改。 `git reset`", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "初级", "language": "en", "created_at": "2025-07-19T19:22:02.038786"}}
{"text": "根据使用情况，可以修改索引或更改分支头当前指向的提交。 </p> </b></details> <details> <summary>你想将提交移至顶部。 你将如何实现?</summary><br><b> 使用 <code>git rebase></code> 命令 </b></details> <details> <summary>那种情形你会使用 <code>git rebase</code>?</summary><br><b> </b></details> <details> <summary>你熟悉哪些合并策略?</summary><br><b> 提及两个或三个就足够了，最好提到“递归”作为默认值。 recursive resolve ours theirs 这篇文章解释是最好的: https://git-scm.com/docs/merge-strategies </b></details> <details> <summary>在提交更改之前，如何查看已完成的更改?</summary><br><b> <code>git diff</code> </b></details> <details> <summary>如何将特定文件还原为先前的提交?</summary><br><b> ``` git checkout HEAD~1 -- /path/of/the/file ``` </b></details> <a name=\"git-advanced\"></a>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "初级", "language": "en", "created_at": "2025-07-19T19:22:02.038816"}}
{"text": "<details> <summary>解释 Git octopus merge</summary><br><b> 也许不错，它是： * 对于合并多个分支的情况（以及此类用例的默认情况）非常有用 * 主要用于将主题分支捆绑在一起 有一篇文章关于 Octopus merge: http://www.freblogg.com/2016/12/git-octopus-merge.html </b></details>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "高级", "language": "en", "created_at": "2025-07-19T19:22:02.038846"}}
{"text": "<a name=\"go-beginner\"></a>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "Go", "language": "en", "created_at": "2025-07-19T19:22:02.038861"}}
{"text": "<details> <summary>Go编程语言有哪些特点?</summary><br><b> * 强类型和静态类型 - 变量的类型不能随时间更改，必须在编译时进行定义 * 简单 * 快速编译时间 * 内置并发 * 垃圾回收 * 平台无关 * 编译为独立的二进制文件 - 你运行应用程序所需的所有内容都将被编译为一个二进制文件。 对于运行时的版本管理非常有用。 Go 而且有一个很好的社区. </b></details> <details> <summary><code>var x int = 2</code> 和 <code>x := 2</code>区别是什么?</summary><br><b> 结果相同，变量值为2。 with <code>var x int = 2</code> we are setting the variable type to integer while with <code>x := 2</code> we are letting Go figure out by itself the type. </b></details> <details> <summary>对还是错? 在Go中，我们可以重新声明变量，并且一旦声明就必须使用它.</summary> 错. 我们不能重新声明变量，必须使用声明的变量。 </b></details> <details> <summary>你使用了哪些Go库?</summary><br><b> 应该根据你的使用情况回答此问题，一些示例是： * fmt - formatted I/O </b></details> <details> <summary>下面代码块有什么问题? 怎么解决? ```go func main() { var x float32 = 13.5 var y int y = x } ``` </summary><br><b> </b></details> <details> <summary>下面的代码块尝试将整数101转换为字符串，但相反，我们得到“ e”。 这是为什么？ 怎么解决? ```go package main import \"fmt\" func main() { var x int = 101 var y string y = string(x)", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "初级", "language": "en", "created_at": "2025-07-19T19:22:02.039377"}}
{"text": "fmt.Println(y) } ``` </summary><br><b> 它看起来在101处设置了什么unicode值，并将其用于将整数转换为字符串。 如果要获取“ 101”，则应使用“ strconv” 软件包，然后替换 <code>y = string(x)</code> with <code>y = strconv.Itoa(x)</code> </b></details> <details> <summary>以下代码块什么是错的?: ``` package main func main() { var x = 2 var y = 3 const someConst = x + y } ``` </summary><br><b> </b></details> <details> <summary>以下代码块的输出是什么？: ```go package main import \"fmt\" const ( x = iota y = iota ) const z = iota func main() { fmt.Printf(\"%v\\n\", x) fmt.Printf(\"%v\\n\", y) fmt.Printf(\"%v\\n\", z) } ``` </summary><br><b> </b></details> <details> <summary> _ 在 Go 中的用途是什么?</summary><br><b> </b></details> <details> <summary>以下代码块的输出是什么？: ```go package main import \"fmt\" const ( _ = iota + 3 x ) func main() { fmt.Printf(\"%v\\n\", x) } ``` </summary><br><b> </b></details>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "初级", "language": "en", "created_at": "2025-07-19T19:22:02.039403"}}
{"text": "<a name=\"mongo-beginner\"></a>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "Mongo", "language": "en", "created_at": "2025-07-19T19:22:02.039425"}}
{"text": "<details> <summary>MongoDB有什么优势？ 换句话说，为什么选择 MongoDB 而不选择 NoSQL 的其他实现?</summary><br><b> </b></details> <details> <summary>SQL和NoSQL之间的区别是什么?</summary><br><b> 主要区别在于SQL数据库是结构化的（数据以带有行和列的表格-像是Excel电子表格表格），而NoSQL是 非结构化的，并且数据存储会根据NoSQL DB的设置方式而有所不同，例如 作为键值对，面向文档等 </b></details> <details> <summary>在哪种情况下，这个查询希望使用 NoSQL/Mongo 而不是SQL?</summary><br><b> * 经常变化的异构数据 * 数据一致性和完整性不是重中之重 * 最好，如果数据库需要快速扩展 </b></details> <details> <summary>什么是一个文档? 什么是一个集合?</summary><br><b> </b></details> <details> <summary>什么是一个聚合?</summary><br><b> </b></details> <details> <summary>那个更好? 嵌入文档还是引用?</summary><br><b> </b></details>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "初级", "language": "en", "created_at": "2025-07-19T19:22:02.039475"}}
{"text": "<details> <summary>解释这个查询: <code>db.books.find({\"name\": /abc/})</code></summary><br><b> </b></details> <details> <summary>解释这个查询: <code>db.books.find().sort({x:1})</code></summary><br><b> </b></details>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "Queries", "language": "en", "created_at": "2025-07-19T19:22:02.039641"}}
{"text": "<a name=\"openshift-beginner\"></a>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "OpenShift", "language": "en", "created_at": "2025-07-19T19:22:02.039691"}}
{"text": "<details> <summary>什么是OpenShift? 你用过吗？ 如果有，是怎样使用的？</summary><br><b> </b></details> <details> <summary>你能解释一下 OpenShift 和 Kubernetes 之间的区别吗?</summary><br><b> </b></details> <details> <summary>定义 Pods 以及解释什么是有状态的 pods</summary><br><b> </b></details> <details> <summary>你熟悉哪些类型的构建策略?</summary><br><b> </b></details> <details> <summary>解释标签是什么以及它们的用途</summary><br><b> </b></details> <details> <summary>解释什么是注释以及它们与标签的区别</summary><br><b> </b></details> <details> <summary>解释什么是Downward API</summary><br><b> </b></details>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "初级", "language": "en", "created_at": "2025-07-19T19:22:02.039779"}}
{"text": "<a name=\"shell-scripting-beginner\"></a>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "Shell 脚本", "language": "en", "created_at": "2025-07-19T19:22:02.039805"}}
{"text": "<details> <summary>告诉我你使用Shell脚本的经验</summary><br><b> </b></details> <details> <summary>脚本中的这一行是什么意思?: <code>#!/bin/bash</code></summary><br><b> </b></details> <details> <summary>你倾向于在编写的每个脚本中包含什么?</summary><br><b> </b></details> <details> <summary>对还是错?: 当某个命令行失败时，默认情况下，该脚本将退出并且不会继续运行</summary><br><b> 取决于所使用的语言和设置，例如在Bash中，默认情况下，脚本将继续运行。 </b></details> <details> <summary>今天，我们拥有Ansible之类的工具和技术。 为什么还会有人使用Shell脚本?</summary><br><b> </b></details> <details> <summary>说出下面每个命令的结果是什么： * <code>echo $0</code> * <code>echo $?</code> * <code>echo $$</code> * <code>echo $@</code> * <code>echo $#</code></summary><br><b> </b></details> <details> <summary>你如何调试Shell脚本?</summary><br><b> </b></details> <details> <summary>如何在Shell脚本中从用户获得输入?</summary><br><b> </b></details> <details> <summary>解释一下条件语句以及如何使用它们</summary><br><b> </b></details> <details> <summary>什么是循环? 你熟悉哪些类型的循环?</summary><br><b> </b></details> <details> <summary>解释 <code>continue</code> 和 <code>break</code>. 你什么时候使用它们?</summary><br><b>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "初级", "language": "en", "created_at": "2025-07-19T19:22:02.039905"}}
{"text": "</b></details> <details> <summary>如何将命令的输出存储在变量中?</summary><br><b> </b></details> <details> <summary>你如何检查可变长度?</summary><br><b> </b></details> <details> <summary>单引号和双引号之间的区别是什么?</summary><br><b> </b></details> <a name=\"shell-scripting-advanced\"></a>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "初级", "language": "en", "created_at": "2025-07-19T19:22:02.039925"}}
{"text": "<details> <summary>解释以下代码: <code>:(){ :|:& };:</code> </summary><br><b> </b></details> <details> <summary>你能举一些Bash最佳实践的例子吗?</summary><br><b> </b></details> <details> <summary>什么是三元运算符？ 你如何在bash中使用它?</summary><br><b> 使用 if/else 的一种简短方法。 一个例子: [[ $a = 1 ]] && b=\"yes, equal\" || b=\"nope\" </b></details>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "高级", "language": "en", "created_at": "2025-07-19T19:22:02.039963"}}
{"text": "<a name=\"sql-beginner\"></a>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "SQL", "language": "en", "created_at": "2025-07-19T19:22:02.039979"}}
{"text": "<details> <summary>SQL 代表什么?</summary><br><b> Structured Query Language（结构化查询语言） </b></details> <details> <summary>SQL 和 NoSQL 有那些不同</summary><br><b> 主要区别在于SQL数据库是结构化的（数据以 带有行和列的表格-像是Excel电子表格表格），而NoSQL是 非结构化的，并且数据存储会根据NoSQL DB的设置方式而有所不同，例如 作为键值对，面向文档等 </b></details> <details> <summary>数据库符合ACID的含义是什么?</summary><br> ACID代表原子性，一致性，隔离性，耐久性。为了符合ACID，数据库必须满足四个标准中的每个标准 **原子性** - 数据库发生更改时，它整体上应该成功或失败。 例如，如果你要更新表，则更新应完全执行。如果仅部分执行，则 更新被视为整体失败，并且不会通过-数据库将恢复为原始状态 更新发生之前的状态。还应该提到的是，原子性确保每个 事务以其自身的独立“单元”完成 - 如果任何部分失败，则整个语句都会失败。 **一致性** - 对数据库所做的任何更改都应将其从一种有效状态转变为另一种有效状态。 例如，如果你对数据库进行了更改，则不应破坏它。通过检查和约束来保持一致性 在数据库中预定义。例如，如果你尝试将列的值从字符串更改为int 应该是数据类型字符串，一致的数据库将不允许该事务通过，并且该操作将 不执行 **隔离** - 确保数据库不会被“更新中”-因为多个事务正在运行 同时，它仍应保持数据库处于与按顺序运行事务相同的状态。 例如，假设有20个人同时对数据库进行了更改。在 当你执行查询时，已完成20项更改中的15项，但仍有5项正在进行中。你应该 仅看到已完成的15个更改 - 随着更改的进行，你将看不到数据库的更新中。 **耐用性** - 更改一旦提交，无论发生什么情况都将保持提交状态 （电源故障，系统崩溃等）。这意味着所有已完成的交易 必须记录在非挥发性内存中。 请注意，SQL本质上符合ACID。某些NoSQL DB可能符合ACID，具体取决于 它们的工作方式，但是根据一般经验，NoSQL DB不被视为符合ACID </details>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "初级", "language": "en", "created_at": "2025-07-19T19:22:02.040106"}}
{"text": "<details> <summary>什么时候最好使用SQL/NoSQL？</summary><br><b> SQL - 当数据完整性至关重要时，最适合使用。 由于符合ACID，SQL通常由许多业务实现特别是金融领域。 NoSQL - 非常适合你需要快速扩展的情况。 请记住NoSQL是为Web应用程序设计的 ，如果你需要快速将相同信息散布到多台服务器，它将会很好的用 此外，由于 NoSQL 不遵守具有列和行结构的严格表 关系数据库所要求的，你可以将不同的数据类型存储在一起。 </b></details> <details> <summary>什么是笛卡尔积?</summary><br> 笛卡尔积是指第一个表中的所有行都与第二个表中的所有行连接在一起时的结果 表。 这可以通过不定义要联接的键来隐式完成，也可以通过以下方式显式地完成： 在两个表上调用CROSS JOIN，如下所示： Select * from customers **CROSS JOIN** orders; 请注意，笛卡尔积也可能是一件坏事 - 执行联接时 在两个都没有唯一键的表上，这可能会导致返回信息 是不正确的。 </details>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "初级", "language": "en", "created_at": "2025-07-19T19:22:02.040138"}}
{"text": "对于这些问题，我们将使用下面显示的“客户和订单”表： **Customers** Customer_ID | Customer_Name | Items_in_cart | Cash_spent_to_Date ------------ | ------------- | ------------- | ------------- 100204 | John Smith | 0 | 20.00 100205 | Jane Smith | 3 | 40.00 100206 | Bobby Frank | 1 | 100.20 **ORDERS** Customer_ID | Order_ID | Item | Price | Date_sold ------------ | ------------- | ------------- | ------------- | ------------- 100206 | A123 | Rubber Ducky | 2.20 | 2019-09-18 100206 | A123 | Bubble Bath | 8.00 | 2019-09-18 100206 | Q987 | 80-Pack TP | 90.00 | 2019-09-20 100205 | Z001 | Cat Food - Tuna Fish | 10.00 | 2019-08-05 100205 | Z001 | Cat Food - Chicken | 10.00 | 2019-08-05 100205 | Z001 | Cat Food - Beef | 10.00 | 2019-08-05 100205 | Z001 | Cat Food - Kitty quesadilla | 10.00 | 2019-08-05 100204 | X202 | Coffee | 20.00 | 2019-04-29 <details> <summary>我如何从该表中选择所有字段?</summary><br><b> Select * <br> From Customers; </b></details> <details> <summary>约翰的购物车中有几件？</summary><br><b> Select Items_in_cart <br> From", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "SQL Specific Questions", "language": "en", "created_at": "2025-07-19T19:22:02.040740"}}
{"text": "Customers <br> Where Customer_Name = \"John Smith\"; </b></details> <details> <summary>所有客户花费的所有现金的总和是多少?</summary><br><b> Select SUM(Cash_spent_to_Date) as SUM_CASH <br> From Customers; </b></details> <details> <summary>在购物车有商品的有多少人?</summary><br><b> Select count(1) as Number_of_People_w_items <br> From Customers <br> where Items_in_cart > 0; </b></details> <details> <summary>你如何将客户表加入订单表?</summary><br><b> 你可以加入他们的唯一键。 在这种情况下，唯一键为中的Customer_ID 客户表和订单表 </b></details> <details> <summary>你如何显示哪些客户订购了哪些物品?</summary><br><b> Select c.Customer_Name, o.Item <br> From Customers c <br> Left Join Orders o <br> On c.Customer_ID = o.Customer_ID; </b></details> <a name=\"sql-advanced\"></a>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "SQL Specific Questions", "language": "en", "created_at": "2025-07-19T19:22:02.040774"}}
{"text": "<details> <summary>使用with语句，你将如何显示谁订购了猫粮以及花费的总金额?</summary><br><b> with cat_food as ( <br> Select Customer_ID, SUM(Price) as TOTAL_PRICE <br> From Orders <br> Where Item like \"%Cat Food%\" <br> Group by Customer_ID <br> ) <br> Select Customer_name, TOTAL_PRICE <br> From Customers c <br> Inner JOIN cat_food f <br> ON c.Customer_ID = f.Customer_ID <br> where c.Customer_ID in (Select Customer_ID from cat_food); 尽管这是一个简单的声明，但“ with”子句在 在连接到另一个表之前，需要在一个表上运行一个复杂的查询。 用语句很好， 因为你在运行查询时会创建一个伪临时文件，而不是创建一个新表。 目前尚无法获得所有猫粮的总和，因此我们使用了with语句来创建 伪表检索每个客户花费的价格总和，然后正常加入该表。 </b></details>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "高级", "language": "en", "created_at": "2025-07-19T19:22:02.040873"}}
{"text": "<a name=\"azure-beginner\"></a>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "Azure", "language": "en", "created_at": "2025-07-19T19:22:02.040893"}}
{"text": "<details> <summary>解释一下可用性集和可用性区域</summary><br><b> </b></details> <details> <summary>什么是Azure资源管理器？ 你可以描述ARM模板的格式吗？</summary><br><b> </b></details> <details> <summary>解释一下Azure托管磁盘</summary><br><b> </b></details>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "初级", "language": "en", "created_at": "2025-07-19T19:22:02.040912"}}
{"text": "<a name=\"gcp-beginner\"></a>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "GCP", "language": "en", "created_at": "2025-07-19T19:22:02.040927"}}
{"text": "<details> <summary>GCP的主要组件和服务是什么?</summary><br><b> </b></details> <details> <summary>你熟悉哪些GCP管理工具?</summary><br><b> </b></details> <details> <summary>告诉我对GCP联网了解多少</summary><br><b> </b></details>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "初级", "language": "en", "created_at": "2025-07-19T19:22:02.040943"}}
{"text": "<a name=\"openstack-beginner\"></a>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "OpenStack", "language": "en", "created_at": "2025-07-19T19:22:02.040957"}}
{"text": "<details> <summary>告诉我你使用OpenStack的经验。 你认为OpenStack的优缺点是什么?</summary><br><b> </b></details> <details> <summary>你熟悉OpenStack的哪些组件/项目?</summary><br><b> </b></details> <details> <summary>你能告诉我以下每个组件/项目负责什么吗?: * Nova * Neutron * Cinder * Glance * Keystone</summary><br><b> </b></details> <details> <summary>详细描述如何使用可以从云外部访问的IP来启动实例</summary><br><b> </b></details> <details> <summary>你收到客户打来的电话，说：“我可以ping我的实例，但不能连接（ssh）它”。 可能是什么问题？</summary><br><b> </b></details> <details> <summary>OpenStack支持哪些类型的网络？</summary><br><b> </b></details> <details> <summary>你如何调试OpenStack存储问题？ （工具，日志等）</summary><br><b> </b></details> <details> <summary>你如何调试OpenStack计算问题？ （工具，日志等）</summary><br><b> </b></details> <details> <summary>你熟悉 TripleO吗? 它有那些优点?</summary><br><b> </b></details>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "初级", "language": "en", "created_at": "2025-07-19T19:22:02.041016"}}
{"text": "<details> <summary>什么是供应商网络?</summary><br><b> </b></details> <details> <summary>L2和L3中存在哪些组件和服务?</summary><br><b> </b></details> <details> <summary>什么是ML2 plug-in? 解释一下它的架构</summary><br><b> </b></details> <details> <summary>什么是L2 代理? 它是怎么工作的以及它主要负责什么?</summary><br><b> </b></details> <details> <summary>什么是L3 代理? 它是怎么工作的以及它主要负责什么?</summary><br><b> </b></details> <details> <summary>解释元数据代理是怎么工作的以及它主要负责什么</summary><br><b> </b></details> <details> <summary>你如何调试OpenStack网络问题？ （工具，日志等）</summary><br><b> </b></details> <a name=\"openstack-advanced\"></a>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "网络", "language": "en", "created_at": "2025-07-19T19:22:02.041062"}}
{"text": "<details> <summary>解释 BGP 动态路由</summary> </b></details>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "网络", "language": "en", "created_at": "2025-07-19T19:22:02.041082"}}
{"text": "<a name=\"security-beginner\"></a>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "安全", "language": "en", "created_at": "2025-07-19T19:22:02.041095"}}
{"text": "<details> <summary>你能描述一下DevSecOps的核心原理吗?</summary><br><b> </b></details> <details> <summary>你熟悉哪些DevOps安全最佳实践?</summary><br><b> </b></details> <details> <summary>你熟悉哪些安全技术?</summary><br><b> </b></details> <details> <summary>如何在不同的工具和平台中管理密码?</summary><br><b> </b></details> <details> <summary>你如何识别和管理漏洞?</summary><br><b> </b></details> <details> <summary>什么是权限限制?</summary><br><b> </b></details>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "初级", "language": "en", "created_at": "2025-07-19T19:22:02.041120"}}
{"text": "<a name=\"puppet-beginner\"></a>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "Puppet", "language": "en", "created_at": "2025-07-19T19:22:02.041136"}}
{"text": "<details> <summary>什么是Puppet? 它是怎么工作的?</summary><br><b> </b></details> <details> <summary>解释一下 Puppet 结构</summary><br><b> </b></details> <details> <summary>你可以将Puppet与其他配置管理工具进行比较吗？ 你为什么选择使用Puppet？</summary><br><b> </b></details> <details> <summary>解释以下: * Module * Manifest * Node</summary><br><b> </b></details> <details> <summary>解释一下Facter</summary><br><b> </b></details> <details> <summary>什么是MCollective?</summary><br><b> </b></details> <a name=\"puppet-advanced\"></a>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "初级", "language": "en", "created_at": "2025-07-19T19:22:02.041200"}}
{"text": "<details> <summary>你有编写模块的经验吗？ 你创建了哪个模块以及用于什么?</summary><br><b> </b></details> <details> <summary>解释一下什么是Hiera</summary><br><b> </b></details>", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "中级", "language": "en", "created_at": "2025-07-19T19:22:02.041222"}}
{"text": "方案是没有口头回答的问题，需要你满足以下条件之一： * 设置环境 * 编写脚本 * 设计和/或开发基础设施项目 这些问题通常作为应聘者的一项家庭任务作为候选，可以将多个主题结合在一起。 在下面，你可以找到一些场景问题： * [Elasticsearch & Kibana on AWS](scenarios/elk_kibana_aws.md) * [Ansible, Minikube and Docker](scenarios/ansible_minikube_docker.md) * [Cloud Slack bot](scenarios/cloud_slack_bot.md) * [Writing Jenkins Scripts](scenarios/jenkins_scripts.md) * [Writing Jenkins Pipelines](scenarios/jenkins_pipelines.md)", "metadata": {"source_file": "learning-materials/README-zh_CN.md", "section": "场景", "language": "en", "created_at": "2025-07-19T19:22:02.041264"}}
{"text": "Most frequently asked questions.", "metadata": {"source_file": "learning-materials/faq.md", "section": "FAQ", "language": "en", "created_at": "2025-07-19T19:22:02.041467"}}
{"text": "Learning, of course.", "metadata": {"source_file": "learning-materials/faq.md", "section": "What is the purpose of repository?", "language": "en", "created_at": "2025-07-19T19:22:02.041487"}}
{"text": "Overall, this repository should help you learn some concepts but, don't assume at any point that your interview will include similar questions to those that included in this repository. Regarding interviews, I've added a couple of suggestions [here](prepare_for_interview.md)<br>", "metadata": {"source_file": "learning-materials/faq.md", "section": "My goal is to prepare for a DevOps interviews. Should I use this repository?", "language": "en", "created_at": "2025-07-19T19:22:02.041519"}}
{"text": "All good things come to an end...", "metadata": {"source_file": "learning-materials/faq.md", "section": "Will you stop at some point adding questions and exercises?", "language": "en", "created_at": "2025-07-19T19:22:02.041536"}}
{"text": "That's a great question.<br> I don't have a definitive answer for this question, I'm exploring it myself from time to time. What I believe helps is to: * Practice - Practicing DevOps practically should be the primary way to become a DevOps engineer in my opinion * Read - blogs, books, ... anything that can enrich your knowledge about DevOps or related DevOps topics * Participate - there are great DevOps communities. I personally like [Reddit DevOps community](https://www.reddit.com/r/devops). Visiting there, I learn quite a lot on different topics. * Share - This is one of the reasons I created this project. Primary goal was to help others but a secondary goal quickly became to learn more. By asking questions, you actually learn better a certain topic. Try it out, take a certain subject and try to come up with questions you would ask someone to test his/her skills about that topic.", "metadata": {"source_file": "learning-materials/faq.md", "section": "How do I become a better DevOps Engineer?", "language": "en", "created_at": "2025-07-19T19:22:02.041765"}}
{"text": "1. Because we need more contributors 2. Because often asking questions is easier than answering them", "metadata": {"source_file": "learning-materials/faq.md", "section": "Why most of the questions don't have answers?", "language": "en", "created_at": "2025-07-19T19:22:02.041802"}}
{"text": "1. Search for them using search engines, documentation pages, ... this is part of being a DevOps engineer 2. Use the communities: many people will be happy to help and answer your questions 3. Ask us. If you want, you can contact me or start a discussion on this project.", "metadata": {"source_file": "learning-materials/faq.md", "section": "Where can I find answers to some of the questions in this repository?", "language": "en", "created_at": "2025-07-19T19:22:02.041844"}}
{"text": "Well, everywhere! - past experience, colleagues, contributors, ... but please note we do not allow copying interview questions from interview questions sites to here. There are people who worked hard on adding those to their sites and we respect that.<br> As an evidence, we did deny pull requests with copied content from other sites.", "metadata": {"source_file": "learning-materials/faq.md", "section": "Where the questions and answers are coming from?", "language": "en", "created_at": "2025-07-19T19:22:02.041891"}}
{"text": "It's a hard question and the reason is that if you'll ask 20 different people, you'll probably get at least 10 different answers but here is what I believe is common today: * OS - DevOps require you good understanding of operating system concepts. The level required is mainly depends on the company although in my opinion it should be the same level. You should understand how the operating system works, how to troubleshoot and debug issues, etc. * Programming is part of DevOps. The level again depends on the company. Some will require you to know basic level of scripting while others deep understanding of common algorithms, data structure, design patterns etc. * Cloud and Containers - while not 100% must in all companies/positions, this skill is on the rise every year and many (if not most) of the positions/companies require this skill. This specifically means: AWS/Azure/GCP, Docker/Podman, Kubernetes, ... * CI/CD - Be able to to answer questions like \"Why do we need CI/CD?\" and \"What", "metadata": {"source_file": "learning-materials/faq.md", "section": "What are the top DevOps skills required for being a DevOps Engineer?", "language": "en", "created_at": "2025-07-19T19:22:02.042135"}}
{"text": "ways and models are there to perform CI/CD?\". Eventually, practice assembling such processes and workflow, using whatever tools you feel comfortable with.", "metadata": {"source_file": "learning-materials/faq.md", "section": "What are the top DevOps skills required for being a DevOps Engineer?", "language": "en", "created_at": "2025-07-19T19:22:02.042157"}}
{"text": "Is that a question? :)<br> If you don't like some of the questions or think that some questions should be removed you can open an issue or submit a PR and we can discuss it there. We don't have rules against deleting questions (for now :P)", "metadata": {"source_file": "learning-materials/faq.md", "section": "I feel like there are some questions that shouldn't be included in this project", "language": "en", "created_at": "2025-07-19T19:22:02.042195"}}
{"text": "You can (although I have no idea why would you want to), but: * Not without attribution. Many people worked hard on adding these questions and they deserve a proper credit for their work * Not if you plan to make money out of it. Directly or indirectly (e.g. ADS) as this is a free content and we would like it to stay this way :) Same goes for copying questions from different sources to this repository. We saw it happened already with a couple of pull requests and we rejected them. We will not merge pull requests with copied questions and answers from other sources.", "metadata": {"source_file": "learning-materials/faq.md", "section": "Can I copy the questions from here to my site?", "language": "en", "created_at": "2025-07-19T19:22:02.042305"}}
{"text": "I'll simply imagine you didn't ask that on an open source project... :)", "metadata": {"source_file": "learning-materials/faq.md", "section": "Can I add questions and/or answers to this project?", "language": "en", "created_at": "2025-07-19T19:22:02.042357"}}
{"text": "In general, I prefer questions added to this repository will have certain educational value for the user. Either regarding a certain concept or even a very general question, but one that will make the user research on a certain topic and will make him eventually more familiar with some of its core concepts.<br> I know that this is not the case for every question in this repo as of today (e.g. questions about specific commands) but this is definitely something to aspire for. I see little to none value in what is known as \"Installation Questions\". Let's say I ask you \"how to install Jenkins?\". Should I conclude from your answer that you are familiar with what is Jenkins and/or how it works? In other words, is there a value in knowing how to install Jenkins? In my opinion, no.", "metadata": {"source_file": "learning-materials/faq.md", "section": "Why can't I add installation questions?", "language": "en", "created_at": "2025-07-19T19:22:02.042527"}}
{"text": "Personally, I really like the following sites * [HackerRank](https://www.hackerrank.com) * [LeetCode](https://leetcode.com) * [Exercism](https://exercism.io)", "metadata": {"source_file": "learning-materials/faq.md", "section": "Where can I practice coding?", "language": "en", "created_at": "2025-07-19T19:22:02.042552"}}
{"text": "I listed some roadmaps in [devops-resources](https://github.com/bregman-arie/devops-resources)", "metadata": {"source_file": "learning-materials/faq.md", "section": "How to learn more DevOps?", "language": "en", "created_at": "2025-07-19T19:22:02.042567"}}
{"text": "If you see two identical questions, that's a bug.<br> If you see two similar questions, that's a feature :D (= it's intentional) For example: 1. What is horizontal scaling? 2. The act of adding additional instances to the pool to handle scaling is called ________ scaling You are right, both ask about horizontal scaling but it's done from a different angle in every question and in addition, I do believe repetition helps you to learn something in a way where you are not fixed on the way it's asked, rather you understand the concept itself.", "metadata": {"source_file": "learning-materials/faq.md", "section": "Why some questions repeat themselves?", "language": "en", "created_at": "2025-07-19T19:22:02.042660"}}
{"text": "Absolutely. Don't be afraid to raise ideas and start discussions.<br> I'll be more than happy to discuss any change you think we should make to improve the learning experience", "metadata": {"source_file": "learning-materials/faq.md", "section": "Are you open for making big changes in the repository?", "language": "en", "created_at": "2025-07-19T19:22:02.042690"}}
{"text": "Use pull requests to contribute to the project. Stick to the following format: \\<details> <summary>[Question]</summary><br><b> [Answer] \\</b></details> * If you added several questions and you would like to know how many questions are there you can use the script \"count_questions.sh\" in scripts directory.", "metadata": {"source_file": "learning-materials/CONTRIBUTING.md", "section": "How to contribute", "language": "en", "created_at": "2025-07-19T19:22:02.042844"}}
{"text": "* Avoid adding installation questions. Those are the worst type of questions... * Don't copy questions and answers from other sources. They probably worked hard for adding them. * If you add new images, make sure they are free and can be used.", "metadata": {"source_file": "learning-materials/CONTRIBUTING.md", "section": "What to avoid", "language": "en", "created_at": "2025-07-19T19:22:02.042890"}}
{"text": "You can test your changes locally with the script `run_ci.sh` in scripts directory.", "metadata": {"source_file": "learning-materials/CONTRIBUTING.md", "section": "Before submitting the pull request", "language": "en", "created_at": "2025-07-19T19:22:02.042909"}}
{"text": "Note: the following is opinionated.", "metadata": {"source_file": "learning-materials/prepare_for_interview.md", "section": "How to prepare for DevOps/SRE/Production Engineer interviews?", "language": "en", "created_at": "2025-07-19T19:22:02.043057"}}
{"text": "Every DevOps Engineer should have a deep understanding of at least one operating system and if you have the option to choose then I would say it should definitely be Linux as I believe it's a requirement of at least 90% of the DevOps jobs postings out there. In addition, Linux is almost integral part of any sub-area or domain in DevOps like Cloud, Containers, etc. Usually, the followup question is \"How extensive should my knowledge be?\" Out of all the DevOps skills, I would say this, along with coding, should be your strongest skills. Be familiar with OS processes, debugging tools, filesystem, networking, ... know your operating system, understand how it works, how to troubleshoot issues, etc. Not long ago, I've created a list of Linux resources right [here](https://dev.to/abregman/collection-of-linux-resources-3nhk). There are some good sites there that you can use for learning more about Linux.", "metadata": {"source_file": "learning-materials/prepare_for_interview.md", "section": "Linux", "language": "en", "created_at": "2025-07-19T19:22:02.043240"}}
{"text": "My personal belief is that any DevOps engineer should know programming, at least to some degree. Having this skill you can automate manual processes, improve some of the open source tools you are using today or build new tools & projects to provide a solution to existing problems. Knowing how to code = a lot of power. When it comes to interviews you'll notice that the level of knowledge very much depends on the company or position you are interviewing for. Some will require you just to be able to write simple scripts while others will deep dive into complex algorithms and data structures. The best way to practice this skill is by doing some actual coding - scripts, online challenges, CLI tools, web applications, ... just code :) Also, the following is probably clear to most people but let's still clarify it: when given the chance to choose any language for answering coding tasks/questions, choose the one you have experience with! Some candidates prefer to choose the language they think", "metadata": {"source_file": "learning-materials/prepare_for_interview.md", "section": "Programming", "language": "en", "created_at": "2025-07-19T19:22:02.043582"}}
{"text": "the company is using and this is a huge mistake since giving the right answer is always better than a wrong answer, no matter which language you have used :) I recommend the following sites for practicing coding: * [HackerRank](https://www.hackerrank.com) * [LeetCode](https://leetcode.com) * [Exercism](https://exercism.io) Starting your own project is also a good idea. More on that later on.", "metadata": {"source_file": "learning-materials/prepare_for_interview.md", "section": "Programming", "language": "en", "created_at": "2025-07-19T19:22:02.043613"}}
{"text": "This is also an important aspect of DevOps. You should be able to describe how to design different systems, workflows, and architectures. Also, the scale is an important aspect of that. A design which might work for a dozen of hosts or X amount of data, will not necessarily work well with bigger scale. Some ideas for you to explore: * How to design and implement a CI pipeline (or pipelines) for verifying PRs, run multiple different types of tests, package the project and deploy it somewhere * How to design and implement secured ELK architecture which will get logs from 10,000 apps and will display the data eventually to the user * Microservices designs are also quite popular these days In general, you should be able to describe some designs, projects, architectures, ... you performed.", "metadata": {"source_file": "learning-materials/prepare_for_interview.md", "section": "Architecture and Design", "language": "en", "created_at": "2025-07-19T19:22:02.043808"}}
{"text": "Some interviews will focus on specific tools or technologies. Which tools? this is mainly based on a combination of what you mentioned in your C.V & those that are mentioned in the job posting and used in the company. Here are some questions I believe anyone should know to answer regarding the tools he/she is familiar with: * What the tool does? What it allows us to achieve that we couldn't do without it? * What its advantages over other tools in the same area, with the same purpose? Why you specifically using it? * How it works? * How to use it? * Best practices you apply/use when using it Let's deep dive into practical preparation steps", "metadata": {"source_file": "learning-materials/prepare_for_interview.md", "section": "Tooling", "language": "en", "created_at": "2025-07-19T19:22:02.043980"}}
{"text": "This is a very common way to interview today for DevOps roles. The candidate is given a task which represents a common task of DevOps Engineers or a piece of common knowledge and the candidate has several hours or days to accomplish the task.<br> This is a great way to prepare for interviews and I recommend to try it out before actually interviewing. How? Take requirements from job posts and convert them into scenarios. Let's see an example: \"Knowledge in CI/CD\" -> Scenario: create a CI/CD pipeline for a project. At this point, some people ask: \"but what project?\" and the answer is: what about GitHub? it has only 9125912851285192 projects...and a free way to set up CI to any of them (also a great way to learn how to collaborate with others :) ) Let's convert another scenario: \"Experience with provisioning servers\" -> Scenario: provision a server (to make it more interesting: create a web server). And the last example: \"Experience with scripting\" -> Scenario: write a script. Don't waste", "metadata": {"source_file": "learning-materials/prepare_for_interview.md", "section": "Scenarios || Challenges || Tasks", "language": "en", "created_at": "2025-07-19T19:22:02.044237"}}
{"text": "too much time thinking \"what script should I write?\". Simply automate something you are doing manually or even implement your own version of common small utils.", "metadata": {"source_file": "learning-materials/prepare_for_interview.md", "section": "Scenarios || Challenges || Tasks", "language": "en", "created_at": "2025-07-19T19:22:02.044259"}}
{"text": "Starting a DevOps project is a good idea because: * It will make you practice coding * It will be something you can add to your resume and talk about with the interviewer * Depends on size and complexity, it can teach you something about design in general * Depends on adoption, it can teach you about managing Open Source projects Same here, don't overthink what your project should be about. Just go and build something :)", "metadata": {"source_file": "learning-materials/prepare_for_interview.md", "section": "Start your own DevOps project", "language": "en", "created_at": "2025-07-19T19:22:02.044329"}}
{"text": "Make a sample list of interview questions on various topics/areas like technical, company, role, ... and try to answer them. See if you can manage to answer them in a fluent, detailed way. Better yet, ask a good friend/colleague to challenge you with some questions. Your self-awareness might be an obstacle in objective self-review of your knowledge :)", "metadata": {"source_file": "learning-materials/prepare_for_interview.md", "section": "Sample interview questions", "language": "en", "created_at": "2025-07-19T19:22:02.044389"}}
{"text": "For those who attend technical meetups and conferences, it can be a great opportunity to chat with people from other companies on their interviewing process. But don't start with it, it can be quite awkward. Say at least hello first... (: Doing so can give you a lot of information on what to expect from an interview at some companies or how to better prepare.", "metadata": {"source_file": "learning-materials/prepare_for_interview.md", "section": "Networking", "language": "en", "created_at": "2025-07-19T19:22:02.044448"}}
{"text": "It may sound trivial but the idea here is simple: be ready to answer any question regarding any line you included in your resume. Sometimes candidates surprised when they are asked on a skill or line which seems to be not related to the position but the simple truth is: if you mentioned something on your resume, it's only fair to ask you about it.", "metadata": {"source_file": "learning-materials/prepare_for_interview.md", "section": "Know your resume", "language": "en", "created_at": "2025-07-19T19:22:02.044504"}}
{"text": "Be familiar with the company you are interviewing at. Some ideas: * What the company does? * What products it has? * Why its products are unique (or better than other products)? This can also be a good question for you to ask", "metadata": {"source_file": "learning-materials/prepare_for_interview.md", "section": "Know the company", "language": "en", "created_at": "2025-07-19T19:22:02.044542"}}
{"text": "From my experience, this is not done by many candidates but it's one of the best ways to deep dive into topics like operating system, virtualization, scale, distributed systems, etc. In most cases, you will do fine without reading books but for the AAA interviews (hardest level) you'll want to read some books and overall if you inspire to be better DevOps Engineer, books (also articles, blog posts) is a great way develop yourself :)", "metadata": {"source_file": "learning-materials/prepare_for_interview.md", "section": "Books", "language": "en", "created_at": "2025-07-19T19:22:02.044609"}}
{"text": "While not a preparation step, you should know that landing DevOps as a first position can be challenging. No, it's not impossible but still, since DevOps covers many different practices, tools, ... it can be quite challenging and also overwhelming for someone to try and achieve it as a first position.<br> A possible path to becoming a DevOps engineer is to start with actually a different (but related) position and switch from there after 1-2 years or more. Some ideas: * System Administrator - This is perfect because every DevOps Engineer should have a solid understanding of the OS and sysadmins know their OS :) * Software Developer/Engineer - A DevOps should have coding skills and this position will provide more than the required knowledge in most cases * QA Engineer - This is a more tricky one because IMHO there are less overlapping areas/skills with DevOps Engineer. Sure, DevOps engineers should have some knowledge about testing but usually, it seems their solid skills/background is", "metadata": {"source_file": "learning-materials/prepare_for_interview.md", "section": "Consider starting in non-DevOps position", "language": "en", "created_at": "2025-07-19T19:22:02.044878"}}
{"text": "mainly composed out of system internals and coding skills.", "metadata": {"source_file": "learning-materials/prepare_for_interview.md", "section": "Consider starting in non-DevOps position", "language": "en", "created_at": "2025-07-19T19:22:02.044907"}}
{"text": "DevOps interviews can be very different. Some will include design questions, some will focus on coding, others will include short technical questions and you might even have an interview where the interviewer only goes over your resume and discussing your past experience. There are a couple of things you can do about it so it will be a less overwhelming experience: 1. You can and probably should ask the HR (in some cases even the team lead) how the interview process looks like. Some will be kind enough to even tell you how to prepare. 2. Usually, the job posting gives more than a hint on where the focus will be and what you should focus on in your preparations so read it carefully. 3. There are plenty of sites that have notes or a summary of the interview process in different companies, especially big enterprises.", "metadata": {"source_file": "learning-materials/prepare_for_interview.md", "section": "What to expect from a DevOps interview?", "language": "en", "created_at": "2025-07-19T19:22:02.045089"}}
{"text": "Some people tend to look at interviews as a one-way road of \"Determining whether a candidate is qualified\" but in reality, a candidate should also determine whether the company he/she is interviewing at, is the right place for him/her. * Do I care about team size? More specifically, do I care about being a one-man show or being part of a bigger team? * Do I care about work-life balance? * Do I care about personal growth and how it's practically done? * Do I care about knowing what are my responsibilities as part of the role? If you do, you should also play the interviewer role :)", "metadata": {"source_file": "learning-materials/prepare_for_interview.md", "section": "Don't forget to be an interviewer as well", "language": "en", "created_at": "2025-07-19T19:22:02.045209"}}
{"text": "[Good luck](https://youtu.be/AFUrG1-BAt4?t=59) :)", "metadata": {"source_file": "learning-materials/prepare_for_interview.md", "section": "One Last Thing", "language": "en", "created_at": "2025-07-19T19:22:02.045247"}}
{"text": "<details> <summary>What is cloud computing?</summary><br><b> [Wikipedia](https://en.wikipedia.org/wiki/Cloud_computing): \"Cloud computing is the on-demand availability of computer system resources, especially data storage (cloud storage) and computing power, without direct active management by the user\" </b></details> <details> <summary>What types of clouds (or cloud deployments) are there?</summary><br><b> * Public - Cloud services sharing computing resources among multiple customers * Private - Cloud services having computing resources limited to specific customer or organization, managed by third party or organizations itself * Hybrid - Combination of public and private clouds </b></details> <details> <summary>What is Azure Firewall?</summary><br><b> Azure Firewall is a cloud-native and intelligent network firewall security service that provides the best of breed threat protection for your cloud workloads running in Azure. </b></details> <details> <summary>What is Network Security", "metadata": {"source_file": "learning-materials/certificates/azure-fundamentals-az-900.md", "section": "AZ-900", "language": "en", "created_at": "2025-07-19T19:22:02.045687"}}
{"text": "Group?</summary><br><b> A network security group contains security rules that allow or deny inbound network traffic to, or outbound network traffic from, several types of Azure resources. For each rule, you can specify source and destination, port, and protocol. </b></details>", "metadata": {"source_file": "learning-materials/certificates/azure-fundamentals-az-900.md", "section": "AZ-900", "language": "en", "created_at": "2025-07-19T19:22:02.045755"}}
{"text": "A summary of what you need to know for the exam can be found [here](https://aws.amazon.com/certification/certified-sysops-admin-associate)", "metadata": {"source_file": "learning-materials/certificates/aws-cloud-sysops-associate.md", "section": "AWS Cloud SysOps Administration Associate", "language": "en", "created_at": "2025-07-19T19:22:02.045863"}}
{"text": "<br> AWS Certified SysOps Administrator - Associate is intended for system administrators in cloud operations roles to validate technical skills. <summary>Before you take this exam, we recommend you have :</summary><br><b> * A minimum of one year of hands-on experience with AWS technology * Experience deploying, managing, and operating workloads on AWS as well as implementing security controls and compliance requirements * Familiarity with using both the AWS Management Console and the AWS Command Line Interface (CLI) * Understanding of the AWS Well-Architected Framework as well as AWS networking and security services </b>", "metadata": {"source_file": "learning-materials/certificates/aws-cloud-sysops-associate.md", "section": "<b>  Who should take this exam? </b>", "language": "en", "created_at": "2025-07-19T19:22:02.045960"}}
{"text": "<br> Get started with free resources or explore additional resources, including Official Practice Exams, with a subscription to AWS Skill Builder. * AWS Cloud SysOps Guide (SOA-C02) [here](https://d1.awsstatic.com/training-and-certification/docs-sysops-associate/AWS-Certified-SysOps-Administrator-Associate_Exam-Guide.pdf) * AWS Certified SysOps Administrator - Associate Official Practice Question Set (FREE) [here](https://explore.skillbuilder.aws/learn/course/external/view/elearning/12485/aws-certified-sysops-administrator-associate-practice-question-set-soa-c02-english?syops=sec&sec=prep) * Exam Prep: AWS Certified SysOps Administrator - Associate (FREE) [here](https://explore.skillbuilder.aws/learn/course/external/view/elearning/9313/exam-prep-aws-certified-sysops-administrator-associate) ### <b> Certification resource </b> <br> This is a resource for studying and preparing for the AWS Cloud SysOps Associate exam. * Architecting for the cloud", "metadata": {"source_file": "learning-materials/certificates/aws-cloud-sysops-associate.md", "section": "<b>  Prepare for your exam </b>", "language": "en", "created_at": "2025-07-19T19:22:02.046072"}}
{"text": "[here](https://d1.awsstatic.com/whitepapers/AWS_Cloud_Best_Practices.pdf) * AWS Well-Architected Framework [here](https://d0.awsstatic.com/whitepapers/architecture/AWS_Well-Architected_Framework.pdf) * Development and Test on Amazon Web Services [here](https://media.amazonwebservices.com/AWS_Development_Test_Environments.pdf) * Backup, Archive, and Restore Approaches Using AWS [here](https://d0.awsstatic.com/whitepapers/Backup_Archive_and_Restore_Approaches_Using_AWS.pdf) * How AWS Pricing Works - AWS Pricing Overview [here](https://d0.awsstatic.com/whitepapers/aws_pricing_overview.pdf) * Sample Question AWS SOA-C02 [here](https://d1.awsstatic.com/training-and-certification/docs-sysops-associate/AWS-Certified-SysOps-Administrator-Associate_Sample-Questions.pdf)", "metadata": {"source_file": "learning-materials/certificates/aws-cloud-sysops-associate.md", "section": "<b>  Prepare for your exam </b>", "language": "en", "created_at": "2025-07-19T19:22:02.046091"}}
{"text": "A summary of what you need to know for the exam can be found [here](https://aws.amazon.com/certification/certified-cloud-practitioner/)", "metadata": {"source_file": "learning-materials/certificates/aws-cloud-practitioner.md", "section": "AWS - Cloud Practitioner", "language": "en", "created_at": "2025-07-19T19:22:02.046793"}}
{"text": "<details> <summary>What is cloud computing?</summary><br><b> [Wikipedia](https://en.wikipedia.org/wiki/Cloud_computing): \"Cloud computing is the on-demand availability of computer system resources, especially data storage (cloud storage) and computing power, without direct active management by the user\" Cloud computing also allows you to scale resources up or down as needed, paying only for what you use. </b></details> <details> <summary>What types of Cloud Computing services are there?</summary><br><b> IAAS PAAS SAAS </b></details> <details> <summary>Explain each of the following and give an example: * IAAS * PAAS * SAAS</summary><br><b> - IAAS - Infrastructure As A Service is a cloud computing service where a cloud provider rents out IT infrastructure such as compute, networking resources and storage over the internet (e.g., AWS EC2).<br> - PAAS - Platform As A Service is a cloud hosting platform with an on-demand access to ready-to-use set of deployment, application management and", "metadata": {"source_file": "learning-materials/certificates/aws-cloud-practitioner.md", "section": "Cloud 101", "language": "en", "created_at": "2025-07-19T19:22:02.047181"}}
{"text": "DevOps tools (e.g., AWS Elastic Beanstalk).<br> - SAAS - Software As A Service is a software distribution model in which services are hosted by a cloud service provider (e.g., AWS WorkSpaces or any web-based email service). </b></details> <details> <summary>What types of clouds (or cloud deployments) are there?</summary><br><b> * Public * Hybrid * Private </b></details> <details> <summary>Explain each of the following Cloud Computing Deployments: * Public * Hybrid * Private</summary><br><b> - Public - Public cloud is when you leverage cloud services over the open internet on hardware owned by the cloud provider, but its usage is shared by other companies. It offers cost-effectiveness and ease of scaling.<br> - Hybrid - A hybrid cloud is a cloud computing environment that uses a mix of combining a public and private cloud environment, like an on-premises data center, and public CSPs. It provides greater flexibility and more deployment options.<br> - Private - Private cloud means that", "metadata": {"source_file": "learning-materials/certificates/aws-cloud-practitioner.md", "section": "Cloud 101", "language": "en", "created_at": "2025-07-19T19:22:02.047206"}}
{"text": "the cloud infrastructure is provisioned for exclusive use by a single organization. Resources are not shared with others, so it offers more control over security and data. [Read more](https://aws.amazon.com/types-of-cloud-computing/) </b></details>", "metadata": {"source_file": "learning-materials/certificates/aws-cloud-practitioner.md", "section": "Cloud 101", "language": "en", "created_at": "2025-07-19T19:22:02.047226"}}
{"text": "<details> <summary>Explain the following * Availability zone * Region * Edge location</summary><br><b> AWS regions are data centers hosted across different geographical locations worldwide, each region is completely independent of one another.<br> Within each region, there are multiple isolated locations known as Availability Zones. Multiple availability zones ensure high availability in case one of them goes down. Each Availability Zone is physically separated from others, with its own power, networking, and connectivity.<br> Edge locations are basically content delivery network endpoints which cache data and ensure lower latency and faster delivery to the users in any location. They are located in major cities around the world. </b></details>", "metadata": {"source_file": "learning-materials/certificates/aws-cloud-practitioner.md", "section": "AWS Global Infrastructure", "language": "en", "created_at": "2025-07-19T19:22:02.047367"}}
{"text": "<details> <summary>What is VPC?</summary><br><b> \"A logically isolated section of the AWS cloud where you can launch AWS resources in a virtual network that you define\". Read more about it [here](https://aws.amazon.com/vpc). A VPC spans all the Availability Zones within a single region. </b></details> <details> <summary>True or False? VPC spans multiple regions</summary><br><b> False. A VPC is region-specific and cannot span multiple regions. </b></details> <details> <summary>True or False? Subnets belong to the same VPC, can be in different availability zones</summary><br><b> True. Just to clarify, a subnet must reside entirely in one AZ, but a single VPC can contain subnets across multiple AZs. </b></details> <details> <summary>What is an Internet Gateway?</summary><br><b> \"component that allows communication between instances in your VPC and the internet\" (AWS docs). Read more about it [here](https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Internet_Gateway.html) It scales", "metadata": {"source_file": "learning-materials/certificates/aws-cloud-practitioner.md", "section": "AWS Networking", "language": "en", "created_at": "2025-07-19T19:22:02.047869"}}
{"text": "horizontally and is highly available, allowing inbound and outbound traffic to flow without imposing availability risks or bandwidth constraints. </b></details> <details> <summary>True or False? NACL allow or deny traffic on the subnet level</summary><br><b> True </b></details> <details> <summary>True or False? Multiple Internet Gateways can be attached to one VPC</summary><br><b> False. Only one internet gateway can be attached to a single VPC. </b></details> <details> <summary>True or False? Route Tables used to allow or deny traffic from the internet to AWS instances</summary><br><b> False. Route tables are used to direct traffic to the right destination (e.g., Internet Gateway, NAT Gateway, etc.), not to allow or deny traffic. </b></details> <details> <summary>Explain Security Groups and Network ACLs</summary><br><b> * NACL - security layer on the subnet level. They are stateless, meaning inbound and outbound rules are evaluated separately.<br> * Security Group - security layer on", "metadata": {"source_file": "learning-materials/certificates/aws-cloud-practitioner.md", "section": "AWS Networking", "language": "en", "created_at": "2025-07-19T19:22:02.047907"}}
{"text": "the instance level. They are stateful, meaning if you allow inbound traffic, outbound traffic is automatically allowed, and vice versa. Read more about it [here](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html) and [here](https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html) </b></details> <details> <summary>What is AWS Direct Connect?</summary><br><b> Allows you to connect your corporate network to AWS network. It provides a dedicated network connection that can offer more consistent performance than internet-based connections. </b></details>", "metadata": {"source_file": "learning-materials/certificates/aws-cloud-practitioner.md", "section": "AWS Networking", "language": "en", "created_at": "2025-07-19T19:22:02.047928"}}
{"text": "<details> <summary>What is EC2?</summary><br><b> \"a web service that provides secure, resizable compute capacity in the cloud\". Read more [here](https://aws.amazon.com/ec2) EC2 allows you to quickly scale up or down to match resource needs, paying only for the compute time you consume. </b></details> <details> <summary>What is AMI?</summary><br><b> Amazon Machine Images is \"An Amazon Machine Image (AMI) provides the information required to launch an instance\". Read more [here](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html) An AMI typically includes an operating system, application server, and applications, so you can quickly spin up new instances with the same configuration. </b></details> <details> <summary>What are the different source for AMIs?</summary><br><b> * Personal AMIs - AMIs you create * AWS Marketplace for AMIs - Paid AMIs usually bundled with licensed software * Community AMIs - Free You can also share AMIs across accounts if needed. </b></details>", "metadata": {"source_file": "learning-materials/certificates/aws-cloud-practitioner.md", "section": "AWS Compute", "language": "en", "created_at": "2025-07-19T19:22:02.048541"}}
{"text": "<details> <summary>What is instance type?</summary><br><b> \"the instance type that you specify determines the hardware of the host computer used for your instance\" Read more about instance types [here](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-types.html) Instance types vary by CPU, memory, storage, and networking capacity, e.g., t2.micro, c5.large, etc. </b></details> <details> <summary>True or False? The following are instance types available for a user in AWS: * Compute optimized * Network optimized * Web optimized</summary><br><b> False. From the above list only compute optimized is available. There's no \"Web optimized\" or \"Network optimized\" instance type. You do have memory optimized, storage optimized, etc. </b></details> <details> <summary>What is EBS?</summary><br><b> \"provides block level storage volumes for use with EC2 instances. EBS volumes behave like raw, unformatted block devices.\" More on EBS", "metadata": {"source_file": "learning-materials/certificates/aws-cloud-practitioner.md", "section": "AWS Compute", "language": "en", "created_at": "2025-07-19T19:22:02.048564"}}
{"text": "[here](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AmazonEBS.html) EBS volumes are tied to an Availability Zone. They can be snapshotted to Amazon S3 for durability and can be detached/reattached between EC2 instances in the same AZ. </b></details> <details> <summary>What EC2 pricing models are there?</summary><br><b> On Demand - pay a fixed rate by the hour/second with no commitment. You can provision and terminate at any time.<br> Reserved - you get capacity reservation, basically purchase an instance for a fixed time period (1 or 3 years). The longer, the cheaper.<br> Spot - Enables you to bid whatever price you want for instances or pay the spot price. Ideal for workloads that can be interrupted.<br> Dedicated Hosts - physical EC2 server dedicated for your use. Helps you address compliance requirements and use your own software licenses. </b></details> <details> <summary>What are Security Groups?</summary><br><b> \"A security group acts as a virtual firewall that controls", "metadata": {"source_file": "learning-materials/certificates/aws-cloud-practitioner.md", "section": "AWS Compute", "language": "en", "created_at": "2025-07-19T19:22:02.048583"}}
{"text": "the traffic for one or more instances\" More on this subject [here](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html) They are stateful, so any rule applied for inbound automatically applies to outbound, and vice versa (if the inbound rule is allowed). </b></details> <details> <summary>What can you attach to an EC2 instance in order to store data?</summary><br><b> EBS Additionally, some instance types support Instance Store (ephemeral storage), and you can also mount EFS (file storage) if you need a shared filesystem across multiple instances. </b></details> <details> <summary>What EC2 RI types are there?</summary><br><b> Standard RI - most significant discount + suited for steady-state usage<br> Convertible RI - discount + change attribute of RI + suited for steady-state usage<br> Scheduled RI - launch within time windows you reserve Learn more about EC2 RI [here](https://aws.amazon.com/ec2/pricing/reserved-instances) Some RIs also offer different payment", "metadata": {"source_file": "learning-materials/certificates/aws-cloud-practitioner.md", "section": "AWS Compute", "language": "en", "created_at": "2025-07-19T19:22:02.048601"}}
{"text": "options (no upfront, partial upfront, or all upfront) affecting the discount level. </b></details>", "metadata": {"source_file": "learning-materials/certificates/aws-cloud-practitioner.md", "section": "AWS Compute", "language": "en", "created_at": "2025-07-19T19:22:02.048649"}}
{"text": "<details> <summary>What is Amazon ECS?</summary><br><b> Amazon definition: \"Amazon Elastic Container Service (Amazon ECS) is a fully managed container orchestration service. Customers such as Duolingo, Samsung, GE, and Cook Pad use ECS to run their most sensitive and mission critical applications because of its security, reliability, and scalability.\" Learn more [here](https://aws.amazon.com/ecs) </b></details> <details> <summary>What is Amazon ECR?</summary><br><b> Amazon definition: \"Amazon Elastic Container Registry (ECR) is a fully-managed Docker container registry that makes it easy for developers to store, manage, and deploy Docker container images.\" Learn more [here](https://aws.amazon.com/ecr) </b></details> <details> <summary>What is AWS Fargate?</summary><br><b> Amazon definition: \"AWS Fargate is a serverless compute engine for containers that works with both Amazon Elastic Container Service (ECS) and Amazon Elastic Kubernetes Service (EKS).\" Learn more", "metadata": {"source_file": "learning-materials/certificates/aws-cloud-practitioner.md", "section": "AWS Containers", "language": "en", "created_at": "2025-07-19T19:22:02.048819"}}
{"text": "[here](https://aws.amazon.com/fargate) </b></details>", "metadata": {"source_file": "learning-materials/certificates/aws-cloud-practitioner.md", "section": "AWS Containers", "language": "en", "created_at": "2025-07-19T19:22:02.048845"}}
{"text": "<details> <summary>Explain what is AWS S3?</summary><br><b> S3 stands for 3 S, Simple Storage Service. S3 is a object storage service which is fast, scalable and durable. S3 enables customers to upload, download or store any file or object that is up to 5 TB in size. More on S3 [here](https://aws.amazon.com/s3) </b></details> <details> <summary>What is a bucket?</summary><br><b> An S3 bucket is a resource which is similar to folders in a file system and allows storing objects, which consist of data. </b></details> <details> <summary>True or False? A bucket name must be globally unique</summary><br><b> True </b></details> <details> <summary>Explain folders and objects in regards to buckets</summary><br><b> * Folder - any sub folder in an s3 bucket * Object - The files which are stored in a bucket </b></details> <details> <summary>Explain the following: * Object Lifecycles * Object Sharing * Object Versioning</summary><br><b> * Object Lifecycles - Transfer objects between storage classes", "metadata": {"source_file": "learning-materials/certificates/aws-cloud-practitioner.md", "section": "AWS Storage", "language": "en", "created_at": "2025-07-19T19:22:02.049640"}}
{"text": "based on defined rules of time periods * Object Sharing - Share objects via a URL link * Object Versioning - Manage multiple versions of an object </b></details> <details> <summary>Explain Object Durability and Object Availability</summary><br><b> Object Durability: The percent over a one-year time period that a file will not be lost Object Availability: The percent over a one-year time period that a file will be accessible </b></details> <details> <summary>What is a storage class? What storage classes are there?</summary><br><b> Each object has a storage class assigned to, affecting its availability and durability. This also has effect on costs. Storage classes offered today: * Standard: * Used for general, all-purpose storage (mostly storage that needs to be accessed frequently) * The most expensive storage class * 11x9% durability * 2x9% availability * Default storage class * Standard-IA (Infrequent Access) * Long lived, infrequently accessed data but must be available the moment", "metadata": {"source_file": "learning-materials/certificates/aws-cloud-practitioner.md", "section": "AWS Storage", "language": "en", "created_at": "2025-07-19T19:22:02.049662"}}
{"text": "it's being accessed * 11x9% durability * 99.90% availability * One Zone-IA (Infrequent Access): * Long-lived, infrequently accessed, non-critical data * Less expensive than Standard and Standard-IA storage classes * 2x9% durability * 99.50% availability * Intelligent-Tiering: * Long-lived data with changing or unknown access patterns. Basically, In this class the data automatically moves to the class most suitable for you based on usage patterns * Price depends on the used class * 11x9% durability * 99.90% availability * Glacier: Archive data with retrieval time ranging from minutes to hours * Glacier Deep Archive: Archive data that rarely, if ever, needs to be accessed with retrieval times in hours * Both Glacier and Glacier Deep Archive are: * The most cheap storage classes * have 9x9% durability More on storage classes [here](https://aws.amazon.com/s3/storage-classes) </b></details> <details> <summary>A customer would like to move data which is rarely accessed from standard storage", "metadata": {"source_file": "learning-materials/certificates/aws-cloud-practitioner.md", "section": "AWS Storage", "language": "en", "created_at": "2025-07-19T19:22:02.049682"}}
{"text": "class to the most cheapest class there is. Which storage class should be used? * One Zone-IA * Glacier Deep Archive * Intelligent-Tiering</summary><br><b> Glacier Deep Archive </b></details> <details> <summary>What Glacier retrieval options are available for the user?</summary><br><b> Expedited, Standard and Bulk </b></details> <details> <summary>True or False? Each AWS account can store up to 500 PetaByte of data. Any additional storage will cost double</summary><br><b> False. Unlimited capacity. </b></details> <details> <summary>Explain what is Storage Gateway</summary><br><b> \"AWS Storage Gateway is a hybrid cloud storage service that gives you on-premises access to virtually unlimited cloud storage\". More on Storage Gateway [here](https://aws.amazon.com/storagegateway) </b></details> <details> <summary>Explain the following Storage Gateway deployments types * File Gateway * Volume Gateway * Tape Gateway</summary><br><b> Explained in detail", "metadata": {"source_file": "learning-materials/certificates/aws-cloud-practitioner.md", "section": "AWS Storage", "language": "en", "created_at": "2025-07-19T19:22:02.049700"}}
{"text": "[here](https://aws.amazon.com/storagegateway/faqs) </b></details> <details> <summary>What is the difference between stored volumes and cached volumes?</summary><br><b> Stored Volumes - Data is located at customer's data center and periodically backed up to AWS Cached Volumes - Data is stored in AWS cloud and cached at customer's data center for quick access </b></details> <details> <summary>What is \"Amazon S3 Transfer Acceleration\"?</summary><br><b> AWS definition: \"Amazon S3 Transfer Acceleration enables fast, easy, and secure transfers of files over long distances between your client and an S3 bucket\" Learn more [here](https://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration.html) </b></details> <details> <summary>What is Amazon EFS?</summary><br><b> Amazon definition: \"Amazon Elastic File System (Amazon EFS) provides a simple, scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources.\" Learn more", "metadata": {"source_file": "learning-materials/certificates/aws-cloud-practitioner.md", "section": "AWS Storage", "language": "en", "created_at": "2025-07-19T19:22:02.049745"}}
{"text": "[here](https://aws.amazon.com/efs) </b></details> <details> <summary>What is AWS Snowmobile?</summary><br><b> \"AWS Snowmobile is an Exabyte-scale data transfer service used to move extremely large amounts of data to AWS.\" Learn more [here](https://aws.amazon.com/snowmobile) </b></details>", "metadata": {"source_file": "learning-materials/certificates/aws-cloud-practitioner.md", "section": "AWS Storage", "language": "en", "created_at": "2025-07-19T19:22:02.049770"}}
{"text": "<details> <summary>What is IAM? What are some of its features?</summary><br><b> IAM stands for Identity and Access Management, and is used for managing users, groups, access policies & roles Full explanation is [here](https://aws.amazon.com/iam) </b></details> <details> <summary>True or False? IAM configuration is defined globally and not per region</summary><br><b> True </b></details> <details> <summary>True or False? When creating an AWS account, root account is created by default. This is the recommended account to use and share in your organization</summary><br><b> False. Instead of using the root account, you should be creating users and use them. </b></details> <details> <summary>True or False? Groups in AWS IAM, can contain only users and not other groups</summary><br><b> True </b></details> <details> <summary>True or False? Users in AWS IAM, can belong only to a single group</summary><br><b> False. Users can belong to multiple groups. </b></details> <details> <summary>What are", "metadata": {"source_file": "learning-materials/certificates/aws-cloud-practitioner.md", "section": "AWS IAM", "language": "en", "created_at": "2025-07-19T19:22:02.050131"}}
{"text": "Roles?</summary><br><b> A way for allowing a service of AWS to use another service of AWS. You assign roles to AWS resources. For example, you can make use of a role which allows EC2 service to accesses s3 buckets (read and write). </b></details> <details> <summary>What are Policies?</summary><br><b> Policies documents used to give permissions as to what a user, group or role are able to do. Their format is JSON. </b></details> <details> <summary>A user is unable to access an s3 bucket. What might be the problem?</summary><br><b> There can be several reasons for that. One of them is lack of policy. To solve that, the admin has to attach the user with a policy what allows him to access the s3 bucket. </b></details> <details> <summary>What should you use to: * Grant access between two services/resources? * Grant user access to resources/services?</summary><br><b> * Role * Policy </b></details> <details> <summary>What permissions does a new user have?</summary><br><b> Only a login access.", "metadata": {"source_file": "learning-materials/certificates/aws-cloud-practitioner.md", "section": "AWS IAM", "language": "en", "created_at": "2025-07-19T19:22:02.050179"}}
{"text": "</b></details>", "metadata": {"source_file": "learning-materials/certificates/aws-cloud-practitioner.md", "section": "AWS IAM", "language": "en", "created_at": "2025-07-19T19:22:02.050201"}}
{"text": "<details> <summary>What is ELB (Elastic Load Balancing)?</summary><br><b> AWS definition: \"Elastic Load Balancing automatically distributes incoming application traffic across multiple targets, such as Amazon EC2 instances, containers, IP addresses, and Lambda functions.\" More on ELB [here](https://aws.amazon.com/elasticloadbalancing) </b></details> <details> <summary>What is auto scaling?</summary><br><b> AWS definition: \"AWS Auto Scaling monitors your applications and automatically adjusts capacity to maintain steady, predictable performance at the lowest possible cost\" Read more about auto scaling [here](https://aws.amazon.com/autoscaling) </b></details> <details> <summary>True or False? Auto Scaling is about adding resources (such as instances) and not about removing resource</summary><br><b> False. Auto scaling adjusts capacity and this can mean removing some resources based on usage and performances. </b></details> <details> <summary>What types of load balancers are supported in", "metadata": {"source_file": "learning-materials/certificates/aws-cloud-practitioner.md", "section": "AWS ELB", "language": "en", "created_at": "2025-07-19T19:22:02.050344"}}
{"text": "EC2 and what are they used for?</summary><br><b> * Application LB - layer 7 traffic * Network LB - ultra-high performances or static IP address * Classic LB - low costs, good for test or dev environments </b></details>", "metadata": {"source_file": "learning-materials/certificates/aws-cloud-practitioner.md", "section": "AWS ELB", "language": "en", "created_at": "2025-07-19T19:22:02.050369"}}
{"text": "<details> <summary>What is Route 53?</summary><br><b> \"Amazon Route 53 is a highly available and scalable cloud Domain Name System (DNS) web service\" Some of Route 53 features: * Register domain * DNS service - domain name translations * Health checks - verify your app is available More on Route 53 [here](https://aws.amazon.com/route53) </b></details>", "metadata": {"source_file": "learning-materials/certificates/aws-cloud-practitioner.md", "section": "AWS DNS", "language": "en", "created_at": "2025-07-19T19:22:02.050413"}}
{"text": "<details> <summary>Explain what is CloudFront</summary><br><b> AWS definition: \"Amazon CloudFront is a fast content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to customers globally with low latency, high transfer speeds, all within a developer-friendly environment.\" More on CloudFront [here](https://aws.amazon.com/cloudfront) </b></details> <details> <summary>Explain the following * Origin * Edge location * Distribution</summary><br><b> </b></details>", "metadata": {"source_file": "learning-materials/certificates/aws-cloud-practitioner.md", "section": "AWS CloudFront", "language": "en", "created_at": "2025-07-19T19:22:02.050463"}}
{"text": "<details> <summary>What is AWS CloudWatch?</summary><br><b> AWS definition: \"Amazon CloudWatch is a monitoring and observability service...\" More on CloudWatch [here](https://aws.amazon.com/cloudwatch) </b></details> <details> <summary>What is AWS CloudTrail?</summary><br><b> AWS definition: \"AWS CloudTrail is a service that enables governance, compliance, operational auditing, and risk auditing of your AWS account.\" Read more on CloudTrail [here](https://aws.amazon.com/cloudtrail) </b></details> <details> <summary>What is Simply Notification Service?</summary><br><b> AWS definition: \"a highly available, durable, secure, fully managed pub/sub messaging service that enables you to decouple microservices, distributed systems, and serverless applications.\" Read more about it [here](https://aws.amazon.com/sns) </b></details> <details> <summary>Explain the following in regards to SNS: * Topics * Subscribers * Publishers</summary><br><b> * Topics - used for grouping multiple endpoints *", "metadata": {"source_file": "learning-materials/certificates/aws-cloud-practitioner.md", "section": "AWS Monitoring & Logging", "language": "en", "created_at": "2025-07-19T19:22:02.050594"}}
{"text": "Subscribers - the endpoints where topics send messages to * Publishers - the provider of the message (event, person, ...) </b></details>", "metadata": {"source_file": "learning-materials/certificates/aws-cloud-practitioner.md", "section": "AWS Monitoring & Logging", "language": "en", "created_at": "2025-07-19T19:22:02.050613"}}
{"text": "<details> <summary>What is the shared responsibility model? What AWS is responsible for and what the user is responsible for based on the shared responsibility model?</summary><br><b> The shared responsibility model defines what the customer is responsible for and what AWS is responsible for. For example, AWS is responsible for security \"of\" the cloud, while the customer is responsible for security \"in\" the cloud. More on the shared responsibility model [here](https://aws.amazon.com/compliance/shared-responsibility-model) </b></details> <details> <summary>True or False? Based on the shared responsibility model, Amazon is responsible for physical CPUs and security groups on instances</summary><br><b> False. It is responsible for Hardware in its sites but not for security groups which created and managed by the users. </b></details> <details> <summary>Explain \"Shared Controls\" in regards to the shared responsibility model</summary><br><b> AWS definition: \"apply to both the infrastructure", "metadata": {"source_file": "learning-materials/certificates/aws-cloud-practitioner.md", "section": "AWS Security", "language": "en", "created_at": "2025-07-19T19:22:02.051428"}}
{"text": "layer and customer layers, but in completely separate contexts or perspectives. In a shared control, AWS provides the requirements for the infrastructure and the customer must provide their own control implementation within their use of AWS services\" Learn more about it [here](https://aws.amazon.com/compliance/shared-responsibility-model) </b></details> <details> <summary>What is the AWS compliance program?</summary><br><b> </b></details> <details> <summary>What is AWS Artifact?</summary><br><b> AWS definition: \"AWS Artifact is your go-to, central resource for compliance-related information that matters to you. It provides on-demand access to AWS’ security and compliance reports and select online agreements.\" Read more about it [here](https://aws.amazon.com/artifact) </b></details> <details> <summary>What is AWS Inspector?</summary><br><b> AWS definition: \"Amazon Inspector is an automated security assessment service that helps improve the security and compliance of applications", "metadata": {"source_file": "learning-materials/certificates/aws-cloud-practitioner.md", "section": "AWS Security", "language": "en", "created_at": "2025-07-19T19:22:02.051458"}}
{"text": "deployed on AWS. Amazon Inspector automatically assesses applications for exposure, vulnerabilities, and deviations from best practices.\"\" Learn more [here](https://aws.amazon.com/inspector) </b></details> <details> <summary>What is AWS Guarduty?</summary><br><b> Guarduty is a threat detection service that monitors your AWS accounts to help detect and mitigate malicious activity </b></details> <details> <summary>What is AWS Shield?</summary><br><b> AWS definition: \"AWS Shield is a managed Distributed Denial of Service (DDoS) protection service that safeguards applications running on AWS.\" </b></details> <details> <summary>What is AWS WAF? Give an example of how it can used and describe what resources or services you can use it with</summary><br><b> An AWS Web Application Firewall (WAF) can filter out unwanted web traffic (bots), and protect against attacks like SQL injection and cross-site scripting. One service you could use it with would be Amazon CloudFront, a CDN service, to block", "metadata": {"source_file": "learning-materials/certificates/aws-cloud-practitioner.md", "section": "AWS Security", "language": "en", "created_at": "2025-07-19T19:22:02.051483"}}
{"text": "attacks before they reach your origin servers </b></details> <details> <summary>What AWS VPN is used for?</summary><br><b> </b></details> <details> <summary>What is the difference between Site-to-Site VPN and Client VPN?</summary><br><b> </b></details> <details> <summary>What is AWS CloudHSM?</summary><br><b> Amazon definition: \"AWS CloudHSM is a cloud-based hardware security module (HSM) that enables you to easily generate and use your own encryption keys on the AWS Cloud.\" Learn more [here](https://aws.amazon.com/cloudhsm) </b></details> <details> <summary>True or False? AWS Inspector can perform both network and host assessments</summary><br><b> True </b></details> <details> <summary>What is AWS Acceptable Use Policy?</summary><br><b> It describes prohibited uses of the web services offered by AWS. More on AWS Acceptable Use Policy [here](https://aws.amazon.com/aup) </b></details> <details> <summary>What is AWS Key Management Service (KMS)?</summary><br><b> AWS definition: \"KMS", "metadata": {"source_file": "learning-materials/certificates/aws-cloud-practitioner.md", "section": "AWS Security", "language": "en", "created_at": "2025-07-19T19:22:02.051575"}}
{"text": "makes it easy for you to create and manage cryptographic keys and control their use across a wide range of AWS services and in your applications.\" More on KMS [here](https://aws.amazon.com/kms) </b></details> <details> <summary>True or False? A user is not allowed to perform penetration testing on any of the AWS services</summary><br><b> False. On some services, like EC2, CloudFront and RDS, penetration testing is allowed. </b></details> <details> <summary>True or False? DDoS attack is an example of allowed penetration testing activity</summary><br><b> False. </b></details> <details> <summary>True or False? AWS Access Key is a type of MFA device used for AWS resources protection</summary><br><b> False. Security key is an example of an MFA device. </b></details> <details> <summary>What is Amazon Cognito?</summary><br><b> Amazon definition: \"Amazon Cognito handles user authentication and authorization for your web and mobile apps.\" Learn more", "metadata": {"source_file": "learning-materials/certificates/aws-cloud-practitioner.md", "section": "AWS Security", "language": "en", "created_at": "2025-07-19T19:22:02.051602"}}
{"text": "[here](https://docs.aws.amazon.com/cognito/index.html) </b></details> <details> <summary>What is AWS ACM?</summary><br><b> Amazon definition: \"AWS Certificate Manager is a service that lets you easily provision, manage, and deploy public and private Secure Sockets Layer/Transport Layer Security (SSL/TLS) certificates for use with AWS services and your internal connected resources.\" Learn more [here](https://aws.amazon.com/certificate-manager) </b></details>", "metadata": {"source_file": "learning-materials/certificates/aws-cloud-practitioner.md", "section": "AWS Security", "language": "en", "created_at": "2025-07-19T19:22:02.051622"}}
{"text": "<details> <summary>What is AWS RDS?</summary><br><b> Amazon Relational Database Service (RDS) is a service for setting up and managing resizable, cost-efficient relational databases resource Learn more [here](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Welcome.html) </b></details> <details> <summary>What is AWS DynamoDB?</summary><br><b> </b></details> <details> <summary>Explain \"Point-in-Time Recovery\" feature in DynamoDB</summary><br><b> Amazon definition: \"You can create on-demand backups of your Amazon DynamoDB tables, or you can enable continuous backups using point-in-time recovery. For more information about on-demand backups, see On-Demand Backup and Restore for DynamoDB.\" Learn more [here](https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/PointInTimeRecovery.html) </b></details> <details> <summary>Explain \"Global Tables\" in DynamoDB</summary><br><b> Amazon definition: \"A global table is a collection of one or more replica tables, all owned by a single", "metadata": {"source_file": "learning-materials/certificates/aws-cloud-practitioner.md", "section": "AWS Databases", "language": "en", "created_at": "2025-07-19T19:22:02.052205"}}
{"text": "AWS account.\" Learn more [here](https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/V2globaltables_HowItWorks.html) </b></details> <details> <summary>What is DynamoDB Accelerator?</summary><br><b> Amazon definition: \"Amazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for DynamoDB that delivers up to a 10x performance improvement – from milliseconds to microseconds...\" Learn more [here](https://aws.amazon.com/dynamodb/dax) </b></details> <details> <summary>What is AWS Redshift and how is it different than RDS?</summary><br><b> AWS Redshift is a cloud data warehousing service that is geared towards handling massive amounts of data (think petabytes) and being able to execute complex queries. In contrast, Amazon RDS is best suited for things like web applications requiring simple queries with more frequent transactions, and on a smaller scale. </b></details> <details> <summary>What is AWS ElastiCache? For what cases is it", "metadata": {"source_file": "learning-materials/certificates/aws-cloud-practitioner.md", "section": "AWS Databases", "language": "en", "created_at": "2025-07-19T19:22:02.052233"}}
{"text": "used?</summary><br><b> Amazon Elasticache is a fully managed Redis or Memcached in-memory data store. It's great for use cases like two-tier web applications where the most frequently accesses data is stored in ElastiCache so response time is optimal. </b></details> <details> <summary>What is Amazon Aurora</summary><br><b> A MySQL & Postgresql based relational database. Also, the default database proposed for the user when using RDS for creating a database. Great for use cases like two-tier web applications that has a MySQL or Postgresql database layer and you need automated backups for your application. </b></details> <details> <summary>What is Amazon DocumentDB?</summary><br><b> Amazon definition: \"Amazon DocumentDB (with MongoDB compatibility) is a fast, scalable, highly available, and fully managed document database service that supports MongoDB workloads. As a document database, Amazon DocumentDB makes it easy to store, query, and index JSON data.\" Learn more", "metadata": {"source_file": "learning-materials/certificates/aws-cloud-practitioner.md", "section": "AWS Databases", "language": "en", "created_at": "2025-07-19T19:22:02.052257"}}
{"text": "[here](https://aws.amazon.com/documentdb) </b></details> <details> <summary>What \"AWS Database Migration Service\" is used for?</summary><br><b> </b></details> <details> <summary>What type of storage is used by Amazon RDS?</summary><br><b> EBS </b></details> <details> <summary>Explain Amazon RDS Read Replicas</summary><br><b> AWS definition: \"Amazon RDS Read Replicas provide enhanced performance and durability for RDS database (DB) instances. They make it easy to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads.\" Read more about [here](https://aws.amazon.com/rds/features/read-replicas) </b></details>", "metadata": {"source_file": "learning-materials/certificates/aws-cloud-practitioner.md", "section": "AWS Databases", "language": "en", "created_at": "2025-07-19T19:22:02.052276"}}
{"text": "<details> <summary>Explain what is AWS Lambda</summary><br><b> AWS definition: \"AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume.\" Read more on it [here](https://aws.amazon.com/lambda) </b></details> <details> <summary>True or False? In AWS Lambda, you are charged as long as a function exists, regardless of whether it's running or not</summary><br><b> False. Charges are being made when the code is executed. </b></details> <details> <summary>Which of the following set of languages Lambda supports? * R, Swift, Rust, Kotlin * Python, Ruby, Go * Python, Ruby, PHP</summary><br><b> * Python, Ruby, Go </b></details>", "metadata": {"source_file": "learning-materials/certificates/aws-cloud-practitioner.md", "section": "AWS Serverless Compute", "language": "en", "created_at": "2025-07-19T19:22:02.052385"}}
{"text": "<details> <summary>What would you use for automating code/software deployments?</summary><br><b> AWS CodeDeploy </b></details> <details> <summary>What would you use for easily creating similar AWS environments/resources for different customers?</summary><br><b> CloudFormation </b></details> <details> <summary>Which service would you use for building a website or web application?</summary><br><b> Lightsail or Elastic Beanstalk </b></details> <details> <summary>Which tool would you use for choosing between Reserved instances or On-Demand instances?</summary><br><b> Cost Explorer </b></details> <details> <summary>What would you use to check how many unassociated Elastic IP address you have?</summary><br><b> Trusted Advisor </b></details> <details> <summary>What service allows you to transfer large amounts (Petabytes) of data in and out of the AWS cloud?</summary><br><b> AWS Snowball </b></details> <details> <summary>What provides a virtual network dedicated to your AWS", "metadata": {"source_file": "learning-materials/certificates/aws-cloud-practitioner.md", "section": "Identify the service or tool", "language": "en", "created_at": "2025-07-19T19:22:02.052924"}}
{"text": "account?</summary><br><b> VPC </b></details> <details> <summary>What you would use for having automated backups for an application that has MySQL database layer?</summary><br><b> Amazon Aurora </b></details> <details> <summary>What would you use to migrate on-premise database to AWS?</summary><br><b> AWS Database Migration Service (DMS) </b></details> <details> <summary>What would you use to check why certain EC2 instances were terminated?</summary><br><b> AWS CloudTrail </b></details> <details> <summary>What would you use for SQL database?</summary><br><b> AWS RDS </b></details> <details> <summary>What would you use for NoSQL database?</summary><br><b> AWS DynamoDB </b></details> <details> <summary>What would you use for running SQL queries interactively on S3?</summary><br><b> AWS Athena </b></details> <details> <summary>What would you use for adding image and video analysis to your application?</summary><br><b> AWS Rekognition </b></details> <details> <summary>Which service would", "metadata": {"source_file": "learning-materials/certificates/aws-cloud-practitioner.md", "section": "Identify the service or tool", "language": "en", "created_at": "2025-07-19T19:22:02.052953"}}
{"text": "you use for debugging and improving performances issues with your applications?</summary><br><b> AWS X-Ray </b></details> <details> <summary>Which service is used for sending notifications?</summary><br><b> SNS </b></details> <details> <summary>Which service would you use for monitoring malicious activity and unauthorized behavior in regards to AWS accounts and workloads?</summary><br><b> Amazon GuardDuty </b></details> <details> <summary>Which service would you use for centrally manage billing, control access, compliance, and security across multiple AWS accounts?</summary><br><b> AWS Organizations </b></details> <details> <summary>Which service would you use for web application protection?</summary><br><b> AWS WAF </b></details> <details> <summary>You would like to monitor some of your resources in the different services. Which service would you use for that?</summary><br><b> CloudWatch </b></details> <details> <summary>Which service would you use for performing security", "metadata": {"source_file": "learning-materials/certificates/aws-cloud-practitioner.md", "section": "Identify the service or tool", "language": "en", "created_at": "2025-07-19T19:22:02.052972"}}
{"text": "assessment?</summary><br><b> AWS Inspector </b></details> <details> <summary>Which service would you use for creating DNS record?</summary><br><b> Route 53 </b></details> <details> <summary>What would you use if you need a fully managed document database?</summary><br><b> Amazon DocumentDB </b></details> <details> <summary>Which service would you use to add access control (or sign-up, sign-in forms) to your web/mobile apps?</summary><br><b> AWS Cognito </b></details> <details> <summary>Which service would you use if you need messaging queue?</summary><br><b> Simple Queue Service (SQS) </b></details> <details> <summary>Which service would you use if you need managed DDOS protection?</summary><br><b> AWS Shield </b></details> <details> <summary>Which service would you use if you need store frequently used data for low latency access?</summary><br><b> ElastiCache </b></details> <details> <summary>What would you use to transfer files over long distances between a client and an S3", "metadata": {"source_file": "learning-materials/certificates/aws-cloud-practitioner.md", "section": "Identify the service or tool", "language": "en", "created_at": "2025-07-19T19:22:02.052990"}}
{"text": "bucket?</summary><br><b> Amazon S3 Transfer Acceleration </b></details>", "metadata": {"source_file": "learning-materials/certificates/aws-cloud-practitioner.md", "section": "Identify the service or tool", "language": "en", "created_at": "2025-07-19T19:22:02.053007"}}
{"text": "<details> <summary>What is AWS Organizations?</summary><br><b> AWS definition: \"AWS Organizations helps you centrally govern your environment as you grow and scale your workloads on AWS.\" More on Organizations [here](https://aws.amazon.com/organizations) </b></details> <details> <summary>Explain AWS pricing model</summary><br><b> It mainly works on \"pay-as-you-go\" meaning you pay only for what are using and when you are using it. In s3 you pay for 1. How much data you are storing 2. Making requests (PUT, POST, ...) In EC2 it's based on the purchasing option (on-demand, spot, ...), instance type, AMI type and the region used. More on AWS pricing model [here](https://aws.amazon.com/pricing) </b></details> <details> <summary>How one should estimate AWS costs when for example comparing to on-premise solutions?</summary><br><b> * TCO calculator * AWS simple calculator * Cost Explorer </b></details> <details> <summary>What basic support in AWS includes?</summary><br><b> * 24x7 customer", "metadata": {"source_file": "learning-materials/certificates/aws-cloud-practitioner.md", "section": "AWS Billing & Support", "language": "en", "created_at": "2025-07-19T19:22:02.053372"}}
{"text": "service * Trusted Advisor * AWS personal Health Dashoard </b></details> <details> <summary>How are EC2 instances billed?</summary><br><b> </b></details> <details> <summary>What AWS Pricing Calculator is used for?</summary><br><b> </b></details> <details> <summary>What is Amazon Connect?</summary><br><b> Amazon definition: \"Amazon Connect is an easy to use omnichannel cloud contact center that helps companies provide superior customer service at a lower cost.\" Learn more [here](https://aws.amazon.com/connect) </b></details> <details> <summary>What are \"APN Consulting Partners\"?</summary><br><b> Amazon definition: \"APN Consulting Partners are professional services firms that help customers of all types and sizes design, architect, build, migrate, and manage their workloads and applications on AWS, accelerating their journey to the cloud.\" Learn more [here](https://aws.amazon.com/partners/consulting) </b></details> <details> <summary>Which of the following are AWS accounts types (and are", "metadata": {"source_file": "learning-materials/certificates/aws-cloud-practitioner.md", "section": "AWS Billing & Support", "language": "en", "created_at": "2025-07-19T19:22:02.053393"}}
{"text": "sorted by order)? * Basic, Developer, Business, Enterprise * Newbie, Intermediate, Pro, Enterprise * Developer, Basic, Business, Enterprise * Beginner, Pro, Intermediate Enterprise</summary><br><b> * Basic, Developer, Business, Enterprise </b></details> <details> <summary>True or False? Region is a factor when it comes to EC2 costs/pricing</summary><br><b> True. You pay differently based on the chosen region. </b></details> <details> <summary>What is \"AWS Infrastructure Event Management\"?</summary><br><b> AWS Definition: \"AWS Infrastructure Event Management is a structured program available to Enterprise Support customers (and Business Support customers for an additional fee) that helps you plan for large-scale events such as product or application launches, infrastructure migrations, and marketing events.\" </b></details>", "metadata": {"source_file": "learning-materials/certificates/aws-cloud-practitioner.md", "section": "AWS Billing & Support", "language": "en", "created_at": "2025-07-19T19:22:02.053412"}}
{"text": "<details> <summary>What is AWS CodeDeploy?</summary><br><b> Amazon definition: \"AWS CodeDeploy is a fully managed deployment service that automates software deployments to a variety of compute services such as Amazon EC2, AWS Fargate, AWS Lambda, and your on-premises servers.\" Learn more [here](https://aws.amazon.com/codedeploy) </b></details> <details> <summary>Explain what is CloudFormation</summary><br><b> </b></details>", "metadata": {"source_file": "learning-materials/certificates/aws-cloud-practitioner.md", "section": "AWS Automation", "language": "en", "created_at": "2025-07-19T19:22:02.053474"}}
{"text": "<details> <summary>What is AWS Lightsail?</summary><br><b> AWS definition: \"Lightsail is an easy-to-use cloud platform that offers you everything needed to build an application or website, plus a cost-effective, monthly plan.\" </b></details> <details> <summary>What is AWS Rekognition?</summary><br><b> AWS definition: \"Amazon Rekognition makes it easy to add image and video analysis to your applications using proven, highly scalable, deep learning technology that requires no machine learning expertise to use.\" Learn more [here](https://aws.amazon.com/rekognition) </b></details> <details> <summary>What AWS Resource Groups used for?</summary><br><b> Amazon definition: \"You can use resource groups to organize your AWS resources. Resource groups make it easier to manage and automate tasks on large numbers of resources at one time. \" Learn more [here](https://docs.aws.amazon.com/ARG/latest/userguide/welcome.html) </b></details> <details> <summary>What is AWS Global", "metadata": {"source_file": "learning-materials/certificates/aws-cloud-practitioner.md", "section": "AWS Misc", "language": "en", "created_at": "2025-07-19T19:22:02.054271"}}
{"text": "Accelerator?</summary><br><b> Amazon definition: \"AWS Global Accelerator is a service that improves the availability and performance of your applications with local or global users...\" Learn more [here](https://aws.amazon.com/global-accelerator) </b></details> <details> <summary>What is AWS Config?</summary><br><b> Amazon definition: \"AWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources.\" Learn more [here](https://aws.amazon.com/config) </b></details> <details> <summary>What is AWS X-Ray?</summary><br><b> AWS definition: \"AWS X-Ray helps developers analyze and debug production, distributed applications, such as those built using a microservices architecture.\" Learn more [here](https://aws.amazon.com/xray) </b></details> <details> <summary>What is AWS OpsWorks?</summary><br><b> Amazon definition: \"AWS OpsWorks is a configuration management service that provides managed instances of Chef and Puppet.\" Learn more about it", "metadata": {"source_file": "learning-materials/certificates/aws-cloud-practitioner.md", "section": "AWS Misc", "language": "en", "created_at": "2025-07-19T19:22:02.054301"}}
{"text": "[here](https://aws.amazon.com/opsworks) </b></details> <details> <summary>What is AWS Service Catalog?</summary><br><b> Amazon definition: \"AWS Service Catalog allows organizations to create and manage catalogs of IT services that are approved for use on AWS.\" Learn more [here](https://aws.amazon.com/servicecatalog) </b></details> <details> <summary>What is AWS CAF?</summary><br><b> Amazon definition: \"AWS Professional Services created the AWS Cloud Adoption Framework (AWS CAF) to help organizations design and travel an accelerated path to successful cloud adoption. \" Learn more [here](https://aws.amazon.com/professional-services/CAF) </b></details> <details> <summary>What is AWS Cloud9?</summary><br><b> AWS definition: \"AWS Cloud9 is a cloud-based integrated development environment (IDE) that lets you write, run, and debug your code with just a browser\" </b></details> <details> <summary>What is AWS Application Discovery Service?</summary><br><b> Amazon definition: \"AWS Application", "metadata": {"source_file": "learning-materials/certificates/aws-cloud-practitioner.md", "section": "AWS Misc", "language": "en", "created_at": "2025-07-19T19:22:02.054321"}}
{"text": "Discovery Service helps enterprise customers plan migration projects by gathering information about their on-premises data centers.\" Learn more [here](https://aws.amazon.com/application-discovery) </b></details> <details> <summary>What is the Trusted Advisor?</summary><br><b> </b></details> <details> <summary>What is the AWS well-architected framework and what pillars it's based on?</summary><br><b> AWS definition: \"The Well-Architected Framework has been developed to help cloud architects build secure, high-performing, resilient, and efficient infrastructure for their applications. Based on five pillars — operational excellence, security, reliability, performance efficiency, and cost optimization\" Learn more [here](https://aws.amazon.com/architecture/well-architected) </b></details> <details> <summary>What AWS services are serverless (or have the option to be serverless)?</summary><br><b> AWS Lambda AWS Athena </b></details> <details> <summary>What is AWS EMR?</summary><br><b> AWS", "metadata": {"source_file": "learning-materials/certificates/aws-cloud-practitioner.md", "section": "AWS Misc", "language": "en", "created_at": "2025-07-19T19:22:02.054340"}}
{"text": "definition: \"big data platform for processing vast amounts of data using open source tools such as Apache Spark, Apache Hive, Apache HBase, Apache Flink, Apache Hudi, and Presto.\" Learn more [here](https://aws.amazon.com/emr) </b></details> <details> <summary>What is AWS Athena?</summary><br><b> \"Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL.\" Learn more about AWS Athena [here](https://aws.amazon.com/athena) </b></details> <details> <summary>What is Amazon Cloud Directory?</summary><br><b> Amazon definition: \"Amazon Cloud Directory is a highly available multi-tenant directory-based store in AWS. These directories scale automatically to hundreds of millions of objects as needed for applications.\" Learn more [here](https://docs.aws.amazon.com/clouddirectory/latest/developerguide/what_is_cloud_directory.html) </b></details> <details> <summary>What is AWS Elastic Beanstalk?</summary><br><b> AWS definition: \"AWS Elastic", "metadata": {"source_file": "learning-materials/certificates/aws-cloud-practitioner.md", "section": "AWS Misc", "language": "en", "created_at": "2025-07-19T19:22:02.054368"}}
{"text": "Beanstalk is an easy-to-use service for deploying and scaling web applications and services...You can simply upload your code and Elastic Beanstalk automatically handles the deployment\" Learn more about it [here](https://aws.amazon.com/elasticbeanstalk) </b></details> <details> <summary>What is AWS SWF?</summary><br><b> Amazon definition: \"Amazon SWF helps developers build, run, and scale background jobs that have parallel or sequential steps. You can think of Amazon SWF as a fully-managed state tracker and task coordinator in the Cloud.\" Learn more on Amazon Simple Workflow Service [here](https://aws.amazon.com/swf) </b></details> <details> <summary>What is Simple Queue Service (SQS)?</summary><br><b> AWS definition: \"Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications\". Learn more about it [here](https://aws.amazon.com/sqs) </b></details>", "metadata": {"source_file": "learning-materials/certificates/aws-cloud-practitioner.md", "section": "AWS Misc", "language": "en", "created_at": "2025-07-19T19:22:02.054387"}}
{"text": "<details> <summary>In regards to disaster recovery, what is RTO and RPO?</summary><br><b> RTO - The maximum acceptable length of time that your application can be offline. RPO - The maximum acceptable length of time during which data might be lost from your application due to an incident. </b></details> <details> <summary>What types of disaster recovery techniques AWS supports?</summary><br><b> * The Cold Method - Periodically backups and sending the backups off-site<br> * Pilot Light - Data is mirrored to an environment which is always running * Warm Standby - Running scaled down version of production environment * Multi-site - Duplicated environment that is always running </b></details> <details> <summary>Which disaster recovery option has the highest downtime and which has the lowest?</summary><br><b> Lowest - Multi-site Highest - The cold method </b></details>", "metadata": {"source_file": "learning-materials/certificates/aws-cloud-practitioner.md", "section": "AWS Disaster Recovery", "language": "en", "created_at": "2025-07-19T19:22:02.054539"}}
{"text": "Good luck! You can do it :)", "metadata": {"source_file": "learning-materials/certificates/aws-cloud-practitioner.md", "section": "Final Note", "language": "en", "created_at": "2025-07-19T19:22:02.054580"}}
{"text": "Last update: 2021", "metadata": {"source_file": "learning-materials/certificates/aws-solutions-architect-associate.md", "section": "AWS - Solutions Architect Associate", "language": "en", "created_at": "2025-07-19T19:22:02.055281"}}
{"text": "<details> <summary>Explain the following * Availability zone * Region * Edge location</summary><br><b> AWS regions are data centers hosted across different geographical locations worldwide, each region is completely independent of one another.<br> Within each region, there are multiple isolated locations known as Availability Zones. Multiple availability zones ensure high availability in case one of them goes down.<br> Edge locations are basically content delivery network which caches data and insures lower latency and faster delivery to the users in any location. They are located in major cities in the world. </b></details>", "metadata": {"source_file": "learning-materials/certificates/aws-solutions-architect-associate.md", "section": "AWS Global Infrastructure", "language": "en", "created_at": "2025-07-19T19:22:02.055389"}}
{"text": "<details> <summary>What is IAM? What are some of its features?</summary><br><b> Full explanation is [here](https://aws.amazon.com/iam) In short: it's used for managing users, groups, access policies & roles </b></details> <details> <summary>True or False? IAM configuration is defined globally and not per region</summary><br><b> True </b></details> <details> <summary>True or False? When creating an AWS account, root account is created by default. This is the recommended account to use and share in your organization</summary><br><b> False. Instead of using the root account, you should be creating users and use them. </b></details> <details> <summary>True or False? Groups in AWS IAM, can contain only users and not other groups</summary><br><b> True </b></details> <details> <summary>True or False? Users in AWS IAM, can belong only to a single group</summary><br><b> False. Users can belong to multiple groups. </b></details> <details> <summary>What are Roles?</summary><br><b> A way for", "metadata": {"source_file": "learning-materials/certificates/aws-solutions-architect-associate.md", "section": "AWS - IAM", "language": "en", "created_at": "2025-07-19T19:22:02.055822"}}
{"text": "allowing a service of AWS to use another service of AWS. You assign roles to AWS resources. For example, you can make use of a role which allows EC2 service to accesses s3 buckets (read and write). </b></details> <details> <summary>What are Policies?</summary><br><b> Policies documents used to give permissions as to what a user, group or role are able to do. Their format is JSON. </b></details> <details> <summary>A user is unable to access an s3 bucket. What might be the problem?</summary><br><b> There can be several reasons for that. One of them is lack of policy. To solve that, the admin has to attach the user with a policy what allows him to access the s3 bucket. </b></details> <details> <summary>What should you use to: * Grant access between two services/resources? * Grant user access to resources/services?</summary><br><b> * Role * Policy </b></details> <details> <summary>What permissions does a new user have?</summary><br><b> Only a login access. </b></details>", "metadata": {"source_file": "learning-materials/certificates/aws-solutions-architect-associate.md", "section": "AWS - IAM", "language": "en", "created_at": "2025-07-19T19:22:02.055852"}}
{"text": "<details> <summary>What is VPC?</summary><br><b> \"A logically isolated section of the AWS cloud where you can launch AWS resources in a virtual network that you define\" Read more about it [here](https://aws.amazon.com/vpc). </b></details> <details> <summary>True or False? VPC spans multiple regions</summary><br><b> False </b></details> <details> <summary>True or False? Subnets belong to the same VPC, can be in different availability zones</summary><br><b> True. Just to clarify, a subnet must reside entirely in one AZ. </b></details> <details> <summary>What is an Internet Gateway?</summary><br><b> \"component that allows communication between instances in your VPC and the internet\" (AWS docs). Read more about it [here](https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Internet_Gateway.html) </b></details> <details> <summary>True or False? NACL allow or deny traffic on the subnet level</summary><br><b> True </b></details> <details> <summary>True or False? Multiple Internet Gateways can", "metadata": {"source_file": "learning-materials/certificates/aws-solutions-architect-associate.md", "section": "AWS Networking", "language": "en", "created_at": "2025-07-19T19:22:02.056074"}}
{"text": "be attached to one VPC</summary><br><b> False. Only one internet gateway can be attached to a single VPC. </b></details> <details> <summary>True or False? Route Tables used to allow or deny traffic from the internet to AWS instances</summary><br><b> False. </b></details> <details> <summary>Explain Security Groups and Network ACLs</summary><br><b> * NACL - security layer on the subnet level. * Security Group - security layer on the instance level. Read more about it [here](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html) and [here](https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html) </b></details> <details> <summary>What is AWS Direct Connect?</summary><br><b> Allows you to connect your corporate network to AWS network. </b></details>", "metadata": {"source_file": "learning-materials/certificates/aws-solutions-architect-associate.md", "section": "AWS Networking", "language": "en", "created_at": "2025-07-19T19:22:02.056095"}}
{"text": "<details> <summary>What is EC2?</summary><br><b> \"a web service that provides secure, resizable compute capacity in the cloud\". Read more [here](https://aws.amazon.com/ec2) </b></details> <details> <summary>True or False? EC2 is a regional service</summary><br><b> True. As opposed to IAM for example, which is a global service, EC2 is a regional service. </b></details> <details> <summary>What is AMI?</summary><br><b> Amazon Machine Images is \"An Amazon Machine Image (AMI) provides the information required to launch an instance\". Read more [here](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html) </b></details> <details> <summary>What are the different source for AMIs?</summary><br><b> * Personal AMIs - AMIs you create * AWS Marketplace for AMIs - Paid AMIs usually with bundled with licensed software * Community AMIs - Free </b></details> <details> <summary>What is instance type?</summary><br><b> \"the instance type that you specify determines the hardware of the host computer", "metadata": {"source_file": "learning-materials/certificates/aws-solutions-architect-associate.md", "section": "AWS Compute", "language": "en", "created_at": "2025-07-19T19:22:02.056503"}}
{"text": "used for your instance\" Read more about instance types [here](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-types.html) </b></details> <details> <summary>True or False? The following are instance types available for a user in AWS: * Compute optimized * Network optimized * Web optimized</summary><br><b> False. From the above list only compute optimized is available. </b></details> <details> <summary>What is EBS?</summary><br><b> \"provides block level storage volumes for use with EC2 instances. EBS volumes behave like raw, unformatted block devices.\" More on EBS [here](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AmazonEBS.html) </b></details> <details> <summary>What EC2 pricing models are there?</summary><br><b> On Demand - pay a fixed rate by the hour/second with no commitment. You can provision and terminate it at any given time. Reserved - you get capacity reservation, basically purchase an instance for a fixed time of period. The longer, the cheaper. Spot -", "metadata": {"source_file": "learning-materials/certificates/aws-solutions-architect-associate.md", "section": "AWS Compute", "language": "en", "created_at": "2025-07-19T19:22:02.056525"}}
{"text": "Enables you to bid whatever price you want for instances or pay the spot price. Dedicated Hosts - physical EC2 server dedicated for your use. </b></details> <details> <summary>What are Security Groups?</summary><br><b> \"A security group acts as a virtual firewall that controls the traffic for one or more instances\" More on this subject [here](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html) </b></details> <details> <summary>What can you attach to an EC2 instance in order to store data?</summary><br><b> EBS </b></details> <details> <summary>What EC2 RI types are there?</summary><br><b> Standard RI - most significant discount + suited for steady-state usage Convertible RI - discount + change attribute of RI + suited for steady-state usage Scheduled RI - launch within time windows you reserve Learn more about EC2 RI [here](https://aws.amazon.com/ec2/pricing/reserved-instances) </b></details>", "metadata": {"source_file": "learning-materials/certificates/aws-solutions-architect-associate.md", "section": "AWS Compute", "language": "en", "created_at": "2025-07-19T19:22:02.056572"}}
{"text": "<details> <summary>What is the shared responsibility model? What AWS is responsible for and what the user is responsible for based on the shared responsibility model?</summary><br><b> The shared responsibility model defines what the customer is responsible for and what AWS is responsible for. More on the shared responsibility model [here](https://aws.amazon.com/compliance/shared-responsibility-model) </b></details> <details> <summary>True or False? Based on the shared responsibility model, Amazon is responsible for physical CPUs and security groups on instances</summary><br><b> False. It is responsible for Hardware in its sites but not for security groups which created and managed by the users. </b></details> <details> <summary>Explain \"Shared Controls\" in regards to the shared responsibility model</summary><br><b> AWS definition: \"apply to both the infrastructure layer and customer layers, but in completely separate contexts or perspectives. In a shared control, AWS provides the", "metadata": {"source_file": "learning-materials/certificates/aws-solutions-architect-associate.md", "section": "AWS Security", "language": "en", "created_at": "2025-07-19T19:22:02.058582"}}
{"text": "requirements for the infrastructure and the customer must provide their own control implementation within their use of AWS services\" Learn more about it [here](https://aws.amazon.com/compliance/shared-responsibility-model) </b></details> <details> <summary>What is the AWS compliance program?</summary><br><b> </b></details> <details> <summary>What is AWS Artifact?</summary><br><b> AWS definition: \"AWS Artifact is your go-to, central resource for compliance-related information that matters to you. It provides on-demand access to AWS’ security and compliance reports and select online agreements.\" Read more about it [here](https://aws.amazon.com/artifact) </b></details> <details> <summary>What is AWS Inspector?</summary><br><b> AWS definition: \"Amazon Inspector is an automated security assessment service that helps improve the security and compliance of applications deployed on AWS. Amazon Inspector automatically assesses applications for exposure, vulnerabilities, and deviations from best", "metadata": {"source_file": "learning-materials/certificates/aws-solutions-architect-associate.md", "section": "AWS Security", "language": "en", "created_at": "2025-07-19T19:22:02.058614"}}
{"text": "practices.\"\" Learn more [here](https://aws.amazon.com/inspector) </b></details> <details> <summary>What is AWS Guarduty?</summary><br><b> </b></details> <details> <summary>What is AWS Shield?</summary><br><b> AWS definition: \"AWS Shield is a managed Distributed Denial of Service (DDoS) protection service that safeguards applications running on AWS.\" </b></details> <details> <summary>What is AWS WAF? Give an example of how it can used and describe what resources or services you can use it with</summary><br><b> </b></details> <details> <summary>What AWS VPN is used for?</summary><br><b> </b></details> <details> <summary>What is the difference between Site-to-Site VPN and Client VPN?</summary><br><b> </b></details> <details> <summary>What is AWS CloudHSM?</summary><br><b> Amazon definition: \"AWS CloudHSM is a cloud-based hardware security module (HSM) that enables you to easily generate and use your own encryption keys on the AWS Cloud.\" Learn more [here](https://aws.amazon.com/cloudhsm)", "metadata": {"source_file": "learning-materials/certificates/aws-solutions-architect-associate.md", "section": "AWS Security", "language": "en", "created_at": "2025-07-19T19:22:02.058639"}}
{"text": "</b></details> <details> <summary>True or False? AWS Inspector can perform both network and host assessments</summary><br><b> True </b></details> <details> <summary>What is AWS Acceptable Use Policy?</summary><br><b> It describes prohibited uses of the web services offered by AWS. More on AWS Acceptable Use Policy [here](https://aws.amazon.com/aup) </b></details> <details> <summary>What is AWS Key Management Service (KMS)?</summary><br><b> AWS definition: \"KMS makes it easy for you to create and manage cryptographic keys and control their use across a wide range of AWS services and in your applications.\" More on KMS [here](https://aws.amazon.com/kms) </b></details> <details> <summary>True or False? A user is not allowed to perform penetration testing on any of the AWS services</summary><br><b> False. On some services, like EC2, CloudFront and RDS, penetration testing is allowed. </b></details> <details> <summary>True or False? DDoS attack is an example of allowed penetration testing", "metadata": {"source_file": "learning-materials/certificates/aws-solutions-architect-associate.md", "section": "AWS Security", "language": "en", "created_at": "2025-07-19T19:22:02.058665"}}
{"text": "activity</summary><br><b> False. </b></details> <details> <summary>True or False? AWS Access Key is a type of MFA device used for AWS resources protection</summary><br><b> False. Security key is an example of an MFA device. </b></details> <details> <summary>What is Amazon Cognito?</summary><br><b> Amazon definition: \"Amazon Cognito handles user authentication and authorization for your web and mobile apps.\" Learn more [here](https://docs.aws.amazon.com/cognito/index.html) </b></details> <details> <summary>What is AWS ACM?</summary><br><b> Amazon definition: \"AWS Certificate Manager is a service that lets you easily provision, manage, and deploy public and private Secure Sockets Layer/Transport Layer Security (SSL/TLS) certificates for use with AWS services and your internal connected resources.\" Learn more [here](https://aws.amazon.com/certificate-manager) </b></details>", "metadata": {"source_file": "learning-materials/certificates/aws-solutions-architect-associate.md", "section": "AWS Security", "language": "en", "created_at": "2025-07-19T19:22:02.058685"}}
{"text": "<details> <summary>What is AWS RDS?</summary><br><b> </b></details> <details> <summary>What is AWS DynamoDB?</summary><br><b> </b></details> <details> <summary>Explain \"Point-in-Time Recovery\" feature in DynamoDB</summary><br><b> Amazon definition: \"You can create on-demand backups of your Amazon DynamoDB tables, or you can enable continuous backups using point-in-time recovery. For more information about on-demand backups, see On-Demand Backup and Restore for DynamoDB.\" Learn more [here](https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/PointInTimeRecovery.html) </b></details> <details> <summary>Explain \"Global Tables\" in DynamoDB</summary><br><b> Amazon definition: \"A global table is a collection of one or more replica tables, all owned by a single AWS account.\" Learn more [here](https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/V2globaltables_HowItWorks.html) </b></details> <details> <summary>What is DynamoDB Accelerator?</summary><br><b> Amazon", "metadata": {"source_file": "learning-materials/certificates/aws-solutions-architect-associate.md", "section": "AWS Databases", "language": "en", "created_at": "2025-07-19T19:22:02.059263"}}
{"text": "definition: \"Amazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for DynamoDB that delivers up to a 10x performance improvement – from milliseconds to microseconds...\" Learn more [here](https://aws.amazon.com/dynamodb/dax) </b></details> <details> <summary>What is AWS Redshift and how is it different than RDS?</summary><br><b> cloud data warehouse </b></details> <details> <summary>What is AWS ElastiCache? For what cases is it used?</summary><br><b> Amazon Elasticache is a fully managed Redis or Memcached in-memory data store. It's great for use cases like two-tier web applications where the most frequently accesses data is stored in ElastiCache so response time is optimal. </b></details> <details> <summary>What is Amazon Aurora</summary><br><b> A MySQL & Postgresql based relational database. Also, the default database proposed for the user when using RDS for creating a database. Great for use cases like two-tier web applications that has a MySQL or", "metadata": {"source_file": "learning-materials/certificates/aws-solutions-architect-associate.md", "section": "AWS Databases", "language": "en", "created_at": "2025-07-19T19:22:02.059312"}}
{"text": "Postgresql database layer and you need automated backups for your application. </b></details> <details> <summary>What is Amazon DocumentDB?</summary><br><b> Amazon definition: \"Amazon DocumentDB (with MongoDB compatibility) is a fast, scalable, highly available, and fully managed document database service that supports MongoDB workloads. As a document database, Amazon DocumentDB makes it easy to store, query, and index JSON data.\" Learn more [here](https://aws.amazon.com/documentdb) </b></details> <details> <summary>What \"AWS Database Migration Service\" is used for?</summary><br><b> </b></details> <details> <summary>What type of storage is used by Amazon RDS?</summary><br><b> EBS </b></details> <details> <summary>Explain Amazon RDS Read Replicas</summary><br><b> AWS definition: \"Amazon RDS Read Replicas provide enhanced performance and durability for RDS database (DB) instances. They make it easy to elastically scale out beyond the capacity constraints of a single DB instance for", "metadata": {"source_file": "learning-materials/certificates/aws-solutions-architect-associate.md", "section": "AWS Databases", "language": "en", "created_at": "2025-07-19T19:22:02.059338"}}
{"text": "read-heavy database workloads.\" Read more about [here](https://aws.amazon.com/rds/features/read-replicas) </b></details>", "metadata": {"source_file": "learning-materials/certificates/aws-solutions-architect-associate.md", "section": "AWS Databases", "language": "en", "created_at": "2025-07-19T19:22:02.059364"}}
{"text": "<details> <summary>What would you use for automating code/software deployments?</summary><br><b> AWS CodeDeploy </b></details> <details> <summary>What would you use for easily creating similar AWS environments/resources for different customers?</summary><br><b> CloudFormation </b></details> <details> <summary>Which service would you use for building a website or web application?</summary><br><b> Lightsail </b></details> <details> <summary>Which tool would you use for choosing between Reserved instances or On-Demand instances?</summary><br><b> Cost Explorer </b></details> <details> <summary>What would you use to check how many unassociated Elastic IP address you have?</summary><br><b> Trusted Advisor </b></details> <details> <summary>What service allows you to transfer large amounts (Petabytes) of data in and out of the AWS cloud?</summary><br><b> AWS Snowball </b></details> <details> <summary>What provides a virtual network dedicated to your AWS account?</summary><br><b> VPC", "metadata": {"source_file": "learning-materials/certificates/aws-solutions-architect-associate.md", "section": "Identify the service or tool", "language": "en", "created_at": "2025-07-19T19:22:02.060023"}}
{"text": "</b></details> <details> <summary>What you would use for having automated backups for an application that has MySQL database layer?</summary><br><b> Amazon Aurora </b></details> <details> <summary>What would you use to migrate on-premise database to AWS?</summary><br><b> AWS Database Migration Service (DMS) </b></details> <details> <summary>What would you use to check why certain EC2 instances were terminated?</summary><br><b> AWS CloudTrail </b></details> <details> <summary>What would you use for SQL database?</summary><br><b> AWS RDS </b></details> <details> <summary>What would you use for NoSQL database?</summary><br><b> AWS DynamoDB </b></details> <details> <summary>What would you use for running SQL queries interactively on S3?</summary><br><b> AWS Athena </b></details> <details> <summary>What would you use for adding image and video analysis to your application?</summary><br><b> AWS Rekognition </b></details> <details> <summary>Which service would you use for debugging and", "metadata": {"source_file": "learning-materials/certificates/aws-solutions-architect-associate.md", "section": "Identify the service or tool", "language": "en", "created_at": "2025-07-19T19:22:02.060053"}}
{"text": "improving performances issues with your applications?</summary><br><b> AWS X-Ray </b></details> <details> <summary>Which service is used for sending notifications?</summary><br><b> SNS </b></details> <details> <summary>Which service would you use for monitoring malicious activity and unauthorized behavior in regards to AWS accounts and workloads?</summary><br><b> Amazon GuardDuty </b></details> <details> <summary>Which service would you use for centrally manage billing, control access, compliance, and security across multiple AWS accounts?</summary><br><b> AWS Organizations </b></details> <details> <summary>Which service would you use for web application protection?</summary><br><b> AWS WAF </b></details> <details> <summary>You would like to monitor some of your resources in the different services. Which service would you use for that?</summary><br><b> CloudWatch </b></details> <details> <summary>Which service would you use for performing security assessment?</summary><br><b> AWS", "metadata": {"source_file": "learning-materials/certificates/aws-solutions-architect-associate.md", "section": "Identify the service or tool", "language": "en", "created_at": "2025-07-19T19:22:02.060073"}}
{"text": "Inspector </b></details> <details> <summary>Which service would you use for creating DNS record?</summary><br><b> Route 53 </b></details> <details> <summary>What would you use if you need a fully managed document database?</summary><br><b> Amazon DocumentDB </b></details> <details> <summary>Which service would you use to add access control (or sign-up, sign-in forms) to your web/mobile apps?</summary><br><b> AWS Cognito </b></details> <details> <summary>Which service would you use if you need messaging queue?</summary><br><b> Simple Queue Service (SQS) </b></details> <details> <summary>Which service would you use if you need managed DDOS protection?</summary><br><b> AWS Shield </b></details> <details> <summary>Which service would you use if you need store frequently used data for low latency access?</summary><br><b> ElastiCache </b></details> <details> <summary>What would you use to transfer files over long distances between a client and an S3 bucket?</summary><br><b> Amazon S3", "metadata": {"source_file": "learning-materials/certificates/aws-solutions-architect-associate.md", "section": "Identify the service or tool", "language": "en", "created_at": "2025-07-19T19:22:02.060098"}}
{"text": "Transfer Acceleration </b></details>", "metadata": {"source_file": "learning-materials/certificates/aws-solutions-architect-associate.md", "section": "Identify the service or tool", "language": "en", "created_at": "2025-07-19T19:22:02.060116"}}
{"text": "[AWS Certification Paths based on Cloud Roles and Responsibilities](https://d1.awsstatic.com/training-and-certification/docs/AWS_certification_paths.pdf)", "metadata": {"source_file": "learning-materials/certificates/aws-certification-paths.md", "section": "AWS Certification Paths", "language": "en", "created_at": "2025-07-19T19:22:02.061538"}}
{"text": "<details> <summary>Deploy a pod called web-1985 using the nginx:alpine image</code></summary><br><b> `kubectl run web-1985 --image=nginx:alpine --restart=Never` </b></details> <details> <summary>How to find out on which node a certain pod is running?</summary><br><b> `kubectl get po -o wide` </b></details>", "metadata": {"source_file": "learning-materials/certificates/cka.md", "section": "Pods", "language": "en", "created_at": "2025-07-19T19:22:02.061639"}}
{"text": "<details> <summary>List all namespaces</code></summary><br><b> kubectl get ns </b></details> <details> <summary>List all the pods in the namespace 'neverland'</code></summary><br><b> kubectl get po -n neverland </b></details> <details> <summary>List all the pods in all the namespaces</code></summary><br><b> kubectl get po --all-namespaces </b></details>", "metadata": {"source_file": "learning-materials/certificates/ckad.md", "section": "Namespaces", "language": "en", "created_at": "2025-07-19T19:22:02.061965"}}
{"text": "[](./sample.png)", "metadata": {"source_file": "learning-materials/scripts/aws s3 event triggering/README.md", "section": "Introduction", "language": "en", "created_at": "2025-07-19T19:22:02.062132"}}
{"text": "boto3==1.17.95", "metadata": {"source_file": "learning-materials/scripts/aws s3 event triggering/s3-lambda/requirements.txt", "section": "General", "language": "en", "created_at": "2025-07-19T19:22:02.062228"}}
{"text": "Set up the following using any log you would like: * Run the following: elasticsearch, logstash, kibana and filebeat (each running in its own container) * Make filebeat transfer a log to logstash for process * Once logstash is done, index with elasticsearch * Finally, make sure data is available in Kibana", "metadata": {"source_file": "learning-materials/topics/eflk.md", "section": "ELK + Filebeat", "language": "en", "created_at": "2025-07-19T19:22:02.062397"}}
{"text": "Create a slack bot to manage cloud instances. You can choose whatever cloud provider you want (e.g. Openstack, AWS, GCP, Azure) You should provide: * Instructions on how to use it * Source code of the slack bot * A running slack bot account or a deployment script so we can test it The bot should be able to support: * Creating new instances * Removing existing instances * Starting an instance * Stopping an instance * Displaying the status of an instance * List all available instances The bot should also be able to show help message.", "metadata": {"source_file": "learning-materials/topics/cloud_slack_bot.md", "section": "Cloud Slack Bot", "language": "en", "created_at": "2025-07-19T19:22:02.062574"}}
{"text": "Write/Create the following Jenkins pipelines: * A pipeline which will run unit tests upon git push to a certain repository * A pipeline which will do to the following: * Provision an instance (can also be a container) * Configure the instance as Apache web server * Deploy a web application on the provisioned instance", "metadata": {"source_file": "learning-materials/topics/jenkins_pipelines.md", "section": "Jenkins Pipelines", "language": "en", "created_at": "2025-07-19T19:22:02.062679"}}
{"text": "Write a pipeline, on any CI/CD system you prefer, that will build am image out of a given Dockerfile and will publish that image to running Kubernetes cluster.", "metadata": {"source_file": "learning-materials/topics/pipeline_deploy_image_to_k8.md", "section": "Build & Publish Docker Images to Kubernetes Cluster", "language": "en", "created_at": "2025-07-19T19:22:02.062799"}}
{"text": "Write the following scripts: * Remove all the jobs which include the string \"REMOVE_ME\" in their name * Remove builds older than 14 days", "metadata": {"source_file": "learning-materials/topics/jenkins_scripts.md", "section": "Jenkins Scripts", "language": "en", "created_at": "2025-07-19T19:22:02.062876"}}
{"text": "* [Remove jobs which include specific string](jenkins/scripts/jobs_with_string.groovy) * [Remove builds older than 14 days](jenkins/scripts/old_builds.groovy)", "metadata": {"source_file": "learning-materials/topics/jenkins_scripts.md", "section": "Answer", "language": "en", "created_at": "2025-07-19T19:22:02.062899"}}
{"text": "Your task is to build an elasticsearch cluster along with Kibana dashboard on one of the following clouds: * AWS * OpenStack * Azure * GCP You have to describe in details (preferably with some drawings) how you are going to set it up. Please describe in detail: - How you scale it up or down - How you quickly (less 20 minutes) provision the cluster - How you apply security policy for access control - How you transfer the logs from the app to ELK - How you deal with multi apps running in different regions", "metadata": {"source_file": "learning-materials/topics/misc/elk_kibana_aws.md", "section": "Elasticsearch, Kibana and AWS", "language": "en", "created_at": "2025-07-19T19:22:02.063305"}}
{"text": "This one out of many possible solutions. This solution is relying heavily on AWS. * Create a VPC with subnet so we can place Elasticsearch node(s) in internal environment only. If required, we will also setup NAT for public access. * Create an IAM role for the access to the cluster. Also, create a separate role for admin access. * To provision the solution quickly, we will use the elasticsearch service directly from AWS for production deployment. This way we also cover multiple AZs. As for authentication, we either use Amazon cognito or the organization LDAP server. * To transfer data, we will have to install logstash agent on the instances. The agent will be responsible for pushing the data to elasticsearch. * For monitoring we will use: * Cloud watch to monitor cluster resource utilization * Cloud metrics dashboard * If access required from multiple regions we will transfer all the data to S3 which will allow us to view the data from different regions and consolidate it in one", "metadata": {"source_file": "learning-materials/topics/misc/elk_kibana_aws.md", "section": "Solution", "language": "en", "created_at": "2025-07-19T19:22:02.063619"}}
{"text": "dashboard", "metadata": {"source_file": "learning-materials/topics/misc/elk_kibana_aws.md", "section": "Solution", "language": "en", "created_at": "2025-07-19T19:22:02.063650"}}
{"text": "1. Run Pod with a web service (e.g. httpd) 2. Verify the web service is running with the `ps` command 3. Check how many restarts the pod has performed 4. Kill the web service process 5. Check how many restarts the pod has performed 6. Verify again the web service is running", "metadata": {"source_file": "learning-materials/topics/kubernetes/killing_containers.md", "section": "\"Killing\" Containers", "language": "en", "created_at": "2025-07-19T19:22:02.063835"}}
{"text": "* Why did the \"RESTARTS\" count raised?", "metadata": {"source_file": "learning-materials/topics/kubernetes/killing_containers.md", "section": "After you complete the exercise", "language": "en", "created_at": "2025-07-19T19:22:02.063861"}}
{"text": "<!-- {% raw %} --> What's your goal? * I would like to prepare for CKA certification * See [CKA](CKA.md) page * I would like to learn Kubernetes by practicing both theoritcal and practical material * Solve [exercises](#kubernetes-exercises) * Solve [questions](#kubernetes-questions) * I would like to learn practical Kubernetes * Solve [exercises](#kubernetes-exercises) - [Kubernetes](#kubernetes) - [Kubernetes Exercises](#kubernetes-exercises) - [Pods](#pods) - [Service](#service) - [ReplicaSet](#replicaset) - [Labels and Selectors](#labels-and-selectors) - [Scheduler](#scheduler) - [Kustomize](#kustomize) - [Kubernetes Questions](#kubernetes-questions) - [Kubernetes 101](#kubernetes-101) - [Cluster and Architecture](#cluster-and-architecture) - [Kubelet](#kubelet) - [Nodes Commands](#nodes-commands) - [Pods](#pods-1) - [Static Pods](#static-pods) - [Pods Commands](#pods-commands) - [Pods Troubleshooting and Debugging](#pods-troubleshooting-and-debugging) - [Labels and", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Kubernetes", "language": "en", "created_at": "2025-07-19T19:22:02.066443"}}
{"text": "Selectors](#labels-and-selectors-1) - [Deployments](#deployments) - [Deployments Commands](#deployments-commands) - [Services](#services) - [Ingress](#ingress) - [ReplicaSets](#replicasets) - [DaemonSet](#daemonset) - [DaemonSet - Commands](#daemonset---commands) - [StatefulSet](#statefulset) - [Storage](#storage) - [Volumes](#volumes) - [Networking](#networking) - [Network Policies](#network-policies) - [etcd](#etcd) - [Namespaces](#namespaces) - [Namespaces - commands](#namespaces---commands) - [Resources Quota](#resources-quota) - [Operators](#operators) - [Secrets](#secrets) - [Volumes](#volumes-1) - [Access Control](#access-control) - [Patterns](#patterns) - [CronJob](#cronjob) - [Misc](#misc) - [Gatekeeper](#gatekeeper) - [Policy Testing](#policy-testing) - [Helm](#helm) - [Commands](#commands) - [Security](#security) - [Troubleshooting Scenarios](#troubleshooting-scenarios) - [Istio](#istio) - [Controllers](#controllers) - [Scheduler](#scheduler-1) - [Node", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Kubernetes", "language": "en", "created_at": "2025-07-19T19:22:02.066484"}}
{"text": "Affinity](#node-affinity) - [Taints](#taints) - [Resource Limits](#resource-limits) - [Resources Limits - Commands](#resources-limits---commands) - [Monitoring](#monitoring) - [Kustomize](#kustomize-1) - [Deployment Strategies](#deployment-strategies) - [Scenarios](#scenarios)", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Kubernetes", "language": "en", "created_at": "2025-07-19T19:22:02.066635"}}
{"text": "|Name|Topic|Objective & Instructions|Solution|Comments| |--------|--------|------|----|----| | My First Pod | Pods | [Exercise](pods_01.md) | [Solution](solutions/pods_01_solution.md) | \"Killing\" Containers | Pods | [Exercise](killing_containers.md) | [Solution](solutions/killing_containers.md)", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Pods", "language": "en", "created_at": "2025-07-19T19:22:02.066674"}}
{"text": "|Name|Topic|Objective & Instructions|Solution|Comments| |--------|--------|------|----|----| | Creating a Service | Service | [Exercise](services_01.md) | [Solution](solutions/services_01_solution.md)", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Service", "language": "en", "created_at": "2025-07-19T19:22:02.066695"}}
{"text": "|Name|Topic|Objective & Instructions|Solution|Comments| |--------|--------|------|----|----| | Creating a ReplicaSet | ReplicaSet | [Exercise](replicaset_01.md) | [Solution](solutions/replicaset_01_solution.md) | Operating ReplicaSets | ReplicaSet | [Exercise](replicaset_02.md) | [Solution](solutions/replicaset_02_solution.md) | ReplicaSets Selectors | ReplicaSet | [Exercise](replicaset_03.md) | [Solution](solutions/replicaset_03_solution.md)", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "ReplicaSet", "language": "en", "created_at": "2025-07-19T19:22:02.066801"}}
{"text": "|Name|Topic|Objective & Instructions|Solution|Comments| |--------|--------|------|----|----| | Labels and Selectors 101 | Labels, Selectors | [Exercise](exercises/labels_and_selectors/exercise.md) | [Solution](exercises/labels_and_selectors/solution.md) | Node Selectors | Labels, Selectors | [Exercise](exercises/node_selectors/exercise.md) | [Solution](exercises/node_selectors/solution.md)", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Labels and Selectors", "language": "en", "created_at": "2025-07-19T19:22:02.066835"}}
{"text": "|Name|Topic|Objective & Instructions|Solution|Comments| |--------|--------|------|----|----| | Taints 101 | Taints | [Exercise](exercises/taints_101/exercise.md) | [Solution](exercises/taints_101/solution.md)", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Scheduler", "language": "en", "created_at": "2025-07-19T19:22:02.066867"}}
{"text": "|Name|Topic|Objective & Instructions|Solution|Comments| |--------|--------|------|----|----| | common labels | Kustomize | [Exercise](exercises/kustomize_common_labels/exercise.md) | [Solution](exercises/kustomize_common_labels/solution.md)", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Kustomize", "language": "en", "created_at": "2025-07-19T19:22:02.066888"}}
{"text": "<details> <summary>What is Kubernetes? Why organizations are using it?</summary><br><b> Kubernetes is an open-source system that provides users with the ability to manage, scale and deploy containerized applications. To understand what Kubernetes is good for, let's look at some examples: * You would like to run a certain application in a container on multiple different locations and sync changes across all of them, no matter where they run * Performing updates and changes across hundreds of containers * Handle cases where the current load requires to scale up (or down) </b></details> <details> <summary>When or why NOT to use Kubernetes?</summary><br><b> - If you manage low level infrastructure or baremetals, Kubernetes is probably not what you need or want - If you are a small team (like less than 20 engineers) running less than a dozen of containers, Kubernetes might be an overkill (even if you need scale, rolling out updates, etc.). You might still enjoy the benefits of using managed", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Kubernetes 101", "language": "en", "created_at": "2025-07-19T19:22:02.067595"}}
{"text": "Kubernetes, but you definitely want to think about it carefully before making a decision on whether to adopt it. </b></details> <details> <summary>What are some of Kubernetes features?</summary><br><b> - Self-Healing: Kubernetes uses health checks to monitor containers and run certain actions upon failure or other type of events, like restarting the container - Load Balancing: Kubernetes can split and/or balance requests to applications running in the cluster, based on the state of the Pods running the application - Operators: Kubernetes packaged applications that can use the API of the cluster to update its state and trigger actions based on events and application state changes - Automated Rollout: Gradual updates roll out to applications and support in roll back in case anything goes wrong - Scaling: Scaling horizontally (down and up) based on different state parameters and custom defined criteria - Secrets: you have a mechanism for storing user names, passwords and service endpoints", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Kubernetes 101", "language": "en", "created_at": "2025-07-19T19:22:02.067625"}}
{"text": "in a private way, where not everyone using the cluster are able to view it </b></details> <details> <summary>What Kubernetes objects are there?</summary><br><b> * Pod * Service * ReplicationController * ReplicaSet * DaemonSet * Namespace * ConfigMap ... </b></details> <details> <summary>What fields are mandatory with any Kubernetes object?</summary><br><b> metadata, kind and apiVersion </b></details> <details> <summary>What is kubectl?</summary><br><b> Kubectl is the Kubernetes command line tool that allows you to run commands against Kubernetes clusters. For example, you can use kubectl to deploy applications, inspect and manage cluster resources, and view logs. </b></details> <details> <summary>What Kubernetes objects do you usually use when deploying applications in Kubernetes?</summary><br><b> * Deployment - creates the Pods () and watches them * Service: route traffic to Pods internally * Ingress: route traffic from outside the cluster </b></details> <details> <summary>Why there", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Kubernetes 101", "language": "en", "created_at": "2025-07-19T19:22:02.067647"}}
{"text": "is no such command in Kubernetes? <code>kubectl get containers</code></summary><br><b> Becaused container is not a Kubernetes object. The smallest object unit in Kubernetes is a Pod. In a single Pod you can find one or more containers. </b></details> <details> <summary>What actions or operations you consider as best practices when it comes to Kubernetes?</summary><br><b> - Always make sure Kubernetes YAML files are valid. Applying automated checks and pipelines is recommended. - Always specify requests and limits to prevent situation where containers are using the entire cluster memory which may lead to OOM issue - Specify labels to logically group Pods, Deployments, etc. Use labels to identify the type of the application for example, among other things </b></details>", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Kubernetes 101", "language": "en", "created_at": "2025-07-19T19:22:02.067666"}}
{"text": "<details> <summary>What is a Kubernetes Cluster?</summary><br><b> Red Hat Definition: \"A Kubernetes cluster is a set of node machines for running containerized applications. If you’re running Kubernetes, you’re running a cluster. At a minimum, a cluster contains a worker node and a master node.\" Read more [here](https://www.redhat.com/en/topics/containers/what-is-a-kubernetes-cluster) </b></details> <details> <summary>What is a Node?</summary><br><b> A node is a virtual or a physical machine that serves as a worker for running the applications.<br> It's recommended to have at least 3 nodes in a production environment. </b></details> <details> <summary>What the master node is responsible for?</summary><br><b> The master coordinates all the workflows in the cluster: * Scheduling applications * Managing desired state * Rolling out new updates </b></details> <details> <summary>Describe shortly and in high-level, what happens when you run <code>kubectl get nodes</code></summary><br><b> 1.", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Cluster and Architecture", "language": "en", "created_at": "2025-07-19T19:22:02.068442"}}
{"text": "Your user is getting authenticated 2. Request is validated by the kube-apiserver 3. Data is retrieved from etcd </b></details> <details> <summary>True or False? Every cluster must have 0 or more master nodes and at least 1 worker</summary><br><b> False. A Kubernetes cluster consists of at least 1 master and can have 0 workers (although that wouldn't be very useful...) </b></details> <details> <summary>What are the components of the master node (aka control plane)?</summary><br><b> * API Server - the Kubernetes API. All cluster components communicate through it * Scheduler - assigns an application with a worker node it can run on * Controller Manager - cluster maintenance (replications, node failures, etc.) * etcd - stores cluster configuration </b></details> <details> <summary>What are the components of a worker node (aka data plane)?</summary><br><b> * Kubelet - an agent responsible for node communication with the master. * Kube-proxy - load balancing traffic between app components *", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Cluster and Architecture", "language": "en", "created_at": "2025-07-19T19:22:02.068646"}}
{"text": "Container runtime - the engine runs the containers (Podman, Docker, ...) </b></details> <details> <summary>Place the components on the right side of the image in the right place in the drawing<br> <img src=\"images/cluster_architecture_exercise.png\"/> </summary><br><b> <img src=\"images/cluster_architecture_solution.png\"/> </b></details> <details> <summary>You are managing multiple Kubernetes clusters. How do you quickly change between the clusters using kubectl?</summary><br><b> `kubectl config use-context` </b></details> <details> <summary>How do you prevent high memory usage in your Kubernetes cluster and possibly issues like memory leak and OOM?</summary><br><b> Apply requests and limits, especially on third party applications (where the uncertainty is even bigger) </b></details> <details> <summary>Do you have experience with deploying a Kubernetes cluster? If so, can you describe the process in high-level?</summary><br><b> 1. Create multiple instances you will use as Kubernetes", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Cluster and Architecture", "language": "en", "created_at": "2025-07-19T19:22:02.068678"}}
{"text": "nodes/workers. Create also an instance to act as the Master. The instances can be provisioned in a cloud or they can be virtual machines on bare metal hosts. 2. Provision a certificate authority that will be used to generate TLS certificates for the different components of a Kubernetes cluster (kubelet, etcd, ...) 1. Generate a certificate and private key for the different components 3. Generate kubeconfigs so the different clients of Kubernetes can locate the API servers and authenticate. 4. Generate encryption key that will be used for encrypting the cluster data 5. Create an etcd cluster </b></details> <details> <summary>Which command will list all the object types in a cluster?</summary><br><b> `kubectl api-resources` </b></details> <details> <summary>What <code>kubectl get componentstatus</code> does?</summary><br><b> Outputs the status of each of the control plane components. </b></details>", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Cluster and Architecture", "language": "en", "created_at": "2025-07-19T19:22:02.068698"}}
{"text": "<details> <summary>What happens to running pods if if you stop Kubelet on the worker nodes?</summary><br><b> When you stop the kubelet service on a worker node, it will no longer be able to communicate with the Kubernetes API server. As a result, the node will be marked as NotReady and the pods running on that node will be marked as Unknown. The Kubernetes control plane will then attempt to reschedule the pods to other available nodes in the cluster. </b></details>", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Kubelet", "language": "en", "created_at": "2025-07-19T19:22:02.068810"}}
{"text": "<details> <summary>Run a command to view all nodes of the cluster</summary><br><b> `kubectl get nodes` Note: You might want to create an alias (`alias k=kubectl`) and get used to `k get no` </b></details> <details> <summary>Create a list of all nodes in JSON format and store it in a file called \"some_nodes.json\"</summary><br><b> `k get nodes -o json > some_nodes.json` </b></details> <details> <summary>Check what labels one of your nodes in the cluster has</summary><br><b> `k get no minikube --show-labels` </b></details>", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Nodes Commands", "language": "en", "created_at": "2025-07-19T19:22:02.068890"}}
{"text": "<details> <summary>Explain what is a Pod</summary><br><b> A Pod is a group of one or more containers, with shared storage and network resources, and a specification for how to run the containers. Pods are the smallest deployable units of computing that you can create and manage in Kubernetes. </b></details> <details> <summary>Deploy a pod called \"my-pod\" using the nginx:alpine image</summary><br><b> `kubectl run my-pod --image=nginx:alpine` If you are a Kubernetes beginner you should know that this is not a common way to run Pods. The common way is to run a Deployment which in turn runs Pod(s). In addition, Pods and/or Deployments are usually defined in files rather than executed directly using only the CLI arguments. </b></details> <details> <summary>What are your thoughts on \"Pods are not meant to be created directly\"?</summary><br><b> Pods are usually indeed not created directly. You'll notice that Pods are usually created as part of another entities such as Deployments or", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Pods", "language": "en", "created_at": "2025-07-19T19:22:02.070687"}}
{"text": "ReplicaSets. If a Pod dies, Kubernetes will not bring it back. This is why it's more useful for example to define ReplicaSets that will make sure that a given number of Pods will always run, even after a certain Pod dies. </b></details> <details> <summary>How many containers can a pod contain?</summary><br><b> A pod can include multiple containers but in most cases it would probably be one container per pod. There are some patterns where it makes to run more than one container like the \"side-car\" pattern where you might want to perform logging or some other operation that is executed by another container running with your app container in the same Pod. </b></details> <details> <summary>What use cases exist for running multiple containers in a single pod?</summary><br><b> A web application with separate (= in their own containers) logging and monitoring components/adapters is one examples.<br> A CI/CD pipeline (using Tekton for example) can run multiple containers in one Pod if a Task", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Pods", "language": "en", "created_at": "2025-07-19T19:22:02.070755"}}
{"text": "contains multiple commands. </b></details> <details> <summary>What are the possible Pod phases?</summary><br><b> * Running - The Pod bound to a node and at least one container is running * Failed/Error - At least one container in the Pod terminated with a failure * Succeeded - Every container in the Pod terminated with success * Unknown - Pod's state could not be obtained * Pending - Containers are not yet running (Perhaps images are still being downloaded or the pod wasn't scheduled yet) </b></details> <details> <summary>True or False? By default, pods are isolated. This means they are unable to receive traffic from any source</summary><br><b> False. By default, pods are non-isolated = pods accept traffic from any source. </b></details> <details> <summary>True or False? The \"Pending\" phase means the Pod was not yet accepted by the Kubernetes cluster so the scheduler can't run it unless it's accepted</summary><br><b> False. \"Pending\" is after the Pod was accepted by the cluster, but", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Pods", "language": "en", "created_at": "2025-07-19T19:22:02.070781"}}
{"text": "the container can't run for different reasons like images not yet downloaded. </b></details> <details> <summary>True or False? A single Pod can be split across multiple nodes</summary><br><b> False. A single Pod can run on a single node. </b></details> <details> <summary>You run a pod and you see the status <code>ContainerCreating</code></summary><br><b> </b></details> <details> <summary>True or False? A volume defined in Pod can be accessed by all the containers of that Pod</summary><br><b> True. </b></details> <details> <summary>What happens when you run a Pod with kubectl?</summary><br><b> 1. Kubectl sends a request to the API server (kube-apiserver) to create the Pod 1. In the the process the user gets authenticated and the request is being validated. 2. etcd is being updated with the data 2. The Scheduler detects that there is an unassigned Pod by monitoring the API server (kube-apiserver) 3. The Scheduler chooses a node to assign the Pod to 1. etcd is being updated with the", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Pods", "language": "en", "created_at": "2025-07-19T19:22:02.070917"}}
{"text": "information 4. The Scheduler updates the API server about which node it chose 5. Kubelet (which also monitors the API server) notices there is a Pod assigned to the same node on which it runs and that Pod isn't running 6. Kubelet sends request to the container engine (e.g. Docker) to create and run the containers 7. An update is sent by Kubelet to the API server (notifying it that the Pod is running) 1. etcd is being updated by the API server again </b></details> <details> <summary>How to confirm a container is running after running the command <code>kubectl run web --image nginxinc/nginx-unprivileged</code></summary><br><b> * When you run `kubectl describe pods <POD_NAME>` it will tell whether the container is running: `Status: Running` * Run a command inside the container: `kubectl exec web -- ls` </b></details> <details> <summary>After running <code>kubectl run database --image mongo</code> you see the status is \"CrashLoopBackOff\". What could possibly went wrong and what do you do", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Pods", "language": "en", "created_at": "2025-07-19T19:22:02.070947"}}
{"text": "to confirm?</summary><br><b> \"CrashLoopBackOff\" means the Pod is starting, crashing, starting...and so it repeats itself.<br> There are many different reasons to get this error - lack of permissions, init-container misconfiguration, persistent volume connection issue, etc. One of the ways to check why it happened it to run `kubectl describe po <POD_NAME>` and having a look at the exit code ``` Last State: Terminated Reason: Error Exit Code: 100 ``` Another way to check what's going on, is to run `kubectl logs <POD_NAME>`. This will provide us with the logs from the containers running in that Pod. </b></details> <details> <summary>Explain the purpose of the following lines ``` livenessProbe: exec: command: - cat - /appStatus initialDelaySeconds: 10 periodSeconds: 5 ``` </summary><br><b> These lines make use of `liveness probe`. It's used to restart a container when it reaches a non-desired state.<br> In this case, if the command `cat /appStatus` fails, Kubernetes will kill the container", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Pods", "language": "en", "created_at": "2025-07-19T19:22:02.070969"}}
{"text": "and will apply the restart policy. The `initialDelaySeconds: 10` means that Kubelet will wait 10 seconds before running the command/probe for the first time. From that point on, it will run it every 5 seconds, as defined with `periodSeconds` </b></details> <details> <summary>Explain the purpose of the following lines ``` readinessProbe: tcpSocket: port: 2017 initialDelaySeconds: 15 periodSeconds: 20 ``` </summary><br><b> They define a readiness probe where the Pod will not be marked as \"Ready\" before it will be possible to connect to port 2017 of the container. The first check/probe will start after 15 seconds from the moment the container started to run and will continue to run the check/probe every 20 seconds until it will manage to connect to the defined port. </b></details> <details> <summary>What does the \"ErrImagePull\" status of a Pod means?</summary><br><b> It wasn't able to pull the image specified for running the container(s). This can happen if the client didn't authenticated", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Pods", "language": "en", "created_at": "2025-07-19T19:22:02.070988"}}
{"text": "for example.<br> More details can be obtained with `kubectl describe po <POD_NAME>`. </b></details> <details> <summary>What happens when you delete a Pod?</summary><br><b> 1. The `TERM` signal is sent to kill the main processes inside the containers of the given Pod 2. Each container is given a period of 30 seconds to shut down the processes gracefully 3. If the grace period expires, the `KILL` signal is used to kill the processes forcefully and the containers as well </b></details> <details> <summary>Explain liveness probes</summary><br><b> Liveness probes is a useful mechanism used for restarting the container when a certain check/probe, the user has defined, fails.<br> For example, the user can define that the command `cat /app/status` will run every X seconds and the moment this command fails, the container will be restarted. You can read more about it in [kubernetes.io](https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes)", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Pods", "language": "en", "created_at": "2025-07-19T19:22:02.071007"}}
{"text": "</b></details> <details> <summary>Explain readiness probes</summary><br><b> readiness probes used by Kubelet to know when a container is ready to start running, accepting traffic.<br> For example, a readiness probe can be to connect port 8080 on a container. Once Kubelet manages to connect it, the Pod is marked as ready You can read more about it in [kubernetes.io](https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes) </b></details> <details> <summary>How readiness probe status affect Services when they are combined?</summary><br><b> Only containers whose state set to Success will be able to receive requests sent to the Service. </b></details> <details> <summary>Why it's common to have only one container per Pod in most cases?</summary><br><b> One reason is that it makes it harder to scale when you need to scale only one of the containers in a given Pod. </b></details> <details> <summary>True or False? Once a Pod is assisgned to a worker", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Pods", "language": "en", "created_at": "2025-07-19T19:22:02.071025"}}
{"text": "node, it will only run on that node, even if it fails at some point and spins up a new Pod</summary><br><b> True. </b></details> <details> <summary>True or False? Each Pod, when created, gets its own public IP address</summary><br><b> False. Each Pod gets an IP address but an internal one and not publicly accessible. To make a Pod externally accessible, we need to use an object called Service in Kubernetes. </b></details>", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Pods", "language": "en", "created_at": "2025-07-19T19:22:02.071049"}}
{"text": "<details> <summary>What are Static Pods?</summary><br><b> [Kubernetes.io](https://kubernetes.io/docs/tasks/configure-pod-container/static-pod/): \"Static Pods are managed directly by the kubelet daemon on a specific node, without the API server observing them. Unlike Pods that are managed by the control plane (for example, a Deployment); instead, the kubelet watches each static Pod (and restarts it if it fails).\" </b></details> <details> <summary>True or False? The same as there are \"Static Pods\" there are other static resources like \"deployments\" and \"replicasets\"</summary><br><b> False. </b></details> <details> <summary>What are some use cases for using Static Pods?</summary><br><b> One clear use case is running Control Plane Pods - running Pods such as kube-apiserver, scheduler, etc. These should run and operate regardless of whether some components of the cluster work or not and they should run on specific nodes of the cluster. </b></details> <details> <summary>How to identify which", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Static Pods", "language": "en", "created_at": "2025-07-19T19:22:02.071435"}}
{"text": "Pods are Static Pods?</summary><br><b> The suffix of the Pods is the same as the name of the nodes on which they are running TODO: check if it's always the case. </b></details> <details> <summary>Which of the following is not a static pod?: * kube-scheduler * kube-proxy * kube-apiserver </summary><br><b> kube-proxy - it's a DaemonSet (since it has to be presented on every node in the cluster). There is no one specific node on which it has to run. </b></details> <details> <summary>Where static Pods manifests are located?</summary><br><b> Most of the time it's in /etc/kubernetes/manifests but you can verify with `grep -i static /var/lib/kubelet/config.yaml` to locate the value of `statisPodsPath`. It might be that your config is in different path. To verify run `ps -ef | grep kubelet` and see what is the value of --config argument of the process `/usr/bin/kubelet` The key itself for defining the path of static Pods is `staticPodPath`. So if your config is in", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Static Pods", "language": "en", "created_at": "2025-07-19T19:22:02.071700"}}
{"text": "`/var/lib/kubelet/config.yaml` you can run `grep staticPodPath /var/lib/kubelet/config.yaml`. </b></details> <details> <summary>Describe how would you delete a static Pod </summary><br><b> Locate the static Pods directory (look at `staticPodPath` in kubelet configuration file). Go to that directory and remove the manifest/definition of the staic Pod (`rm <STATIC_POD_PATH>/<POD_DEFINITION_FILE>`) </b></details>", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Static Pods", "language": "en", "created_at": "2025-07-19T19:22:02.071761"}}
{"text": "<details> <summary>How to check to which worker node the pods were scheduled to? In other words, how to check on which node a certain Pod is running?</summary><br><b> `kubectl get pods -o wide` </b></details> <details> <summary>How to delete a pod?</summary><br><b> `kubectl delete pod pod_name` </b></details> <details> <summary>List all the pods with the label \"env=prod\"</summary><br><b> `k get po -l env=prod` To count them: `k get po -l env=prod --no-headers | wc -l` </b></details> <details> <summary>How to list the pods in the current namespace?</summary><br><b> `kubectl get po` </b></details> <details> <summary>How view all the pods running in all the namespaces?</summary><br><b> `kubectl get pods --all-namespaces` </b></details>", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Pods Commands", "language": "en", "created_at": "2025-07-19T19:22:02.071872"}}
{"text": "<details> <summary>You try to run a Pod but it's in \"Pending\" state. What might be the reason?</summary><br><b> One possible reason is that the scheduler which supposed to schedule Pods on nodes, is not running. To verify it, you can run `kubectl get po -A | grep scheduler` or check directly in `kube-system` namespace. </b></details> <details> <summary>What <code>kubectl logs [pod-name]</code> command does?</summary><br><b> Prints the logs for a container in a pod. </b></details> <details> <summary>What <code>kubectl describe pod [pod name] does?</code> command does?</summary><br><b> Show details of a specific resource or group of resources. </b></details> <details> <summary>Create a static pod with the image <code>python</code> that runs the command <code>sleep 2017</code></summary><br><b> First change to the directory tracked by kubelet for creating static pod: `cd /etc/kubernetes/manifests` (you can verify path by reading kubelet conf file) Now create the definition/manifest in that", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Pods Troubleshooting and Debugging", "language": "en", "created_at": "2025-07-19T19:22:02.072050"}}
{"text": "directory `k run some-pod --image=python --command sleep 2017 --restart=Never --dry-run=client -o yaml > statuc-pod.yaml` </b></details>", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Pods Troubleshooting and Debugging", "language": "en", "created_at": "2025-07-19T19:22:02.072070"}}
{"text": "<details> <summary>Explain Labels</summary><br><b> [Kubernetes.io](https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/): \"Labels are key/value pairs that are attached to objects, such as pods. Labels are intended to be used to specify identifying attributes of objects that are meaningful and relevant to users, but do not directly imply semantics to the core system. Labels can be used to organize and to select subsets of objects. Labels can be attached to objects at creation time and subsequently added and modified at any time. Each object can have a set of key/value labels defined. Each Key must be unique for a given object.\" </b></details> <details> <summary>Explain selectors</summary><br><b> [Kubernetes.io](https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#label-selectors): \"Unlike names and UIDs, labels do not provide uniqueness. In general, we expect many objects to carry the same label(s). Via a label selector, the client/user can", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Labels and Selectors", "language": "en", "created_at": "2025-07-19T19:22:02.072535"}}
{"text": "identify a set of objects. The label selector is the core grouping primitive in Kubernetes. The API currently supports two types of selectors: equality-based and set-based. A label selector can be made of multiple requirements which are comma-separated. In the case of multiple requirements, all must be satisfied so the comma separator acts as a logical AND (&&) operator.\" </b></details> <details> <summary>Provide some actual examples of how labels are used</summary><br><b> * Can be used by the scheduler to place certain Pods (with certain labels) on specific nodes * Used by replicasets to track pods which have to be scaled </b></details> <details> <summary>What are Annotations?</summary><br><b> [Kubernetes.io](https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/): \"You can use Kubernetes annotations to attach arbitrary non-identifying metadata to objects. Clients such as tools and libraries can retrieve this metadata.\" </b></details> <details> <summary>How", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Labels and Selectors", "language": "en", "created_at": "2025-07-19T19:22:02.072556"}}
{"text": "annotations different from labels?</summary><br><b> [Kuberenets.io](Labels can be used to select objects and to find collections of objects that satisfy certain conditions. In contrast, annotations are not used to identify and select objects. The metadata in an annotation can be small or large, structured or unstructured, and can include characters not permitted by labels.): \"Labels can be used to select objects and to find collections of objects that satisfy certain conditions. In contrast, annotations are not used to identify and select objects. The metadata in an annotation can be small or large, structured or unstructured, and can include characters not permitted by labels.\" </b></details> <details> <summary>How to view the logs of a container running in a Pod?</summary><br><b> `k logs POD_NAME` </b></details> <details> <summary>There are two containers inside a Pod called \"some-pod\". What will happen if you run <code>kubectl logs some-pod</code></summary><br><b> It won't work", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Labels and Selectors", "language": "en", "created_at": "2025-07-19T19:22:02.072575"}}
{"text": "because there are two containers inside the Pod and you need to specify one of them with `kubectl logs POD_NAME -c CONTAINER_NAME` </b></details>", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Labels and Selectors", "language": "en", "created_at": "2025-07-19T19:22:02.072594"}}
{"text": "<details> <summary>What is a \"Deployment\" in Kubernetes?</summary><br><b> A Kubernetes Deployment is used to tell Kubernetes how to create or modify instances of the pods that hold a containerized application. Deployments can scale the number of replica pods, enable rollout of updated code in a controlled manner, or roll back to an earlier deployment version if necessary. A Deployment is a declarative statement for the desired state for Pods and Replica Sets. </b></details> <details> <summary>How to create a deployment with the image \"nginx:alpine\"?</code></summary><br><b> `kubectl create deployment my-first-deployment --image=nginx:alpine` OR ``` cat << EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: nginx spec: replicas: 1 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:alpine ``` </b></details> <details> <summary>How to verify a deployment was created?</code></summary><br><b>", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Deployments", "language": "en", "created_at": "2025-07-19T19:22:02.073208"}}
{"text": "`kubectl get deployments` or `kubectl get deploy` This command lists all the Deployment objects created and exist in the cluster. It doesn't mean the deployments are readt and running. This can be checked with the \"READY\" and \"AVAILABLE\" columns. </b></details> <details> <summary>How to edit a deployment?</code></summary><br><b> `kubectl edit deployment <DEPLOYMENT_NAME>` </b></details> <details> <summary>What happens after you edit a deployment and change the image?</summary><br><b> The pod will terminate and another, new pod, will be created. Also, when looking at the replicaset, you'll see the old replica doesn't have any pods and a new replicaset is created. </b></details> <details> <summary>How to delete a deployment?</summary><br><b> One way is by specifying the deployment name: `kubectl delete deployment [deployment_name]` Another way is using the deployment configuration file: `kubectl delete -f deployment.yaml` </b></details> <details> <summary>What happens when you delete a", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Deployments", "language": "en", "created_at": "2025-07-19T19:22:02.073408"}}
{"text": "deployment?</summary><br><b> The pod related to the deployment will terminate and the replicaset will be removed. </b></details> <details> <summary>What happens behind the scenes when you create a Deployment object?</summary><br><b> The following occurs when you run `kubectl create deployment some_deployment --image=nginx` 1. HTTP request sent to kubernetes API server on the cluster to create a new deployment 2. A new Pod object is created and scheduled to one of the workers nodes 3. Kublet on the worker node notices the new Pod and instructs the Container runtime engine to pull the image from the registry 4. A new container is created using the image that was just pulled </b></details> <details> <summary>How make an app accessible on private or external network?</summary><br><b> Using a Service. </b></details> <details> <summary>Can you use a Deployment for stateful applications?</summary><br><b> </b></details> <details> <summary>Fix the following deployment manifest ```yaml", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Deployments", "language": "en", "created_at": "2025-07-19T19:22:02.073441"}}
{"text": "apiVersion: apps/v1 kind: Deploy metadata: creationTimestamp: null labels: app: dep name: dep spec: replicas: 3 selector: matchLabels: app: dep strategy: {} template: metadata: creationTimestamp: null labels: app: dep spec: containers: - image: redis name: redis resources: {} status: {} ``` </summary><br><b> Change `kind: Deploy` to `kind: Deployment` </b></details> <details> <summary>Fix the following deployment manifest ```yaml apiVersion: apps/v1 kind: Deployment metadata: creationTimestamp: null labels: app: dep name: dep spec: replicas: 3 selector: matchLabels: app: depdep strategy: {} template: metadata: creationTimestamp: null labels: app: dep spec: containers: - image: redis name: redis resources: {} status: {} ``` </summary><br><b> The selector doesn't match the label (dep vs depdep). To solve it, fix depdep so it's dep instead. </b></details>", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Deployments", "language": "en", "created_at": "2025-07-19T19:22:02.073462"}}
{"text": "<details> <summary>Create a file definition/manifest of a deployment called \"dep\", with 3 replicas that uses the image 'redis'</summary><br><b> `k create deploy dep -o yaml --image=redis --dry-run=client --replicas 3 > deployment.yaml ` </b></details> <details> <summary>Delete the deployment `depdep`</summary><br><b> `k delete deploy depdep` </b></details> <details> <summary>Create a deployment called \"pluck\" using the image \"redis\" and make sure it runs 5 replicas</summary><br><b> `kubectl create deployment pluck --image=redis` `kubectl scale deployment pluck --replicas=5` </b></details> <details> <summary>Create a deployment with the following properties: * called \"blufer\" * using the image \"python\" * runs 3 replicas * all pods will be placed on a node that has the label \"blufer\" </summary><br><b> `kubectl create deployment blufer --image=python --replicas=3 -o yaml --dry-run=client > deployment.yaml` Add the following section (`vi deployment.yaml`): ``` spec: affinity: nodeAffinity:", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Deployments Commands", "language": "en", "created_at": "2025-07-19T19:22:02.073625"}}
{"text": "requiredDuringSchedlingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: blufer operator: Exists ``` `kubectl apply -f deployment.yaml` </b></details>", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Deployments Commands", "language": "en", "created_at": "2025-07-19T19:22:02.073646"}}
{"text": "<details> <summary>What is a Service in Kubernetes?</summary><br><b> \"An abstract way to expose an application running on a set of Pods as a network service.\" - read more [here](https://kubernetes.io/docs/concepts/services-networking/service) In simpler words, it allows you to add an internal or external connectivity to a certain application running in a container. </b></details> <details> <summary>Place the components in the right placeholders in regards to Kubernetes service<br> <img src=\"images/service_exercise.png\"/> </summary><br><b> <img src=\"images/service_solution.png\"/> </b></details> <details> <summary>How to create a service for an existing deployment called \"alle\" on port 8080 so the Pod(s) accessible via a Load Balancer?</summary><br><b> The imperative way: `kubectl expose deployment alle --type=LoadBalancer --port 8080` </b></details> <details> <summary>True or False? The lifecycle of Pods and Services isn't connected so when a Pod dies, the Service still stays", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Services", "language": "en", "created_at": "2025-07-19T19:22:02.075135"}}
{"text": "</summary><br><b> True </b></details> <details> <summary>After creating a service, how to check it was created?</summary><br><b> `kubectl get svc` </b></details> <details> <summary>What's the default Service type?</summary><br><b> ClusterIP - used for internal communication. </b></details> <details> <summary>What Service types are there?</summary><br><b> * ClusterIP * NodePort * LoadBalancer * ExternalName More on this topic [here](https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types) </b></details> <details> <summary>How Service and Deployment are connected?</summary><br><b> The truth is they aren't connected. Service points to Pod(s) directly, without connecting to the Deployment in any way. </b></details> <details> <summary>What are important steps in defining/adding a Service?</summary><br><b> 1. Making sure that targetPort of the Service is matching the containerPort of the Pod 2. Making sure that selector matches at least one of the", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Services", "language": "en", "created_at": "2025-07-19T19:22:02.075171"}}
{"text": "Pod's labels </b></details> <details> <summary>What is the default service type in Kubernetes and what is it used for?</summary><br><b> The default is ClusterIP and it's used for exposing a port internally. It's useful when you want to enable internal communication between Pods and prevent any external access. </b></details> <details> <summary>How to get information on a certain service?</summary><br><b> `kubctl describe service <SERVICE_NAME>` It's more common to use `kubectl describe svc ...` </b></details> <details> <summary>What the following command does? ``` kubectl expose rs some-replicaset --name=replicaset-svc --target-port=2017 --type=NodePort ``` </summary><br><b> It exposes a ReplicaSet by creating a service called 'replicaset-svc'. The exposed port is 2017 (this is the port used by the application) and the service type is NodePort which means it will be reachable externally. </b></details> <details> <summary>True or False? the target port, in the case of running the", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Services", "language": "en", "created_at": "2025-07-19T19:22:02.075191"}}
{"text": "following command, will be exposed only on one of the Kubernetes cluster nodes but it will routed to all the pods ``` kubectl expose rs some-replicaset --name=replicaset-svc --target-port=2017 --type=NodePort ``` </summary><br><b> False. It will be exposed on every node of the cluster and will be routed to one of the Pods (which belong to the ReplicaSet) </b></details> <details> <summary>How to verify that a certain service configured to forward the requests to a given pod</summary><br><b> Run `kubectl describe service` and see if the IPs from \"Endpoints\" match any IPs from the output of `kubectl get pod -o wide` </b></details> <details> <summary>Explain what will happen when running apply on the following block ``` apiVersion: v1 kind: Service metadata: name: some-app spec: type: NodePort ports: - port: 8080 nodePort: 2017 protocol: TCP selector: type: backend service: some-app ``` </summary><br><b> It creates a new Service of the type \"NodePort\" which means it can be used for", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Services", "language": "en", "created_at": "2025-07-19T19:22:02.075429"}}
{"text": "internal and external communication with the app.<br> The port of the application is 8080 and the requests will forwarded to this port. The exposed port is 2017. As a note, this is not a common practice, to specify the nodePort.<br> The port used TCP (instead of UDP) and this is also the default so you don't have to specify it.<br> The selector used by the Service to know to which Pods to forward the requests. In this case, Pods with the label \"type: backend\" and \"service: some-app\".<br> </b></details> <details> <summary>How to turn the following service into an external one? ``` spec: selector: app: some-app ports: - protocol: TCP port: 8081 targetPort: 8081 ``` </summary><br><b> Adding `type: LoadBalancer` and `nodePort` ``` spec: selector: app: some-app type: LoadBalancer ports: - protocol: TCP port: 8081 targetPort: 8081 nodePort: 32412 ``` </b></details> <details> <summary>What would you use to route traffic from outside the Kubernetes cluster to services within a", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Services", "language": "en", "created_at": "2025-07-19T19:22:02.075462"}}
{"text": "cluster?</summary><br><b> Ingress </b></details> <details> <summary>True or False? When \"NodePort\" is used, \"ClusterIP\" will be created automatically?</summary><br><b> True </b></details> <details> <summary>When would you use the \"LoadBalancer\" type</summary><br><b> Mostly when you would like to combine it with cloud provider's load balancer </b></details> <details> <summary>How would you map a service to an external address?</summary><br><b> Using the 'ExternalName' directive. </b></details> <details> <summary>Describe in detail what happens when you create a service</summary><br><b> 1. Kubectl sends a request to the API server to create a Service 2. The controller detects there is a new Service 3. Endpoint objects created with the same name as the service, by the controller 4. The controller is using the Service selector to identify the endpoints 5. kube-proxy detects there is a new endpoint object + new service and adds iptables rules to capture traffic to the Service port and", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Services", "language": "en", "created_at": "2025-07-19T19:22:02.075489"}}
{"text": "redirect it to endpoints 6. kube-dns detects there is a new Service and adds the container record to the dns server </b></details> <details> <summary>How to list the endpoints of a certain app?</summary><br><b> `kubectl get ep <name>` </b></details> <details> <summary>How can you find out information on a Service related to a certain Pod if all you can use is <code>kubectl exec <POD_NAME> -- </code></summary><br><b> You can run `kubectl exec <POD_NAME> -- env` which will give you a couple environment variables related to the Service.<br> Variables such as `[SERVICE_NAME]_SERVICE_HOST`, `[SERVICE_NAME]_SERVICE_PORT`, ... </b></details> <details> <summary>Describe what happens when a container tries to connect with its corresponding Service for the first time. Explain who added each of the components you include in your description</summary><br><b> - The container looks at the nameserver defined in /etc/resolv.conf - The container queries the nameserver so the address is resolved to the", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Services", "language": "en", "created_at": "2025-07-19T19:22:02.075583"}}
{"text": "Service IP - Requests sent to the Service IP are forwarded with iptables rules (or other chosen software) to the endpoint(s). Explanation as to who added them: - The nameserver in the container is added by kubelet during the scheduling of the Pod, by using kube-dns - The DNS record of the service is added by kube-dns during the Service creation - iptables rules are added by kube-proxy during Endpoint and Service creation </b></details> <details> <summary>Describe in high level what happens when you run <code>kubctl expose deployment remo --type=LoadBalancer --port 8080</code></summary><br><b> 1. Kubectl sends a request to Kubernetes API to create a Service object 2. Kubernetes asks the cloud provider (e.g. AWS, GCP, Azure) to provision a load balancer 3. The newly created load balancer forwards incoming traffic to relevant worker node(s) which forwards the traffic to the relevant containers </b></details> <details> <summary>After creating a service that forwards incoming external", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Services", "language": "en", "created_at": "2025-07-19T19:22:02.075605"}}
{"text": "traffic to the containerized application, how to make sure it works?</summary><br><b> You can run `curl <SERVICE IP>:<SERVICE PORT>` to examine the output. </b></details> <details> <summary>An internal load balancer in Kubernetes is called <code>____</code> and an external load balancer is called <code>____</code></summary><br><b> An internal load balancer in Kubernetes is called Service and an external load balancer is Ingress </b></details>", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Services", "language": "en", "created_at": "2025-07-19T19:22:02.075625"}}
{"text": "<details> <summary>What is Ingress?</summary><br><b> From Kubernetes docs: \"Ingress exposes HTTP and HTTPS routes from outside the cluster to services within the cluster. Traffic routing is controlled by rules defined on the Ingress resource.\" Read more [here](https://kubernetes.io/docs/concepts/services-networking/ingress/) </b></details> <details> <summary>Complete the following configuration file to make it Ingress ``` metadata: name: someapp-ingress spec: ``` </summary><br><b> There are several ways to answer this question. ``` apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: someapp-ingress spec: rules: - host: my.host http: paths: - backend: serviceName: someapp-internal-service servicePort: 8080 ``` </b></details> <details> <summary>Explain the meaning of \"http\", \"host\" and \"backend\" directives ``` apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: someapp-ingress spec: rules: - host: my.host http: paths: - backend: serviceName: someapp-internal-service", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Ingress", "language": "en", "created_at": "2025-07-19T19:22:02.076260"}}
{"text": "servicePort: 8080 ``` </summary><br><b> host is the entry point of the cluster so basically a valid domain address that maps to cluster's node IP address<br> the http line used for specifying that incoming requests will be forwarded to the internal service using http.<br> backend is referencing the internal service (serviceName is the name under metadata and servicePort is the port under the ports section). </b></details> <details> <summary>Why using a wildcard in ingress host may lead to issues?</summary><br><b> The reason you should not wildcard value in a host (like `- host: *`) is because you basically tell your Kubernetes cluster to forward all the traffic to the container where you used this ingress. This may cause the entire cluster to go down. </b></details> <details> <summary>What is Ingress Controller?</summary><br><b> An implementation for Ingress. It's basically another pod (or set of pods) that does evaluates and processes Ingress rules and this it manages all the", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Ingress", "language": "en", "created_at": "2025-07-19T19:22:02.076294"}}
{"text": "redirections. There are multiple Ingress Controller implementations (the one from Kubernetes is Kubernetes Nginx Ingress Controller). </b></details> <details> <summary>What are some use cases for using Ingress?</summary><br><b> * Multiple sub-domains (multiple host entries, each with its own service) * One domain with multiple services (multiple paths where each one is mapped to a different service/application) </b></details> <details> <summary>How to list Ingress in your namespace?</summary><br><b> kubectl get ingress </b></details> <details> <summary>What is Ingress Default Backend?</summary><br><b> It specifies what do with an incoming request to the Kubernetes cluster that isn't mapped to any backend (= no rule to for mapping the request to a service). If the default backend service isn't defined, it's recommended to define so users still see some kind of message instead of nothing or unclear error. </b></details> <details> <summary>How to configure a default", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Ingress", "language": "en", "created_at": "2025-07-19T19:22:02.076456"}}
{"text": "backend?</summary><br><b> Create Service resource that specifies the name of the default backend as reflected in `kubectl describe ingress ...` and the port under the ports section. </b></details> <details> <summary>How to configure TLS with Ingress?</summary><br><b> Add tls and secretName entries. ``` spec: tls: - hosts: - some_app.com secretName: someapp-secret-tls ``` </b></details> <details> <summary>True or False? When configuring Ingress with TLS, the Secret component must be in the same namespace as the Ingress component</summary><br><b> True </b></details> <details> <summary>Which Kubernetes concept would you use to control traffic flow at the IP address or port level? </summary><br><b> Network Policies </b></details> <details> <summary>How to scale an application (deplyoment) so it runs more than one instance of the application?</summary><br><b> To run two instances of the applicaation? `kubectl scale deployment <DEPLOYMENT_NAME> --replicas=2` You can specify any other number,", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Ingress", "language": "en", "created_at": "2025-07-19T19:22:02.076487"}}
{"text": "given that your application knows how to scale. </b></details>", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Ingress", "language": "en", "created_at": "2025-07-19T19:22:02.076507"}}
{"text": "<details> <summary>What is the purpose of ReplicaSet?</summary><br><b> [kubernetes.io](https://kubernetes.io/docs/concepts/workloads/controllers/replicaset): \"A ReplicaSet's purpose is to maintain a stable set of replica Pods running at any given time. As such, it is often used to guarantee the availability of a specified number of identical Pods.\" In simpler words, a ReplicaSet will ensure the specified number of Pods replicas is running for a selected Pod. If there are more Pods than defined in the ReplicaSet, some will be removed. If there are less than what is defined in the ReplicaSet then, then more replicas will be added. </b></details> <details> <summary>What the following block of lines does? ``` spec: replicas: 2 selector: matchLabels: type: backend template: metadata: labels: type: backend spec: containers: - name: httpd-yup image: httpd ``` </summary><br><b> It defines a replicaset for Pods whose type is set to \"backend\" so at any given point of time there will be 2", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "ReplicaSets", "language": "en", "created_at": "2025-07-19T19:22:02.077839"}}
{"text": "concurrent Pods running. </b></details> <details> <summary>What will happen when a Pod, created by ReplicaSet, is deleted directly with <code>kubectl delete po ...</code>?</summary><br><b> The ReplicaSet will create a new Pod in order to reach the desired number of replicas. </b></details> <details> <summary>True or False? If a ReplicaSet defines 2 replicas but there 3 Pods running matching the ReplicaSet selector, it will do nothing</summary><br><b> False. It will terminate one of the Pods to reach the desired state of 2 replicas. </b></details> <details> <summary>Describe the sequence of events in case of creating a ReplicaSet</summary><br><b> * The client (e.g. kubectl) sends a request to the API server to create a ReplicaSet * The Controller detects there is a new event requesting for a ReplicaSet * The controller creates new Pod definitions (the exact number depends on what is defined in the ReplicaSet definition) * The scheduler detects unassigned Pods and decides to which nodes", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "ReplicaSets", "language": "en", "created_at": "2025-07-19T19:22:02.077870"}}
{"text": "to assign the Pods. This information sent to the API server * Kubelet detects that two Pods were assigned to the node it's running on (as it constantly watching the API server) * Kubelet sends requests to the container engine, to create the containers that are part of the Pod * Kubelet sends a request to the API server to notify it the Pods were created </b></details> <details> <summary>How to list ReplicaSets in the current namespace?</summary><br><b> `kubectl get rs` </b></details> <details> <summary>Is it possible to delete ReplicaSet without deleting the Pods it created?</summary><br><b> Yes, with `--cascase=false`. `kubectl delete -f rs.yaml --cascade=false` </b></details> <details> <summary>What is the default number of replicas if not explicitly specified?</summary><br><b> 1 </b></details> <details> <summary>What the following output of <code>kubectl get rs</code> means? NAME DESIRED CURRENT READY AGE web 2 2 0 2m23s </summary><br><b> The replicaset `web` has 2 replicas. It", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "ReplicaSets", "language": "en", "created_at": "2025-07-19T19:22:02.077891"}}
{"text": "seems that the containers inside the Pod(s) are not yet running since the value of READY is 0. It might be normal since it takes time for some containers to start running and it might be due to an error. Running `kubectl describe po POD_NAME` or `kubectl logs POD_NAME` can give us more information. </b></details> <details> <summary>True or False? Pods specified by the selector field of ReplicaSet must be created by the ReplicaSet itself</summary><br><b> False. The Pods can be already running and initially they can be created by any object. It doesn't matter for the ReplicaSet and not a requirement for it to acquire and monitor them. </b></details> <details> <summary>True or False? In case of a ReplicaSet, if Pods specified in the selector field don't exists, the ReplicaSet will wait for them to run before doing anything</summary><br><b> False. It will take care of running the missing Pods. </b></details> <details> <summary>In case of a ReplicaSet, Which field is mandatory in the spec", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "ReplicaSets", "language": "en", "created_at": "2025-07-19T19:22:02.077910"}}
{"text": "section?</summary><br><b> The field `template` in spec section is mandatory. It's used by the ReplicaSet to create new Pods when needed. </b></details> <details> <summary>You've created a ReplicaSet, how to check whether the ReplicaSet found matching Pods or it created new Pods?</summary><br><b> `kubectl describe rs <ReplicaSet Name>` It will be visible under `Events` (the very last lines) </b></details> <details> <summary>True or False? Deleting a ReplicaSet will delete the Pods it created</summary><br><b> True (and not only the Pods but anything else it created). </b></details> <details> <summary>True or False? Removing the label from a Pod that is tracked by a ReplicaSet, will cause the ReplicaSet to create a new Pod</summary><br><b> True. When the label, used by a ReplicaSet in the selector field, removed from a Pod, that Pod no longer controlled by the ReplicaSet and the ReplicaSet will create a new Pod to compensate for the one it \"lost\". </b></details> <details> <summary>How to", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "ReplicaSets", "language": "en", "created_at": "2025-07-19T19:22:02.077928"}}
{"text": "scale a deployment to 8 replicas?</code></summary><br><b> kubectl scale deploy <DEPLOYMENT_NAME> --replicas=8 </b></details> <details> <summary>ReplicaSets are running the moment the user executed the command to create them (like <code>kubectl create -f rs.yaml</code>)</summary><br><b> False. It can take some time, depends on what exactly you are running. To see if they are up and running, run `kubectl get rs` and watch the 'READY' column. </b></details> <details> <summary>How to expose a ReplicaSet as a new service?</summary><br><b> `kubectl expose rs <ReplicaSet Name> --name=<Service Name> --target-port=<Port to expose> --type=NodePort` Few notes: - the target port depends on which port the app is using in the container - type can be different and doesn't has to be specifically \"NodePort\" </b></details> <details> <summary>Fix the following ReplicaSet definition ```yaml apiVersion: apps/v1 kind: ReplicaCet metadata: name: redis labels: app: redis tier: cache spec: selector:", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "ReplicaSets", "language": "en", "created_at": "2025-07-19T19:22:02.078109"}}
{"text": "matchLabels: tier: cache template: metadata: labels: tier: cachy spec: containers: - name: redis image: redis ``` </summary><br><b> kind should be ReplicaSet and not ReplicaCet :) </b></details> <details> <summary>Fix the following ReplicaSet definition ```yaml apiVersion: apps/v1 kind: ReplicaSet metadata: name: redis labels: app: redis tier: cache spec: selector: matchLabels: tier: cache template: metadata: labels: tier: cachy spec: containers: - name: redis image: redis ``` </summary><br><b> The selector doesn't match the label (cache vs cachy). To solve it, fix cachy so it's cache instead. </b></details> <details> <summary>How to check which container image was used as part of replica set called \"repli\"?</summary><br><b> `k describe rs repli | grep -i image` </b></details> <details> <summary>How to check how many Pods are ready as part of a replica set called \"repli\"?</summary><br><b> `k describe rs repli | grep -i \"Pods Status\"` </b></details> <details> <summary>How to delete a", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "ReplicaSets", "language": "en", "created_at": "2025-07-19T19:22:02.078139"}}
{"text": "replica set called \"rori\"?</summary><br><b> `k delete rs rori` </b></details> <details> <summary>How to modify a replica set called \"rori\" to use a different image?</summary><br><b> `k edis rs rori` </b></details> <details> <summary>Scale up a replica set called \"rori\" to run 5 Pods instead of 2</summary><br><b> `k scale rs rori --replicas=5` </b></details> <details> <summary>Scale down a replica set called \"rori\" to run 1 Pod instead of 5</summary><br><b> `k scale rs rori --replicas=1` </b></details>", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "ReplicaSets", "language": "en", "created_at": "2025-07-19T19:22:02.078160"}}
{"text": "<details> <summary>What's a DaemonSet?</summary><br><b> [Kubernetes.io](https://kubernetes.io/docs/concepts/workloads/controllers/daemonset): \"A DaemonSet ensures that all (or some) Nodes run a copy of a Pod. As nodes are added to the cluster, Pods are added to them. As nodes are removed from the cluster, those Pods are garbage collected. Deleting a DaemonSet will clean up the Pods it created.\" </b></details> <details> <summary>What's the difference between a ReplicaSet and DaemonSet?</summary><br><b> A ReplicaSet's purpose is to maintain a stable set of replica Pods running at any given time. A DaemonSet ensures that all Nodes run a copy of a Pod. </b></details> <details> <summary>What are some use cases for using a DaemonSet?</summary><br><b> * Monitoring: You would like to perform monitoring on every node part of cluster. For example datadog pod runs on every node using a daemonset * Logging: You would like to having logging set up on every node part of your cluster * Networking:", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "DaemonSet", "language": "en", "created_at": "2025-07-19T19:22:02.078380"}}
{"text": "there is networking component you need on every node for all nodes to communicate between them </b></details> <details> <summary>How DaemonSet works?</summary><br><b> Historically, up 1.12, it was done with NodeName attribute. Starting 1.12, it's achieved with regular scheduler and node affinity. </b></details>", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "DaemonSet", "language": "en", "created_at": "2025-07-19T19:22:02.078400"}}
{"text": "<details> <summary>How to list all daemonsets in the current namespace?</summary><br><b> `kubectl get ds` </b></details>", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "DaemonSet - Commands", "language": "en", "created_at": "2025-07-19T19:22:02.078420"}}
{"text": "<details> <summary>Explain StatefulSet</summary><br><b> StatefulSet is the workload API object used to manage stateful applications. Manages the deployment and scaling of a set of Pods, and provides guarantees about the ordering and uniqueness of these Pods.[Learn more](https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/) </b></details>", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "StatefulSet", "language": "en", "created_at": "2025-07-19T19:22:02.078451"}}
{"text": "<details> <summary>What is a volume in regards to Kubernetes?</summary><br><b> A directory accessible by the containers inside a certain Pod and containers. The mechanism responsible for creating the directory, managing it, ... mainly depends on the volume type. </b></details> <details> <summary>What volume types are you familiar with?</summary><br><b> * emptyDir: created when a Pod assigned to a node and ceases to exist when the Pod is no longer running on that node * hostPath: mounts a path from the host itself. Usually not used due to security risks but has multiple use-cases where it's needed like access to some internal host paths (`/sys`, `/var/lib`, etc.) </b></details> <details> <summary>Which problems, volumes in Kubernetes solve?</summary><br><b> 1. Sharing files between containers running in the same Pod 2. Storage in containers is ephemeral - it usually doesn't last for long. For example, when a container crashes, you lose all on-disk data. Certain volumes allows to manage", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Volumes", "language": "en", "created_at": "2025-07-19T19:22:02.078771"}}
{"text": "such situation by persistent volumes </b></details> <details> <summary>Explain ephemeral volume types vs. persistent volumes in regards to Pods</summary><br><b> Ephemeral volume types have the lifetime of a pod as opposed to persistent volumes which exist beyond the lifetime of a Pod. </b></details> <details> <summary>Provide at least one use-case for each of the following volume types: * emptyDir * hostPath </summary><br><b> * EmptyDir: You need a temporary data that you can afford to lose if the Pod is deleted. For example short-lived data required for one-time operations. * hostPath: You need access to paths on the host itself (like data from `/sys` or data generated in `/var/lib`) </b></details>", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Volumes", "language": "en", "created_at": "2025-07-19T19:22:02.078799"}}
{"text": "<details> <summary>True or False? By default there is no communication between two Pods in two different namespaces</summary><br><b> False. By default two Pods in two different namespaces are able to communicate with each other. Try it for yourself: kubectl run test-prod -n prod --image ubuntu -- sleep 2000000000 kubectl run test-dev -n dev --image ubuntu -- sleep 2000000000 `k describe po test-prod -n prod` to get the IP of test-prod Pod. Access dev Pod: `kubectl exec --stdin --tty test-dev -n dev -- /bin/bash` And ping the IP of test-prod Pod you get earlier.You'll see that there is communication between the two pods, in two separate namespaces. </b></details>", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Networking", "language": "en", "created_at": "2025-07-19T19:22:02.078914"}}
{"text": "<details> <summary>Explain Network Policies</summary><br><b> [kubernetes.io](https://kubernetes.io/docs/concepts/services-networking/network-policies): \"NetworkPolicies are an application-centric construct which allow you to specify how a pod is allowed to communicate with various network \"entities\"...\" In simpler words, Network Policies specify how pods are allowed/disallowed to communicate with each other and/or other network endpoints. </b></details> <details> <summary>What are some use cases for using Network Policies?</summary><br><b> - Security: You want to prevent from everyone to communicate with a certain pod for security reasons - Controlling network traffic: You would like to deny network flow between two specific nodes </b></details> <details> <summary>True or False? If no network policies are applied to a pod, then no connections to or from it are allowed</summary><br><b> False. By default pods are non-isolated. </b></details> <details> <summary>In case of two pods, if", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Network Policies", "language": "en", "created_at": "2025-07-19T19:22:02.079211"}}
{"text": "there is an egress policy on the source denining traffic and ingress policy on the destination that allows traffic then, traffic will be allowed or denied?</summary><br><b> Denied. Both source and destination policies has to allow traffic for it to be allowed. </b></details> <details> <summary>Where Kubernetes cluster stores the cluster state?</summary><br><b> etcd </b></details>", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Network Policies", "language": "en", "created_at": "2025-07-19T19:22:02.079241"}}
{"text": "<details> <summary>What is etcd?</summary><br><b> etcd is an open source distributed key-value store used to hold and manage the critical information that distributed systems need to keep running. [Read more here](https://www.redhat.com/en/topics/containers/what-is-etcd) </b></details> <details> <summary>True or False? Etcd holds the current status of any kubernetes component</summary><br><b> True </b></details> <details> <summary>True or False? The API server is the only component which communicates directly with etcd</summary><br><b> True </b></details> <details> <summary>True or False? application data is not stored in etcd</summary><br><b> True </b></details> <details> <summary>Why etcd? Why not some SQL or NoSQL database?</summary><br><b> When chosen as the data store etcd was (and still is of course): * Highly Available - you can deploy multiple nodes * Fully Replicated - any node in etcd cluster is \"primary\" node and has full access to the data * Consistent - reads return latest", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "etcd", "language": "en", "created_at": "2025-07-19T19:22:02.079418"}}
{"text": "data * Secured - supports both TLS and SSL * Speed - high performance data store (10k writes per sec!) </b></details>", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "etcd", "language": "en", "created_at": "2025-07-19T19:22:02.079439"}}
{"text": "<details> <summary>What are namespaces?</summary><br><b> Namespaces allow you split your cluster into virtual clusters where you can group your applications in a way that makes sense and is completely separated from the other groups (so you can for example create an app with the same name in two different namespaces) </b></details> <details> <summary>Why to use namespaces? What is the problem with using one default namespace?</summary><br><b> When using the default namespace alone, it becomes hard over time to get an overview of all the applications you manage in your cluster. Namespaces make it easier to organize the applications into groups that makes sense, like a namespace of all the monitoring applications and a namespace for all the security applications, etc. Namespaces can also be useful for managing Blue/Green environments where each namespace can include a different version of an app and also share resources that are in other namespaces (namespaces like logging, monitoring,", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Namespaces", "language": "en", "created_at": "2025-07-19T19:22:02.079902"}}
{"text": "etc.). Another use case for namespaces is one cluster, multiple teams. When multiple teams use the same cluster, they might end up stepping on each others toes. For example if they end up creating an app with the same name it means one of the teams overridden the app of the other team because there can't be too apps in Kubernetes with the same name (in the same namespace). </b></details> <details> <summary>True or False? When a namespace is deleted all resources in that namespace are not deleted but moved to another default namespace</summary><br><b> False. When a namespace is deleted, the resources in that namespace are deleted as well. </b></details> <details> <summary>What special namespaces are there by default when creating a Kubernetes cluster?</summary><br><b> * default * kube-system * kube-public * kube-node-lease </b></details> <details> <summary>What can you find in kube-system namespace?</summary><br><b> * Master and Kubectl processes * System processes </b></details>", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Namespaces", "language": "en", "created_at": "2025-07-19T19:22:02.079938"}}
{"text": "<details> <summary>While namespaces do provide scope for resources, they are not isolating them</summary><br><b> True. Try create two pods in two separate namespaces for example, and you'll see there is a connection between the two. </b></details>", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Namespaces", "language": "en", "created_at": "2025-07-19T19:22:02.079958"}}
{"text": "<details> <summary>How to list all namespaces?</code></summary><br><b> `kubectl get namespaces` OR `kubectl get ns` </b></details> <details> <summary>Create a namespace called 'alle'</summary><br><b> `k create ns alle` </b></details> <details> <summary>Check how many namespaces are there</summary><br><b> `k get ns --no-headers | wc -l` </b></details> <details> <summary>Check how many pods exist in the \"dev\" namespace</summary><br><b> `k get po -n dev` </b></details> <details> <summary>Create a pod called \"kartos\" in the namespace dev. The pod should be using the \"redis\" image.</summary><br><b> If the namespace doesn't exist already: `k create ns dev` `k run kratos --image=redis -n dev` </b></details> <details> <summary>You are looking for a Pod called \"atreus\". How to check in which namespace it runs?</summary><br><b> `k get po -A | grep atreus` </b></details> <details> <summary>What kube-public contains?</summary><br><b> * A configmap, which contains cluster information * Publicly", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Namespaces - commands", "language": "en", "created_at": "2025-07-19T19:22:02.080204"}}
{"text": "accessible data </b></details> <details> <summary>How to get the name of the current namespace?</code></summary><br><b> `kubectl config view | grep namespace` </b></details> <details> <summary>What kube-node-lease contains?</summary><br><b> It holds information on heartbeats of nodes. Each node gets an object which holds information about its availability. </b></details> <details> <summary>True or False? With namespaces you can limit the resources consumed by the users/teams</summary><br><b> True. With namespaces you can limit CPU, RAM and storage usage. </b></details> <details> <summary>How to switch to another namespace? In other words how to change active namespace?</code></summary><br><b> `kubectl config set-context --current --namespace=some-namespace` and validate with `kubectl config view --minify | grep namespace:` OR `kubens some-namespace` </b></details>", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Namespaces - commands", "language": "en", "created_at": "2025-07-19T19:22:02.080225"}}
{"text": "<details> <summary>What is Resource Quota?</code></summary><br><b> Resource quota provides constraints that limit aggregate resource consumption per namespace. It can limit the quantity of objects that can be created in a namespace by type, as well as the total amount of compute resources that may be consumed by resources in that namespace. </b></details> <details> <summary>How to create a Resource Quota?</code></summary><br><b> kubectl create quota some-quota --hard-cpu=2,pods=2 </b></details> <details> <summary>Which resources are accessible from different namespaces?</code></summary><br><b> Services. </b></details> <details> <summary>Which service and in which namespace the following file is referencing? ``` apiVersion: v1 kind: ConfigMap metadata: name: some-configmap data: some_url: samurai.jack ``` </summary><br><b> It's referencing the service \"samurai\" in the namespace called \"jack\". </b></details> <details> <summary>Which components can't be created within a", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Resources Quota", "language": "en", "created_at": "2025-07-19T19:22:02.081018"}}
{"text": "namespace?</code></summary><br><b> Volume and Node. </b></details> <details> <summary>How to list all the components that bound to a namespace?</code></summary><br><b> `kubectl api-resources --namespaced=true` </b></details> <details> <summary>How to create components in a namespace?</code></summary><br><b> One way is by specifying --namespace like this: `kubectl apply -f my_component.yaml --namespace=some-namespace` Another way is by specifying it in the YAML itself: ``` apiVersion: v1 kind: ConfigMap metadata: name: some-configmap namespace: some-namespace ``` and you can verify with: `kubectl get configmap -n some-namespace` </b></details> <details> <summary>How to execute the command \"ls\" in an existing pod?</code></summary><br><b> kubectl exec some-pod -it -- ls </b></details> <details> <summary>How to create a service that exposes a deployment?</code></summary><br><b> kubectl expose deploy some-deployment --port=80 --target-port=8080 </b></details> <details> <summary>How to", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Resources Quota", "language": "en", "created_at": "2025-07-19T19:22:02.081052"}}
{"text": "create a pod and a service with one command?</code></summary><br><b> kubectl run nginx --image=nginx --restart=Never --port 80 --expose </b></details> <details> <summary>Describe in detail what the following command does <code>kubectl create deployment kubernetes-httpd --image=httpd</code></summary><br><b> </b></details> <details> <summary>Why to create kind deployment, if pods can be launched with replicaset?</summary><br><b> </b></details> <details> <summary>How to get list of resources which are not bound to a specific namespace?</code></summary><br><b> kubectl api-resources --namespaced=false </b></details> <details> <summary>How to delete all pods whose status is not \"Running\"?</code></summary><br><b> kubectl delete pods --field-selector=status.phase!='Running' </b></details> <details> <summary>How to display the resources usages of pods?</summary><br><b> kubectl top pod </b></details> <details> <summary>Perhaps a general question but, you suspect one of the pods is having issues,", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Resources Quota", "language": "en", "created_at": "2025-07-19T19:22:02.081072"}}
{"text": "you don't know what exactly. What do you do?</summary><br><b> Start by inspecting the pods status. we can use the command `kubectl get pods` (--all-namespaces for pods in system namespace)<br> If we see \"Error\" status, we can keep debugging by running the command `kubectl describe pod [name]`. In case we still don't see anything useful we can try stern for log tailing.<br> In case we find out there was a temporary issue with the pod or the system, we can try restarting the pod with the following `kubectl scale deployment [name] --replicas=0`<br> Setting the replicas to 0 will shut down the process. Now start it with `kubectl scale deployment [name] --replicas=1` </b></details> <details> <summary>What happens what pods are using too much memory? (more than its limit)</summary><br><b> They become candidates to for termination. </b></details> <details> <summary>Describe how roll-back works</summary><br><b> </b></details> <details> <summary>True or False? Memory is a compressible resource,", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Resources Quota", "language": "en", "created_at": "2025-07-19T19:22:02.081090"}}
{"text": "meaning that when a container reach the memory limit, it will keep running</summary><br><b> False. CPU is a compressible resource while memory is a non compressible resource - once a container reached the memory limit, it will be terminated. </b></details>", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Resources Quota", "language": "en", "created_at": "2025-07-19T19:22:02.081109"}}
{"text": "<details> <summary>What is an Operator?</summary><br><b> Explained [here](https://kubernetes.io/docs/concepts/extend-kubernetes/operator) \"Operators are software extensions to Kubernetes that make use of custom resources to manage applications and their components. Operators follow Kubernetes principles, notably the control loop.\" In simpler words, you can think about an operator as a custom control loop in Kubernetes. </b></details> <details> <summary>Why do we need Operators?</summary><br><b> The process of managing stateful applications in Kubernetes isn't as straightforward as managing stateless applications where reaching the desired status and upgrades are both handled the same way for every replica. In stateful applications, upgrading each replica might require different handling due to the stateful nature of the app, each replica might be in a different status. As a result, we often need a human operator to manage stateful applications. Kubernetes Operator is suppose to assist", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Operators", "language": "en", "created_at": "2025-07-19T19:22:02.081812"}}
{"text": "with this. This also help with automating a standard process on multiple Kubernetes clusters </b></details> <details> <summary>What components the Operator consists of?</summary><br><b> 1. CRD (Custom Resource Definition) - You are fanmiliar with Kubernetes resources like Deployment, Pod, Service, etc. CRD is also a resource, but one that you or the developer the operator defines. 2. Controller - Custom control loop which runs against the CRD </b></details> <details> <summary>Explain CRD</summary><br><b> CRD is Custom Resource Definitions. It's custom Kubernetes component which extends K8s API. TODO(abregman): add more info. </b></details> <details> <summary>How Operator works?</summary><br><b> It uses the control loop used by Kubernetes in general. It watches for changes in the application state. The difference is that is uses a custom control loop. In addition, it also makes use of CRD's (Custom Resources Definitions) so basically it extends Kubernetes API. </b></details> <details>", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Operators", "language": "en", "created_at": "2025-07-19T19:22:02.081842"}}
{"text": "<summary>True or False? Kubernetes Operator used for stateful applications</summary><br><b> True </b></details> <details> <summary>Explain what is the OLM (Operator Lifecycle Manager) and what is it used for</summary><br><b> </b></details> <details> <summary>What is the Operator Framework?</summary><br><b> open source toolkit used to manage k8s native applications, called operators, in an automated and efficient way. </b></details> <details> <summary>What components the Operator Framework consists of?</summary><br><b> 1. Operator SDK - allows developers to build operators 2. Operator Lifecycle Manager - helps to install, update and generally manage the lifecycle of all operators 3. Operator Metering - Enables usage reporting for operators that provide specialized services 4. </b></details> <details> <summary>Describe in detail what is the Operator Lifecycle Manager</summary><br><b> It's part of the Operator Framework, used for managing the lifecycle of operators. It basically extends", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Operators", "language": "en", "created_at": "2025-07-19T19:22:02.081862"}}
{"text": "Kubernetes so a user can use a declarative way to manage operators (installation, upgrade, ...). </b></details> <details> <summary>What openshift-operator-lifecycle-manager namespace includes?</summary><br><b> It includes: * catalog-operator - Resolving and installing ClusterServiceVersions the resource they specify. * olm-operator - Deploys applications defined by ClusterServiceVersion resource </b></details> <details> <summary>What is kubconfig? What do you use it for?</summary><br><b> A kubeconfig file is a file used to configure access to Kubernetes when used in conjunction with the kubectl commandline tool (or other clients). Use kubeconfig files to organize information about clusters, users, namespaces, and authentication mechanisms. </b></details> <details> <summary>Would you use Helm, Go or something else for creating an Operator?</summary><br><b> Depends on the scope and maturity of the Operator. If it mainly covers installation and upgrades, Helm might be enough. If you want", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Operators", "language": "en", "created_at": "2025-07-19T19:22:02.082121"}}
{"text": "to go for Lifecycle management, insights and auto-pilot, this is where you'd probably use Go. </b></details> <details> <summary>Are there any tools, projects you are using for building Operators?</summary><br><b> This one is based more on a personal experience and taste... * Operator Framework * Kubebuilder * Controller Runtime ... </b></details>", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Operators", "language": "en", "created_at": "2025-07-19T19:22:02.082151"}}
{"text": "<details> <summary>Explain Kubernetes Secrets</summary><br><b> Secrets let you store and manage sensitive information (passwords, ssh keys, etc.) </b></details> <details> <summary>How to create a Secret from a key and value?</summary><br><b> `kubectl create secret generic some-secret --from-literal=password='donttellmypassword'` </b></details> <details> <summary>How to create a Secret from a file?</summary><br><b> `kubectl create secret generic some-secret --from-file=/some/file.txt` </b></details> <details> <summary>What <code>type: Opaque</code> in a secret file means? What other types are there?</summary><br><b> Opaque is the default type used for key-value pairs. </b></details> <details> <summary>True or False? storing data in a Secret component makes it automatically secured</summary><br><b> False. Some known security mechanisms like \"encryption\" aren't enabled by default. </b></details> <details> <summary>What is the problem with the following Secret file: ``` apiVersion: v1", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Secrets", "language": "en", "created_at": "2025-07-19T19:22:02.082473"}}
{"text": "kind: Secret metadata: name: some-secret type: Opaque data: password: mySecretPassword ``` </summary><br><b> Password isn't encrypted. You should run something like this: `echo -n 'mySecretPassword' | base64` and paste the result to the file instead of using plain-text. </b></details> <details> <summary>What the following in a Deployment configuration file means? ``` spec: containers: - name: USER_PASSWORD valueFrom: secretKeyRef: name: some-secret key: password ``` </summary><br><b> USER_PASSWORD environment variable will store the value from password key in the secret called \"some-secret\" In other words, you reference a value from a Kubernetes Secret. </b></details> <details> <summary>How to commit secrets to Git and in general how to use encrypted secrets?</summary><br><b> One possible process would be as follows: 1. You create a Kubernetes secret (but don't commit it) 2. You encrypt it using some 3rd party project (.e.g kubeseal) 3. You apply the seald/encrypted secret 4. You", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Secrets", "language": "en", "created_at": "2025-07-19T19:22:02.082495"}}
{"text": "commit the the sealed secret to Git 5. You deploy an application that requires the secret and it can be automatically decrypted by using for example a Bitnami Sealed secrets controller </b></details>", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Secrets", "language": "en", "created_at": "2025-07-19T19:22:02.082514"}}
{"text": "<details> <summary>True or False? Kubernetes provides data persistence out of the box, so when you restart a pod, data is saved</summary><br><b> False </b></details> <details> <summary>Explain \"Persistent Volumes\". Why do we need it?</summary><br><b> Persistent Volumes allow us to save data so basically they provide storage that doesn't depend on the pod lifecycle. </b></details> <details> <summary>True or False? Persistent Volume must be available to all nodes because the pod can restart on any of them</summary><br><b> True </b></details> <details> <summary>What types of persistent volumes are there?</summary><br><b> * NFS * iSCSI * CephFS * ... </b></details> <details> <summary>What is PersistentVolumeClaim?</summary><br><b> </b></details> <details> <summary>Explain Volume Snapshots</summary><br><b> Volume snapshots let you create a copy of your volume at a specific point in time. </b></details> <details> <summary>True or False? Kubernetes manages data persistence</summary><br><b>", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Volumes", "language": "en", "created_at": "2025-07-19T19:22:02.082781"}}
{"text": "False </b></details> <details> <summary>Explain Storage Classes</summary><br><b> </b></details> <details> <summary>Explain \"Dynamic Provisioning\" and \"Static Provisioning\"</summary><br><b> The main difference relies on the moment when you want to configure storage. For instance, if you need to pre-populate data in a volume, you choose static provisioning. Whereas, if you need to create volumes on demand, you go for dynamic provisioning. </b></details> <details> <summary>Explain Access Modes</summary><br><b> </b></details> <details> <summary>What is CSI Volume Cloning?</summary><br><b> </b></details> <details> <summary>Explain \"Ephemeral Volumes\"</summary><br><b> </b></details> <details> <summary>What types of ephemeral volumes Kubernetes supports?</summary><br><b> </b></details> <details> <summary>What is Reclaim Policy?</summary><br><b> </b></details> <details> <summary>What reclaim policies are there?</summary><br><b> * Retain * Recycle * Delete </b></details>", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Volumes", "language": "en", "created_at": "2025-07-19T19:22:02.082809"}}
{"text": "<details> <summary>What is RBAC?</summary><br><b> RBAC in Kubernetes is the mechanism that enables you to configure fine-grained and specific sets of permissions that define how a given user, or group of users, can interact with any Kubernetes object in cluster, or in a specific Namespace of cluster. </b></details> <details> <summary>Explain the <code>Role</code> and <code>RoleBinding\"</code> objects</summary><br><b> </b></details> <details> <summary>What is the difference between <code>Role</code> and <code>ClusterRole</code> objects?</summary><br><b> The difference between them is that a Role is used at a namespace level whereas a ClusterRole is for the entire cluster. </b></details> <details> <summary>Explain what are \"Service Accounts\" and in which scenario would use create/use one</summary><br><b> [Kubernetes.io](https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account): \"A service account provides an identity for processes that run in a Pod.\" An example", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Access Control", "language": "en", "created_at": "2025-07-19T19:22:02.083093"}}
{"text": "of when to use one: You define a pipeline that needs to build and push an image. In order to have sufficient permissions to build an push an image, that pipeline would require a service account with sufficient permissions. </b></details> <details> <summary>What happens you create a pod and you DON'T specify a service account?</summary><br><b> The pod is automatically assigned with the default service account (in the namespace where the pod is running). </b></details> <details> <summary>Explain how Service Accounts are different from User Accounts</summary><br><b> - User accounts are global while Service accounts unique per namespace - User accounts are meant for humans or client processes while Service accounts are for processes which run in pods </b></details> <details> <summary>How to list Service Accounts?</summary><br><b> `kubectl get serviceaccounts` </b></details> <details> <summary>Explain \"Security Context\"</summary><br><b>", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Access Control", "language": "en", "created_at": "2025-07-19T19:22:02.083113"}}
{"text": "[kubernetes.io](https://kubernetes.io/docs/tasks/configure-pod-container/security-context): \"A security context defines privilege and access control settings for a Pod or Container.\" </b></details>", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Access Control", "language": "en", "created_at": "2025-07-19T19:22:02.083333"}}
{"text": "<details> <summary>Explain what is CronJob and what is it used for</summary><br><b> A CronJob creates Jobs on a repeating schedule. One CronJob object is like one line of a crontab (cron table) file. It runs a job periodically on a given schedule, written in Cron format. </b></details> <details> <summary>What possible issue can arise from using the following spec and how to fix it? ``` apiVersion: batch/v1beta1 kind: CronJob metadata: name: some-cron-job spec: schedule: '*/1 * * * *' startingDeadlineSeconds: 10 concurrencyPolicy: Allow ``` </summary><br><b> If the cron job fails, the next job will not replace the previous one due to the \"concurrencyPolicy\" value which is \"Allow\". It will keep spawning new jobs and so eventually the system will be filled with failed cron jobs. To avoid such problem, the \"concurrencyPolicy\" value should be either \"Replace\" or \"Forbid\". </b></details> <details> <summary>What issue might arise from using the following CronJob and how to fix it? ```", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "CronJob", "language": "en", "created_at": "2025-07-19T19:22:02.083695"}}
{"text": "apiVersion: batch/v1beta1 kind: CronJob metadata: name: \"some-cron-job\" spec: schedule: '*/1 * * * *' jobTemplate: spec: template: spec: restartPolicy: Never concurrencyPolicy: Forbid successfulJobsHistoryLimit: 1 failedJobsHistoryLimit: 1 ``` </summary><br><b> The following lines placed under the template: ``` concurrencyPolicy: Forbid successfulJobsHistoryLimit: 1 failedJobsHistoryLimit: 1 ``` As a result this configuration isn't part of the cron job spec hence the cron job has no limits which can cause issues like OOM and potentially lead to API server being down.<br> To fix it, these lines should placed in the spec of the cron job, above or under the \"schedule\" directive in the above example. </b></details>", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "CronJob", "language": "en", "created_at": "2025-07-19T19:22:02.083753"}}
{"text": "<details> <summary>Explain Imperative Management vs. Declarative Management</summary><br><b> </b></details> <details> <summary>Explain what Kubernetes Service Discovery means</summary><br><b> </b></details> <details> <summary>You have one Kubernetes cluster and multiple teams that would like to use it. You would like to limit the resources each team consumes in the cluster. Which Kubernetes concept would you use for that?</summary><br><b> Namespaces will allow to limit resources and also make sure there are no collisions between teams when working in the cluster (like creating an app with the same name). </b></details> <details> <summary>What Kube Proxy does?</summary><br><b> Kube Proxy is a network proxy that runs on each node in your cluster, implementing part of the Kubernetes Service concept </b></details> <details> <summary>What \"Resources Quotas\" are used for and how?</summary><br><b> </b></details> <details> <summary>Explain ConfigMap</summary><br><b> Separate configuration from", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Misc", "language": "en", "created_at": "2025-07-19T19:22:02.084291"}}
{"text": "pods. It's good for cases where you might need to change configuration at some point but you don't want to restart the application or rebuild the image so you create a ConfigMap and connect it to a pod but externally to the pod. Overall it's good for: * Sharing the same configuration between different pods * Storing external to the pod configuration </b></details> <details> <summary>How to use ConfigMaps?</summary><br><b> 1. Create it (from key&value, a file or an env file) 2. Attach it. Mount a configmap as a volume </b></details> <details> <summary>True or False? Sensitive data, like credentials, should be stored in a ConfigMap</summary><br><b> False. Use secret. </b></details> <details> <summary>Explain \"Horizontal Pod Autoscaler\"</summary><br><b> In Kubernetes, a HorizontalPodAutoscaler automatically updates a workload resource with the aim of automatically scaling the workload to match demand. </b></details> <details> <summary>When you delete a pod, is it deleted instantly? (a", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Misc", "language": "en", "created_at": "2025-07-19T19:22:02.084312"}}
{"text": "moment after running the command)</summary><br><b> </b></details> <details> <summary>What does being cloud-native mean?</summary><br><b> The term cloud native refers to the concept of building and running applications to take advantage of the distributed computing offered by the cloud delivery model. </b></details> <details> <summary>Explain the pet and cattle approach of infrastructure with respect to kubernetes</summary><br><b> </b></details> <details> <summary>Describe how you one proceeds to run a containerized web app in K8s, which should be reachable from a public URL.</summary><br><b> </b></details> <details> <summary>How would you troubleshoot your cluster if some applications are not reachable any more?</summary><br><b> </b></details> <details> <summary>Describe what CustomResourceDefinitions there are in the Kubernetes world? What they can be used for?</summary><br><b> </b></details> <details> <summary> How does scheduling work in kubernetes?</summary><br><b> The control", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Misc", "language": "en", "created_at": "2025-07-19T19:22:02.084332"}}
{"text": "plane component kube-scheduler asks the following questions, 1. What to schedule? It tries to understand the pod-definition specifications 2. Which node to schedule? It tries to determine the best node with available resources to spin a pod 3. Binds the Pod to a given node View more [here](https://www.youtube.com/watch?v=rDCWxkvPlAw) </b></details> <details> <summary> How are labels and selectors used?</summary><br><b> </b></details> <details> <summary>What QoS classes are there?</summary><br><b> * Guaranteed * Burstable * BestEffort </b></details> <details> <summary>Explain Labels. What are they and why would one use them?</summary><br><b> Kubernetes labels are key-value pairs that can connect identifying metadata with Kubernetes objects. </b></details> <details> <summary>Explain Selectors</summary><br><b> </b></details> <details> <summary>What is Kubeconfig?</summary><br><b> </b></details>", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Misc", "language": "en", "created_at": "2025-07-19T19:22:02.084356"}}
{"text": "<details> <summary>What is Gatekeeper?</summary><br><b> [Gatekeeper docs](https://open-policy-agent.github.io/gatekeeper/website/docs): \"Gatekeeper is a validating (mutating TBA) webhook that enforces CRD-based policies executed by Open Policy Agent\" </b></details> <details> <summary>Explain how Gatekeeper works</summary><br><b> On every request sent to the Kubernetes cluster, Gatekeeper sends the policies and the resources to OPA (Open Policy Agent) to check if it violates any policy. If it does, Gatekeeper will return the policy error message back. If it isn't violates any policy, the request will reach the cluster. </b></details>", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Gatekeeper", "language": "en", "created_at": "2025-07-19T19:22:02.084435"}}
{"text": "<details> <summary>What is Conftest?</summary><br><b> Conftest allows you to write tests against structured files. You can think of it as tests library for Kubernetes resources.<br> It is mostly used in testing environments such as CI pipelines or local hooks. </b></details> <details> <summary>What is Datree? How is it different from Conftest?</summary><br><b> Same as Conftest, it is used for policy testing and enforcement. The difference is that it comes with built-in policies. </b></details>", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Policy Testing", "language": "en", "created_at": "2025-07-19T19:22:02.084502"}}
{"text": "<details> <summary>What is Helm?</summary><br><b> Package manager for Kubernetes. Basically the ability to package YAML files and distribute them to other users and apply them in the cluster(s). As a concept it's quite common and can be found in many platforms and services. Think for example on package managers in operating systems. If you use Fedora/RHEL that would be dnf. If you use Ubuntu then, apt. If you don't use Linux, then a different question should be asked and it's why? but that's another topic :) </b></details> <details> <summary>Why do we need Helm? What would be the use case for using it?</summary><br><b> Sometimes when you would like to deploy a certain application to your cluster, you need to create multiple YAML files/components like: Secret, Service, ConfigMap, etc. This can be tedious task. So it would make sense to ease the process by introducing something that will allow us to share these bundle of YAMLs every time we would like to add an application to our", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Helm", "language": "en", "created_at": "2025-07-19T19:22:02.085288"}}
{"text": "cluster. This something is called Helm. A common scenario is having multiple Kubernetes clusters (prod, dev, staging). Instead of individually applying different YAMLs in each cluster, it makes more sense to create one Chart and install it in every cluster. Another scenario is, you would like to share what you've created with the community. For people and companies to easily deploy your application in their cluster. </b></details> <details> <summary>Explain \"Helm Charts\"</summary><br><b> Helm Charts is a bundle of YAML files. A bundle that you can consume from repositories or create your own and publish it to the repositories. </b></details> <details> <summary>It is said that Helm is also Templating Engine. What does it mean?</summary><br><b> It is useful for scenarios where you have multiple applications and all are similar, so there are minor differences in their configuration files and most values are the same. With Helm you can define a common blueprint for all of them and the", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Helm", "language": "en", "created_at": "2025-07-19T19:22:02.085321"}}
{"text": "values that are not fixed and change can be placeholders. This is called a template file and it looks similar to the following ``` apiVersion: v1 kind: Pod metadata: name: {[ .Values.name ]} spec: containers: - name: {{ .Values.container.name }} image: {{ .Values.container.image }} port: {{ .Values.container.port }} ``` The values themselves will in separate file: ``` name: some-app container: name: some-app-container image: some-app-image port: 1991 ``` </b></details> <details> <summary>What are some use cases for using Helm template file?</summary><br><b> * Deploy the same application across multiple different environments * CI/CD </b></details> <details> <summary>Explain the Helm Chart Directory Structure</summary><br><b> someChart/ -> the name of the chart Chart.yaml -> meta information on the chart values.yaml -> values for template files charts/ -> chart dependencies templates/ -> templates files :) </b></details> <details> <summary>How Helm supports release", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Helm", "language": "en", "created_at": "2025-07-19T19:22:02.085342"}}
{"text": "management?</summary><br><b> Helm allows you to upgrade, remove and rollback to previous versions of charts. In version 2 of Helm it was with what is known as \"Tiller\". In version 3, it was removed due to security concerns. </b></details>", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Helm", "language": "en", "created_at": "2025-07-19T19:22:02.085367"}}
{"text": "<details> <summary>How do you search for charts?</summary><br><b> `helm search hub [some_keyword]` </b></details> <details> <summary>Is it possible to override values in values.yaml file when installing a chart?</summary><br><b> Yes. You can pass another values file: `helm install --values=override-values.yaml [CHART_NAME]` Or directly on the command line: `helm install --set some_key=some_value` </b></details> <details> <summary>How do you list deployed releases?</summary><br><b> `helm ls` or `helm list` </b></details> <details> <summary>How to execute a rollback?</summary><br><b> `helm rollback RELEASE_NAME REVISION_ID` </b></details> <details> <summary>How to view revision history for a certain release?</summary><br><b> `helm history RELEASE_NAME` </b></details> <details> <summary>How to upgrade a release?</summary><br><b> `helm upgrade RELEASE_NAME CHART_NAME` </b></details>", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Commands", "language": "en", "created_at": "2025-07-19T19:22:02.085492"}}
{"text": "<details> <summary>What security best practices do you follow in regards to the Kubernetes cluster?</summary><br><b> * Secure inter-service communication (one way is to use Istio to provide mutual TLS) * Isolate different resources into separate namespaces based on some logical groups * Use supported container runtime (if you use Docker then drop it because it's deprecated. You might want to CRI-O as an engine and podman for CLI) * Test properly changes to the cluster (e.g. consider using Datree to prevent kubernetes misconfigurations) * Limit who can do what (by using for example OPA gatekeeper) in the cluster * Use NetworkPolicy to apply network security * Consider using tools (e.g. Falco) for monitoring threats </b></details>", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Security", "language": "en", "created_at": "2025-07-19T19:22:02.085658"}}
{"text": "<details> <summary>Running <code>kubectl get pods</code> you see Pods in \"Pending\" status. What would you do?</summary><br><b> One possible path is to run `kubectl describe pod <pod name>` to get more details.<br> You might see one of the following: * Cluster is full. In this case, extend the cluster. * ResourcesQuota limits are met. In this case you might want to modify them * Check if PersistentVolumeClaim mount is pending If none of the above helped, run the command (`get pods`) with `-o wide` to see if the node is assigned to a node. If not, there might be an issue with scheduler. </b></details> <details> <summary>Users unable to reach an application running on a Pod on Kubernetes. What might be the issue and how to check?</summary><br><b> One possible path is to start with checking the Pod status. 1. Is the Pod pending? if yes, check for the reason with `kubectl describe pod <pod name>` TODO: finish this... </b></details>", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Troubleshooting Scenarios", "language": "en", "created_at": "2025-07-19T19:22:02.085959"}}
{"text": "<details> <summary>What is Istio? What is it used for?</summary><br><b> Istio is an open source service mesh that helps organizations run distributed, microservices-based apps anywhere. Istio enables organizations to secure, connect, and monitor microservices, so they can modernize their enterprise apps more swiftly and securely. </b></details>", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Istio", "language": "en", "created_at": "2025-07-19T19:22:02.086019"}}
{"text": "<details> <summary>What are controllers?</summary><br><b> [Kubernetes.io](https://kubernetes.io/docs/concepts/architecture/controller): \"In Kubernetes, controllers are control loops that watch the state of your cluster, then make or request changes where needed. Each controller tries to move the current cluster state closer to the desired state.\" </b></details> <details> <summary>Name two controllers you are familiar with</summary><br><b> 1. Node Controller: manages the nodes of a cluster. Among other things, the controller is responsible for monitoring nodes' health - if the node is suddenly unreachable it will evacuate all the pods running on it and will mark the node status accordingly. 2. Replication Controller - monitors the status of pod replicas based on what should be running. It makes sure the number of pods that should be running is actually running </b></details> <details> <summary>What process is responsible for running and installing the different", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Controllers", "language": "en", "created_at": "2025-07-19T19:22:02.086294"}}
{"text": "controllers?</summary><br><b> Kube-Controller-Manager </b></details> <details> <summary>What is the control loop? How it works?</summary><br><b> Explained [here](https://www.youtube.com/watch?v=i9V4oCa5f9I) </b></details> <details> <summary>What are all the phases/steps of a control loop?</summary><br><b> - Observe - identify the cluster current state - Diff - Identify whether a diff exists between current state and desired state - Act - Bring current cluster state to the desired state (basically reach a state where there is no diff) </b></details>", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Controllers", "language": "en", "created_at": "2025-07-19T19:22:02.086481"}}
{"text": "<details> <summary>True of False? The scheduler is responsible for both deciding where a Pod will run and actually running it</summary><br><b> False. While the scheduler is responsible for choosing the node on which the Pod will run, Kubelet is the one that actually runs the Pod. </b></details> <details> <summary>How to schedule a pod on a node called \"node1\"?</summary><br><b> `k run some-pod --image=redix -o yaml --dry-run=client > pod.yaml` `vi pod.yaml` and add: ``` spec: nodeName: node1 ``` `k apply -f pod.yaml` Note: if you don't have a node1 in your cluster the Pod will be stuck on \"Pending\" state. </b></details>", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Scheduler", "language": "en", "created_at": "2025-07-19T19:22:02.086610"}}
{"text": "<details> <summary>Using node affinity, set a Pod to schedule on a node where the key is \"region\" and value is either \"asia\" or \"emea\"</summary><br><b> `vi pod.yaml` ```yaml affinity: nodeAffinity: requiredDuringSchedlingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: region operator: In values: - asia - emea ``` </b></details> <details> <summary>Using node affinity, set a Pod to never schedule on a node where the key is \"region\" and value is \"neverland\"</summary><br><b> `vi pod.yaml` ```yaml affinity: nodeAffinity: requiredDuringSchedlingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: region operator: NotIn values: - neverland ``` </b></details> <details> <summary>True of False? Using the node affinity type \"requiredDuringSchedlingIgnoredDuringExecution\" means the scheduler can't schedule unless the rule is met</summary><br><b> True </b></details> <details> <summary>True of False? Using the node affinity type", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Node Affinity", "language": "en", "created_at": "2025-07-19T19:22:02.086938"}}
{"text": "\"preferredDuringSchedlingIgnoredDuringExecution\" means the scheduler can't schedule unless the rule is met</summary><br><b> False. The scheduler tries to find a node that meets the requirements/rules and if it doesn't it will schedule the Pod anyway. </b></details> <details> <summary>Can you deploy multiple schedulers?</summary><br><b> Yes, it is possible. You can run another pod with a command similar to: ``` spec: containers: - command: - kube-scheduler - --address=127.0.0.1 - --leader-elect=true - --scheduler-name=some-custom-scheduler ... ``` </b></details> <details> <summary>Assuming you have multiple schedulers, how to know which scheduler was used for a given Pod?</summary><br><b> Running `kubectl get events` you can see which scheduler was used. </b></details> <details> <summary>You want to run a new Pod and you would like it to be scheduled by a custom scheduler. How to achieve it?</summary><br><b> Add the following to the spec of the Pod: ``` spec: schedulerName:", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Node Affinity", "language": "en", "created_at": "2025-07-19T19:22:02.086966"}}
{"text": "some-custom-scheduler ``` </b></details>", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Node Affinity", "language": "en", "created_at": "2025-07-19T19:22:02.086986"}}
{"text": "<details> <summary>Check if there are taints on node \"master\"</summary><br><b> `k describe no master | grep -i taints` </b></details> <details> <summary>Create a taint on one of the nodes in your cluster with key of \"app\" and value of \"web\" and effect of \"NoSchedule\". Verify it was applied</summary><br><b> `k taint node minikube app=web:NoSchedule` `k describe no minikube | grep -i taints` </b></details> <details> <summary>You applied a taint with <code>k taint node minikube app=web:NoSchedule</code> on the only node in your cluster and then executed <code>kubectl run some-pod --image=redis</code>. What will happen?</summary><br><b> The Pod will remain in \"Pending\" status due to the only node in the cluster having a taint of \"app=web\". </b></details> <details> <summary>You applied a taint with <code>k taint node minikube app=web:NoSchedule</code> on the only node in your cluster and then executed <code>kubectl run some-pod --image=redis</code> but the Pod is in pending state. How to", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Taints", "language": "en", "created_at": "2025-07-19T19:22:02.087299"}}
{"text": "fix it?</summary><br><b> `kubectl edit po some-pod` and add the following ``` - effect: NoSchedule key: app operator: Equal value: web ``` Exit and save. The pod should be in Running state now. </b></details> <details> <summary>Remove an existing taint from one of the nodes in your cluster</summary><br><b> `k taint node minikube app=web:NoSchedule-` </b></details> <details> <summary>What taint effects are there? Explain each one of them</summary><br><b> `NoSchedule`: prevents from resources to be scheduled on a certain node `PreferNoSchedule`: will prefer to shcedule resources on other nodes before resorting to scheduling the resource on the chosen node (on which the taint was applied) `NoExecute`: Applying \"NoSchedule\" will not evict already running Pods (or other resources) from the node as opposed to \"NoExecute\" which will evict any already running resource from the Node </b></details>", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Taints", "language": "en", "created_at": "2025-07-19T19:22:02.087319"}}
{"text": "<details> <summary>Explain why one would specify resource limits in regards to Pods</summary><br><b> * You know how much RAM and/or CPU your app should be consuming and anything above that is not valid * You would like to make sure that everyone can run their apps in the cluster and resources are not being solely used by one type of application </b></details> <details> <summary>True or False? Resource limits applied on a Pod level meaning, if limits is 2gb RAM and there are two container in a Pod that it's 1gb RAM each</summary><br><b> False. It's per container and not per Pod. </b></details>", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Resource Limits", "language": "en", "created_at": "2025-07-19T19:22:02.087433"}}
{"text": "<details> <summary>Check if there are any limits on one of the pods in your cluster</summary><br><b> `kubectl describe po <POD_NAME> | grep -i limits` </b></details> <details> <summary>Run a pod called \"yay\" with the image \"python\" and resources request of 64Mi memory and 250m CPU</summary><br><b> `kubectl run yay --image=python --dry-run=client -o yaml > pod.yaml` `vi pod.yaml` ``` spec: containers: - image: python imagePullPolicy: Always name: yay resources: requests: cpu: 250m memory: 64Mi ``` `kubectl apply -f pod.yaml` </b></details> <details> <summary>Run a pod called \"yay2\" with the image \"python\". Make sure it has resources request of 64Mi memory and 250m CPU and the limits are 128Mi memory and 500m CPU</summary><br><b> `kubectl run yay2 --image=python --dry-run=client -o yaml > pod.yaml` `vi pod.yaml` ``` spec: containers: - image: python imagePullPolicy: Always name: yay2 resources: limits: cpu: 500m memory: 128Mi requests: cpu: 250m memory: 64Mi ``` `kubectl apply -f", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Resources Limits - Commands", "language": "en", "created_at": "2025-07-19T19:22:02.087662"}}
{"text": "pod.yaml` </b></details>", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Resources Limits - Commands", "language": "en", "created_at": "2025-07-19T19:22:02.087922"}}
{"text": "<details> <summary>What monitoring solutions are you familiar with in regards to Kubernetes?</summary><br><b> There are many types of monitoring solutions for Kubernetes. Some open-source, some are in-memory, some of them cost money, ... here is a short list: * metrics-server: in-memory open source monitoring * datadog: $$$ * promethues: open source monitoring solution </b></details> <details> <summary>Describe how the monitoring solution you are working with monitors Kubernetes and </summary><br><b> This very much depends on what you chose to use. Let's address some of the solutions: * metrics-server: an open source and free monitoring solution that uses the cAdvisor component of kubelet to retrieve information on the cluster and its resources and stores them in-memory. Once installed, after some time you can run commands like `kubectl top node` and `kubectl top pod` to view performance metrics on nodes, pods and other resources. TODO: add more monitoring solutions </b></details>", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Monitoring", "language": "en", "created_at": "2025-07-19T19:22:02.088116"}}
{"text": "<details> <summary>What is Kustomize?</summary><br><b> </b></details> <details> <summary>Explain the need for Kustomize by describing actual use cases</summary><br><b> * You have an helm chart of an application used by multiple teams in your organization and there is a requirement to add annotation to the app specifying the name of the of team owning the app * Without Kustomize you would need to copy the files (chart template in this case) and modify it to include the specific annotations we need * With Kustomize you don't need to copy the entire repo or files * You are asked to apply a change/patch to some app without modifying the original files of the app * With Kustomize you can define kustomization.yml file that defines these customizations so you don't need to touch the original app files </b></details> <details> <summary>Describe in high-level how Kustomize works</summary><br><b> 1. You add kustomization.yml file in the folder of the app you would like to customize. 1. You", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Kustomize", "language": "en", "created_at": "2025-07-19T19:22:02.088342"}}
{"text": "define the customizations you would like to perform 2. You run `kustomize build APP_PATH` where your kustomization.yml also resides </b></details>", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Kustomize", "language": "en", "created_at": "2025-07-19T19:22:02.088370"}}
{"text": "<details> <summary>What rollout/deployment strategies are you familiar with?</summary><br><b> * Blue/Green Deployments: You deploy a new version of your app, while old version still running, and you start redirecting traffic to the new version of the app * Canary Deployments: You deploy a new version of your app and start redirecting **portion** of your users/traffic to the new version. So you the migration to the new version is much more gradual </b></details> <details> <summary>Explain Blue/Green deployments/rollouts in detail</summary><br><b> Blue/Green deployment steps: 1. Traffic coming from users through a load balancer to the application which is currently version 1 Users -> Load Balancer -> App Version 1 2. A new application version 2 is deployed (while version 1 still running) Users -> Load Balancer -> App Version 1 App Version 2 3. If version 2 runs properly, traffic switched to it instead of version 1 User -> Load Balancer App version 1 -> App Version 2 4. Whether old", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Deployment Strategies", "language": "en", "created_at": "2025-07-19T19:22:02.088888"}}
{"text": "version is removed or keep running but without users being redirected to it, is based on team or company decision Pros: * We can rollback/switch quickly to previous version at any point Cons: * In case of an issue with new version, ALL users are affected (instead of small portion/percentage) </b></details> <details> <summary>Explain Canary deployments/rollouts in detail</summary><br><b> Canary deployment steps: 1. Traffic coming from users through a load balancer to the application which is currently version 1 Users -> Load Balancer -> App Version 1 2. A new application version 2 is deployed (while version 1 still running) and part of the traffic is redirected to the new version Users -> Load Balancer ->(95% of the traffic) App Version 1 ->(5% of the traffic) App Version 2 3. If the new version (2) runs well, more traffic is redirected to it Users -> Load Balancer ->(70% of the traffic) App Version 1 ->(30% of the traffic) App Version 2 3. If everything runs well, at some point all", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Deployment Strategies", "language": "en", "created_at": "2025-07-19T19:22:02.088916"}}
{"text": "traffic is redirected to the new version Users -> Load Balancer -> App Version 2 Pros: * If there is any issue with the new deployed app version, only some portion of the users affected, instead of all of them Cons: * Testing of new version is neccesrialy in the production environment (as the user traffic is exists only there) </b></details> <details> <summary>What ways are you familiar with to implement deployment strategies (like canary, blue/green) in Kubernetes?</summary><br><b> There are multiple ways. One of them is Argo Rollouts. </b></details>", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Deployment Strategies", "language": "en", "created_at": "2025-07-19T19:22:02.088936"}}
{"text": "<details> <summary>An engineer form your organization told you he is interested only in seeing his team resources in Kubernetes. Instead, in reality, he sees resources of the whole organization, from multiple different teams. What Kubernetes concept can you use in order to deal with it?</summary><br><b> Namespaces. See the following [namespaces question and answer](#namespaces-use-cases) for more information. </b></details> <details> <summary>An engineer in your team runs a Pod but the status he sees is \"CrashLoopBackOff\". What does it means? How to identify the issue?</summary><br><b> The container failed to run (due to different reasons) and Kubernetes tries to run the Pod again after some delay (= BackOff time). Some reasons for it to fail: - Misconfiguration - misspelling, non supported value, etc. - Resource not available - nodes are down, PV not mounted, etc. Some ways to debug: 1. `kubectl describe pod POD_NAME` 1. Focus on `State` (which should be Waiting, CrashLoopBackOff) and", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Scenarios", "language": "en", "created_at": "2025-07-19T19:22:02.089280"}}
{"text": "`Last State` which should tell what happened before (as in why it failed) 2. Run `kubectl logs mypod` 1. This should provide an accurate output of 2. For specific container, you can add `-c CONTAINER_NAME` </b></details> <details> <summary>An engineer form your organization asked whether there is a way to prevent from Pods (with cretain label) to be scheduled on one of the nodes in the cluster. Your reply is:</summary><br><b> Yes, using taints, we could run the following command and it will prevent from all resources with label \"app=web\" to be scheduled on node1: `kubectl taint node node1 app=web:NoSchedule` </b></details> <details> <summary>You would like to limit the number of resources being used in your cluster. For example no more than 4 replicasets, 2 services, etc. How would you achieve that?</summary><br><b> Using ResourceQuats </b></details> <!-- {% endraw %} -->", "metadata": {"source_file": "learning-materials/topics/kubernetes/README.md", "section": "Scenarios", "language": "en", "created_at": "2025-07-19T19:22:02.089300"}}
{"text": "Learn how to operate ReplicaSets", "metadata": {"source_file": "learning-materials/topics/kubernetes/replicaset_02.md", "section": "Objective", "language": "en", "created_at": "2025-07-19T19:22:02.089625"}}
{"text": "1. Create a ReplicaSet with 2 replicas. The app can be anything. 2. Verify a ReplicaSet was created and there are 2 replicas 3. Remove the ReplicaSet but NOT the pods it created 4. Verify you've deleted the ReplicaSet but the Pods are still running", "metadata": {"source_file": "learning-materials/topics/kubernetes/replicaset_02.md", "section": "Instructions", "language": "en", "created_at": "2025-07-19T19:22:02.089674"}}
{"text": "Learn how to create and view ReplicaSets", "metadata": {"source_file": "learning-materials/topics/kubernetes/replicaset_01.md", "section": "Objective", "language": "en", "created_at": "2025-07-19T19:22:02.089789"}}
{"text": "1. Create a ReplicaSet with 2 replicas. The app can be anything. 2. Verify a ReplicaSet was created and there are 2 replicas 3. Delete one of the Pods the ReplicaSet has created 4. If you'll list all the Pods now, what will you see? 5. Remove the ReplicaSet you've created 6. Verify you've deleted the ReplicaSet", "metadata": {"source_file": "learning-materials/topics/kubernetes/replicaset_01.md", "section": "Instructions", "language": "en", "created_at": "2025-07-19T19:22:02.089843"}}
{"text": "Learn how to create services", "metadata": {"source_file": "learning-materials/topics/kubernetes/services_01.md", "section": "Objective", "language": "en", "created_at": "2025-07-19T19:22:02.089912"}}
{"text": "1. Create a pod running ngnix 2. Create a service for the pod you've just created 3. Verify the app is reachable", "metadata": {"source_file": "learning-materials/topics/kubernetes/services_01.md", "section": "Instructions", "language": "en", "created_at": "2025-07-19T19:22:02.089937"}}
{"text": "- [CKA (Certified Kubernetes Administrator)](#cka-certified-kubernetes-administrator) - [Setup](#setup) - [Pods](#pods) - [Troubleshooting Pods](#troubleshooting-pods) - [Namespaces](#namespaces) - [Nodes](#nodes) - [Services](#services) - [ReplicaSets](#replicasets) - [Troubleshooting ReplicaSets](#troubleshooting-replicasets) - [Deployments](#deployments) - [Troubleshooting Deployments](#troubleshooting-deployments) - [Scheduler](#scheduler) - [Node Affinity](#node-affinity) - [Labels and Selectors](#labels-and-selectors) - [Node Selector](#node-selector) - [Taints](#taints) - [Resources Limits](#resources-limits) - [Monitoring](#monitoring) - [Scheduler](#scheduler-1)", "metadata": {"source_file": "learning-materials/topics/kubernetes/CKA.md", "section": "CKA (Certified Kubernetes Administrator)", "language": "en", "created_at": "2025-07-19T19:22:02.090408"}}
{"text": "* Set up Kubernetes cluster. Use one of the following 1. Minikube for local free & simple cluster 2. Managed Cluster (EKS, GKE, AKS) * Set aliases ``` alias k=kubectl alias kd=kubectl delete alias kds=kubectl describe alias ke=kubectl edit alias kr=kubectl run alias kg=kubectl get ```", "metadata": {"source_file": "learning-materials/topics/kubernetes/CKA.md", "section": "Setup", "language": "en", "created_at": "2025-07-19T19:22:02.090460"}}
{"text": "<details> <summary>Run a command to view all the pods in the current namespace</summary><br><b> `kubectl get pods` Note: create an alias (`alias k=kubectl`) and get used to `k get po` </b></details> <details> <summary>Run a pod called \"nginx-test\" using the \"nginx\" image</summary><br><b> `k run nginx-test --image=nginx` </b></details> <details> <summary>Assuming that you have a Pod called \"nginx-test\", how to remove it?</summary><br><b> `k delete po nginx-test` </b></details> <details> <summary>In what namespace the <code>etcd</code> pod is running? list the pods in that namespace</summary><br><b> `k get po -n kube-system` Let's say you didn't know in what namespace it is. You could then run `k get po -A | grep etc` to find the Pod and see in what namespace it resides. </b></details> <details> <summary>List pods from all namespaces</summary><br><b> `k get po -A` The long version would be `kubectl get pods --all-namespaces`. </b></details> <details> <summary>Write a YAML of a Pod with", "metadata": {"source_file": "learning-materials/topics/kubernetes/CKA.md", "section": "Pods", "language": "en", "created_at": "2025-07-19T19:22:02.091111"}}
{"text": "two containers and use the YAML file to create the Pod (use whatever images you prefer)</summary><br><b> ``` cat > pod.yaml <<EOL apiVersion: v1 kind: Pod metadata: name: test spec: containers: - image: alpine name: alpine - image: nginx-unprivileged name: nginx-unprivileged EOL k create -f pod.yaml ``` If you ask yourself how would I remember writing all of that? no worries, you can simply run `kubectl run some_pod --image=redis -o yaml --dry-run=client > pod.yaml`. If you ask yourself \"how am I supposed to remember this long command\" time to change attitude ;) </b></details> <details> <summary>Create a YAML of a Pod without actually running the Pod with the kubectl command (use whatever image you prefer)</summary><br><b> `k run some-pod -o yaml --image nginx-unprivileged --dry-run=client > pod.yaml` </b></details> <details> <summary>How to test a manifest is valid?</summary><br><b> with `--dry-run` flag which will not actually create it, but it will test it and you can find this way,", "metadata": {"source_file": "learning-materials/topics/kubernetes/CKA.md", "section": "Pods", "language": "en", "created_at": "2025-07-19T19:22:02.091141"}}
{"text": "any syntax issues. `k create -f YAML_FILE --dry-run` </b></details> <details> <summary>How to check which image a certain Pod is using?</summary><br><b> `k describe po <POD_NAME> | grep -i image` </b></details> <details> <summary>How to check how many containers run in single Pod?</summary><br><b> `k get po POD_NAME` and see the number under \"READY\" column. You can also run `k describe po POD_NAME` </b></details> <details> <summary>Run a Pod called \"remo\" with the the latest redis image and the label 'year=2017'</summary><br><b> `k run remo --image=redis:latest -l year=2017` </b></details> <details> <summary>List pods and their labels</summary><br><b> `k get po --show-labels` </b></details> <details> <summary>Delete a Pod called \"nm\"</summary><br><b> `k delete po nm` </b></details> <details> <summary>List all the pods with the label \"env=prod\"</summary><br><b> `k get po -l env=prod` To count them: `k get po -l env=prod --no-headers | wc -l` </b></details> <details> <summary>Create a", "metadata": {"source_file": "learning-materials/topics/kubernetes/CKA.md", "section": "Pods", "language": "en", "created_at": "2025-07-19T19:22:02.091169"}}
{"text": "static pod with the image <code>python</code> that runs the command <code>sleep 2017</code></summary><br><b> First change to the directory tracked by kubelet for creating static pod: `cd /etc/kubernetes/manifests` (you can verify path by reading kubelet conf file) Now create the definition/manifest in that directory `k run some-pod --image=python --command sleep 2017 --restart=Never --dry-run=client -o yaml > static-pod.yaml` </b></details> <details> <summary>Describe how would you delete a static Pod </summary><br><b> Locate the static Pods directory (look at `staticPodPath` in kubelet configuration file). Go to that directory and remove the manifest/definition of the staic Pod (`rm <STATIC_POD_PATH>/<POD_DEFINITION_FILE>`) </b></details>", "metadata": {"source_file": "learning-materials/topics/kubernetes/CKA.md", "section": "Pods", "language": "en", "created_at": "2025-07-19T19:22:02.091189"}}
{"text": "<details> <summary>You try to run a Pod but see the status \"CrashLoopBackOff\". What does it means? How to identify the issue?</summary><br><b> The container failed to run (due to different reasons) and Kubernetes tries to run the Pod again after some delay (= BackOff time). Some reasons for it to fail: - Misconfiguration - misspelling, non supported value, etc. - Resource not available - nodes are down, PV not mounted, etc. Some ways to debug: 1. `kubectl describe pod POD_NAME` 1. Focus on `State` (which should be Waiting, CrashLoopBackOff) and `Last State` which should tell what happened before (as in why it failed) 2. Run `kubectl logs mypod` 1. This should provide an accurate output of 2. For specific container, you can add `-c CONTAINER_NAME` 3. If you still have no idea why it failed, try `kubectl get events` 4. </b></details> <details> <summary>What the error <code>ImagePullBackOff</code> means?</summary><br><b> Most likely you didn't write correctly the name of the image you try", "metadata": {"source_file": "learning-materials/topics/kubernetes/CKA.md", "section": "Troubleshooting Pods", "language": "en", "created_at": "2025-07-19T19:22:02.091698"}}
{"text": "to pull and run. Or perhaps it doesn't exists in the registry. You can confirm with `kubectl describe po POD_NAME` </b></details> <details> <summary>How to check on which node a certain Pod is running?</summary><br><b> `k get po POD_NAME -o wide` </b></details> <details> <summary>Run the following command: <code>kubectl run ohno --image=sheris</code>. Did it work? why not? fix it without removing the Pod and using any image you would like</summary><br><b> Because there is no such image `sheris`. At least for now :) To fix it, run `kubectl edit ohno` and modify the following line `- image: sheris` to `- image: redis` or any other image you prefer. </b></details> <details> <summary>You try to run a Pod but it's in \"Pending\" state. What might be the reason?</summary><br><b> One possible reason is that the scheduler which supposed to schedule Pods on nodes, is not running. To verify it, you can run `kubectl get po -A | grep scheduler` or check directly in `kube-system` namespace.", "metadata": {"source_file": "learning-materials/topics/kubernetes/CKA.md", "section": "Troubleshooting Pods", "language": "en", "created_at": "2025-07-19T19:22:02.091883"}}
{"text": "</b></details> <details> <summary>How to view the logs of a container running in a Pod?</summary><br><b> `k logs POD_NAME` </b></details> <details> <summary>There are two containers inside a Pod called \"some-pod\". What will happen if you run <code>kubectl logs some-pod</code></summary><br><b> It won't work because there are two containers inside the Pod and you need to specify one of them with `kubectl logs POD_NAME -c CONTAINER_NAME` </b></details>", "metadata": {"source_file": "learning-materials/topics/kubernetes/CKA.md", "section": "Troubleshooting Pods", "language": "en", "created_at": "2025-07-19T19:22:02.091915"}}
{"text": "<details> <summary>List all the namespaces</summary><br><b> `k get ns` </b></details> <details> <summary>Create a namespace called 'alle'</summary><br><b> `k create ns alle` </b></details> <details> <summary>Check how many namespaces are there</summary><br><b> `k get ns --no-headers | wc -l` </b></details> <details> <summary>Check how many pods exist in the \"dev\" namespace</summary><br><b> `k get po -n dev` </b></details> <details> <summary>Create a pod called \"kartos\" in the namespace dev. The pod should be using the \"redis\" image.</summary><br><b> If the namespace doesn't exist already: `k create ns dev` `k run kratos --image=redis -n dev` </b></details> <details> <summary>You are looking for a Pod called \"atreus\". How to check in which namespace it runs?</summary><br><b> `k get po -A | grep atreus` </b></details>", "metadata": {"source_file": "learning-materials/topics/kubernetes/CKA.md", "section": "Namespaces", "language": "en", "created_at": "2025-07-19T19:22:02.092042"}}
{"text": "<details> <summary>Run a command to view all nodes of the cluster</summary><br><b> `kubectl get nodes` Note: create an alias (`alias k=kubectl`) and get used to `k get no` </b></details> <details> <summary>Create a list of all nodes in JSON format and store it in a file called \"some_nodes.json\"</summary><br><b> `k get nodes -o json > some_nodes.json` </b></details> <details> <summary>Check what labels one of your nodes in the cluster has</summary><br><b> `k get no minikube --show-labels` </b></details>", "metadata": {"source_file": "learning-materials/topics/kubernetes/CKA.md", "section": "Nodes", "language": "en", "created_at": "2025-07-19T19:22:02.092114"}}
{"text": "<details> <summary>Check how many services are running in the current namespace</summary><br><b> `k get svc` </b></details> <details> <summary>Create an internal service called \"sevi\" to expose the app 'web' on port 1991</summary><br><b> `kubectl expose pod web --port=1991 --name=sevi` </b></details> <details> <summary>How to reference by name a service called \"app-service\" within the same namespace?</summary><br><b> app-service </b></details> <details> <summary>How to check the TargetPort of a service?</summary><br><b> `k describe svc <SERVICE_NAME>` </b></details> <details> <summary>How to check what endpoints the svc has?</summary><br><b> `k describe svc <SERVICE_NAME>` </b></details> <details> <summary>How to reference by name a service called \"app-service\" within a different namespace, called \"dev\"?</summary><br><b> app-service.dev.svc.cluster.local </b></details> <details> <summary>Assume you have a deployment running and you need to create a Service for exposing the pods. This", "metadata": {"source_file": "learning-materials/topics/kubernetes/CKA.md", "section": "Services", "language": "en", "created_at": "2025-07-19T19:22:02.092283"}}
{"text": "is what is required/known: * Deployment name: jabulik * Target port: 8080 * Service type: NodePort * Selector: jabulik-app * Port: 8080 </summary><br><b> `kubectl expose deployment jabulik --name=jabulik-service --target-port=8080 --type=NodePort --port=8080 --dry-run=client -o yaml -> svc.yaml` `vi svc.yaml` (make sure selector is set to `jabulik-app`) `k apply -f svc.yaml` </b></details>", "metadata": {"source_file": "learning-materials/topics/kubernetes/CKA.md", "section": "Services", "language": "en", "created_at": "2025-07-19T19:22:02.092302"}}
{"text": "<details> <summary>How to check how many replicasets defined in the current namespace?</summary><br><b> `k get rs` </b></details> <details> <summary>You have a replica set defined to run 3 Pods. You removed one of these 3 pods. What will happen next? how many Pods will there be?</summary><br><b> There will still be 3 Pods running theoretically because the goal of the replica set is to ensure that. so if you delete one or more Pods, it will run additional Pods so there are always 3 Pods. </b></details> <details> <summary>How to check which container image was used as part of replica set called \"repli\"?</summary><br><b> `k describe rs repli | grep -i image` </b></details> <details> <summary>How to check how many Pods are ready as part of a replica set called \"repli\"?</summary><br><b> `k describe rs repli | grep -i \"Pods Status\"` </b></details> <details> <summary>How to delete a replica set called \"rori\"?</summary><br><b> `k delete rs rori` </b></details> <details> <summary>How to modify", "metadata": {"source_file": "learning-materials/topics/kubernetes/CKA.md", "section": "ReplicaSets", "language": "en", "created_at": "2025-07-19T19:22:02.092552"}}
{"text": "a replica set called \"rori\" to use a different image?</summary><br><b> `k edis rs rori` </b></details> <details> <summary>Scale up a replica set called \"rori\" to run 5 Pods instead of 2</summary><br><b> `k scale rs rori --replicas=5` </b></details> <details> <summary>Scale down a replica set called \"rori\" to run 1 Pod instead of 5</summary><br><b> `k scale rs rori --replicas=1` </b></details>", "metadata": {"source_file": "learning-materials/topics/kubernetes/CKA.md", "section": "ReplicaSets", "language": "en", "created_at": "2025-07-19T19:22:02.092574"}}
{"text": "<details> <summary>Fix the following ReplicaSet definition ```yaml apiVersion: apps/v1 kind: ReplicaCet metadata: name: redis labels: app: redis tier: cache spec: selector: matchLabels: tier: cache template: metadata: labels: tier: cachy spec: containers: - name: redis image: redis ``` </summary><br><b> kind should be ReplicaSet and not ReplicaCet :) </b></details> <details> <summary>Fix the following ReplicaSet definition ```yaml apiVersion: apps/v1 kind: ReplicaSet metadata: name: redis labels: app: redis tier: cache spec: selector: matchLabels: tier: cache template: metadata: labels: tier: cachy spec: containers: - name: redis image: redis ``` </summary><br><b> The selector doesn't match the label (cache vs cachy). To solve it, fix cachy so it's cache instead. </b></details>", "metadata": {"source_file": "learning-materials/topics/kubernetes/CKA.md", "section": "Troubleshooting ReplicaSets", "language": "en", "created_at": "2025-07-19T19:22:02.092683"}}
{"text": "<details> <summary>How to list all the deployments in the current namespace?</summary><br><b> `k get deploy` </b></details> <details> <summary>How to check which image a certain Deployment is using?</summary><br><b> `k describe deploy <DEPLOYMENT_NAME> | grep image` </b></details> <details> <summary>Create a file definition/manifest of a deployment called \"dep\", with 3 replicas that uses the image 'redis'</summary><br><b> `k create deploy dep -o yaml --image=redis --dry-run=client --replicas 3 > deployment.yaml ` </b></details> <details> <summary>Remove the deployment `depdep`</summary><br><b> `k delete deploy depdep` </b></details> <details> <summary>Create a deployment called \"pluck\" using the image \"redis\" and make sure it runs 5 replicas</summary><br><b> `kubectl create deployment pluck --image=redis --replicas=5` </b></details> <details> <summary>Create a deployment with the following properties: * called \"blufer\" * using the image \"python\" * runs 3 replicas * all pods will be", "metadata": {"source_file": "learning-materials/topics/kubernetes/CKA.md", "section": "Deployments", "language": "en", "created_at": "2025-07-19T19:22:02.092892"}}
{"text": "placed on a node that has the label \"blufer\" </summary><br><b> `kubectl create deployment blufer --image=python --replicas=3 -o yaml --dry-run=client > deployment.yaml` Add the following section (`vi deployment.yaml`): ``` spec: affinity: nodeAffinity: requiredDuringSchedlingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: blufer operator: Exists ``` `kubectl apply -f deployment.yaml` </b></details>", "metadata": {"source_file": "learning-materials/topics/kubernetes/CKA.md", "section": "Deployments", "language": "en", "created_at": "2025-07-19T19:22:02.093037"}}
{"text": "<details> <summary>Fix the following deployment manifest ```yaml apiVersion: apps/v1 kind: Deploy metadata: creationTimestamp: null labels: app: dep name: dep spec: replicas: 3 selector: matchLabels: app: dep strategy: {} template: metadata: creationTimestamp: null labels: app: dep spec: containers: - image: redis name: redis resources: {} status: {} ``` </summary><br><b> Change `kind: Deploy` to `kind: Deployment` </b></details> <details> <summary>Fix the following deployment manifest ```yaml apiVersion: apps/v1 kind: Deployment metadata: creationTimestamp: null labels: app: dep name: dep spec: replicas: 3 selector: matchLabels: app: depdep strategy: {} template: metadata: creationTimestamp: null labels: app: dep spec: containers: - image: redis name: redis resources: {} status: {} ``` </summary><br><b> The selector doesn't match the label (dep vs depdep). To solve it, fix depdep so it's dep instead. </b></details>", "metadata": {"source_file": "learning-materials/topics/kubernetes/CKA.md", "section": "Troubleshooting Deployments", "language": "en", "created_at": "2025-07-19T19:22:02.093189"}}
{"text": "<details> <summary>How to schedule a pod on a node called \"node1\"?</summary><br><b> `k run some-pod --image=redix -o yaml --dry-run=client > pod.yaml` `vi pod.yaml` and add: ``` spec: nodeName: node1 ``` `k apply -f pod.yaml` Note: if you don't have a node1 in your cluster the Pod will be stuck on \"Pending\" state. </b></details>", "metadata": {"source_file": "learning-materials/topics/kubernetes/CKA.md", "section": "Scheduler", "language": "en", "created_at": "2025-07-19T19:22:02.093242"}}
{"text": "<details> <summary>Using node affinity, set a Pod to schedule on a node where the key is \"region\" and value is either \"asia\" or \"emea\"</summary><br><b> `vi pod.yaml` ```yaml affinity: nodeAffinity: requiredDuringSchedlingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: region operator: In values: - asia - emea ``` </b></details> <details> <summary>Using node affinity, set a Pod to never schedule on a node where the key is \"region\" and value is \"neverland\"</summary><br><b> `vi pod.yaml` ```yaml affinity: nodeAffinity: requiredDuringSchedlingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: region operator: NotIn values: - neverland ``` </b></details>", "metadata": {"source_file": "learning-materials/topics/kubernetes/CKA.md", "section": "Node Affinity", "language": "en", "created_at": "2025-07-19T19:22:02.093326"}}
{"text": "<details> <summary>How to list all the Pods with the label \"app=web\"?</summary><br><b> `k get po -l app=web` </b></details> <details> <summary>How to list all objects labeled as \"env=staging\"?</summary><br><b> `k get all -l env=staging` </b></details> <details> <summary>How to list all deployments from \"env=prod\" and \"type=web\"?</summary><br><b> `k get deploy -l env=prod,type=web` </b></details>", "metadata": {"source_file": "learning-materials/topics/kubernetes/CKA.md", "section": "Labels and Selectors", "language": "en", "created_at": "2025-07-19T19:22:02.093377"}}
{"text": "<details> <summary>Apply the label \"hw=max\" on one of the nodes in your cluster</summary><br><b> `kubectl label nodes some-node hw=max` </b></details> <details> <summary>Create and run a Pod called `some-pod` with the image `redis` and configure it to use the selector `hw=max`</summary><br><b> ``` kubectl run some-pod --image=redis --dry-run=client -o yaml > pod.yaml vi pod.yaml spec: nodeSelector: hw: max kubectl apply -f pod.yaml ``` </b></details> <details> <summary>Explain why node selectors might be limited</summary><br><b> Assume you would like to run your Pod on all the nodes with with either `hw` set to max or to min, instead of just max. This is not possible with nodeSelectors which are quite simplified and this is where you might want to consider `node affinity`. </b></details>", "metadata": {"source_file": "learning-materials/topics/kubernetes/CKA.md", "section": "Node Selector", "language": "en", "created_at": "2025-07-19T19:22:02.093507"}}
{"text": "fix it?</summary><br><b> `kubectl edit po some-pod` and add the following ``` - effect: NoSchedule key: app operator: Equal value: web ``` Exit and save. The pod should be in Running state now. </b></details> <details> <summary>Remove an existing taint from one of the nodes in your cluster</summary><br><b> `k taint node minikube app=web:NoSchedule-` </b></details>", "metadata": {"source_file": "learning-materials/topics/kubernetes/CKA.md", "section": "Taints", "language": "en", "created_at": "2025-07-19T19:22:02.093750"}}
{"text": "<details> <summary>Deploy metrics-server</summary><br><b> `kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml` </b></details> <details> <summary>Using metrics-server, view the following: * top performing nodes in the cluster * top performing Pods </summary><br><b> * top nodes: `kubectl top nodes` * top pods: `kubectl top pods` </b></details>", "metadata": {"source_file": "learning-materials/topics/kubernetes/CKA.md", "section": "Monitoring", "language": "en", "created_at": "2025-07-19T19:22:02.093965"}}
{"text": "<details> <summary>Can you deploy multiple schedulers?</summary><br><b> Yes, it is possible. You can run another pod with a command similar to: ``` spec: containers: - command: - kube-scheduler - --address=127.0.0.1 - --leader-elect=true - --scheduler-name=some-custom-scheduler ... ``` </b></details> <details> <summary>Assuming you have multiple schedulers, how to know which scheduler was used for a given Pod?</summary><br><b> Running `kubectl get events` you can see which scheduler was used. </b></details> <details> <summary>You want to run a new Pod and you would like it to be scheduled by a custom scheduler. How to achieve it?</summary><br><b> Add the following to the spec of the Pod: ``` spec: schedulerName: some-custom-scheduler ``` </b></details>", "metadata": {"source_file": "learning-materials/topics/kubernetes/CKA.md", "section": "Scheduler", "language": "en", "created_at": "2025-07-19T19:22:02.094094"}}
{"text": "Learn how labels used by ReplicaSets", "metadata": {"source_file": "learning-materials/topics/kubernetes/replicaset_03.md", "section": "Objective", "language": "en", "created_at": "2025-07-19T19:22:02.094227"}}
{"text": "1. Create a ReplicaSet with 2 replicas. Make sure the label used for the selector and in the Pods is \"type=web\" 2. Verify a ReplicaSet was created and there are 2 replicas 3. List the Pods running 4. Remove the label (type=web) from one of the Pods created by the ReplicaSet 5. List the Pods running. Are there more Pods running after removing the label? Why? 6. Verify the ReplicaSet indeed created a new Pod", "metadata": {"source_file": "learning-materials/topics/kubernetes/replicaset_03.md", "section": "Instructions", "language": "en", "created_at": "2025-07-19T19:22:02.094300"}}
{"text": "Learn how to create pods", "metadata": {"source_file": "learning-materials/topics/kubernetes/pods_01.md", "section": "Objective", "language": "en", "created_at": "2025-07-19T19:22:02.094378"}}
{"text": "1. Choose a container image (e.g. redis, nginx, mongo, etc.) 2. Create a pod (in the default namespace) using the image you chose 3. Verify the pod is running", "metadata": {"source_file": "learning-materials/topics/kubernetes/pods_01.md", "section": "Instructions", "language": "en", "created_at": "2025-07-19T19:22:02.094408"}}
{"text": "``` kubectl run nginx --image=nginx --restart=Never --port=80 --labels=\"app=dev-nginx\" cat << EOF > nginx-service.yaml apiVersion: v1 kind: Service metadata: name: nginx-service spec: selector: app: dev-nginx ports: - protocol: TCP port: 80 targetPort: 9372 EOF ```", "metadata": {"source_file": "learning-materials/topics/kubernetes/solutions/services_01_solution.md", "section": "Services 01 - Solution", "language": "en", "created_at": "2025-07-19T19:22:02.094593"}}
{"text": "1. Run Pod with a web service (e.g. httpd) - `kubectl run web --image registry.redhat.io/rhscl/httpd-24-rhel7` 2. Verify the web service is running with the `ps` command - `kubectl exec web -- ps` 3. Check how many restarts the pod has performed - `kubectl get po web` 4. Kill the web service process -`kubectl exec web -- kill 1` 5. Check how many restarts the pod has performed - `kubectl get po web` 6. Verify again the web service is running - `kubectl exec web -- ps`", "metadata": {"source_file": "learning-materials/topics/kubernetes/solutions/killing_containers.md", "section": "\"Killing\" Containers - Solution", "language": "en", "created_at": "2025-07-19T19:22:02.094957"}}
{"text": "* Why did the \"RESTARTS\" count raised? - `Kubernetes restarted the Pod because we killed the process and the container was not running properly.`", "metadata": {"source_file": "learning-materials/topics/kubernetes/solutions/killing_containers.md", "section": "After you complete the exercise", "language": "en", "created_at": "2025-07-19T19:22:02.094996"}}
{"text": "1. Create a ReplicaSet with 2 replicas. The app can be anything. ``` cat >> rs.yaml <<EOL apiVersion: apps/v1 kind: ReplicaSet metadata: name: web labels: app: somewebapp type: web spec: replicas: 2 selector: matchLabels: type: web template: metadata: labels: type: web spec: containers: - name: httpd image: registry.redhat.io/rhscl/httpd-24-rhel7 EOL kubectl apply -f rs.yaml ``` 2. Verify a ReplicaSet was created and there are 2 replicas ``` kubectl get rs", "metadata": {"source_file": "learning-materials/topics/kubernetes/solutions/replicaset_01_solution.md", "section": "ReplicaSet 01 - Solution", "language": "en", "created_at": "2025-07-19T19:22:02.095129"}}
{"text": "``` 3. Delete one of the Pods the ReplicaSet has created ``` kubectl delete po <POD_NAME> ``` 4. If you'll list all the Pods now, what will you see? ``` The same number of Pods. Since we defined 2 replicas, the ReplicaSet will make sure to create another Pod that will replace the one you've deleted. ``` 5. Remove the ReplicaSet you've created ``` kubectl delete -f rs.yaml ``` 6. Verify you've deleted the ReplicaSet ``` kubectl get rs", "metadata": {"source_file": "learning-materials/topics/kubernetes/solutions/replicaset_01_solution.md", "section": "OR a more specific way: kubectl get -f rs.yaml", "language": "en", "created_at": "2025-07-19T19:22:02.095207"}}
{"text": "```", "metadata": {"source_file": "learning-materials/topics/kubernetes/solutions/replicaset_01_solution.md", "section": "OR a more specific way: kubectl get -f rs.yaml", "language": "en", "created_at": "2025-07-19T19:22:02.095224"}}
{"text": "``` 3. Remove the ReplicaSet but NOT the pods it created ``` kubectl delete -f rs.yaml --cascade=orphan ``` 4. Verify you've deleted the ReplicaSet but the Pods are still running ``` kubectl get rs # no replicas kubectl get po # Pods still running ``` 5. Create again the same ReplicaSet, without changing anything ``` kubectl apply -f rs.yaml ``` 6. Verify that the ReplicaSet used the existing Pods and didn't create new Pods ``` kubectl describe rs web # You should see there are no new events and if you list the pods with 'kubectl get po -f rs.yaml` you'll see they have the same names ```", "metadata": {"source_file": "learning-materials/topics/kubernetes/solutions/replicaset_02_solution.md", "section": "OR a more specific way: kubectl get -f rs.yaml", "language": "en", "created_at": "2025-07-19T19:22:02.095461"}}
{"text": "1. Create a ReplicaSet with 2 replicas. Make sure the label used for the selector and in the Pods is \"type=web\" ``` cat >> rs.yaml <<EOL apiVersion: apps/v1 kind: ReplicaSet metadata: name: web labels: app: somewebapp type: web spec: replicas: 2 selector: matchLabels: type: web template: metadata: labels: type: web spec: containers: - name: httpd image: registry.redhat.io/rhscl/httpd-24-rhel7 EOL kubectl apply -f rs.yaml ``` 2. Verify a ReplicaSet was created and there are 2 replicas ``` kubectl get rs", "metadata": {"source_file": "learning-materials/topics/kubernetes/solutions/replicaset_03_solution.md", "section": "ReplicaSet 03 - Solution", "language": "en", "created_at": "2025-07-19T19:22:02.095671"}}
{"text": "``` 3. List the Pods running and save the output somewhere ``` kubectl get po > running_pods.txt ``` 4. Remove the label (type=web) from one of the Pods created by the ReplicaSet ``` kubectl label pod <POD_NAME> type- ``` 5. List the Pods running. Are there more Pods running after removing the label? Why? ``` Yes, there is an additional Pod running because once the label (used as a matching selector) was removed, the Pod became independent meaning, it's not controlled by the ReplicaSet anymore and the ReplicaSet was missing replicas based on its definition so, it created a new Pod. ``` 6. Verify the ReplicaSet indeed created a new Pod ``` kubectl describe rs web ```", "metadata": {"source_file": "learning-materials/topics/kubernetes/solutions/replicaset_03_solution.md", "section": "OR a more specific way: kubectl get -f rs.yaml", "language": "en", "created_at": "2025-07-19T19:22:02.095843"}}
{"text": "``` kubectl run nginx --image=nginx --restart=Never kubectl get pods ```", "metadata": {"source_file": "learning-materials/topics/kubernetes/solutions/pods_01_solution.md", "section": "Pods 01 - Solution", "language": "en", "created_at": "2025-07-19T19:22:02.095930"}}
{"text": "1. Apply the label \"hw=max\" on one of the nodes in your cluster 2. Create and run a Pod called `some-pod` with the image `redis` and configure it to use the selector `hw=max` 3. Explain why node selectors might be limited", "metadata": {"source_file": "learning-materials/topics/kubernetes/exercises/node_selectors/solution.md", "section": "Objectives", "language": "en", "created_at": "2025-07-19T19:22:02.096112"}}
{"text": "Click [here](solution.md) to view the solution 1. `kubectl label nodes some-node hw=max` 2. ``` kubectl run some-pod --image=redis --dry-run=client -o yaml > pod.yaml vi pod.yaml spec: nodeSelector: hw: max kubectl apply -f pod.yaml ``` 3. Assume you would like to run your Pod on all the nodes with with either `hw` set to max or to min, instead of just max. This is not possible with nodeSelectors which are quite simplified and this is where you might want to consider `node affinity`.", "metadata": {"source_file": "learning-materials/topics/kubernetes/exercises/node_selectors/solution.md", "section": "Solution", "language": "en", "created_at": "2025-07-19T19:22:02.096206"}}
{"text": "Click [here](solution.md) to view the solution", "metadata": {"source_file": "learning-materials/topics/kubernetes/exercises/node_selectors/exercise.md", "section": "Solution", "language": "en", "created_at": "2025-07-19T19:22:02.096302"}}
{"text": "1. How to list all the Pods with the label \"app=web\"? 2. How to list all objects labeled as \"env=staging\"? 3. How to list all deployments from \"env=prod\" and \"type=web\"?", "metadata": {"source_file": "learning-materials/topics/kubernetes/exercises/labels_and_selectors/solution.md", "section": "Objectives", "language": "en", "created_at": "2025-07-19T19:22:02.096411"}}
{"text": "`k get po -l app=web` `k get all -l env=staging` `k get deploy -l env=prod,type=web`", "metadata": {"source_file": "learning-materials/topics/kubernetes/exercises/labels_and_selectors/solution.md", "section": "Solution", "language": "en", "created_at": "2025-07-19T19:22:02.096436"}}
{"text": "Click [here](solution.md) to view the solution.", "metadata": {"source_file": "learning-materials/topics/kubernetes/exercises/labels_and_selectors/exercise.md", "section": "Solution", "language": "en", "created_at": "2025-07-19T19:22:02.096520"}}
{"text": "1. Check if one of the nodes in the cluster has taints (doesn't matter which node) 2. Create a taint on one of the nodes in your cluster with key of \"app\" and value of \"web\" and effect of \"NoSchedule\" 1. Explain what it does exactly 2. Verify it was applied", "metadata": {"source_file": "learning-materials/topics/kubernetes/exercises/taints_101/solution.md", "section": "Objectives", "language": "en", "created_at": "2025-07-19T19:22:02.096640"}}
{"text": "1. `kubectl describe no minikube | grep -i taints` 2. `kubectl taint node minikube app=web:NoSchedule` 1. Any resource with \"app=web\" key value will not be scheduled on node `minikube` 2. `kubectl describe no minikube | grep -i taints` 3. ``` kubectl run some-pod --image=redis kubectl edit po some-pod ``` ``` - effect: NoSchedule key: app operator: Equal value: web ``` Save and exit. The Pod should be running.", "metadata": {"source_file": "learning-materials/topics/kubernetes/exercises/taints_101/solution.md", "section": "Solution", "language": "en", "created_at": "2025-07-19T19:22:02.096871"}}
{"text": "1. Check if one of the nodes in the cluster has taints (doesn't matter which node) 2. Create a taint on one of the nodes in your cluster with key of \"app\" and value of \"web\" and effect of \"NoSchedule\" 1. Explain what it does exactly 2. Verify it was applied 3. Run a Pod that will be able to run on the node on which you applied the taint", "metadata": {"source_file": "learning-materials/topics/kubernetes/exercises/taints_101/exercise.md", "section": "Objectives", "language": "en", "created_at": "2025-07-19T19:22:02.097022"}}
{"text": "1. Running Kubernetes cluster 2. Kubctl version 1.14 or above", "metadata": {"source_file": "learning-materials/topics/kubernetes/exercises/kustomize_common_labels/solution.md", "section": "Requirements", "language": "en", "created_at": "2025-07-19T19:22:02.097135"}}
{"text": "In the current directory there is an app composed of a Deployment and Service. 1. Write a kustomization.yml file that will add to both the Service and Deployment the label \"team-name: aces\" 2. Execute a kustomize command that will generate the customized k8s files with the label appended", "metadata": {"source_file": "learning-materials/topics/kubernetes/exercises/kustomize_common_labels/solution.md", "section": "Objectives", "language": "en", "created_at": "2025-07-19T19:22:02.097181"}}
{"text": "1. Add the following to kustomization.yml in someApp directory: ``` apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization commonLabels: team-name: aces resources: - service.yml - deployment.yml ``` 2. Run `kubectl apply -k someApp`", "metadata": {"source_file": "learning-materials/topics/kubernetes/exercises/kustomize_common_labels/solution.md", "section": "Solution", "language": "en", "created_at": "2025-07-19T19:22:02.097210"}}
{"text": "These are not DevOps related questions as you probably noticed, but since they are part of the DevOps interview process I've decided it might be good to keep them<br> There are no answers for these questions for obvious reasons :) <details> <summary>Tell us little bit about yourself</summary><br><b> </b></details> <details> <summary>Tell me about the best type of environment you've worked in (team, solo, pairs, ...)</summary><br><b> </b></details> <details> <summary>Tell me about your last big project/task you worked on</summary><br><b> </b></details> <details> <summary>What was most challenging part in the project you worked on?</summary><br><b> </b></details> <details> <summary>How did you hear about us?</summary><br><b> </b></details> <details> <summary>How would you describe a good leadership?</summary><br><b> </b></details> <details> <summary>Describe yourself in one word</summary><br><b> </b></details> <details> <summary>Tell me about a time where you didn't agree on an", "metadata": {"source_file": "learning-materials/topics/soft_skills/README.md", "section": "HR & Soft Skills", "language": "en", "created_at": "2025-07-19T19:22:02.097887"}}
{"text": "implementation</summary><br><b> </b></details> <details> <summary>How do you deal with a situation where key stakeholders are not around and a big decision needs to be made? </summary><br><b> </b></details> <details> <summary>Where do you see yourself 5 years down the line?</summary><br><b> </b></details> <details> <summary>Give an example of a time when you were able to change the view of a team about a particular tool/project/technology</summary><br><b> </b></details> <details> <summary>Have you ever caused a service outage? (or broke a working project, tool, ...?)</summary><br><b> </b></details> <details> <summary>Rank the following in order 1 to 5, where 1 is most important: salaray, benefits, career, team/people, work life balance</summary><br><b> </b></details> <details> <summary>You have three important tasks scheduled for today. One is for your boss, second for a colleague who is also a friend, third is for a customer. All tasks are equally important. What do you do", "metadata": {"source_file": "learning-materials/topics/soft_skills/README.md", "section": "HR & Soft Skills", "language": "en", "created_at": "2025-07-19T19:22:02.097921"}}
{"text": "first?</summary><br><b> </b></details> <details> <summary>You have a colleague you don‘t get along with. Tell us some strategies how you create a good work relationship with them anyway.</summary><br><b> </b></details> <details> <summary>What do you love about your work?</summary><br><b> </b></details> <details> <summary>What are your responsibilities in your current position?</summary><br><b> </b></details> <details> <summary>Why should we hire you for the role?</summary><br><b> </b></details>", "metadata": {"source_file": "learning-materials/topics/soft_skills/README.md", "section": "HR & Soft Skills", "language": "en", "created_at": "2025-07-19T19:22:02.097942"}}
{"text": "<details> <summary>Why do you want to work here?</summary><br><b> </b></details> <details> <summary>Why are you looking to leave your current place?</summary><br><b> </b></details> <details> <summary>What are your strengths and weaknesses?</summary><br><b> </b></details> <details> <summary>Where do you see yourself in five years?</summary><br><b> </b></details> <details> <summary>When you faced with problem, what do you do?</summary><br><b> </b></details> <details> <summary>When was the last time you had to learn a new technology and what was your approach in doing so?</summary><br><b> </b></details>", "metadata": {"source_file": "learning-materials/topics/soft_skills/README.md", "section": "Pointless Questions", "language": "en", "created_at": "2025-07-19T19:22:02.098015"}}
{"text": "<details> <summary>How would you improve productivity in your team?</summary><br><b> </b></details>", "metadata": {"source_file": "learning-materials/topics/soft_skills/README.md", "section": "Team Lead", "language": "en", "created_at": "2025-07-19T19:22:02.098036"}}
{"text": "Answer the questions given the following program (without running it): ```", "metadata": {"source_file": "learning-materials/topics/os/fork_102.md", "section": "Fork 101", "language": "en", "created_at": "2025-07-19T19:22:02.098161"}}
{"text": "int main() { fork(); fork(); printf(\"\\nyay\\n\"); return 0; } ``` 1. How many times the word \"yay\" will be printed? 2. How many processes will be created?", "metadata": {"source_file": "learning-materials/topics/os/fork_102.md", "section": "include <unistd.h>", "language": "en", "created_at": "2025-07-19T19:22:02.098192"}}
{"text": "int main() { fork(); printf(\"\\nyay\\n\"); return 0; } ``` 1. How many times the word \"yay\" will be printed? 2. How many processes will be created?", "metadata": {"source_file": "learning-materials/topics/os/fork_101.md", "section": "include <unistd.h>", "language": "en", "created_at": "2025-07-19T19:22:02.098277"}}
{"text": "1. 2 2. 2", "metadata": {"source_file": "learning-materials/topics/os/solutions/fork_101_solution.md", "section": "Fork 101 - Solution", "language": "en", "created_at": "2025-07-19T19:22:02.098381"}}
{"text": "1. 4 2. 4", "metadata": {"source_file": "learning-materials/topics/os/solutions/fork_102_solution.md", "section": "Fork 102 - Solution", "language": "en", "created_at": "2025-07-19T19:22:02.098439"}}
{"text": "<details> <summary>What is Cloud Computing? What is a Cloud Provider?</summary><br><b> Cloud computing refers to the delivery of on-demand computing services over the internet on a pay-as-you-go basis. In simple words, Cloud computing is a service that lets you use any computing service such as a server, storage, networking, databases, and intelligence, right through your browser without owning anything. You can do anything you can think of unless it doesn’t require you to stay close to your hardware. Cloud service providers are companies that establish public clouds, manage private clouds, or offer on-demand cloud computing components (also known as cloud computing services) like Infrastructure-as-a-Service (IaaS), Platform-as-a-Service (PaaS), and Software-as-a-Service(SaaS). Cloud services can reduce business process costs when compared to on-premise IT. </b></details> <details> <summary>What are the advantages of cloud computing? Mention at least 3 advantages</summary><br><b> * Pay", "metadata": {"source_file": "learning-materials/topics/cloud/README.md", "section": "Cloud", "language": "en", "created_at": "2025-07-19T19:22:02.099658"}}
{"text": "as you go: you are paying only for what you are using. No upfront payments and payment stops when resources are no longer used. * Scalable: resources are scaled down or up based on demand * High availability: resources and applications provide seamless experience, even when some services are down * Disaster recovery </b></details> <details> <summary>True or False? Cloud computing is a consumption-based model (users only pay for for resources they use)</summary><br><b> True </b></details> <details> <summary>What types of Cloud Computing services are there?</summary><br><b> IAAS - Infrastructure as a Service PAAS - Platform as a Service SAAS - Software as a Service </b></details> <details> <summary>Explain each of the following and give an example: * IAAS * PAAS * SAAS</summary><br><b> * IAAS - Users have control over complete Operating System and don't need to worry about the physical resources, which is managed by Cloud Service Provider. * PAAS - CLoud Service Provider takes care of", "metadata": {"source_file": "learning-materials/topics/cloud/README.md", "section": "Cloud", "language": "en", "created_at": "2025-07-19T19:22:02.099847"}}
{"text": "Operating System, Middlewares and users only need to focus on our Data and Application. * SAAS - A cloud based method to provide software to users, software logics running on cloud, can be run on-premises or managed by Cloud Service Provider. </b></details> <details> <summary>What types of clouds (or cloud deployments) are there?</summary><br><b> * Public - Cloud services sharing computing resources among multiple customers * Private - Cloud services having computing resources limited to specific customer or organization, managed by third party or organizations itself * Hybrid - Combination of public and private clouds </b></details> <details> <summary>What are the differences between Cloud Providers and On-Premise solution?</summary><br><b> In cloud providers, someone else owns and manages the hardware, hire the relevant infrastructure teams and pays for real-estate (for both hardware and people). You can focus on your business. In On-Premise solution, it's quite the opposite. You", "metadata": {"source_file": "learning-materials/topics/cloud/README.md", "section": "Cloud", "language": "en", "created_at": "2025-07-19T19:22:02.099881"}}
{"text": "need to take care of hardware, infrastructure teams and pay for everything which can be quite expensive. On the other hand it's tailored to your needs. </b></details> <details> <summary>What is Serverless Computing?</summary><br><b> The main idea behind serverless computing is that you don't need to manage the creation and configuration of server. All you need to focus on is splitting your app into multiple functions which will be triggered by some actions. It's important to note that: * Serverless Computing is still using servers. So saying there are no servers in serverless computing is completely wrong * Serverless Computing allows you to have a different paying model. You basically pay only when your functions are running and not when the VM or containers are running as in other payment models </b></details> <details> <summary>Can we replace any type of computing on servers with serverless?</summary><br><b> </b></details> <details> <summary>Is there a difference between managed", "metadata": {"source_file": "learning-materials/topics/cloud/README.md", "section": "Cloud", "language": "en", "created_at": "2025-07-19T19:22:02.099902"}}
{"text": "service to SaaS or is it the same thing?</summary><br><b> </b></details> <details> <summary>What is auto scaling?</summary><br><b> AWS definition: \"AWS Auto Scaling monitors your applications and automatically adjusts capacity to maintain steady, predictable performance at the lowest possible cost\" Read more about auto scaling [here](https://aws.amazon.com/autoscaling) </b></details> <details> <summary>What is the difference between horizontal scaling and vertical scaling?</summary><br><b> [AWS Docs](https://wa.aws.amazon.com/wellarchitected/2020-07-02T19-33-23/wat.concept.horizontal-scaling.en.html): A \"horizontally scalable\" system is one that can increase capacity by adding more computers to the system. This is in contrast to a \"vertically scalable\" system, which is constrained to running its processes on only one computer; in such systems the only way to increase performance is to add more resources into one computer in the form of faster (or more) CPUs, memory or storage.", "metadata": {"source_file": "learning-materials/topics/cloud/README.md", "section": "Cloud", "language": "en", "created_at": "2025-07-19T19:22:02.099922"}}
{"text": "Horizontally scalable systems are oftentimes able to outperform vertically scalable systems by enabling parallel execution of workloads and distributing those across many different computers. </b></details> <details> <summary>True or False? Auto Scaling is about adding resources (such as instances) and not about removing resource</summary><br><b> False. Auto scaling adjusts capacity and this can mean removing some resources based on usage and performances. </b></details>", "metadata": {"source_file": "learning-materials/topics/cloud/README.md", "section": "Cloud", "language": "en", "created_at": "2025-07-19T19:22:02.099940"}}
{"text": "<details> <summary>How to secure instances in the cloud?</summary><br><b> * Instance should have minimal permissions needed. You don't want an instance-level incident to become an account-level incident * Instances should be accessed through load balancers or bastion hosts. In other words, they should be off the internet (in a private subnet behind a NAT). * Using latest OS images with your instances (or at least apply latest patches) </b></details>", "metadata": {"source_file": "learning-materials/topics/cloud/README.md", "section": "Cloud - Security", "language": "en", "created_at": "2025-07-19T19:22:02.100007"}}
{"text": "Implement the following grep command in Python (numbers can be different): `grep error -A 2 -B 2 some_file`", "metadata": {"source_file": "learning-materials/topics/programming/grep_berfore_and_after.md", "section": "Introduction", "language": "en", "created_at": "2025-07-19T19:22:02.100212"}}
{"text": "1. Pick a web site to scrape 2. Using any language you would like, write a web scraper to save some data from the site you chose 3. Save the results to a database (doesn't matter which database, just pick one) * Note: if you don't know which site to pick up have a look [here](http://toscrape.com)", "metadata": {"source_file": "learning-materials/topics/programming/web_scraper.md", "section": "Web Scraper", "language": "en", "created_at": "2025-07-19T19:22:02.100317"}}
{"text": "- [DataDog](#datadog) - [Questions](#questions) - [Basics](#basics) - [Datadog Agent](#datadog-agent) - [Datadog Integrations](#datadog-integrations)", "metadata": {"source_file": "learning-materials/topics/datadog/README.md", "section": "DataDog", "language": "en", "created_at": "2025-07-19T19:22:02.100459"}}
{"text": "<details> <summary>Describe at least three use cases for using something like Datadog. Can be as specific as you would like</summary><br><b> * Monitor instances/servers downtime * Detect anomalies and send an alert when it happens * Service request or response latency </b></details> <details> <summary>What ways are there to collect or send data to Datadog?</summary><br><b> * Datadog agent installed on the device or location which you would like to monitor * Using Datadog API * Built-in integrations </b></details> <details> <summary>What is a host in regards to Datadog?</summary><br><b> Any physical or virtual instance that is monitored with Datadog. Few examples: - Cloud Instance, Virtual Machine - Bare metal node - Platform or service specific nodes like Kubernetes node Basically any device or location that has Datadog agent installed and running on. </b></details> <details> <summary>What is a Datadog agent?</summary><br><b> A software runs on a Datadog host. Its purpose is to collect", "metadata": {"source_file": "learning-materials/topics/datadog/README.md", "section": "Basics", "language": "en", "created_at": "2025-07-19T19:22:02.100689"}}
{"text": "data from the host and sent it to Datadog (data like metrics, logs, etc.) </b></details> <details> <summary>What are Datadog tags?</summary><br><b> Datadog tags are used to mark different information with unique properties. For example, you might want to tag some data with \"environment: production\" while tagging information from staging or dev environment with \"environment: staging\". </b></details>", "metadata": {"source_file": "learning-materials/topics/datadog/README.md", "section": "Basics", "language": "en", "created_at": "2025-07-19T19:22:02.100913"}}
{"text": "<details> <summary>What are the component of a Datadog agent?</summary><br><b> * Collector: its role is to collect data from the host on which it's installed. The default period of time as of today is every 15 seconds. * Forwarder: responsible for sending the data to Datadog over HTTPS </b></details>", "metadata": {"source_file": "learning-materials/topics/datadog/README.md", "section": "Datadog Agent", "language": "en", "created_at": "2025-07-19T19:22:02.100972"}}
{"text": "<details> <summary>What can you tell about Datadog integrations?</summary><br><b> - Datadog has many supported integrations with different services, platforms, etc. - Each integration includes information on how to apply it, how to use it and what configuration options it supports </b></details> <details> <summary>What opening some of the integrations windows/pages, there is a ection called \"Monitors\". What can be found there?</summary><br><b> Usually you can find there some anomaly types that Datadog suggests to monitor and track. </b></details>", "metadata": {"source_file": "learning-materials/topics/datadog/README.md", "section": "Datadog Integrations", "language": "en", "created_at": "2025-07-19T19:22:02.101045"}}
{"text": "<details> <summary>What is Agile in regards to software development?</summary><br><b> [Atlassian](https://www.atlassian.com/agile/kanban/kanban-vs-scrum): \"is a structured and iterative approach to project management and product development. It recognizes the volatility of product development, and provides a methodology for self-organizing teams to respond to change without going off the rails.\" </b></details> <details> <summary>What is Kanban in regards to software development?</summary><br><b> * Kanban is an agile software development framework * It focuses on having flexible and fluid process - no deadlines, fewer meetings, less formal roles * While arguable, Kanban seems to fit better small teams rather than big teams who might benefit more from structurized process </b></details> <details> <summary>What is Scrum in regards to software development?</summary><br><b> * Scrum is an agile software development framework * Fixed length iterations * Requires the team to have roles like", "metadata": {"source_file": "learning-materials/topics/software_development/README.md", "section": "Agile Software Development", "language": "en", "created_at": "2025-07-19T19:22:02.101510"}}
{"text": "scrum master and product owner </b></details> <details> <summary>Can you compare between Kanban and Scrum?</summary><br><b> * Kanban is continuous, fluid and visualized process whereas Scrum is short and structured, where work is shipped during fixed intervals known as sprints * Kanban is less structured compared to other frameworks like scrum * Kanban is more visualized way of managing the development process * Kanban has fewer meetings and formal roles compared to other frameworks like scrum </b></details>", "metadata": {"source_file": "learning-materials/topics/software_development/README.md", "section": "Agile Software Development", "language": "en", "created_at": "2025-07-19T19:22:02.101539"}}
{"text": "<details> <summary>What programming language do you prefer to use for DevOps related tasks? Why specifically this one?</summary><br><b> For example, Python. It's multipurpose, easy-to-learn, continuously-evolving, and open-source. And it's very popular today </b></details> <details> <summary>What are static typed (or simply typed) languages?</summary><br><b> In static typed languages the variable type is known at compile-time instead of at run-time. Such languages are: C, C++ and Java </b></details> <details> <summary>Explain expressions and statements</summary><br><b> An expression is anything that results in a value (even if the value is None). Basically, any sequence of literals so, you can say that a string, integer, list, ... are all expressions. Statements are instructions executed by the interpreter like variable assignments, for loops and conditionals (if-else). </b></details> <details> <summary>What is Object Oriented Programming? Why is it important?</summary><br><b>", "metadata": {"source_file": "learning-materials/topics/software_development/README.md", "section": "Programming", "language": "en", "created_at": "2025-07-19T19:22:02.102898"}}
{"text": "[educative.io](https://www.educative.io/blog/object-oriented-programming) \"Object-Oriented Programming (OOP) is a programming paradigm in computer science that relies on the concept of classes and objects. It is used to structure a software program into simple, reusable pieces of code blueprints (usually called classes), which are used to create individual instances of objects.\" OOP is the mainstream paradigm today. Most of the big services are wrote with OOP </b></details> <details> <summary>Explain Composition</summary><br><b> Composition - ability to build a complex object from other objects </b></details> <details> <summary>What is a compiler and interpreter?</summary><br><b> [bzfar.org](https://www.bzfar.org/publ/algorithms_programming/programming_languages/translators_compiler_vs_interpetator/42-1-0-50) Compiler: \"A compiler is a translator used to convert high-level programming language to low-level programming language. It converts the whole program in one session and reports", "metadata": {"source_file": "learning-materials/topics/software_development/README.md", "section": "Programming", "language": "en", "created_at": "2025-07-19T19:22:02.102931"}}
{"text": "errors detected after the conversion. Compiler takes time to do its work as it translates high-level code to lower-level code all at once and then saves it to memory.\" Interpreter: \"Just like a compiler, is a translator used to convert high-level programming language to low-level programming language. It converts the program line by line and reports errors detected at once, while doing the conversion. With this, it is easier to detect errors than in a compiler.\" </b></details> <details> <summary>Are you familiar with SOLID design principles?</summary><br><b> SOLID design principles are about: * Make it easier to extend the functionality of the system * Make the code more readable and easier to maintain SOLID is: * Single Responsibility - A class* should have one ~responsibility~ reason to change. It was edited by Robert Martin due to wrong understanding of principle * Open-Closed - A class should be open for extension, but closed for modification. What this practically means is that", "metadata": {"source_file": "learning-materials/topics/software_development/README.md", "section": "Programming", "language": "en", "created_at": "2025-07-19T19:22:02.102952"}}
{"text": "you should extend functionality by adding a new code and not by modifying it. Your system should be separated into components so it can be easily extended without breaking everything * Liskov Substitution - Any derived class should be able to substitute the its parent without altering its corrections. Practically, every part of the code will get the expected result no matter which part is using it * Interface Segregation - A client should never depend on anything it doesn't uses. Big interfaces must be split to smaller interfaces if needed * Dependency Inversion - High level modules should depend on abstractions, not low level modules *there also can be module, component, entity, etc. Depends on project structure and programming language </b></details> <details> <summary>What is YAGNI? What is your opinion on it?</summary><br><b> YAGNI - You aren't gonna need it. You must add functionality that will be used. No need to add functionality that is not directly needed </b></details>", "metadata": {"source_file": "learning-materials/topics/software_development/README.md", "section": "Programming", "language": "en", "created_at": "2025-07-19T19:22:02.102972"}}
{"text": "<details> <summary>What is DRY? What is your opinion on it?</summary><br><b> DRY - Don't repeat yourself. Actually it means that you shouldn't duplicate logic and use functions/classes instead. But this must be done smartly and pay attention to the domain logic. Same code lines don't always mean duplication </b></details> <details> <summary>What are the four pillars of object oriented programming?</summary><br><b> * Abstraction - you don't need to know how this class implemented. You need to know what functionality does it provide (interface) and how to use it * Encapsulation - keep fields for class purposes private (or protected) and provide public methods if needed. We must keep the data and code safe within the class itself * Inheritance - gives ability to create class that shares some of attributes of existing classes * Polymorphism - same methods in different contexts can do different things. Method overloading and overriding are some forms of polymorphism </b></details> <details>", "metadata": {"source_file": "learning-materials/topics/software_development/README.md", "section": "Programming", "language": "en", "created_at": "2025-07-19T19:22:02.103115"}}
{"text": "<summary>Explain recursion</summary><br><b> Recursion - process (or strategy), when function calls itself. It has recursive case and exit case. In recursive case we call function again, in exit case we finish function without calling it again. If we don't have exit case - function will work infinite, until memory overload or call stack limit </b></details> <details> <summary>Explain Inversion of Control (IoC)</summary><br><b> Inversion of Control - design principle, used to achieve loose coupling. You must use some abstraction layer to access some functionality (similar to SOLID Dependency Inversion) </b></details> <details> <summary>Explain Dependency Injection (DI)</summary><br><b> Dependency Injection - design pattern, used with IoC. Our object fields (dependencies) must be configurated by external objects </b></details> <details> <summary>True or False? In Dynamically typed languages the variable type is known at run-time instead of at compile-time</summary><br><b> True", "metadata": {"source_file": "learning-materials/topics/software_development/README.md", "section": "Programming", "language": "en", "created_at": "2025-07-19T19:22:02.103146"}}
{"text": "</b></details> <details> <summary>Explain what are design patterns</summary><br><b> [refactoring.guru](https://refactoring.guru/): \"Design patterns are typical solutions to commonly occurring problems in software design. They are like pre-made blueprints that you can customize to solve a recurring design problem in your code.\" </b></details> <details> <summary>Explain big O notation</summary><br><b> [habr.com](https://habr.com/ru/post/559518/) \"We can use Big O notation to compare and search different solutions to find which solution is best. The best solution is one that consumes less amount of time and space. Generally, time and space are two parameters that determine the efficiency of the algorithm. Big O Notation tells accurately how long an algorithm takes to run. It is a basic analysis of algorithm efficiency. It describes the execution time required. It depends on the size of input data that essentially passes in. Big O notation gives us algorithm complexity in terms of input", "metadata": {"source_file": "learning-materials/topics/software_development/README.md", "section": "Programming", "language": "en", "created_at": "2025-07-19T19:22:02.103165"}}
{"text": "size. For the large size of input data, the execution time will be slow as compared to the small size of input data. Big O notation is used to analyze space and time.\" </b></details> <details> <summary>What is \"Duck Typing\"?</summary><br><b> \"When I see a bird that walks like a duck and swims like a duck and quacks like a duck, I call that bird a duck.\" This is direction in programming, where we are checking properties of object, but not it's type </b></details> <details> <summary>Explain string interpolation</summary><br><b> String interpolation - process of evaluating of string literal. For example (JS):</b> ```js const messages = 5; console.log(`You have ${messages} new messages`); // You have 5 new messages ``` </details>", "metadata": {"source_file": "learning-materials/topics/software_development/README.md", "section": "Programming", "language": "en", "created_at": "2025-07-19T19:22:02.103184"}}
{"text": "<details> <summary>Binary search: * How does it works? * Can you implement it? (in any language you prefer) * What is the average performance of the algorithm you wrote?</summary><br><b> It's a search algorithm used with sorted arrays/lists to find a target value by dividing the array each iteration and comparing the middle value to the target value. If the middle value is smaller than target value, then the target value is searched in the right part of the divided array, else in the left side. This continues until the value is found (or the array divided max times) [python implementation](coding/python/binary_search.py) The average performance of the above algorithm is O(log n). Best performance can be O(1) and worst O(log n). </b></details>", "metadata": {"source_file": "learning-materials/topics/software_development/README.md", "section": "Common algorithms", "language": "en", "created_at": "2025-07-19T19:22:02.103326"}}
{"text": "<details> <summary>What are your code-review best practices?</summary><br><b> </b></details> <details> <summary>Do you agree/disagree with each of the following statements and why?: * The commit message is not important. When reviewing a change/patch one should focus on the actual change * You shouldn't test your code before submitting it. This is what CI/CD exists for.</summary><br><b> </b></details>", "metadata": {"source_file": "learning-materials/topics/software_development/README.md", "section": "Code Review", "language": "en", "created_at": "2025-07-19T19:22:02.103392"}}
{"text": "<details> <summary>In any language you want, write a function to determine if a given string is a palindrome</summary><br><b> </b></details> <details> <summary>In any language you want, write a function to determine if two strings are Anagrams </summary><br><b> </b></details>", "metadata": {"source_file": "learning-materials/topics/software_development/README.md", "section": "Strings", "language": "en", "created_at": "2025-07-19T19:22:02.103427"}}
{"text": "<details> <summary>In any language you would like, print the numbers from 1 to a given integer. For example for input: 5, the output is: 12345</summary><br><b> </b></details>", "metadata": {"source_file": "learning-materials/topics/software_development/README.md", "section": "Integers", "language": "en", "created_at": "2025-07-19T19:22:02.103452"}}
{"text": "<details> <summary>Describe what would be the time complexity of the operations <code>access</code>, <code>search</code> <code>insert</code> and <code>remove</code> for the following data structures:</summary><br><b> * Stack * Queue * Linked List * Binary Search Tree </b></details> <details> <summary>What is the complexity for the best, worst and average cases of each of the following algorithms?: * Quick sort * Merge sort * Bucket Sort * Radix Sort</summary><br><b> </b></details>", "metadata": {"source_file": "learning-materials/topics/software_development/README.md", "section": "Time Complexity", "language": "en", "created_at": "2025-07-19T19:22:02.103592"}}
{"text": "<details> <summary>Implement Stack in any language you would like</summary><br><b> </b></details> <details> <summary>Tell me everything you know about Linked Lists</summary><br><b> * A linked list is a data structure * It consists of a collection of nodes. Together these nodes represent a sequence * Useful for use cases where you need to insert or remove an element from any position of the linked list * Some programming languages don't have linked lists as a built-in data type (like Python for example) but it can be easily implemented </b></details> <details> <summary>Describe (no need to implement) how to detect a loop in a Linked List</summary><br><b> There are multiple ways to detect a loop in a linked list. I'll mention three here: Worst solution:<br> Two pointers where one points to the head and one points to the last node. Each time you advance the last pointer by one and check whether the distance between head pointer to the moved pointer is bigger than the last time you", "metadata": {"source_file": "learning-materials/topics/software_development/README.md", "section": "Data Structures & Types", "language": "en", "created_at": "2025-07-19T19:22:02.104392"}}
{"text": "measured the same distance (if not, you have a loop).<br> The reason it's probably the worst solution, is because time complexity here is O(n^2) Decent solution:<br> Create an hash table and start traversing the linked list. Every time you move, check whether the node you moved to is in the hash table. If it isn't, insert it to the hash table. If you do find at any point the node in the hash table, it means you have a loop. When you reach None/Null, it's the end and you can return \"no loop\" value. This one is very easy to implement (just create a hash table, update it and check whether the node is in the hash table every time you move to the next node) but since the auxiliary space is O(n) because you create a hash table then, it's not the best solution Good solution:<br> Instead of creating a hash table to document which nodes in the linked list you have visited, as in the previous solution, you can modify the Linked List (or the Node to be precise) to have a \"visited\" attribute.", "metadata": {"source_file": "learning-materials/topics/software_development/README.md", "section": "Data Structures & Types", "language": "en", "created_at": "2025-07-19T19:22:02.104557"}}
{"text": "Every time you visit a node, you set \"visited\" to True.<br> Time compleixty is O(n) and Auxiliary space is O(1), so it's a good solution but the only problem, is that you have to modify the Linked List. Best solution:<br> You set two pointers to traverse the linked list from the beginning. You move one pointer by one each time and the other pointer by two. If at any point they meet, you have a loop. This solution is also called \"Floyd's Cycle-Finding\"<br> Time complexity is O(n) and auxiliary space is O(1). Perfect :) </b></details> <details> <summary>Implement Hash table in any language you would like</summary><br><b> </b></details> <details> <summary>What is Integer Overflow? How is it handled?</summary><br><b> </b></details> <details> <summary>Name 3 design patterns. Do you know how to implement (= provide an example) these design pattern in any language you'll choose?</summary><br><b> </b></details> <details> <summary>Given an array/list of integers, find 3 integers which are", "metadata": {"source_file": "learning-materials/topics/software_development/README.md", "section": "Data Structures & Types", "language": "en", "created_at": "2025-07-19T19:22:02.104590"}}
{"text": "adding up to 0 (in any language you would like)</summary><br><b> ``` def find_triplets_sum_to_zero(li): li = sorted(li) for i, val in enumerate(li): low, up = 0, len(li)-1 while low < i < up: tmp = var + li[low] + li[up] if tmp > 0: up -= 1 elif tmp < 0: low += 1 else: yield li[low], val, li[up] low += 1 up -= 1 ``` </b></details>", "metadata": {"source_file": "learning-materials/topics/software_development/README.md", "section": "Data Structures & Types", "language": "en", "created_at": "2025-07-19T19:22:02.104611"}}
{"text": "flask", "metadata": {"source_file": "learning-materials/topics/flask_container_ci/requirements.txt", "section": "General", "language": "en", "created_at": "2025-07-19T19:22:02.104765"}}
{"text": "Your mission, should you choose to accept it, involves fixing the app in this directory, containerize it and set up a CI for it. Please read carefully all the instructions. If any of the following steps is not working, it is expected from you to fix them", "metadata": {"source_file": "learning-materials/topics/flask_container_ci/README.md", "section": "Introduction", "language": "en", "created_at": "2025-07-19T19:22:02.104895"}}
{"text": "1. Create a virtual environment with `python3 -m venv challenge_venv` 2. Activate it with `source challenge_venv/bin/activate` 3. Install the requirements in this directory `pip install -r requirements.txt`", "metadata": {"source_file": "learning-materials/topics/flask_container_ci/README.md", "section": "Installation", "language": "en", "created_at": "2025-07-19T19:22:02.104927"}}
{"text": "1. Move to `challenges/flask_container_ci` directory, if you are not already there 1. Run `export FLASK_APP=app/main.py` 1. To run the app execute `flask run`. If it doesn't work, fix it 3. Access `http://127.0.0.1:5000`. You should see the following: ``` { \"resources_uris\": { \"user\": \"/users/\\<username\\>\", \"users\": \"/users\" }, \"current_uri\": \"/\" } ``` 4. You should be able to access any of the resources and get the following data: * /users - all users data * /users/<username> - data on the specific chosen user 5. When accessing /users, the data returned should not include the id of the user, only its name and description. Also, the data should be ordered by usernames.", "metadata": {"source_file": "learning-materials/topics/flask_container_ci/README.md", "section": "Run the app", "language": "en", "created_at": "2025-07-19T19:22:02.105045"}}
{"text": "Using Docker or Podman, containerize the flask app so users can run the following two commands: ``` docker build -t app:latest /path/to/Dockerfile docker run -d -p 5000:5000 app ``` 1. You can use any image base you would like 2. Containerize only what you need for running the application, nothing else.", "metadata": {"source_file": "learning-materials/topics/flask_container_ci/README.md", "section": "Containers", "language": "en", "created_at": "2025-07-19T19:22:02.105093"}}
{"text": "Great, now that we have a working app and also can run it in a container, let's set up a CI for it so it won't break again in the future In current directory you have a file called tests.py which includes the tests for the app. What is required from you, is: 1. The CI should run the app tests. You are free to choose whatever CI system or service you prefer. Use `python tests.py` for running the tests. 2. There should be some kind of test for the Dockerfile you wrote 3. Add additional unit test (or another level of tests) for testing the app", "metadata": {"source_file": "learning-materials/topics/flask_container_ci/README.md", "section": "CI", "language": "en", "created_at": "2025-07-19T19:22:02.105204"}}
{"text": "* Except the app functionality, you can change whatever you want - structure, tooling, libraries, ... If possible add `notes.md` file which explains reasons, logic, thoughts and anything else you would like to share * The CI part should include the source code for the pipeline definition", "metadata": {"source_file": "learning-materials/topics/flask_container_ci/README.md", "section": "Guidelines", "language": "en", "created_at": "2025-07-19T19:22:02.105248"}}
{"text": "1. Improve the following query ``` SELECT COUNT(purchased_at) FROM shawarma_purchases WHERE purchased_at BETWEEN '2017-01-01' AND '2017-12-31'; ```", "metadata": {"source_file": "learning-materials/topics/sql/improve_query.md", "section": "Comparisons vs. Functions", "language": "en", "created_at": "2025-07-19T19:22:02.105398"}}
{"text": "``` SELECT count(*) FROM shawarma_purchases WHERE purchased_at >= '2017-01-01' AND purchased_at <= '2017-31-12' ```", "metadata": {"source_file": "learning-materials/topics/sql/solutions/improve_query.md", "section": "Comparisons vs. Functions - Solution", "language": "en", "created_at": "2025-07-19T19:22:02.105498"}}
{"text": "|Name|Topic|Objective & Instructions|Solution|Comments| |--------|--------|------|----|----| | Set up a CI pipeline | CI | [Exercise](ci_for_open_source_project.md) | | | | Deploy to Kubernetes | Deployment | [Exercise](deploy_to_kubernetes.md) | [Solution](solutions/deploy_to_kubernetes/README.md) | | | Jenkins - Remove Jobs | Jenkins Scripts | [Exercise](remove_jobs.md) | [Solution](solutions/remove_jobs_solution.groovy) | | | Jenkins - Remove Builds | Jenkins Scripts | [Exercise](remove_builds.md) | [Solution](solutions/remove_builds_solution.groovy) | |", "metadata": {"source_file": "learning-materials/topics/cicd/README.md", "section": "CI/CD Exercises", "language": "en", "created_at": "2025-07-19T19:22:02.106395"}}
{"text": "<details> <summary>What is Continuous Integration?</summary><br><b> A development practice where developers integrate code into a shared repository frequently. It can range from a couple of changes every day or a week to a couple of changes in one hour in larger scales. Each piece of code (change/patch) is verified to make sure that the change is safe to merge. Today, it's a common practice to test the change using an automated build that makes sure the code can be integrated. It can be one build which runs several tests in different levels (unit, functional, etc.) or several separate builds that all or some has to pass in order for the change to be merged into the repository. </b></details> <details> <summary>What is Continuous Deployment?</summary><br><b> A development strategy used by developers to release software automatically into production where any code commit must pass through an automated testing phase. Only when this is successful is the release considered production", "metadata": {"source_file": "learning-materials/topics/cicd/README.md", "section": "CI/CD Self Assessment", "language": "en", "created_at": "2025-07-19T19:22:02.108473"}}
{"text": "worthy. This eliminates any human interaction and should be implemented only after production-ready pipelines have been set with real-time monitoring and reporting of deployed assets. If any issues are detected in production it should be easy to rollback to previous working state. For more info please read [here](https://www.atlassian.com/continuous-delivery/continuous-deployment) </b></details> <details> <summary>Can you describe an example of a CI (and/or CD) process starting the moment a developer submitted a change/PR to a repository?</summary><br><b> There are many answers for such a question, as CI processes vary, depending on the technologies used and the type of the project to where the change was submitted. Such processes can include one or more of the following stages: * Compile * Build * Install * Configure * Update * Test An example of one possible answer: A developer submitted a pull request to a project. The PR (pull request) triggered two jobs (or one combined job). One", "metadata": {"source_file": "learning-materials/topics/cicd/README.md", "section": "CI/CD Self Assessment", "language": "en", "created_at": "2025-07-19T19:22:02.108646"}}
{"text": "job for running lint test on the change and the second job for building a package which includes the submitted change, and running multiple api/scenario tests using that package. Once all tests passed and the change was approved by a maintainer/core, it's merged/pushed to the repository. If some of the tests failed, the change will not be allowed to merged/pushed to the repository. A complete different answer or CI process, can describe how a developer pushes code to a repository, a workflow then triggered to build a container image and push it to the registry. Once in the registry, the k8s cluster is applied with the new changes. </b></details> <details> <summary>What is Continuous Delivery?</summary><br><b> A development strategy used to frequently deliver code to QA and Ops for testing. This entails having a staging area that has production like features where changes can only be accepted for production after a manual review. Because of this human entanglement there is usually a", "metadata": {"source_file": "learning-materials/topics/cicd/README.md", "section": "CI/CD Self Assessment", "language": "en", "created_at": "2025-07-19T19:22:02.108680"}}
{"text": "time lag between release and review making it slower and error prone as compared to continuous deployment. For more info please read [here](https://www.atlassian.com/continuous-delivery/continuous-deployment) </b></details> <details> <summary>What is difference between Continuous Delivery and Continuous Deployment?</summary><br><b> Both encapsulate the same process of deploying the changes which were compiled and/or tested in the CI pipelines.<br> The difference between the two is that Continuous Delivery isn't fully automated process as opposed to Continuous Deployment where every change that is tested in the process is eventually deployed to production. In continuous delivery someone is either approving the deployment process or the deployment process is based on constraints and conditions (like time constraint of deploying every week/month/...) </b></details> <details> <summary>What CI/CD best practices are you familiar with? Or what do you consider as CI/CD best", "metadata": {"source_file": "learning-materials/topics/cicd/README.md", "section": "CI/CD Self Assessment", "language": "en", "created_at": "2025-07-19T19:22:02.108701"}}
{"text": "practice?</summary><br><b> * Commit and test often. * Testing/Staging environment should be a clone of production environment. * Clean up your environments (e.g. your CI/CD pipelines may create a lot of resources. They should also take care of cleaning up everything they create) * The CI/CD pipelines should provide the same results when executed locally or remotely * Treat CI/CD as another application in your organization. Not as a glue code. * On demand environments instead of pre-allocated resources for CI/CD purposes * Stages/Steps/Tasks of pipelines should be shared between applications or microservices (don't re-invent common tasks like \"cloning a project\") </b></details> <details> <summary>You are given a pipeline and a pool with 3 workers: virtual machine, baremetal and a container. How will you decide on which one of them to run the pipeline?</summary><br><b> The decision on which type of worker (virtual machine, bare-metal, or container) to use for running a pipeline would", "metadata": {"source_file": "learning-materials/topics/cicd/README.md", "section": "CI/CD Self Assessment", "language": "en", "created_at": "2025-07-19T19:22:02.108757"}}
{"text": "depend on several factors, including the nature of the pipeline, the requirements of the software being built, the available resources, and the specific goals and constraints of the development and deployment process. Here are some considerations that can help in making the decision: 1. Pipeline requirements 2. Resource availability 3. Scalability and flexibility 4. Deployment and isolation requirements 5. Security considerations 6. Development and operational workflows 7. Cost considerations Based on these considerations, the appropriate choice of worker (virtual machine, bare-metal, or container) for running the pipeline would be determined by weighing the pros and cons of each option and aligning with the specific requirements, resources, and goals of the development and deployment process. It may also be useful to consult with relevant stakeholders, such as developers, operations, and infrastructure teams, to gather input and make an informed decision. </b></details> <details>", "metadata": {"source_file": "learning-materials/topics/cicd/README.md", "section": "CI/CD Self Assessment", "language": "en", "created_at": "2025-07-19T19:22:02.108783"}}
{"text": "<summary>Where do you store CI/CD pipelines? Why?</summary><br><b> There are multiple approaches as to where to store the CI/CD pipeline definitions: 1. App Repository - store them in the same repository of the application they are building or testing (perhaps the most popular one) 2. Central Repository - store all organization's/project's CI/CD pipelines in one separate repository (perhaps the best approach when multiple teams test the same set of projects and they end up having many pipelines) 3. CI repo for every app repo - you separate CI related code from app code but you don't put everything in one place (perhaps the worst option due to the maintenance) 4. The platform where the CI/CD pipelines are being executed (e.g. Kubernetes Cluster in case of Tekton/OpenShift Pipelines). </b></details> <details> <summary>How do you perform plan capacity for your CI/CD resources? (e.g. servers, storage, etc.)</summary><br><b> Capacity planning for CI/CD resources involves estimating the", "metadata": {"source_file": "learning-materials/topics/cicd/README.md", "section": "CI/CD Self Assessment", "language": "en", "created_at": "2025-07-19T19:22:02.108803"}}
{"text": "resources required to support the CI/CD pipeline and ensuring that the infrastructure has enough capacity to meet the demands of the pipeline. Here are some steps to perform capacity planning for CI/CD resources: 1. Analyze workload 2. Monitor current usage 3. Identify resource bottlenecks 4. Forecast future demand 5. Plan for growth 6. Consider scalability and elasticity 7. Evaluate cost and budget 8. Continuously monitor and adjust By following these steps, you can effectively plan the capacity for your CI/CD resources, ensuring that your pipeline has sufficient resources to operate efficiently and meet the demands of your development process. </b></details> <details> <summary>How would you structure/implement CD for an application which depends on several other applications?</summary><br><b> Implementing Continuous Deployment (CD) for an application that depends on several other applications requires careful planning and coordination to ensure smooth and efficient deployment of", "metadata": {"source_file": "learning-materials/topics/cicd/README.md", "section": "CI/CD Self Assessment", "language": "en", "created_at": "2025-07-19T19:22:02.108822"}}
{"text": "changes across the entire ecosystem. Here are some general steps to structure/implement CD for an application with dependencies: 1. Define the deployment pipeline 2. Automate the deployment process 3. Version control and dependency management 4. Continuous integration and testing 5. Rolling deployments 6. Monitor and manage dependencies 7. Testing across the ecosystem 8. Rollback and recovery strategies 9. Security and compliance 10. Documentation and communication Implementing CD for an application with dependencies requires careful planning, coordination, and automation to ensure efficient and reliable deployments. By following best practices such as automation, version control, testing, monitoring, rollback strategies, and effective communication, you can ensure a smooth and successful CD process for your application ecosystem. </b></details> <details> <summary>How do you measure your CI/CD quality? Are there any metrics or KPIs you are using for measuring the", "metadata": {"source_file": "learning-materials/topics/cicd/README.md", "section": "CI/CD Self Assessment", "language": "en", "created_at": "2025-07-19T19:22:02.108945"}}
{"text": "quality?</summary><br><b> Measuring the quality of CI/CD processes is crucial to identify areas for improvement, ensure efficient and reliable software delivery, and achieve continuous improvement. Here are some commonly used metrics and KPIs (Key Performance Indicators) to measure CI/CD quality: 1. Build Success Rate: This metric measures the percentage of successful builds compared to the total number of builds. A high build success rate indicates that the majority of builds are successful and the CI/CD pipeline is stable. 2. Build and Deployment Time: This metric measures the time it takes to build and deploy changes from code commit to production. Faster build and deployment times indicate shorter feedback loops and faster time to market. 3. Deployment Frequency: This metric measures the frequency of deployments to production within a given time period. Higher deployment frequency indicates faster release cycles and more frequent updates to production. 4. Mean Time to Detect", "metadata": {"source_file": "learning-materials/topics/cicd/README.md", "section": "CI/CD Self Assessment", "language": "en", "created_at": "2025-07-19T19:22:02.108974"}}
{"text": "(MTTD): This metric measures the average time it takes to detect issues or defects in the CI/CD pipeline or production environment. Lower MTTD indicates faster detection and resolution of issues, leading to higher quality and more reliable deployments. 5. Mean Time to Recover (MTTR): This metric measures the average time it takes to recover from issues or incidents in the CI/CD pipeline or production environment. Lower MTTR indicates faster recovery and reduced downtime, leading to higher availability and reliability. 6. Feedback Loop Time: This metric measures the time it takes to receive feedback on code changes, including code reviews, test results, and other feedback mechanisms. Faster feedback loop times enable quicker iterations and faster improvements in the CI/CD process. 7. Customer Satisfaction: This metric measures the satisfaction of end-users or customers with the quality and reliability of the deployed software. Higher customer satisfaction indicates that the CI/CD", "metadata": {"source_file": "learning-materials/topics/cicd/README.md", "section": "CI/CD Self Assessment", "language": "en", "created_at": "2025-07-19T19:22:02.108994"}}
{"text": "process is delivering high-quality software that meets customer expectations. These are just some examples of metrics and KPIs that can be used to measure the quality of CI/CD processes. It's important to choose metrics that align with the goals and objectives of your organization and regularly track and analyze them to continuously improve the CI/CD process and ensure high-quality software delivery. </b></details>", "metadata": {"source_file": "learning-materials/topics/cicd/README.md", "section": "CI/CD Self Assessment", "language": "en", "created_at": "2025-07-19T19:22:02.109013"}}
{"text": "<details> <summary>What is Jenkins? What have you used it for?</summary><br><b> Jenkins is an open source automation tool written in Java with plugins built for Continuous Integration purpose. Jenkins is used to build and test your software projects continuously making it easier for developers to integrate changes to the project, and making it easier for users to obtain a fresh build. It also allows you to continuously deliver your software by integrating with a large number of testing and deployment technologies. Jenkins integrates development life-cycle processes of all kinds, including build, document, test, package, stage, deploy, static analysis and much more. </b></details> <details> <summary>What are the advantages of Jenkins over its competitors? Can you compare it to one of the following systems? * Travis * Bamboo * Teamcity * CircleCI</summary><br><b> Jenkins has several advantages over its competitors, including Travis, Bamboo, TeamCity, and CircleCI. Here are some of the", "metadata": {"source_file": "learning-materials/topics/cicd/README.md", "section": "CI/CD - Jenkins", "language": "en", "created_at": "2025-07-19T19:22:02.113516"}}
{"text": "key advantages: 1. Open-source and free 2. Customizable and flexible 3. Wide range of integrations and Plugins 4. Active and supportive community When comparing Jenkins to its competitors, there are some key differences in terms of features and capabilities. For example: - Travis: Travis is a cloud-based CI/CD platform that is known for its ease of use and fast setup. However, it has fewer customization options and integrations compared to Jenkins. - Bamboo: Bamboo is a CI/CD tool from Atlassian, the makers of JIRA and Confluence. It provides a range of features for building, testing, and deploying software, but it can be more expensive and complex to set up compared to Jenkins. - TeamCity: TeamCity is a CI/CD tool from JetBrains, the makers of IntelliJ IDEA. It provides a range of features for building, testing, and deploying software, but it can be more complex and resource-intensive compared to Jenkins. - CircleCI: CircleCI is a cloud-based CI/CD platform that is known for its fast", "metadata": {"source_file": "learning-materials/topics/cicd/README.md", "section": "CI/CD - Jenkins", "language": "en", "created_at": "2025-07-19T19:22:02.113578"}}
{"text": "build times and easy integration with GitHub. However, it can be more expensive compared to Jenkins, especially for larger projects. </b></details> <details> <summary>What are the limitations or disadvantages of Jenkins?</summary><br><b> This might be considered to be an opinionated answer: * Old fashioned dashboards with not many options to customize it * Containers readiness (this has improved with Jenkins X) * By itself, it doesn't have many features. On the other hand, there many plugins created by the community to expand its abilities * Managing Jenkins and its pipelines as a code can be one hell of a nightmare </b></details> <details> <summary>Explain the following: - Job - Build - Plugin - Node or Worker - Executor</summary><br><b> - Job is an automation definition = what and where to execute once the user clicks on \"build\" - Build is a running instance of a job. You can have one or more builds at any given point of time (unless limited by configuration) - A worker is the", "metadata": {"source_file": "learning-materials/topics/cicd/README.md", "section": "CI/CD - Jenkins", "language": "en", "created_at": "2025-07-19T19:22:02.113608"}}
{"text": "machine/instance on which the build is running. When a build starts, it \"acquires\" a worker out of a pool to run on it. - An executor is variable of the worker, defining how many builds can run on that worker in parallel. An executor value of 3 means, that 3 builds can run at any point on that executor (not necessarily of the same job. Any builds) </b></details> <details> <summary>What plugins have you used in Jenkins?</summary><br><b> Jenkins has a vast library of plugins, and the most commonly used plugins depend on the specific needs and requirements of each organization. However, here are some of the most popular and widely used plugins in Jenkins: Pipeline: This plugin allows users to create and manage complex, multi-stage pipelines using a simple and easy-to-use scripting language. It provides a powerful and flexible way to automate the entire software delivery process, from code commit to deployment. Git: This plugin provides integration with Git, one of the most popular version", "metadata": {"source_file": "learning-materials/topics/cicd/README.md", "section": "CI/CD - Jenkins", "language": "en", "created_at": "2025-07-19T19:22:02.113634"}}
{"text": "control systems used today. It allows users to pull code from Git repositories, trigger builds based on code changes, and push code changes back to Git. Docker: This plugin provides integration with Docker, a popular platform for building, shipping, and running distributed applications. It allows users to build and run Docker containers as part of their build process, enabling easy and repeatable deployment of applications. JUnit: This plugin provides integration with JUnit, a popular unit testing framework for Java applications. It allows users to run JUnit tests as part of their build process and generates reports and statistics on test results. Cobertura: This plugin provides code coverage reporting for Java applications. It allows users to measure the code coverage of their tests and generate reports on which parts of the code are covered by tests. Email Extension: This plugin provides advanced email notification capabilities for Jenkins. It allows users to customize the content", "metadata": {"source_file": "learning-materials/topics/cicd/README.md", "section": "CI/CD - Jenkins", "language": "en", "created_at": "2025-07-19T19:22:02.113817"}}
{"text": "and format of email notifications, including attachments, and send notifications to specific users or groups based on build results. Artifactory: This plugin provides integration with Artifactory, a popular artifact repository for storing and managing binaries and dependencies. It allows users to publish and retrieve artifacts from Artifactory as part of their build process. SonarQube: This plugin provides integration with SonarQube, a popular code quality analysis tool. It allows users to run code quality checks and generate reports on code quality metrics such as code complexity, code duplication, and code coverage. </b></details> <details> <summary>Have you used Jenkins for CI or CD processes? Can you describe them?</summary><br><b> Let's assume we have a web application built using Node.js, and we want to automate its build and deployment process using Jenkins. Here is how we can set up a simple CI/CD pipeline using Jenkins: 1. Install Jenkins: We can install Jenkins on a dedicated", "metadata": {"source_file": "learning-materials/topics/cicd/README.md", "section": "CI/CD - Jenkins", "language": "en", "created_at": "2025-07-19T19:22:02.113849"}}
{"text": "server or on a cloud platform such as AWS or Google Cloud. 2. Install necessary plugins: Depending on the specific requirements of the project, we may need to install plugins such as NodeJS, Git, Docker, and any other plugins required by the project. 3. Create a new job: In Jenkins, a job is a defined set of instructions for automating a particular task. We can create a new job and configure it to build our Node.js application. 4. Configure the job: We can configure the job to pull the latest code from the Git repository, install any necessary dependencies using Node.js, run unit tests, and build the application using a build script. 5. Set up a deployment environment: We can set up a separate environment for deploying the application, such as a staging or production environment. We can use Docker to create a container image of the application and deploy it to the environment. 6. Set up continuous deployment: We can configure the job to automatically deploy the application to the", "metadata": {"source_file": "learning-materials/topics/cicd/README.md", "section": "CI/CD - Jenkins", "language": "en", "created_at": "2025-07-19T19:22:02.113869"}}
{"text": "deployment environment if the build and tests pass. 7. Monitor and troubleshoot: We can monitor the pipeline for errors or failures and troubleshoot any issues that arise. This is just a simple example of a CI/CD pipeline using Jenkins, and the specific implementation details may vary depending on the requirements of the project. </b></details> <details> <summary>What type of jobs are there? Which types have you used?</summary><br><b> In Jenkins, there are various types of jobs, including: 1. Freestyle job: This is the most common type of job in Jenkins, which allows users to define custom build steps and configure various options, including build triggers, SCM polling, and post-build actions. 2. Pipeline job: Pipeline job is a newer feature in Jenkins that allows users to define a pipeline of jobs that can be executed in a specific order. The pipeline can be defined using a Jenkinsfile, which provides a script-like syntax for defining the pipeline stages, steps, and conditions. 3.", "metadata": {"source_file": "learning-materials/topics/cicd/README.md", "section": "CI/CD - Jenkins", "language": "en", "created_at": "2025-07-19T19:22:02.113889"}}
{"text": "Multi-configuration job: This type of job allows users to execute the same job with multiple configurations, such as different operating systems, browsers, or devices. Jenkins will execute the job for each configuration specified, providing a matrix of results. 4. Maven job: This type of job is specifically designed for building Java applications using the Maven build tool. Jenkins will execute the Maven build process, including compiling, testing, and packaging the application. 5. Parameterized job: This type of job allows users to define parameters that can be passed into the build process at runtime. Parameters can be used to customize the build process, such as specifying the version number or target environment. </b></details> <details> <summary>How did you report build results to users? What ways are there to report the results?</summary><br><b> You can report via: * Emails * Messaging apps * Dashboards Each has its own disadvantages and advantages. Emails for example, if sent", "metadata": {"source_file": "learning-materials/topics/cicd/README.md", "section": "CI/CD - Jenkins", "language": "en", "created_at": "2025-07-19T19:22:02.113908"}}
{"text": "too often, can be eventually disregarded or ignored. </b></details> <details> <summary>You need to run unit tests every time a change submitted to a given project. Describe in details how your pipeline would look like and what will be executed in each stage</summary><br><b> The pipelines will have multiple stages: * Clone the project * Install test dependencies (for example, if I need tox package to run the tests, I will install it in this stage) * Run unit tests * (Optional) report results (For example an email to the users) * Archive the relevant logs/files </b></details> <details> <summary>How to secure Jenkins?</summary><br><b> [Jenkins documentation](https://www.jenkins.io/doc/book/security/securing-jenkins/) provides some basic intro for securing your Jenkins server. </b></details> <details> <summary>Describe how do you add new nodes (agents) to Jenkins</summary><br><b> You can describe the UI way to add new nodes but better to explain how to do in a way that scales like a script", "metadata": {"source_file": "learning-materials/topics/cicd/README.md", "section": "CI/CD - Jenkins", "language": "en", "created_at": "2025-07-19T19:22:02.113926"}}
{"text": "or using dynamic source for nodes like one of the existing clouds. </b></details> <details> <summary>How to acquire multiple nodes for one specific build?</summary><br><b> To acquire multiple nodes for a specific build in Jenkins, you can use the \"Parallel\" feature in the pipeline script. The \"Parallel\" feature allows you to run multiple stages in parallel, and each stage can run on a different node. Here is an example pipeline script that demonstrates how to acquire multiple nodes for a specific build: ```tsx pipeline { agent any stages { stage('Build') { parallel { stage('Node 1') { agent { label 'node1' } steps { // Run build commands on Node 1 } } stage('Node 2') { agent { label 'node2' } steps { // Run build commands on Node 2 } } stage('Node 3') { agent { label 'node3' } steps { // Run build commands on Node 3 } } } } stage('Deploy') { agent any steps { // Deploy the built artifacts } } } } ``` In this example, the \"Build\" stage has three parallel stages, each running on a", "metadata": {"source_file": "learning-materials/topics/cicd/README.md", "section": "CI/CD - Jenkins", "language": "en", "created_at": "2025-07-19T19:22:02.113944"}}
{"text": "different node labeled as \"node1\", \"node2\", and \"node3\". The \"Deploy\" stage runs after the build is complete and runs on any available node. To use this pipeline script, you will need to have the three nodes (node1, node2, and node3) configured in Jenkins. You will also need to ensure that the necessary build commands and dependencies are installed on each node. </b></details> <details> <summary>Whenever a build fails, you would like to notify the team owning the job regarding the failure and provide failure reason. How would you do that?</summary><br><b> In Jenkins, you can use the \"Email Notification\" plugin to notify a team when a build fails. Here are the steps to set up email notifications for failed builds: 1. Install the \"Email Notification\" plugin if it's not already installed in Jenkins. 2. Go to the Jenkins job configuration page and click on \"Configure\". 3. Scroll down to the \"Post-build Actions\" section and click on \"Add post-build action\". 4. Select \"Editable Email", "metadata": {"source_file": "learning-materials/topics/cicd/README.md", "section": "CI/CD - Jenkins", "language": "en", "created_at": "2025-07-19T19:22:02.114080"}}
{"text": "Notification\" from the list of options. 5. Fill out the required fields, such as the recipient email addresses, subject line, and email content. You can use Jenkins environment variables, such as ${BUILD_URL} and ${BUILD_LOG}, to include build-specific information in the email content. 6. In the \"Advanced Settings\" section, select the \"Send to recipients\" option and choose \"Only on failure\" from the dropdown menu. 7. Click \"Save\" to save the job configuration. With this setup, Jenkins will send an email notification to the specified recipients whenever a build fails, providing them with the failure reason and any other relevant information. </b></details> <details> <summary>There are four teams in your organization. How to prioritize the builds of each team? So the jobs of team x will always run before team y for example</summary><br><b> In Jenkins, you can prioritize the builds of each team by using the \"Priority Sorter\" plugin. Here are the steps to set up build prioritization: 1.", "metadata": {"source_file": "learning-materials/topics/cicd/README.md", "section": "CI/CD - Jenkins", "language": "en", "created_at": "2025-07-19T19:22:02.114112"}}
{"text": "Install the \"Priority Sorter\" plugin if it's not already installed in Jenkins. 2. Go to the Jenkins system configuration page and click on \"Configure Global Security\". Scroll down to the \"Access Control\" section and click on \"Per-project basis\". 3. In the \"Project default actions\" section, select \"Configure build triggers and execution\" from the dropdown menu. Click on \"Add user or group\" and add the groups that represent each team in your organization. 4. Go to each Jenkins job configuration page and click on \"Configure\". Scroll down to the \"Build Environment\" section and click on \"Add build step\". Select \"Set build priority with Priority Sorter\" from the list of options. 5. Set the priority of the job based on the team that owns it. For example, if Team X owns the job, set the priority to a higher value than the jobs owned by Team Y. Click \"Save\" to save the job configuration. With this setup, Jenkins will prioritize the builds of each team based on the priority value set in the job", "metadata": {"source_file": "learning-materials/topics/cicd/README.md", "section": "CI/CD - Jenkins", "language": "en", "created_at": "2025-07-19T19:22:02.114140"}}
{"text": "configuration. Jobs owned by Team X will have a higher priority than jobs owned by Team Y, ensuring that they are executed first. </b></details> <details> <summary>If you are managing a dozen of jobs, you can probably use the Jenkins UI. But how do you manage the creation and deletion of hundreds of jobs every week/month?</summary><br><b> Managing the creation and deletion of hundreds of jobs every week/month in Jenkins can be a daunting task if done manually through the UI. Here are some approaches to manage large numbers of jobs efficiently: 1. Use job templates 2. Use Job DSL 3. Use Jenkins REST API 4. Use a configuration management tool 5. Use a Jenkins job management tool </b></details> <details> <summary>What are some of Jenkins limitations?</summary><br><b> * Testing cross-dependencies (changes from multiple projects together) * Starting builds from any stage (although Cloudbees implemented something called checkpoints) </b></details> <details> <summary>What is the different", "metadata": {"source_file": "learning-materials/topics/cicd/README.md", "section": "CI/CD - Jenkins", "language": "en", "created_at": "2025-07-19T19:22:02.114161"}}
{"text": "between a scripted pipeline to declarative pipeline? Which type are you using?</summary><br><b> Jenkins supports two types of pipelines: Scripted pipelines and Declarative pipelines. Scripted pipelines use Groovy syntax and provide a high degree of flexibility and control over the build process. Scripted pipelines allow developers to write custom code to handle complex scenarios, but can be complex and hard to maintain. Declarative pipelines are a newer feature and provide a simpler way to define pipelines using YAML syntax. Declarative pipelines provide a more structured and opinionated way to define builds, making it easier to get started with pipelines and reducing the risk of errors. Some key differences between the two types of pipelines are: 1. Syntax: Scripted pipelines use Groovy syntax while declarative pipelines use YAML syntax. 2. Structure: Declarative pipelines have a more structured format and define specific stages, while scripted pipelines provide more flexibility in", "metadata": {"source_file": "learning-materials/topics/cicd/README.md", "section": "CI/CD - Jenkins", "language": "en", "created_at": "2025-07-19T19:22:02.114179"}}
{"text": "defining build stages and steps. 3. Error handling: Declarative pipelines provide a more comprehensive error handling system with built-in conditions and actions, while scripted pipelines require more manual error handling. 4. Ease of use: Declarative pipelines are easier to use for beginners and provide a simpler syntax, while scripted pipelines require more expertise in Groovy and can be more complex. 5. Maintenance: Declarative pipelines are easier to maintain and can be modified with less effort compared to scripted pipelines, which can be more difficult to modify and extend over time. I am familiar with both types of pipelines, but generally prefer declarative pipelines for their ease of use and simplicity. </b></details> <details> <summary>How would you implement an option of a starting a build from a certain stage and not from the beginning?</summary><br><b> To implement an option of starting a build from a certain stage and not from the beginning in a Jenkins pipeline, we can", "metadata": {"source_file": "learning-materials/topics/cicd/README.md", "section": "CI/CD - Jenkins", "language": "en", "created_at": "2025-07-19T19:22:02.114198"}}
{"text": "use the `when` directive along with a custom parameter to determine the starting stage. Here are the steps to implement this: 1. Add a custom parameter to the pipeline. This parameter can be a simple string or a more complex data type like a map. ```tsx parameters { string(name: 'START_STAGE', defaultValue: '', description: 'The name of the stage to start the build from') } ``` 2. Use the `when` directive to conditionally execute stages based on the value of the `START_STAGE` parameter. ```tsx stage('Build') { when { expression { params.START_STAGE == '' || currentStage.name == params.START_STAGE } } // Build steps go here } stage('Test') { when { expression { params.START_STAGE == '' || currentStage.name == params.START_STAGE || previousStage.result == 'SUCCESS' } } // Test steps go here } stage('Deploy') { when { expression { params.START_STAGE == '' || currentStage.name == params.START_STAGE || previousStage.result == 'SUCCESS' } } // Deploy steps go here } ``` In this example, we", "metadata": {"source_file": "learning-materials/topics/cicd/README.md", "section": "CI/CD - Jenkins", "language": "en", "created_at": "2025-07-19T19:22:02.114216"}}
{"text": "use the `when` directive to execute each stage only if the `START_STAGE` parameter is empty or matches the current stage's name. Additionally, for the `Test` and `Deploy` stages, we also check if the previous stage executed successfully before running. 3. Trigger the pipeline and pass the `START_STAGE` parameter as needed. ```tsx pipeline { agent any parameters { string(name: 'START_STAGE', defaultValue: '', description: 'The name of the stage to start the build from') } stages { stage('Build') { // Build steps go here } stage('Test') { // Test steps go here } stage('Deploy') { // Deploy steps go here } } } ``` When triggering the pipeline, you can pass the `START_STAGE` parameter to start the build from a specific stage. For example, if you want to start the build from the `Test` stage, you can trigger the pipeline with the `START_STAGE` parameter set to `'Test'`: ```tsx pipeline?START_STAGE=Test ``` This will cause the pipeline to skip the `Build` stage and start directly from the", "metadata": {"source_file": "learning-materials/topics/cicd/README.md", "section": "CI/CD - Jenkins", "language": "en", "created_at": "2025-07-19T19:22:02.114336"}}
{"text": "`Test` stage. </b></details> <details> <summary>Do you have experience with developing a Jenkins plugin? Can you describe this experience?</summary><br><b> Developing a Jenkins plugin requires knowledge of Java and familiarity with Jenkins API. The process typically involves setting up a development environment, creating a new plugin project, defining the plugin's extension points, and implementing the desired functionality using Java code. Once the plugin is developed, it can be packaged and deployed to Jenkins. The Jenkins plugin ecosystem is extensive, and there are many resources available to assist with plugin development, including documentation, forums, and online communities. Additionally, Jenkins provides tools such as Jenkins Plugin POM Generator and Jenkins Plugin Manager to help with plugin development and management. </b></details> <details> <summary>Have you written Jenkins scripts? If yes, what for and how they work?</summary><br><b> </b></details>", "metadata": {"source_file": "learning-materials/topics/cicd/README.md", "section": "CI/CD - Jenkins", "language": "en", "created_at": "2025-07-19T19:22:02.114373"}}
{"text": "<details> <summary>What is a Workflow in GitHub Actions?</summary><br><b> A YAML file that defines the automation actions and instructions to execute upon a specific event.<br> The file is placed in the repository itself. A Workflow can be anything - running tests, compiling code, building packages, ... </b></details> <details> <summary>What is a Runner in GitHub Actions?</summary><br><b> A workflow has to be executed somewhere. The environment where the workflow is executed is called Runner.<br> A Runner can be an on-premise host or GitHub hoste </b></details> <details> <summary>What is a Job in GitHub Actions?</summary><br><b> A job is a series of steps which are executed on the same runner/environment.<br> A workflow must include at least one job. </b></details> <details> <summary>What is an Action in GitHub Actions?</summary><br><b> An action is the smallest unit in a workflow. It includes the commands to execute as part of the job. </b></details> <details> <summary>In GitHub", "metadata": {"source_file": "learning-materials/topics/cicd/README.md", "section": "CI/CD - GitHub Actions", "language": "en", "created_at": "2025-07-19T19:22:02.114741"}}
{"text": "Actions workflow, what the 'on' attribute/directive is used for?</summary><br><b> Specify upon which events the workflow will be triggered.<br> For example, you might configure the workflow to trigger every time a changed is pushed to the repository. </b></details> <details> <summary>True or False? In Github Actions, jobs are executed in parallel by default</summary><br><b> True </b></details> <details> <summary>How to create dependencies between jobs so one job runs after another?</summary><br><b> Using the \"needs\" attribute/directive. ``` jobs: job1: job2: needs: job1 ``` In the above example, job1 must complete successfully before job2 runs </b></details> <details> <summary>How to add a Workflow to a repository?</summary><br><b> CLI: 1. Create the directory `.github/workflows` in the repository 2. Add a YAML file UI: 1. In the repository page, click on \"Actions\" 2. Choose workflow and click on \"Set up this workflow\" </b></details>", "metadata": {"source_file": "learning-materials/topics/cicd/README.md", "section": "CI/CD - GitHub Actions", "language": "en", "created_at": "2025-07-19T19:22:02.114771"}}
{"text": "<details> <summary>In Zuul, What are the <code>check</code> pipelines?</summary><br><b> `check` pipeline are triggered when a patch is uploaded to a code review system (e.g. Gerrit).<br> </b></details> <details> <summary>In Zuul, What are the <code>gate</code> pipelines?</summary><br><b> `gate` pipeline are triggered when a code reviewer approves the change in a code review system (e.g. Gerrit) </b></details> <details> <summary>True or False? <code>gate</code> pipelines run after the <code>check</code> pipelines</summary><br><b> True. `check` pipeline run when the change is uploaded, while the `gate` pipelines run when the change is approved by a reviewer </b></details>", "metadata": {"source_file": "learning-materials/topics/cicd/README.md", "section": "Zuul", "language": "en", "created_at": "2025-07-19T19:22:02.114865"}}
{"text": "Learn how to write a Jenkins script to remove Jenkins jobs", "metadata": {"source_file": "learning-materials/topics/cicd/remove_jobs.md", "section": "Objective", "language": "en", "created_at": "2025-07-19T19:22:02.114995"}}
{"text": "1. Create three jobs called: test-job, test2-job and prod-job 2. Write a script to remove all the jobs that include the string \"test\"", "metadata": {"source_file": "learning-materials/topics/cicd/remove_jobs.md", "section": "Instructions", "language": "en", "created_at": "2025-07-19T19:22:02.115023"}}
{"text": "1. Choose an open source project from Github and fork it 2. Create a CI pipeline/workflow for the project you forked 3. The CI pipeline/workflow will include anything that is relevant to the project you forked. For example: * If it's a Python project, you will run PEP8 * If the project has unit tests directory, you will run these unit tests as part of the CI 4. In a separate file, describe what is running as part of the CI and why you chose to include it. You can also describe any thoughts, dilemmas, challenge you had", "metadata": {"source_file": "learning-materials/topics/cicd/ci_for_open_source_project.md", "section": "CI for Open Source Project", "language": "en", "created_at": "2025-07-19T19:22:02.115183"}}
{"text": "Containerize the app of the project you forked using any container engine you would like (e.g. Docker, Podman).<br> Once you successfully ran the application in a container, submit the Dockerfile to the original project (but be prepared that the maintainer might not need/want that).", "metadata": {"source_file": "learning-materials/topics/cicd/ci_for_open_source_project.md", "section": "Bonus", "language": "en", "created_at": "2025-07-19T19:22:02.115229"}}
{"text": "The following is a list of projects without CI (at least at the moment): Note: I wrote a script to find these (except the first project on the list, of course) based on some parameters in case you wonder why these projects specifically are listed. * [This one](https://github.com/bregman-arie/devops-exercises) - We don't have CI! help! :) * [image retrieval platform](https://github.com/skx6/image_retrieval_platform) * [FollowSpot](https://github.com/jenbrissman/FollowSpot) * [Pyrin](https://github.com/mononobi/pyrin) * [food-detection-yolov5](https://github.com/lannguyen0910/food-detection-yolov5) * [Lifely](https://github.com/sagnik1511/Lifely)", "metadata": {"source_file": "learning-materials/topics/cicd/ci_for_open_source_project.md", "section": "Suggestions for Projects", "language": "en", "created_at": "2025-07-19T19:22:02.115290"}}
{"text": "* Write a pipeline that will deploy an \"hello world\" web app to Kubernetes * The CI/CD system (where the pipeline resides) and the Kubernetes cluster should be on separate systems * The web app should be accessible remotely and only with HTTPS", "metadata": {"source_file": "learning-materials/topics/cicd/deploy_to_kubernetes.md", "section": "Deploy to Kubernetes", "language": "en", "created_at": "2025-07-19T19:22:02.115384"}}
{"text": "Learn how to write a Jenkins script that interacts with builds by removing builds older than X days.", "metadata": {"source_file": "learning-materials/topics/cicd/remove_builds.md", "section": "Objective", "language": "en", "created_at": "2025-07-19T19:22:02.115705"}}
{"text": "1. Pick up (or create) a job which has builds older than X days 2. Write a script to remove only the builds that are older than X days", "metadata": {"source_file": "learning-materials/topics/cicd/remove_builds.md", "section": "Instructions", "language": "en", "created_at": "2025-07-19T19:22:02.115780"}}
{"text": "X can be anything. For example, remove builds that are older than 3 days. Just make sure that you don't simply remove all the builds (since that's different from the objective).", "metadata": {"source_file": "learning-materials/topics/cicd/remove_builds.md", "section": "Hints", "language": "en", "created_at": "2025-07-19T19:22:02.115810"}}
{"text": "Note: this exercise can be solved in various ways. The solution described here is just one possible way. 1. Install Jenkins on one system (follow up the standard Jenkins installation procedure) 2. Deploy Kubernetes on a remote host (minikube can be an easy way to achieve it) 3. Create a simple web app or [page](html) 4. Create Kubernetes [resources](helloworld.yml) - Deployment, Service and Ingress (for HTTPS access) 5. Create an [Ansible inventory](inventory) and insert the address of the Kubernetes cluster 6. Write [Ansible playbook](deploy.yml) to deploy the Kubernetes resources and also generate 7. Create a [pipeline](Jenkinsfile) 8. Run the pipeline :) 9. Try to access the web app remotely", "metadata": {"source_file": "learning-materials/topics/cicd/solutions/deploy_to_kubernetes/README.md", "section": "Deploy to Kubernetes", "language": "en", "created_at": "2025-07-19T19:22:02.116076"}}
{"text": "Your mission, should you choose to accept it, involves developing an app, containerize it and set up a CI for it. Please read carefully all the instructions. If any of the following steps is not working, it is expected from you to fix them", "metadata": {"source_file": "learning-materials/topics/flask_container_ci2/README.md", "section": "Introduction", "language": "en", "created_at": "2025-07-19T19:22:02.116344"}}
{"text": "1. Move to `challenges/flask_container_ci` directory, if you are not already there 1. Run `export FLASK_APP=app/main.py` 1. To run the app execute `flask run`. If it doesn't works, fix it 3. Access `http://127.0.0.1:5000`. You should see the following ``` { \"current_uri\": \"/\", \"example\": \"/matrix/'123n456n789'\", \"resources\": { \"column\": \"/columns/<matrix>/<column_number>\", \"matrix\": \"/matrix/<matrix>\", \"row\": \"/rows/<matrix>/<row_number>\" } } ``` 4. You should be able to access any of the resources and get the following data: * /matrix/\\<matrix\\> for example, for /matrix/123n456n789 the user will get: 1 2 3 4 5 6 7 8 9 * /matrix/\\<matrix\\>/\\<column_number\\> for example, for /matrix/123n456n789/2 the user will get: 2 5 8 * /matrix/\\<matrix\\>/\\<row_number\\> for example, for /matrix/123n456n789/1 the user will get: 1 2 3", "metadata": {"source_file": "learning-materials/topics/flask_container_ci2/README.md", "section": "Run the app", "language": "en", "created_at": "2025-07-19T19:22:02.116504"}}
{"text": "Great, now that we have a working app and also can run it in a container, let's set up a CI for it so it won't break again in the future In current directory you have a file called tests.py which includes the tests for the app. What is required from you, is: 1. Write a CI pipeline that will run the app tests. You are free to choose whatever CI system or service you prefer. Use `python tests.py` for running the tests. 2. There should be some kind of test for the Dockerfile you wrote 3. Add additional unit test (or any other level of tests) for testing the app", "metadata": {"source_file": "learning-materials/topics/flask_container_ci2/README.md", "section": "CI", "language": "en", "created_at": "2025-07-19T19:22:02.116665"}}
{"text": "* Except the app functionality, you can change whatever you want - structure, tooling, libraries, ... If possible, add `notes.md` file which explains reasons, logic, thoughts and anything else you would like to share * The CI part should include the source code for the pipeline definition", "metadata": {"source_file": "learning-materials/topics/flask_container_ci2/README.md", "section": "Guidelines", "language": "en", "created_at": "2025-07-19T19:22:02.116745"}}
{"text": "1. Write a task to create the directory ‘/tmp/new_directory’", "metadata": {"source_file": "learning-materials/topics/ansible/my_first_task.md", "section": "Ansible - My First Task", "language": "en", "created_at": "2025-07-19T19:22:02.116899"}}
{"text": "<!-- {% raw %} -->", "metadata": {"source_file": "learning-materials/topics/ansible/README.md", "section": "Ansible", "language": "en", "created_at": "2025-07-19T19:22:02.117239"}}
{"text": "|Name|Topic|Objective & Instructions|Solution|Comments| |--------|--------|------|----|----| | My First Task | Tasks | [Exercise](my_first_task.md) | [Solution](solutions/my_first_task.md) | Upgrade and Update Task | Tasks | [Exercise](update_upgrade_task.md) | [Solution](solutions/update_upgrade_task.md) | My First Playbook | Playbooks | [Exercise](my_first_playbook.md) | [Solution](solutions/my_first_playbook.md)", "metadata": {"source_file": "learning-materials/topics/ansible/README.md", "section": "Ansible Exercises", "language": "en", "created_at": "2025-07-19T19:22:02.117278"}}
{"text": "<details> <summary>Describe each of the following components in Ansible, including the relationship between them: * Task * Inventory * Module * Play * Playbook * Role </summary><br><b> Task – a call to a specific Ansible module Module – the actual unit of code executed by Ansible on your own host or a remote host. Modules are indexed by category (database, file, network, …) and also referred to as task plugins. Inventory – An inventory file defines hosts and/or groups of hosts on which Ansible tasks executed upon. The inventory file can be in one of many formats, depending on the inventory plugins you have. The most common formats are INI and YAML. Play – One or more tasks executed on a given host(s) Playbook – One or more plays. Each play can be executed on the same or different hosts Role – Ansible roles allows you to group resources based on certain functionality/service such that they can be easily reused. In a role, you have directories for variables, defaults, files, templates,", "metadata": {"source_file": "learning-materials/topics/ansible/README.md", "section": "Ansible Self Assessment", "language": "en", "created_at": "2025-07-19T19:22:02.118162"}}
{"text": "handlers, tasks, and metadata. You can then use the role by simply specifying it in your playbook. </b></details> <details> <summary>How Ansible is different from other automation tools? (e.g. Chef, Puppet, etc.)</summary><br><b> Ansible is: * Agentless * Minimal run requirements (Python & SSH) and simple to use * Default mode is \"push\" (it supports also pull) * Focus on simpleness and ease-of-use </b></details> <details> <summary>True or False? Ansible follows the mutable infrastructure paradigm</summary><br><b> True. In immutable infrastructure approach, you'll replace infrastructure instead of modifying it.<br> Ansible rather follows the mutable infrastructure paradigm where it allows you to change the configuration of different components, but this approach is not perfect and has its own disadvantages like \"configuration drift\" where different components may reach different state for different reasons. </b></details> <details> <summary>True or False? Ansible uses declarative style", "metadata": {"source_file": "learning-materials/topics/ansible/README.md", "section": "Ansible Self Assessment", "language": "en", "created_at": "2025-07-19T19:22:02.118197"}}
{"text": "to describe the expected end state</summary><br><b> False. It uses a procedural style. </b></details> <details> <summary>What kind of automation you wouldn't do with Ansible and why?</summary><br><b> While it's possible to provision resources with Ansible, some prefer to use tools that follow immutable infrastructure paradigm. Ansible doesn't saves state by default. So a task that creates 5 instances for example, when executed again will create additional 5 instances (unless additional check is implemented or explicit names are provided) while other tools might check if 5 instances exist. If only 4 exist (by checking the state file for example), one additional instance will be created to reach the end goal of 5 instances. </b></details> <details> <summary>How do you list all modules and how can you see details on a specific module?</summary><br><br> 1. Ansible online docs 2. `ansible-doc -l` for list of modules and `ansible-doc [module_name]` for detailed information on a specific", "metadata": {"source_file": "learning-materials/topics/ansible/README.md", "section": "Ansible Self Assessment", "language": "en", "created_at": "2025-07-19T19:22:02.118219"}}
{"text": "module </b></details>", "metadata": {"source_file": "learning-materials/topics/ansible/README.md", "section": "Ansible Self Assessment", "language": "en", "created_at": "2025-07-19T19:22:02.118376"}}
{"text": "<details> <summary>What is an inventory file and how do you define one?</summary><br><b> An inventory file defines hosts and/or groups of hosts on which Ansible tasks executed upon. An example of inventory file: ``` 192.168.1.2 192.168.1.3 192.168.1.4 [web_servers] 190.40.2.20 190.40.2.21 190.40.2.22 ``` </b></details> <details> <summary>What is a dynamic inventory file? When you would use one?</summary><br><br> A dynamic inventory file tracks hosts from one or more sources like cloud providers and CMDB systems. You should use one when using external sources and especially when the hosts in your environment are being automatically<br> spun up and shut down, without you tracking every change in these sources. </b></details>", "metadata": {"source_file": "learning-materials/topics/ansible/README.md", "section": "Ansible - Inventory", "language": "en", "created_at": "2025-07-19T19:22:02.118501"}}
{"text": "<details> <summary>Modify the following task to use a variable instead of the value \"zlib\" and have \"zlib\" as the default in case the variable is not defined ``` - name: Install a package package: name: \"zlib\" state: present ``` </summary><br><b> ``` - name: Install a package package: name: \"{{ package_name|default('zlib') }}\" state: present ``` </b></details> <details> <summary>How to make the variable \"use_var\" optional? ``` - name: Install a package package: name: \"zlib\" state: present use: \"{{ use_var }}\" ``` </summary><br><b> With \"default(omit)\" ``` - name: Install a package package: name: \"zlib\" state: present use: \"{{ use_var|default(omit) }}\" ``` </b></details> <details> <summary>What would be the result of the following play?</summary><br><b> ``` --- - name: Print information about my host hosts: localhost gather_facts: 'no' tasks: - name: Print hostname debug: msg: \"It's me, {{ ansible_hostname }}\" ``` When given a written code, always inspect it thoroughly. If your answer", "metadata": {"source_file": "learning-materials/topics/ansible/README.md", "section": "Ansible - Variables", "language": "en", "created_at": "2025-07-19T19:22:02.119693"}}
{"text": "is “this will fail” then you are right. We are using a fact (ansible_hostname), which is a gathered piece of information from the host we are running on. But in this case, we disabled facts gathering (gather_facts: no) so the variable would be undefined which will result in failure. </b></details> <details> <summary>When the value '2017'' will be used in this case: `{{ lookup('env', 'BEST_YEAR') | default('2017', true) }}`?</summary><br><b> when the environment variable 'BEST_YEAR' is empty or false. </b></details> <details> <summary>If the value of certain variable is 1, you would like to use the value \"one\", otherwise, use \"two\". How would you do it?</summary><br><b> `{{ (certain_variable == 1) | ternary(\"one\", \"two\") }}` </b></details> <details> <summary>The value of a certain variable you use is the string \"True\". You would like the value to be a boolean. How would you cast it?</summary><br><b> `{{ some_string_var | bool }}` </b></details> <details> <summary>You want to run Ansible", "metadata": {"source_file": "learning-materials/topics/ansible/README.md", "section": "Ansible - Variables", "language": "en", "created_at": "2025-07-19T19:22:02.119753"}}
{"text": "playbook only on specific minor version of your OS, how would you achieve that?</summary><br><b> </b></details> <details> <summary>What the \"become\" directive used for in Ansible?</summary><br><b> </b></details> <details> <summary>What are facts? How to see all the facts of a certain host?</summary><br><b> </b></details> <details> <summary>What would be the result of running the following task? How to fix it? ``` - hosts: localhost tasks: - name: Install zlib package: name: zlib state: present ``` </summary><br><b> </b></details> <details> <summary>Which Ansible best practices are you familiar with?. Name at least three</summary><br><b> </b></details> <details> <summary>Explain the directory layout of an Ansible role</summary><br><b> </b></details> <details> <summary>What 'blocks' are used for in Ansible?</summary><br><b> </b></details> <details> <summary>How do you handle errors in Ansible?</summary><br><b> </b></details> <details> <summary>You would like to run a certain command if a", "metadata": {"source_file": "learning-materials/topics/ansible/README.md", "section": "Ansible - Variables", "language": "en", "created_at": "2025-07-19T19:22:02.119789"}}
{"text": "task fails. How would you achieve that?</summary><br><b> </b></details> <details> <summary>Write a playbook to install ‘zlib’ and ‘vim’ on all hosts if the file ‘/tmp/mario’ exists on the system.</summary><br><b> ``` --- - hosts: all vars: mario_file: /tmp/mario package_list: - 'zlib' - 'vim' tasks: - name: Check for mario file stat: path: \"{{ mario_file }}\" register: mario_f - name: Install zlib and vim if mario file exists become: \"yes\" package: name: \"{{ item }}\" state: present with_items: \"{{ package_list }}\" when: mario_f.stat.exists ``` </b></details> <details> <summary>Write a single task that verifies all the files in files_list variable exist on the host</summary><br><b> ``` - name: Ensure all files exist assert: that: - item.stat.exists loop: \"{{ files_list }}\" ``` </b></details> <details> <summary>Write a playbook to deploy the file ‘/tmp/system_info’ on all hosts except for controllers group, with the following content</summary><br><b> ``` I'm <HOSTNAME> and my operating", "metadata": {"source_file": "learning-materials/topics/ansible/README.md", "section": "Ansible - Variables", "language": "en", "created_at": "2025-07-19T19:22:02.119809"}}
{"text": "system is <OS> ``` Replace <HOSTNAME> and <OS> with the actual data for the specific host you are running on The playbook to deploy the system_info file ``` --- - name: Deploy /tmp/system_info file hosts: all:!controllers tasks: - name: Deploy /tmp/system_info template: src: system_info.j2 dest: /tmp/system_info ``` The content of the system_info.j2 template ```", "metadata": {"source_file": "learning-materials/topics/ansible/README.md", "section": "Ansible - Variables", "language": "en", "created_at": "2025-07-19T19:22:02.119830"}}
{"text": "I'm {{ ansible_hostname }} and my operating system is {{ ansible_distribution } ``` </b></details> <details> <summary>The variable 'whoami' defined in the following places: * role defaults -> whoami: mario * extra vars (variables you pass to Ansible CLI with -e) -> whoami: toad * host facts -> whoami: luigi * inventory variables (doesn’t matter which type) -> whoami: browser According to variable precedence, which one will be used?</summary><br><b> The right answer is ‘toad’. Variable precedence is about how variables override each other when they set in different locations. If you didn’t experience it so far I’m sure at some point you will, which makes it a useful topic to be aware of. In the context of our question, the order will be extra vars (always override any other variable) -> host facts -> inventory variables -> role defaults (the weakest). Here is the order of precedence from least to greatest (the last listed variables winning prioritization): 1. command line values (eg “-u", "metadata": {"source_file": "learning-materials/topics/ansible/README.md", "section": "{{ ansible_managed }}", "language": "en", "created_at": "2025-07-19T19:22:02.121189"}}
{"text": "user”) 2. role defaults [[1\\]](https://docs.ansible.com/ansible/latest/user_guide/playbooks_variables.html#id15) 3. inventory file or script group vars [[2\\]](https://docs.ansible.com/ansible/latest/user_guide/playbooks_variables.html#id16) 4. inventory group_vars/all [[3\\]](https://docs.ansible.com/ansible/latest/user_guide/playbooks_variables.html#id17) 5. playbook group_vars/all [[3\\]](https://docs.ansible.com/ansible/latest/user_guide/playbooks_variables.html#id17) 6. inventory group_vars/* [[3\\]](https://docs.ansible.com/ansible/latest/user_guide/playbooks_variables.html#id17) 7. playbook group_vars/* [[3\\]](https://docs.ansible.com/ansible/latest/user_guide/playbooks_variables.html#id17) 8. inventory file or script host vars [[2\\]](https://docs.ansible.com/ansible/latest/user_guide/playbooks_variables.html#id16) 9. inventory host_vars/* [[3\\]](https://docs.ansible.com/ansible/latest/user_guide/playbooks_variables.html#id17) 10. playbook host_vars/*", "metadata": {"source_file": "learning-materials/topics/ansible/README.md", "section": "{{ ansible_managed }}", "language": "en", "created_at": "2025-07-19T19:22:02.121223"}}
{"text": "[[3\\]](https://docs.ansible.com/ansible/latest/user_guide/playbooks_variables.html#id17) 11. host facts / cached set_facts [[4\\]](https://docs.ansible.com/ansible/latest/user_guide/playbooks_variables.html#id18) 12. play vars 13. play vars_prompt 14. play vars_files 15. role vars (defined in role/vars/main.yml) 16. block vars (only for tasks in block) 17. task vars (only for the task) 18. include_vars 19. set_facts / registered vars 20. role (and include_role) params 21. include params 22. extra vars (always win precedence) A full list can be found at [PlayBook Variables](https://docs.ansible.com/ansible/latest/user_guide/playbooks_variables.html#ansible-variable-precedence) . Also, note there is a significant difference between Ansible 1.x and 2.x. </b></details> <details> <summary>For each of the following statements determine if it's true or false: * A module is a collection of tasks * It’s better to use shell or command instead of a specific module * Host facts override play", "metadata": {"source_file": "learning-materials/topics/ansible/README.md", "section": "{{ ansible_managed }}", "language": "en", "created_at": "2025-07-19T19:22:02.121377"}}
{"text": "variables * A role might include the following: vars, meta, and handlers * Dynamic inventory is generated by extracting information from external sources * It’s a best practice to use indentation of 2 spaces instead of 4 * ‘notify’ used to trigger handlers * This “hosts: all:!controllers” means ‘run only on controllers group hosts</summary><br><b> </b></details> <details> <summary>Explain the Difference between Forks and Serial & Throttle.</summary><br><b> `Serial` is like running the playbook for each host in turn, waiting for completion of the complete playbook before moving on to the next host. `forks`=1 means run the first task in a play on one host before running the same task on the next host, so the first task will be run for each host before the next task is touched. Default fork is 5 in ansible. ``` [defaults] forks = 30 ``` ``` - hosts: webservers serial: 1 tasks: - name: ... ``` Ansible also supports `throttle` This keyword limits the number of workers up to the maximum set", "metadata": {"source_file": "learning-materials/topics/ansible/README.md", "section": "{{ ansible_managed }}", "language": "en", "created_at": "2025-07-19T19:22:02.121411"}}
{"text": "via the forks setting or serial. This can be useful in restricting tasks that may be CPU-intensive or interact with a rate-limiting API ``` tasks: - command: /path/to/cpu_intensive_command throttle: 1 ``` </b></details> <details> <summary>What is ansible-pull? How is it different from how ansible-playbook works?</summary><br><b> </b></details> <details> <summary>What is Ansible Vault?</summary><br><b> </b></details> <details> <summary>Demonstrate each of the following with Ansible: * Conditionals * Loops </summary><br><b> </b></details> <details> <summary>What are filters? Do you have experience with writing filters?</summary><br><b> </b></details> <details> <summary>Write a filter to capitalize a string</summary><br><b> ``` def cap(self, string): return string.capitalize() ``` </b></details> <details> <summary>You would like to run a task only if previous task changed anything. How would you achieve that?</summary><br><b> </b></details> <details> <summary>What are callback plugins?", "metadata": {"source_file": "learning-materials/topics/ansible/README.md", "section": "{{ ansible_managed }}", "language": "en", "created_at": "2025-07-19T19:22:02.121433"}}
{"text": "What can you achieve by using callback plugins?</summary><br><b> </b></details> <details> <summary>What is the difference between `include_task` and `import_task`?</summary><br><b> </b></details> <details> <summary>File '/tmp/exercise' includes the following content ``` Goku = 9001 Vegeta = 5200 Trunks = 6000 Gotenks = 32 ``` With one task, switch the content to: ``` Goku = 9001 Vegeta = 250 Trunks = 40 Gotenks = 32 ``` </summary><br><b> ``` - name: Change saiyans levels lineinfile: dest: /tmp/exercise regexp: \"{{ item.regexp }}\" line: \"{{ item.line }}\" with_items: - { regexp: '^Vegeta', line: 'Vegeta = 250' } - { regexp: '^Trunks', line: 'Trunks = 40' } ... ``` </b></details>", "metadata": {"source_file": "learning-materials/topics/ansible/README.md", "section": "{{ ansible_managed }}", "language": "en", "created_at": "2025-07-19T19:22:02.121453"}}
{"text": "<details> <summary>True or False? By default, Ansible will execute all the tasks in play on a single host before proceeding to the next host</summary><br><b> False. Ansible will execute a single task on all hosts before moving to the next task in a play. As for today, it uses 5 forks by default.<br> This behavior is described as \"strategy\" in Ansible and it's configurable. </b></details> <details> <summary>What is a \"strategy\" in Ansible? What is the default strategy?</summary><br><b> A strategy in Ansible describes how Ansible will execute the different tasks on the hosts. By default Ansible is using the \"Linear strategy\" which defines that each task will run on all hosts before proceeding to the next task. </b></details> <details> <summary>What strategies are you familiar with in Ansible?</summary><br><b> - Linear: the default strategy in Ansible. Run each task on all hosts before proceeding. - Free: For each host, run all the tasks until the end of the play as soon as possible -", "metadata": {"source_file": "learning-materials/topics/ansible/README.md", "section": "Ansible - Execution and Strategy", "language": "en", "created_at": "2025-07-19T19:22:02.121772"}}
{"text": "Debug: Run tasks in an interactive way </b></details> <details> <summary>What the <code>serial</code> keyword is used for?</summary><br><b> It's used to specify the number (or percentage) of hosts to run the full play on, before moving to next number of hosts in the group. For example: ``` - name: Some play hosts: databases serial: 4 ``` If your group has 8 hosts. It will run the whole play on 4 hosts and then the same play on another 4 hosts. </b></details>", "metadata": {"source_file": "learning-materials/topics/ansible/README.md", "section": "Ansible - Execution and Strategy", "language": "en", "created_at": "2025-07-19T19:22:02.121803"}}
{"text": "<details> <summary>How do you test your Ansible based projects?</summary><br><b> </b></details> <details> <summary>What is Molecule? How does it works?</summary><br><b> It's used to rapidy develop and test Ansbile roles. Molecule can be used to test Ansible roles against a varaitey of Linux Distros at the same time. This testing ability helps instill confidence of the automation today and as time go on while a role is maintined. </b></details> <details> <summary>You run Ansible tests and you get \"idempotence test failed\". What does it mean? Why idempotence is important?</summary><br><b> </b></details>", "metadata": {"source_file": "learning-materials/topics/ansible/README.md", "section": "Ansible Testing", "language": "en", "created_at": "2025-07-19T19:22:02.121891"}}
{"text": "<details> <summary>How to find out the data type of a certain variable in one of the playbooks?</summary><br><b> \"{{ some_var | type_debug }}\" </b></details>", "metadata": {"source_file": "learning-materials/topics/ansible/README.md", "section": "Ansible - Debugging", "language": "en", "created_at": "2025-07-19T19:22:02.121920"}}
{"text": "<details> <summary>What are collections in Ansible?</summary><br><b> Ansible Collections are a way to package and distribute modules, roles, plugins, and documentation in a structured format. They help organize and distribute automation code efficiently, especially for complex environments. </b></details> <details> <summary>Why Use Ansible Collections?</summary><br><b> - Modular and reusable components - Simplifies management of custom and third-party modules - Provides a standardized way to distribute automation content - Helps in version control and dependency management </b></details> <!-- {% endraw %} -->", "metadata": {"source_file": "learning-materials/topics/ansible/README.md", "section": "Ansible - Collections", "language": "en", "created_at": "2025-07-19T19:22:02.121997"}}
{"text": "1. Write a playbook that will: a. Install the package zlib b. Create the file `/tmp/some_file` 2. Run the playbook on a remote host", "metadata": {"source_file": "learning-materials/topics/ansible/my_first_playbook.md", "section": "Ansible - My First Playbook", "language": "en", "created_at": "2025-07-19T19:22:02.122224"}}
{"text": "1. Write a task to update and upgrade apt packages", "metadata": {"source_file": "learning-materials/topics/ansible/update_upgrade_task.md", "section": "Ansible - Update and upgrade APT packages task", "language": "en", "created_at": "2025-07-19T19:22:02.122298"}}
{"text": "``` - name: Create a new directory file: path: \"/tmp/new_directory\" state: directory ```", "metadata": {"source_file": "learning-materials/topics/ansible/solutions/my_first_task.md", "section": "My First Task - Solution", "language": "en", "created_at": "2025-07-19T19:22:02.122500"}}
{"text": "1. `vi first_playbook.yml` ``` - name: Install zlib and create a file hosts: some_remote_host tasks: - name: Install zlib package: name: zlib state: present become: yes - name: Create the file /tmp/some_file file: path: '/tmp/some_file' state: touch ``` 2. First, edit the inventory file: `vi /etc/ansible/hosts` ``` [some_remote_host] some.remoted.host.com ``` Run the playbook `ansible-playbook first_playbook.yml`", "metadata": {"source_file": "learning-materials/topics/ansible/solutions/my_first_playbook.md", "section": "My first playbook - Solution", "language": "en", "created_at": "2025-07-19T19:22:02.122611"}}
{"text": "``` - name: \"update and upgrade apt packages.\" become: yes apt: upgrade: yes update_cache: yes ```", "metadata": {"source_file": "learning-materials/topics/ansible/solutions/update_upgrade_task.md", "section": "Update and Upgrade apt packages task - Solution", "language": "en", "created_at": "2025-07-19T19:22:02.122680"}}
{"text": "- [Observability](#observability) - [Monitoring](#monitoring) - [Data](#data) - [Application Performance Management](#application-performance-management) <details> <summary>What's Observability?</summary><br><b> In distributed systems, observability is the ability to collect data about programs' execution, modules' internal states, and the communication among components.<br> To improve observability, software engineers use a wide range of logging and tracing techniques to gather telemetry information, and tools to analyze and use it.<br> Observability is foundational to site reliability engineering, as it is the first step in triaging a service outage.<sup title=\"wikipedia\"><a href=\"https://en.wikipedia.org/wiki/Observability_(software)\">[1]</a></sup> </b></details>", "metadata": {"source_file": "learning-materials/topics/observability/README.md", "section": "Observability", "language": "en", "created_at": "2025-07-19T19:22:02.122919"}}
{"text": "<details> <summary>What's monitoring? How is it related to Observability?</summary><br><b> Google: \"Monitoring is one of the primary means by which service owners keep track of a system’s health and availability\". </b></details> <details> <summary>What types of monitoring outputs are you familiar with and/or used in the past?</summary><br><b> Alerts<br> Tickets<br> Logging<br> </b></details>", "metadata": {"source_file": "learning-materials/topics/observability/README.md", "section": "Monitoring", "language": "en", "created_at": "2025-07-19T19:22:02.122995"}}
{"text": "<details> <summary>Can you mention what type of things are often montiored in the IT industry?</summary><br><b> - Hardware (CPU, RAM, ...) - Infrastructure (Disk capacity, Network latency, ...) - App (Status code, Errors in logs, ...) </b></details> <details> <summary>Explain \"Time Series\" data</summary><br><b> Time series data is sequenced data, measuring certain parameter in ordered (by time) way. An example would be CPU utilization every hour: ``` 08:00 17 09:00 22 10:00 91 ``` </b></details> <details> <summary>Explain data aggregation</summary><br><b> In monitoring, aggregating data is basically combining collection of values. It can be done in different ways like taking the average of multiple values, the sum of them, the count of many times they appear in the collection and other ways that mainly depend on the type of the collection (e.g. time-series would be one type). </b></details>", "metadata": {"source_file": "learning-materials/topics/observability/README.md", "section": "Data", "language": "en", "created_at": "2025-07-19T19:22:02.123162"}}
{"text": "<details> <summary>What is Application Performance Management?</summary><br><b> - IT metrics translated into business insights - Practices for monitoring applications insights so we can improve performances, reduce issues and improve overall user experience </b></details> <details> <summary>Name three aspects of a project you can monitor with APM (e.g. backend)</summary><br><b> - Frontend - Backend - Infra - ... </b></details> <details> <summary>What can be collected/monitored to perform APM monitoring?</summary><br><b> - Metrics - Logs - Events - Traces </b></details>", "metadata": {"source_file": "learning-materials/topics/observability/README.md", "section": "Application Performance Management", "language": "en", "created_at": "2025-07-19T19:22:02.123238"}}
{"text": "<details> <summary>What is DNS? What is it used for?</summary><br><b> DNS (Domain Name Systems) is a protocol used for converting domain names into IP addresses.<br> computer networking (at layer 3 of the OSP model) is done with IP addresses but for as humans it's hard to remember IP addresses, it's much easier to remember names. This why we need something such as DNS to convert any domain name we type into an IP address. You can think on DNS as a huge phonebook or database where each corresponding name has an IP. </b></details> <details> <summary>What is DNS resolution?</summary><br><b> The process of translating IP addresses to domain names. </b></details> <details> <summary>What is a name server?</summary><br><b> A server which is responsible for resolving DNS queries. </b></details> <details> <summary>What is the resolution sequence of: www.site.com</summary><br><b> It's resolved in this order: 1) . 2) .com 3) site.com 4) www.site.com </b></details> <details> <summary>What is a", "metadata": {"source_file": "learning-materials/topics/dns/README.md", "section": "DNS", "language": "en", "created_at": "2025-07-19T19:22:02.124096"}}
{"text": "domain name registrar?</summary><br><b> [Cloudflare](https://www.cloudflare.com/en-gb/learning/dns/glossary/what-is-a-domain-name-registrar): \"A domain name registrar provides domain name registrations to the general public. A common misconception is that registrars sell domain names; these domain names are actually owned by registries and can only be leased by users.\" </b></details> <details> <summary>Given the following fqdn, <code>www.blipblop.com</code>, what is the root?</summary><br><b> `.` is the root </b></details> <details> <summary>Given the following fqdn, <code>www.blipblop.com</code>, what is the top level domain?</summary><br><b> `.com.` is the top level domain </b></details> <details> <summary>Given the following fqdn, <code>www.blipblop.com</code>, what is the second level domain?</summary><br><b> `blipblop.com.` is the second level domain </b></details> <details> <summary>Given the following fqdn, <code>www.blipblop.com</code>, what is the domain?</summary><br><b>", "metadata": {"source_file": "learning-materials/topics/dns/README.md", "section": "DNS", "language": "en", "created_at": "2025-07-19T19:22:02.124149"}}
{"text": "`www.blipblop.com.` is the domain </b></details> <details> <summary>Describe DNS resolution workflow in high-level</summary><br><b> In general the process is as follows: * The user types an address in the web browser (some_site.com) * The operating system gets a request from the browser to translate the address the user entered * A query created to check if a local entry of the address exists in the system. In case it doesn't, the request is forwarded to the DNS resolver * The Resolver is a server, usually configured by your ISP when you connect to the internet, that responsible for resolving your query by contacting other DNS servers * The Resolver contacts the root nameserver (aka as .) * The root nameserver either responds with the address you are looking for or it responds with the address of the relevant Top Level Domain DNS server (if your address ends with org then the org TLD) * The Resolver then contacts the TLD DNS. TLD DNS might respond with the address you are looking for.", "metadata": {"source_file": "learning-materials/topics/dns/README.md", "section": "DNS", "language": "en", "created_at": "2025-07-19T19:22:02.124188"}}
{"text": "If it doesn't has the information, it will provide the address of SLD DNS server * SLD DNS server will reply with the address to the resolver * The Resolver passes this information to the browser while your OS also stores this information in the cache * The user cab browse the website with happiness and joy :D </b></details>", "metadata": {"source_file": "learning-materials/topics/dns/README.md", "section": "DNS", "language": "en", "created_at": "2025-07-19T19:22:02.124356"}}
{"text": "<details> <summary>What is a DNS record?</summary><br><b> A mapping between domain name and an IP address. </b></details> <details> <summary>What types of DNS records are there?</summary><br><b> * A * CNAME * PTR * MX * AAAA ... A more detailed list, can be found [here](https://www.nslookup.io/learning/dns-record-types) </b></details> <details> <summary>What is a A record?</summary><br><b> A (Address): Maps a host name to an IPv4 address. When a computer has multiple adapter cards and IP addresses, it should have multiple address records. </b></details> <details> <summary>What is a AAAA record?</summary><br><b> An AAAA Record performs the same function as an A Record, but for an IPv6 Address. </b></details> <details> <summary>What is a CNAME record?</summary><br><b> CNAME: maps a hostname to another hostname. The target should be a domain name which must have an A or AAAA record. Think of it as an alias record. </b></details> <details> <summary>What is a PTR record?</summary><br><b>", "metadata": {"source_file": "learning-materials/topics/dns/README.md", "section": "DNS - Records", "language": "en", "created_at": "2025-07-19T19:22:02.124614"}}
{"text": "While an A record points a domain name to an IP address, a PTR record does the opposite and resolves the IP address to a domain name. </b></details> <details> <summary>What is a MX record?</summary><br><b> MX (Mail Exchange) Specifies a mail exchange server for the domain, which allows mail to be delivered to the correct mail servers in the domain. </b></details> <details> <summary>What is a NS record?</summary><br><b> NS: name servers that can respond to DNS queries </b></details>", "metadata": {"source_file": "learning-materials/topics/dns/README.md", "section": "DNS - Records", "language": "en", "created_at": "2025-07-19T19:22:02.124637"}}
{"text": "<details> <summary>Explain DNS Records TTL</summary><br><b> [varonis.com](https://www.varonis.com/blog/dns-ttl): \"DNS TTL (time to live) is a setting that tells the DNS resolver how long to cache a query before requesting a new one. The information gathered is then stored in the cache of the recursive or local resolver for the TTL before it reaches back out to collect new, updated details.\" </b></details>", "metadata": {"source_file": "learning-materials/topics/dns/README.md", "section": "DNS - TTL", "language": "en", "created_at": "2025-07-19T19:22:02.124692"}}
{"text": "<details> <summary>Is DNS using TCP or UDP?</summary><br><b> DNS uses UDP port 53 for resolving queries either regular or reverse. DNS uses TCP for zone transfer. </b></details> <details> <summary>True or False? DNS can be used for load balancing</summary><br><b> True. </b></details> <details> <summary>Which techniques a DNS can use for load balancing?</summary><br><b> There are several techniques that a DNS can use for load balancing, including: * Round-robin DNS * Weighted round-robin DNS * Least connections * GeoDNS </b></details> <details> <summary>What is a DNS zone?</summary><br><b> A DNS zone is a logical container that holds all the DNS resource records for a specific domain name. </b></details> <details> <summary>What types of zones are there?</summary><br><b> There are several types, including: * Primary zone: A primary zone is a read/write zone that is stored in a master DNS server. * Secondary zone: A secondary zone is a read-only copy of a primary zone that is stored in a", "metadata": {"source_file": "learning-materials/topics/dns/README.md", "section": "DNS - Misc", "language": "en", "created_at": "2025-07-19T19:22:02.124942"}}
{"text": "slave DNS server. * Stub zone: A stub zone is a type of zone that contains only the essential information about a domain name. It is used to reduce the amount of DNS traffic and improve the efficiency of the DNS resolution process. </b></details>", "metadata": {"source_file": "learning-materials/topics/dns/README.md", "section": "DNS - Misc", "language": "en", "created_at": "2025-07-19T19:22:02.124970"}}
{"text": "<!-- {% raw %} --> - [Argo](#argo) - [ArgoCD Exercises](#argocd-exercises) - [ArgoCD 101](#argocd-101) - [ArgoCD Secrets](#argocd-secrets) - [ArgoCD Helm](#argocd-helm) - [Argo Rollouts](#argo-rollouts) - [ArgoCD Questions](#argocd-questions) - [ArgoCD 101](#argocd-101-1) - [Practical ArgoCD 101](#practical-argocd-101) - [CLI](#cli) - [ArgoCD Configuration](#argocd-configuration) - [Advanced ArgoCD](#advanced-argocd) - [ArgoCD Application Health](#argocd-application-health) - [ArgoCD Syncs](#argocd-syncs) - [ArgoCD and Helm](#argocd-and-helm) - [Argo Rollouts Questions](#argo-rollouts-questions) - [Argo Rollouts 101](#argo-rollouts-101) - [Argo Advanced Rollouts](#argo-advanced-rollouts) - [Argo Rollouts Commands](#argo-rollouts-commands)", "metadata": {"source_file": "learning-materials/topics/argo/README.md", "section": "Argo", "language": "en", "created_at": "2025-07-19T19:22:02.125386"}}
{"text": "|Name|Topic|Objective & Instructions|Solution|Comments| |--------|--------|------|----|----| | Creating an App | App | [Exercise](exercises/app_creation/exercise.md) | [Solution](exercises/app_creation/solution.md) | Syncing App - Git | Sync | [Exercise](exercises/sync_app_git/exercise.md) | [Solution](exercises/sync_app_git/solution.md) | Syncing App - Cluster | Sync | [Exercise](exercises/sync_app_cluster/exercise.md) | [Solution](exercises/sync_app_cluster/solution.md)", "metadata": {"source_file": "learning-materials/topics/argo/README.md", "section": "ArgoCD 101", "language": "en", "created_at": "2025-07-19T19:22:02.125434"}}
{"text": "|Name|Topic|Objective & Instructions|Solution|Comments| |--------|--------|------|----|----| | Secrets 101 | Secrets | [Exercise](exercises/secrets_101/exercise.md) | [Solution](exercises/secrets_101/solution.md)", "metadata": {"source_file": "learning-materials/topics/argo/README.md", "section": "ArgoCD Secrets", "language": "en", "created_at": "2025-07-19T19:22:02.125456"}}
{"text": "|Name|Topic|Objective & Instructions|Solution|Comments| |--------|--------|------|----|----| | Helm ArgoCD App | Secrets | [Exercise](exercises/argocd_helm_app/exercise.md) | [Solution](exercises/argocd_helm_app/solution.md)", "metadata": {"source_file": "learning-materials/topics/argo/README.md", "section": "ArgoCD Helm", "language": "en", "created_at": "2025-07-19T19:22:02.125474"}}
{"text": "|Name|Topic|Objective & Instructions|Solution|Comments| |--------|--------|------|----|----| | Blue/Green Rollout | Rollouts | [Exercise](exercises/blue_green_rollout/exercise.md) | [Solution](exercises/blue_green_rollout/solution.md) | Canary Rollout | Rollouts | [Exercise](exercises/canary_rollout/exercise.md) | [Solution](exercises/canary_rollout/solution.md)", "metadata": {"source_file": "learning-materials/topics/argo/README.md", "section": "Argo Rollouts", "language": "en", "created_at": "2025-07-19T19:22:02.125496"}}
{"text": "<details> <summary>What is Argo CD?</summary><br><b> [ArgoCD](https://argo-cd.readthedocs.io/en/stable): \"Argo CD is a declarative, GitOps continuous delivery tool for Kubernetes.\" As to why Argo CD, they provide the following explanation: \"Application definitions, configurations, and environments should be declarative and version controlled. Application deployment and lifecycle management should be automated, auditable, and easy to understand.\" </b></details> <details> <summary>There been a lot of CI/CD systems before ArgoCD (Jenkins, Teamcity, CircleCI, etc.) What added value ArgoCD brought?</summary><br><b> Simply said, ArgoCD is CD, not CI. We still need CI systems. Secondly, ArgoCD is running on Kubernetes, it's part of its ecosystem, as opposed to some other CI/CD systems. Finally, ArgoCD was built specifically for Kubernetes, not other platforms and systems. Easier to explain the need for ArgoCD by direct comparison to another system that can do CD. Let's use Jenkins for this.", "metadata": {"source_file": "learning-materials/topics/argo/README.md", "section": "ArgoCD 101", "language": "en", "created_at": "2025-07-19T19:22:02.126823"}}
{"text": "With Jenkins, you need make sure to install k8s related tools and set access for commands like kubectl. With ArgoCD you simply need to install it in your namespace but no need to install additional tools as it's part of k8s. With Jenkins, managing access is usually done per pipeline and even if set globally in Jenkins, you still need to configure each pipeline to use that access configuration. With ArgoCD access management to k8s and other resources is given as it runs already on the cluster, in one or multiple namespaces. With Jenkins, tracking the status of what got deployed to k8s can be done only as an extra step, by running the pipeline. This is because Jenkins isn't part of the k8s cluster. With ArgoCD you get much better tracking and visibility of what gets deployed as it runs in the same cluster and the same namespace. With ArgoCD it's really easy to roll back to a previous version because all the changes done, are done to git which is a versioned source control. So it's enough", "metadata": {"source_file": "learning-materials/topics/argo/README.md", "section": "ArgoCD 101", "language": "en", "created_at": "2025-07-19T19:22:02.126988"}}
{"text": "to get to a previous commit for ArgoCD to detect a change and sync to the cluster. Worth to mention, this point specifically is true for Jenkins as well :) </b></details> <details> <summary>Describe an example of workflow where ArgoCD is used</summary><br><b> 1. A developer submitted change to an application repository 2. Jenkins pipeline is triggered to run CI on the change 3. If the Jenkins Pipeline completed successfully, build an image out of the new code 4. Push to image to a registry 5. Update K8S manifest file(s) in a separate app config repository 6. ArgoCD tracks changes in the app config repository. Since there was a change in the repository, it will apply the changes from the repo </b></details> <details> <summary>True or False? ArgoCD supports Kubernetes YAML files but not other manifests formats like Helm Charts and Kustomize</summary><br><b> False. It supports Kubernetes YAML files as well as Helm Charts and Kustomize. </b></details> <details> <summary>What \"GitOps", "metadata": {"source_file": "learning-materials/topics/argo/README.md", "section": "ArgoCD 101", "language": "en", "created_at": "2025-07-19T19:22:02.127022"}}
{"text": "Repository\" means in regards to ArgoCD?</summary><br><b> It's the repository that holds app configuration, the one updated most of the time by CI/CD processes or DevOps, SRE engineers. In regards to ArgoCD it's the repository ArgoCD tracks for changes and apply them when they are detected. </b></details> <details> <summary>What are the advantages in using GitOps approach/repository?</summary><br><b> * Your whole configuration is one place, defined as code so it's completely transparent, adjustable for changes and easily reproducible * Everyone go through the same interface hence you have more people experiencing and testing the code, even if not intentionally * Engineers can use it for testing, development, ... there is no more running manual commands and hoping to reach the same status as in the cluster/cloud. * Single source of truth: you know that your GitOps is the repo from which changes can be done to the cluster. So even if someone tries to manually override it, it won't work.", "metadata": {"source_file": "learning-materials/topics/argo/README.md", "section": "ArgoCD 101", "language": "en", "created_at": "2025-07-19T19:22:02.127042"}}
{"text": "</b></details> <details> <summary>Sorina, one of the engineers in your team, made manual changes to the cluster that override some of the configuration in a repo traced by ArgoCD. What will happen?</summary><br><b> Once Sorina made the modifications, ArgoCD will detect the state diverged and will sync the changes from the GitOps repository, overwriting the manual changes done by Sorina. </b></details> <details> <summary>Nate, one of the engineers in your organization, asked whether it's possible if ArgoCD didn't sync for changes done manually to the cluster. What would be your answer?</summary><br><b> The answer is yes, it's possible. You can configure ArgoCD to sync to desired state when changes done manually and instead do something like sending alerts. </b></details> <details> <summary>How cluster disaster recovery becomes easier with ArgoCD?</summary><br><b> Imagine you have a cluster in the cloud, in one of the regions. Something happens to that cluster and it's either crashes or", "metadata": {"source_file": "learning-materials/topics/argo/README.md", "section": "ArgoCD 101", "language": "en", "created_at": "2025-07-19T19:22:02.127061"}}
{"text": "simply no longer operational. If you have all your cluster configuration in a GitOps repository, ArgoCD can be pointed to that repository while be configured to use a new cluster you've set up and apply that configuration so your cluster is again up and running with the same status as o </b></details> <details> <summary>Ella, an engineer in your team, claims ArgoCD benefit is that it's an extension Kubernetes, it's part of the cluster. Sarah, also an engineer in your team, claims it's not a real benefit as Jenkins can be also deployed in the cluster hence being part of it. What's your take?</summary><br><b> Ella is right, ArgoCD is an extension of the cluster, that is very different from simply being deployed in the cluster as other CI/CD systems like Jenkins. ArgoCD uses existing k8s resources like K8s controllers (for monitoring and state differences) and etcd for storing data. </b></details> <details> <summary>How the main resource in ArgoCD called?</summary><br><b> \"Application\"", "metadata": {"source_file": "learning-materials/topics/argo/README.md", "section": "ArgoCD 101", "language": "en", "created_at": "2025-07-19T19:22:02.127080"}}
{"text": "</b></details> <details> <summary>Explain what is an \"Application\" in regards to ArgoCD</summary><br><b> It's a custom resource definitions which responsible for the deployment and synchronization of application resources to a Kubernetes cluster. </b></details> <details> <summary>How ArgoCD makes access management in the cluster easier?</summary><br><b> Instead of creating Kubernetes resources, you can use Git to manage who is allowed to push code, to review it, merge it, etc - either human users or 3rd party systems and services. There is no need to use ClusterRole or User resources in Kubernetes hence the management of access is much more simplified. </b></details>", "metadata": {"source_file": "learning-materials/topics/argo/README.md", "section": "ArgoCD 101", "language": "en", "created_at": "2025-07-19T19:22:02.127099"}}
{"text": "<details> <summary>Describe the purpose of the following section in a an Application YAML file ```YAML source: repoURL: https://github.com/bregman-arie/devops-exercises targetRevision: HEAD path: main ``` </summary><br><b> This section of an Application in ArgoCD, defines which Git repository should be synced </b></details> <details> <summary>Describe the purpose of the following section in a an Application YAML file ```YAML destination: server: http://some.kubernetes.cluster.svc namespace: devopsExercises ``` </summary><br><b> This section defines with which Kubernetes cluster the app in the tracked Git repository should be synced with. </b></details> <details> <summary>What CRD would you use if you have multiple applications and you would like to group them together logically?</code></summary><br><b> AddProject </b></details> <details> <summary>True or False? ArgoCD sync period is 3 hours</summary><br><b> False. ArgoCD sync period is 3 minutes as of today (and not hours).", "metadata": {"source_file": "learning-materials/topics/argo/README.md", "section": "Practical ArgoCD 101", "language": "en", "created_at": "2025-07-19T19:22:02.127430"}}
{"text": "</b></details> <details> <summary>Describe shortly what ArgoCD does every sync period</summary><br><b> 1. Gathers list of all the apps to sync (those that are marked with \"auto-sync\") 2. Gets Git state for each repository 3. Performs comparison between the repository Git state and the Kubernetes cluster state 1. If states are different, the application marked as \"out-of-sync\" and further action might be taken (based on the configuration) 2. If states are equal, the application marked as \"synced\" </b></details> <details> <summary>You deployed a new application in a namespace called \"yay\" but when running <code>kubectl get ns yay</code> you see there is no such namespace. What happened?</summary><br><b> Deploying applications in non-existing namespaces doesn't create the namespace. For that you have to explicitly mark \"Auto-create namespace\". To fix it, you can simply run `kubectl create namespace NAMESPACE_NAME` but it's better of course to have it stored in Git rather than running", "metadata": {"source_file": "learning-materials/topics/argo/README.md", "section": "Practical ArgoCD 101", "language": "en", "created_at": "2025-07-19T19:22:02.127457"}}
{"text": "kubectl commands. </b></details>", "metadata": {"source_file": "learning-materials/topics/argo/README.md", "section": "Practical ArgoCD 101", "language": "en", "created_at": "2025-07-19T19:22:02.127645"}}
{"text": "<details> <summary>Create a new application with the following properties: * app name: some-app * repo: https://fake.repo.address * app path: ./app_path * namespace: default * cluster: my.kubernetes.cluster </summary><br><b> ``` argocd app create some-app \\ --project \\ --repo https://fake.repo.address \\ --path ./app_path \\ --dest-namespace default \\ --dest-server my.kubernetes.cluster ``` </b></details> <details> <summary>List all argocd apps</summary><br><b> `argocd app list` </b></details> <details> <summary>Print detailed information on the app called \"some-app\"</summary><br><b> `argocd app get some-app` </b></details> <details> <summary>How to add an additional (external) cluster for ArgoCD to manage?</summary><br><b> `argocd cluster add CLUSTER_ADDRESS/NAME` </b></details> <details> <summary>How to list all the clusters ArgoCD manage?</summary><br><b> `argocd cluster list` </b></details>", "metadata": {"source_file": "learning-materials/topics/argo/README.md", "section": "CLI", "language": "en", "created_at": "2025-07-19T19:22:02.127794"}}
{"text": "<details> <summary>Is it possible to change default sync period of ArgoCD?</summary><br><b> Yes, it is possible by adding the following to the argocd-cm (ConfigMap): ``` data: timeout.reconciliation: 300s ``` The value can be any number of seconds you would like to set. </b></details> <details> <summary>What will be the result of setting <code>timeout.reconciliation: 0s</code>?</summary><br><b> sync functionality will be disabled. </b></details>", "metadata": {"source_file": "learning-materials/topics/argo/README.md", "section": "ArgoCD Configuration", "language": "en", "created_at": "2025-07-19T19:22:02.127860"}}
{"text": "<details> <summary>What is the \"App of Apps Patterns\"?</summary><br><b> A solution from Argo community in regards to managing multiple similar applications. Basically a pattern where you have root application that consists of other child applications. So instead of creating multiple separate applications, you have the root application pointing to a repository with additional applications. </b></details> <details> <summary>Can you provide some use cases for using \"App of Apps Patterns\"?</summary><br><b> * Cluster Preparation: You would like to deploy multiple applications at once to bootstrap a Kubernetes cluster * Multiple environments: If deploying many versions of the same application, but with minor changes. For example, several test deployments to test different features * Multiple clusters: when the same application needs to be deployed across multiple K8s clusters connected to ArgoCD </b></details> <details> <summary>True or False? If you have multiple Kubernetes clusters you", "metadata": {"source_file": "learning-materials/topics/argo/README.md", "section": "Advanced ArgoCD", "language": "en", "created_at": "2025-07-19T19:22:02.128241"}}
{"text": "want to manage sync applications to with ArgoCD then, you must have ArgoCD installed on each one of them</summary><br><b> False, it can be deployed on one of them. ArgoCD is able to manage external clusters on which it doesn't run. </b></details> <details> <summary>You've three clusters - dev, staging and prod. Whenever you update the application GitOps repo, all three clusters are being updated. What's the problem with that and how to deal with it?</summary><br><b> You don't usually want to go and update all of your clusters at once, especially when some for testing and development purposes and some for actual production usage. There are multiple ways to deal with it: 1. Branch driven: Have branches for your GitOps repo where you push first to development, do some testing, merge it then to staging and if everything works fine in staging, you merge it to production. 2. Use overlays and Kustomize to control the context of where your changes synced based on the CI process/pipeline used.", "metadata": {"source_file": "learning-materials/topics/argo/README.md", "section": "Advanced ArgoCD", "language": "en", "created_at": "2025-07-19T19:22:02.128261"}}
{"text": "<details> <summary>What are some possible health statuses for an ArgoCD application?</summary><br><b> * Healthy * Missing: resource doesn't exist in the cluster * Suspended: resource is paused * Progressing: resources isn't healthy but will become healthy or has the chance to become healthy * Degraded: resource isn't healthy * Unknown: it's not known what's the app health </b></details> <details> <summary>True or False? A Deployment considered to be healthy if the Pods are running</summary><br><b> Not exactly. A Deployment (as well as StatefulSet, ReplicaSet and DaemonSet) considered healthy if the desired state equals to actual/current state (this includes the number of replicas). </b></details> <details> <summary>True or False? An ingress is considered healthy if status.loadBalancer.ingress list includes at least one value</summary><br><b> True. </b></details> <details> <summary>What can you tell about the health of custom Kubernetes resources?</summary><br><b> The health of custom", "metadata": {"source_file": "learning-materials/topics/argo/README.md", "section": "ArgoCD Application Health", "language": "en", "created_at": "2025-07-19T19:22:02.128450"}}
{"text": "Kubernetes resources is defined by writing Lua scripts. You find such list of scripts here: https://github.com/argoproj/argo-cd/tree/master/resource_customizations </b></details>", "metadata": {"source_file": "learning-materials/topics/argo/README.md", "section": "ArgoCD Application Health", "language": "en", "created_at": "2025-07-19T19:22:02.128471"}}
{"text": "<details> <summary>Explain manual syncs vs. automatic syncs</summary><br><b> Automatic syncs means that once ArgoCD detected a change or a new version of your app in Git, it will apply the changes so the current/actual state can be equal to desired state. With manual syncs, ArgoCD will identify there is a difference, but will do nothing to correct it. </b></details> <details> <summary>Explain auto-pruning</summary><br><b> If enabled, auto-pruning will remove resources when files or content is removed from a tracked Git repository. If disabled, ArgoCD will not remove anything, even when content or files are removed. </b></details> <details> <summary>Explain self-heal in regards to ArgoCD</summary><br><b> Self-heal is the process of correcting the cluster state based on the desired state, when someone makes manual changes to the cluster. </b></details>", "metadata": {"source_file": "learning-materials/topics/argo/README.md", "section": "ArgoCD Syncs", "language": "en", "created_at": "2025-07-19T19:22:02.128609"}}
{"text": "<details> <summary>What support is provided in ArgoCD for Helm?</summary><br><b> ArgoCD is able to track packaged Helm chart in a sense where it will monitor for new versions. </b></details> <details> <summary>True or False? When ArgoCD tracks Helm chart the chart is no longer an Helm application and it's a ArgoCD app</summary><br><b> True. Trying to execute commands like `helm ls` will fail because helm metadata doesn't exist anymore and the application is tracked as ArgoCD app. </b></details>", "metadata": {"source_file": "learning-materials/topics/argo/README.md", "section": "ArgoCD and Helm", "language": "en", "created_at": "2025-07-19T19:22:02.128683"}}
{"text": "<details> <summary>What is Argo Rollouts?</summary><br><b> A controller for Kubernetes to perform application deployments using different strategies like Blue/Green deployments, Canary deployments, etc. In addition, it supports A/B tests, automatic rollbacks and integrated metric analysis. </b></details> <details> <summary>What happens when you rollout a new version of your app with argo rollouts?</summary><br><b> - Argo Rollouts creates a new replicaset (that is the new app version) - Old version is still alive - ArgoCD marks the app as out-of-sync </b></details> <details> <summary>True or False? You need to install ArgoCD in order to use Argo Rollouts</summary><br><b> False. Quite common misconception today but both cab be used independency even though they work nicely together. </b></details>", "metadata": {"source_file": "learning-materials/topics/argo/README.md", "section": "Argo Rollouts 101", "language": "en", "created_at": "2025-07-19T19:22:02.128832"}}
{"text": "<details> <summary>Scott, an engineer in your team, executes manually some smoke tests and monitors rollouts every time a new version is deployed. This way, if there is an issue he detects, he performs a rollback. What better approach you might suggest him to take?</summary><br><b> Shift towards fully automated rollbacks. Argo Rollouts supports multiple metric providers (Datadog, NewRelic, etc.) so you can use data and metrics for automating the rollbacks based on different conditions </b></details> <details> <summary>Explain the concept of \"Analysis\" in regards to Argo Rollouts</summary><br><b> Analysis is a resource deployed along a Rollout resources and defines the conditions and metrics threshols for performing a rollback </b></details> <details> <summary>Explain the following configuration ```yaml apiVersion: argoproj.io/v1alpha1 kind: AnalysisTemplate metadata: name: success-rate spec: args: - name: service-name metrics: - name: success-rate interval: 4m count: 3", "metadata": {"source_file": "learning-materials/topics/argo/README.md", "section": "Argo Advanced Rollouts", "language": "en", "created_at": "2025-07-19T19:22:02.129158"}}
{"text": "successCondition: result[0] >= 0.90 provider: prometheus: address: http:/some-prometheus-instance:80 query: sum(response_status{app=\"{{args.service-name}}\",role=\"canary\",status=~\"2.*\"})/sum(response_status{app=\"{{args.service-name}}\",role=\"canary\"} ``` </summary><br><b> It's an Analysis resource that fetches response status from Prometheus (monitoring instance). If it's more than 0.90 the rollout will continue, if it's less than 0.90 a rollback will be performed meaning the canary deployment failed. </b></details>", "metadata": {"source_file": "learning-materials/topics/argo/README.md", "section": "Argo Advanced Rollouts", "language": "en", "created_at": "2025-07-19T19:22:02.129188"}}
{"text": "<details> <summary>How to list rollouts?</summary><br><b> `kubectl argo rollouts list rollouts` </b></details> <details> <summary>How to list the rollouts of a given application?</summary><br><b> `kubectl argo rollouts get rollout SOME-APP` </b></details> <details> <summary>How to check the status of a rollout?</summary><br><b> `kubectl argo rollouts status SOME-APP` </b></details> <details> <summary>How to rollout a new version (with new container tag)?</summary><br><b> `kubectl argo rollouts set image SOME-APP web-app=some/registry/and/image:v2.0` </b></details> <details> <summary>How to manually promote to new app version?</summary><br><b> `kubectl argo rollouts promote SOME-APP` </b></details> <details> <summary>How do you monitor a rollout?</summary><br><b> `kubectl argo rollouts get rollout SOME-APP --watch` </b></details> <!-- {% endraw %} -->", "metadata": {"source_file": "learning-materials/topics/argo/README.md", "section": "Argo Rollouts Commands", "language": "en", "created_at": "2025-07-19T19:22:02.129291"}}
{"text": "1. Make sure you have repository with some Kubernetes manifests 2. Make sure you have a Kubernetes cluster running with ArgoCD installed", "metadata": {"source_file": "learning-materials/topics/argo/exercises/sync_app_git/solution.md", "section": "Requirements", "language": "en", "created_at": "2025-07-19T19:22:02.129627"}}
{"text": "1. Create a new application using the UI or CLI 1. App Name: app-demo 2. Project: project-demo 3. Kubernetes namespace: default 2. Sync the application 3. Verify the application is running by executing `kubectl get deploy` in the `default` namespace 4. Now make a change in your repository to one of the Kubernetes manifests (e.g. update deployment tag) 5. Go back to ArgoCD and check the state of the app 6. Sync the state of the application", "metadata": {"source_file": "learning-materials/topics/argo/exercises/sync_app_git/solution.md", "section": "Objectives", "language": "en", "created_at": "2025-07-19T19:22:02.129743"}}
{"text": "1. Click on \"New App\" 1. Insert application name: `app-demo` 2. Insert project: `project-demo` 3. Under source put the repository URL to your GitHub repo with Kubernetes manifests 1. Set path of your application 4. Under destination put the address of your Kubernetes cluster and set namespace to `default` 5. Click on \"Create\" 2. Click on the newly created application 1. Click on the \"sync button\" and click on \"Synchronize\" 3. Make a change in your Git repo where the Kubernetes manifests are 1. `git add .` 2. `git commit -a` 3. `git push origin <BRANCH_NAME>` 4. Go back to ArgoCD UI and check the status of the app 1. You should see it's \"out-of-sync\". If you don't, you may want to click on \"Refresh\" 2. You can also click on \"App diff\" to see the difference that led to \"out-of-sync\" 5. Click on \"Sync\" and \"Synchronize\" to sync the application", "metadata": {"source_file": "learning-materials/topics/argo/exercises/sync_app_git/solution.md", "section": "UI", "language": "en", "created_at": "2025-07-19T19:22:02.129957"}}
{"text": "``` argocd app create app-demo \\ --project project-demo \\ --repo https://fake.repo.address \\ --path ./some_app_path \\ --dest-namespace default \\ --dest-server my.kubernetes.cluster", "metadata": {"source_file": "learning-materials/topics/argo/exercises/sync_app_git/solution.md", "section": "CLI", "language": "en", "created_at": "2025-07-19T19:22:02.129990"}}
{"text": "cd <git repo> vi <k8s manifest> git add . git commit -a git push origin <BRANCH_NAME>", "metadata": {"source_file": "learning-materials/topics/argo/exercises/sync_app_git/solution.md", "section": "In the Git repo", "language": "en", "created_at": "2025-07-19T19:22:02.130009"}}
{"text": "argocd app get app-demo", "metadata": {"source_file": "learning-materials/topics/argo/exercises/sync_app_git/solution.md", "section": "Check app state", "language": "en", "created_at": "2025-07-19T19:22:02.130023"}}
{"text": "argocd app sync app-demo argocd app wait app-demo ```", "metadata": {"source_file": "learning-materials/topics/argo/exercises/sync_app_git/solution.md", "section": "Sync app state", "language": "en", "created_at": "2025-07-19T19:22:02.130037"}}
{"text": "1. Running Kubernetes cluster 2. Argo Rollouts CLI 3. Deployed app in a specific version", "metadata": {"source_file": "learning-materials/topics/argo/exercises/canary_rollout/solution.md", "section": "Requirements", "language": "en", "created_at": "2025-07-19T19:22:02.130540"}}
{"text": "1. Install Argo Rollouts controller 2. Write a rollout manifest that use canary rollout strategy and apply it 1. Set it to 6 replicas 2. Disable auto-promotions 3. Check the rollout list 4. Rollout a new version of your app in any way you prefer 1. Check the status of the rollout", "metadata": {"source_file": "learning-materials/topics/argo/exercises/canary_rollout/solution.md", "section": "Objectives", "language": "en", "created_at": "2025-07-19T19:22:02.130593"}}
{"text": "Installation: 1. `kubectl create namespace argo-rollouts` 1. `kubectl apply -n argo-rollouts -f https://github.com/argoproj/argo-rollouts/releases/latest/download/install.yaml` 2. Rollout resource: ``` --- apiVersion: argoproj.io/v1alpha1 kind: Rollout metadata: name: some-app spec: replicas: 6 strategy: canary: stableService: k8s-service-stable canaryService: k8s-service-canary trafficRouting: ambassador: mappings: - k8s-mapping steps: - setWeight: 30 - pause: {} - setWeight: 60 - pause: {} - setWeight: 100 - pause: {} selector: matchLabels: app: some-web-app template: metadata: labels: app: some-web-app spec: containers: - name: web-app image: some/registry/and/image:v1.0 ports: - name: http containerPort: 8080 protocol: TCP ``` 3. `kubectl argo rollouts list rollouts` 4. `kubectl argo rollouts set image SOME-APP web-app=some/registry/and/image:v2.0` 1. `kubectl argo rollouts get rollout some-app --watch`", "metadata": {"source_file": "learning-materials/topics/argo/exercises/canary_rollout/solution.md", "section": "Solution", "language": "en", "created_at": "2025-07-19T19:22:02.130759"}}
{"text": "1. Install Argo Rollouts controller 2. Write a rollout manifest that use canary rollout strategy and apply it 1. Set it to 3 replicas 2. Disable auto-promotions 3. Check the rollout list 4. Rollout a new version of your app in any way you prefer 1. Check the status of the rollout", "metadata": {"source_file": "learning-materials/topics/argo/exercises/canary_rollout/exercise.md", "section": "Objectives", "language": "en", "created_at": "2025-07-19T19:22:02.130895"}}
{"text": "1. Running Kubernetes cluster 2. Argo Rollouts CLI 3. Deployed app in specific version", "metadata": {"source_file": "learning-materials/topics/argo/exercises/blue_green_rollout/solution.md", "section": "Requirements", "language": "en", "created_at": "2025-07-19T19:22:02.131225"}}
{"text": "1. Install Argo Rollouts controller 2. Write a rollout manifest that use blue/green deployment and apply it 1. Set it to 3 replicas 2. Disable auto-promotions 3. Check the rollout list 4. Rollout a new version of your app in any way you prefer 1. Check the status of the rollout", "metadata": {"source_file": "learning-materials/topics/argo/exercises/blue_green_rollout/solution.md", "section": "Objectives", "language": "en", "created_at": "2025-07-19T19:22:02.131277"}}
{"text": "Installation: 1. `kubectl create namespace argo-rollouts` 1. `kubectl apply -n argo-rollouts -f https://github.com/argoproj/argo-rollouts/releases/latest/download/install.yaml` 2. Rollout resource: ``` --- apiVersion: argoproj.io/v1alpha1 kind: Rollout metadata: name: some-app spec: replicas: 3 strategy: blueGreen: autoPromotionEnabled: false selector: matchLabels: app: some-web-app template: metadata: labels: app: some-web-app spec: containers: - name: web-app image: some/registry/and/image:v1.0 ports: - name: http containerPort: 8080 protocol: TCP ``` 3. `kubectl argo rollouts list rollouts` 4. `kubectl argo rollouts set image SOME-APP web-app=some/registry/and/image:v2.0` 1. `kubectl argo rollouts get rollout some-app --watch`", "metadata": {"source_file": "learning-materials/topics/argo/exercises/blue_green_rollout/solution.md", "section": "Solution", "language": "en", "created_at": "2025-07-19T19:22:02.131364"}}
{"text": "1. Running Kubernetes cluster 2. ArgoCD installed on the k8s cluster 3. Repository of an Helm chart", "metadata": {"source_file": "learning-materials/topics/argo/exercises/argocd_helm_app/solution.md", "section": "Requirements", "language": "en", "created_at": "2025-07-19T19:22:02.131634"}}
{"text": "1. Create a new app in ArgoCD that points to the repo of your Helm chart", "metadata": {"source_file": "learning-materials/topics/argo/exercises/argocd_helm_app/solution.md", "section": "Objectives", "language": "en", "created_at": "2025-07-19T19:22:02.131661"}}
{"text": "``` argocd app create some-app \\ --project default \\ --repo https://repo-with-helm-chart --path \"./helm\" \\ --sync-policy auto \\ --dest-namespace default \\ --dest-server https://kubernetes.cluster ```", "metadata": {"source_file": "learning-materials/topics/argo/exercises/argocd_helm_app/solution.md", "section": "Solution", "language": "en", "created_at": "2025-07-19T19:22:02.131685"}}
{"text": "1. Running Kubernetes cluster 2. Application k8s manifests with secrets 3. Kubeseal binary installed", "metadata": {"source_file": "learning-materials/topics/argo/exercises/secrets_101/exercise.md", "section": "Requirements", "language": "en", "created_at": "2025-07-19T19:22:02.131891"}}
{"text": "1. Install bitnami sealed controller as ArgoCD app 2. Encrypt secrets and commit them to the repo with the k8s manifests 3. Create an app using the secrets you encrypted", "metadata": {"source_file": "learning-materials/topics/argo/exercises/secrets_101/exercise.md", "section": "Objectives", "language": "en", "created_at": "2025-07-19T19:22:02.131924"}}
{"text": "1. Click on \"New App\" 1. app name: controller 2. project: default 3. sync policy: automatic 4. repository URL: a URL to bitnami sealed controller manifests 5. namespace: kube-system 2. Run the following for every secret: `kubeseal < some/secret.yml > sealed_secrets/some/encrypted_secret.yaml -o yaml` 3. Click on \"New App\" 1. app name: some-app 2. project: default 3. sync policy: automatic 4. repository URL: a URL to k8s manifests (including encrypted secrets) 5. namespace: default", "metadata": {"source_file": "learning-materials/topics/argo/exercises/secrets_101/soltuion.md", "section": "Solution", "language": "en", "created_at": "2025-07-19T19:22:02.132068"}}
{"text": "1. Make sure you have a Kubernetes cluster running with ArgoCD installed 1. Make sure you have an app deployed in the cluster and tracked by ArgoCD", "metadata": {"source_file": "learning-materials/topics/argo/exercises/sync_app_cluster/solution.md", "section": "Requirements", "language": "en", "created_at": "2025-07-19T19:22:02.132191"}}
{"text": "1. Verify the app is tracked by ArgoCD and in sync 2. . Make a change to your application by running a `kubectl` command. The change can anything: 1. Changing the tag of the image 2. Changing the number of replicas 3. You can go extreme and delete the resource if you would like :) 3. Check the app state in ArgoCD 4. Sync the app state", "metadata": {"source_file": "learning-materials/topics/argo/exercises/sync_app_cluster/solution.md", "section": "Objectives", "language": "en", "created_at": "2025-07-19T19:22:02.132254"}}
{"text": "1. Click on the app in the UI 1. Make sure it's in sync and in \"healthy\" state 2. Make a check in the cluster 1. `kubectl scale --replicas=0 <DEPLOYMENT_NAME>` 2. `kubectl get rs <DEPLOYMENT_NAME>` 3. Go back to the UI and check the state of the app 1. If it's still in sync, make sure to click on \"Refresh\" 2. The app should be in \"out-of-sync\" state 3. Click on \"Sync\" and then on \"Synchronize\"", "metadata": {"source_file": "learning-materials/topics/argo/exercises/sync_app_cluster/solution.md", "section": "UI", "language": "en", "created_at": "2025-07-19T19:22:02.132326"}}
{"text": "kubectl scale --replicas=0 <DEPLOYMENT_NAME> kubectl get rs <DEPLOYMENT_NAME>", "metadata": {"source_file": "learning-materials/topics/argo/exercises/sync_app_cluster/solution.md", "section": "Run the following k8s commands (or any other commands that will change the state of your app)", "language": "en", "created_at": "2025-07-19T19:22:02.132359"}}
{"text": "1. Using the CLI or the UI, create a new application with the following properties: 1. app name: app-demo 2. project: app-project 3. repository URL: your repo with some k8s manifests 4. namespace: default 2. Verify the app was created 3. Sync the app 4. Verify Kubernetes resources were created 5. Delete the app", "metadata": {"source_file": "learning-materials/topics/argo/exercises/app_creation/solution.md", "section": "Objectives", "language": "en", "created_at": "2025-07-19T19:22:02.132633"}}
{"text": "1. Click on \"New App\" 1. Insert application name: `app-demo` 2. Insert project: `app-project` 3. Under source put the repository URL to your GitHub repo with Kubernetes manifests 1. Set the path for your application 4. Under destination put the address of your Kubernetes cluster and set namespace to `default` 5. Click on \"Create\" 2. Click on \"Sync\" button on the \"app-demo\" form 1. Click on \"Synchronize\" 3. Verify the Kubernetes resources were created 1. `kubectl get deployments` 4. Delete the app", "metadata": {"source_file": "learning-materials/topics/argo/exercises/app_creation/solution.md", "section": "UI", "language": "en", "created_at": "2025-07-19T19:22:02.132752"}}
{"text": "``` argocd app create app-demo \\ --project app-project \\ --repo https://fake.repo.address \\ --path ./some_app_path \\ --dest-namespace default \\ --dest-server my.kubernetes.cluster", "metadata": {"source_file": "learning-materials/topics/argo/exercises/app_creation/solution.md", "section": "CLI", "language": "en", "created_at": "2025-07-19T19:22:02.132795"}}
{"text": "argocd app list argocd app get app-demo", "metadata": {"source_file": "learning-materials/topics/argo/exercises/app_creation/solution.md", "section": "Check app state", "language": "en", "created_at": "2025-07-19T19:22:02.132812"}}
{"text": "argocd app sync app-demo argocd app wait app-demo", "metadata": {"source_file": "learning-materials/topics/argo/exercises/app_creation/solution.md", "section": "Sync app state", "language": "en", "created_at": "2025-07-19T19:22:02.132826"}}
{"text": "kubectl get deployments", "metadata": {"source_file": "learning-materials/topics/argo/exercises/app_creation/solution.md", "section": "Verify kubernetes resources were created", "language": "en", "created_at": "2025-07-19T19:22:02.132838"}}
{"text": "argocd app delete app-demo ```", "metadata": {"source_file": "learning-materials/topics/argo/exercises/app_creation/solution.md", "section": "Delete the app", "language": "en", "created_at": "2025-07-19T19:22:02.132986"}}
{"text": "<details> <summary>What is Perl?</summary><br><b> From the official [docs](https://perldoc.perl.org/): \"Perl officially stands for Practical Extraction and Report Language, except when it doesn't.\" It's a general purpose programming language developed for manipulating texts mainly. It has been used to perform system administration tasks, networking, building websites and more. </b></details> <details> <summary>What data types Perl has? And how can we define it?</summary><br><b> - Scalar: This is a simple variable that stores single data items. It can be a string, number or reference. ``` my $number = 5; ``` - Arrays: This is a list of scalars. ``` my @numbers = (1, 2, 3, 4, 5);", "metadata": {"source_file": "learning-materials/topics/perl/README.md", "section": "Perl Self Assessment", "language": "en", "created_at": "2025-07-19T19:22:02.133738"}}
{"text": "my @numbers = qw/1 2 3 4 5/;", "metadata": {"source_file": "learning-materials/topics/perl/README.md", "section": "or using the `qw` keyword (quote word):", "language": "en", "created_at": "2025-07-19T19:22:02.133777"}}
{"text": "``` - Hashes (or associative arrays): This is an unordered collection of key-value pairs. We can access to a hash using the keys. ``` my %numbers = ( First => '1', Second => '2', Third => '3' ); ``` </b></details> <details> <summary>How can you access to a hash value, add and delete a key/value pair and modify a hash?</summary><br><b> ``` my %numbers = ( 'First' => '1', 'Second' => '2', 'Third' => '3' ); ``` - Access: ``` print($numbers{'First'}); ``` - Add: ``` $numbers{'Fourth'} = 4; ``` - Delete: ``` delete $numbers{'Third'}; ``` - Modify: ``` $numbers{'Fifth'} = 6; $numbers{'Fifth'} = 5; ``` </b></details> <details> <summary>How can you iterate an array? And a hash?</summary><br><b> - Array: ``` my @numbers = qw/1 2 3 4 5/;", "metadata": {"source_file": "learning-materials/topics/perl/README.md", "section": "'/' can be another symbol, e.g qw@1 2 3 4 5@", "language": "en", "created_at": "2025-07-19T19:22:02.133925"}}
{"text": "foreach (@numbers) { print($_); }", "metadata": {"source_file": "learning-materials/topics/perl/README.md", "section": "Using `$_` that represents the current iteration in a loop. It starts from index array 0 until the last index.", "language": "en", "created_at": "2025-07-19T19:22:02.133948"}}
{"text": "for my $i (0..$#numbers) { print($numbers[$i]); }", "metadata": {"source_file": "learning-materials/topics/perl/README.md", "section": "\"$#\" returns the max index of an array. That's the reason because we can iterate accessing to the array from the index 0 to the max index.", "language": "en", "created_at": "2025-07-19T19:22:02.133965"}}
{"text": "print map {$_} @numbers;", "metadata": {"source_file": "learning-materials/topics/perl/README.md", "section": "Using the `map` keyword:", "language": "en", "created_at": "2025-07-19T19:22:02.133998"}}
{"text": "while (my $element = shift(@numbers)) { print($element); }", "metadata": {"source_file": "learning-materials/topics/perl/README.md", "section": "After this `loop` the `numbers` array will not have elements.", "language": "en", "created_at": "2025-07-19T19:22:02.134014"}}
{"text": "``` - Hashes: ``` my %capital_cities = ( 'Madrid' => 'Spain', 'Rome' => 'Italy', 'Berlin' => 'Germany' );", "metadata": {"source_file": "learning-materials/topics/perl/README.md", "section": "Output: 12345", "language": "en", "created_at": "2025-07-19T19:22:02.134034"}}
{"text": "foreach my $city (keys %capital_cities) { print($city . \"\\n\"); }", "metadata": {"source_file": "learning-materials/topics/perl/README.md", "section": "Iterate and get the `keys`:", "language": "en", "created_at": "2025-07-19T19:22:02.134050"}}
{"text": "foreach my $country (values %capital_cities) { print($country . \"\\n\"); }", "metadata": {"source_file": "learning-materials/topics/perl/README.md", "section": "Iterate and get the `values`:", "language": "en", "created_at": "2025-07-19T19:22:02.134065"}}
{"text": "foreach my $city (keys %capital_cities) { print(\"City: $city - Country: $capital_cities{$city}\" . \"\\n\"); }", "metadata": {"source_file": "learning-materials/topics/perl/README.md", "section": "Iterate and get the values and keys (first option):", "language": "en", "created_at": "2025-07-19T19:22:02.134081"}}
{"text": "while(my ($city, $country) = each %capital_cities) { print(\"City: $city - Country: $capital_cities{$city}\" . \"\\n\"); } ``` </b></details> <details> <summary>What is a Perl subroutine? How to define it?</summary><br><b> It's the perl model for user defined functions (this is also called function like other programming languages). We can define a subroutine with the keyword `sub`. ``` sub hello { print \"hello\"; } ``` </b></details> <details> <summary>Describe the different ways to receive parameters in a subroutine</summary><br><b> - List assignment: Using the `@_` array. It's a list with the elements that are being passed as parameters. ``` sub power { my ($b, $e) = @_; return $b ** $e; } &power(2, 3); ``` - Individual assignment: We should access to every element of the `@_` array. It starts from zero. ``` sub power { my $b = $_[0]; my $e = $_[1]; return $b ** $e; } &power(2, 3); ``` - Using `shift` keyword: It's used to remove the first value of an array and it's returned. ``` sub", "metadata": {"source_file": "learning-materials/topics/perl/README.md", "section": "Iterate and get the values and keys (first option):", "language": "en", "created_at": "2025-07-19T19:22:02.134549"}}
{"text": "power { my $b = shift; my $3 = shift; return $b ** $e; } &power(2, 3); ``` [Source](https://stackoverflow.com/a/21465275/12771230) We can also read the best way in the same S.O answer. </b></details> <details> <summary>What is lexical and dynamic scoping?</summary><br><b> </b></details> <details> <summary>How to apply referencing and dereferencing?</summary><br><b> </b></details> <details> <summary>Does Perl have conventions?</summary><br><b> You can check [perlstyle](https://perldoc.perl.org/perlstyle) </b></details> <details> <summary>What is Perl POD? Can you code an example?</summary><br><b> From the official [docs](https://perldoc.perl.org/perlpod): \"Pod is a simple-to-use markup language used for writing documentation for Perl, Perl programs, and Perl modules.\" ``` =item This function returns the factorial of a number. Input: $n (number you wanna calculate). Output: number factorial. =cut sub factorial { my ($i, $result, $n) = (1, 1, shift); $result = $result *= $i && $i++ while", "metadata": {"source_file": "learning-materials/topics/perl/README.md", "section": "Iterate and get the values and keys (first option):", "language": "en", "created_at": "2025-07-19T19:22:02.134584"}}
{"text": "$i <= $n; return $result; } ``` </b></details>", "metadata": {"source_file": "learning-materials/topics/perl/README.md", "section": "Iterate and get the values and keys (first option):", "language": "en", "created_at": "2025-07-19T19:22:02.134605"}}
{"text": "<details> <summary>Check if the word `electroencefalografista` exists in a string</summary><br><b> ``` my $string = \"The longest accepted word by RAE is: electroencefalografista\"; if ($string =~ /electroencefalografista/) { print \"Match!\"; } ``` </b></details> <details> <summary>Check if the word `electroencefalografista` does not exists in a string</summary><br><b> ``` my $string = \"The longest not accepted word by RAE is: Ciclopentanoperhidrofenantreno\"; if ($string !~ /electroencefalografista/) { print \"Does not match!\"; } ``` </b></details> <details> <summary>Replace the word `amazing`</summary><br><b> ``` my $string = \"Perl is amazing!\"; $string =~ s/amazing/incredible/; print $string;", "metadata": {"source_file": "learning-materials/topics/perl/README.md", "section": "Perl Regex", "language": "en", "created_at": "2025-07-19T19:22:02.134690"}}
{"text": "``` </b></details> <details> <summary>Extract `hh:mm:ss` with capturing group `()` in the following datetime</summary><br><b> ``` my $date = \"Fri Nov 19 20:09:37 CET 2021\"; my @matches = $date =~ /(.*)(\\d{2}:\\d{2}:\\d{2})(.*)/; print $matches[1];", "metadata": {"source_file": "learning-materials/topics/perl/README.md", "section": "Perl is incredible!", "language": "en", "created_at": "2025-07-19T19:22:02.134755"}}
{"text": "``` </b></details> <details> <summary>Extract all the elements that are numbers in an array</summary><br><b> ``` my @array = ('a', 1, 'b', 2, 'c', 3); my @numbers = grep (/\\d/, @array); # Note: \\d involves more digits than 0-9 map {print $_ . \"\\n\" } @numbers; ``` </b></details> <details> <summary>Print all the linux system users that starts with d or D</summary><br><b> - With a Perl one liner :D ``` open(my $fh, '<', '/etc/passwd'); my @user_info = <$fh>; map { print $& . \"\\n\" if $_ =~ /^d([^:]*)/ } @user_info; close $fh; ``` - Avoiding one-liners ``` foreach my $user_line (@user_info) { if ($user_line =~ /^d([^:]*)/) { print $& . \"\\n\"; } } ``` </b></details>", "metadata": {"source_file": "learning-materials/topics/perl/README.md", "section": "Output: 20:09:37", "language": "en", "created_at": "2025-07-19T19:22:02.135027"}}
{"text": "<details> <summary>Mention the different modes in File Handling</summary><br><b> - Read only: `<` - Write mode. It creates the file if doesn't exist: `>` - Append mode. It creates the file if doesn't exist: `>>` - Read and write mode: `+<` - Read, clear and write mode. It creates the file if doesn't exist: `+>` - Read and append. It creates the file if doesn't exist: `+>>` </b></details> <details> <summary>How to write into a file?</summary><br><b> ```", "metadata": {"source_file": "learning-materials/topics/perl/README.md", "section": "Perl Files Handle", "language": "en", "created_at": "2025-07-19T19:22:02.135111"}}
{"text": "open(my $fh, '>>', 'file_name.ext') or die \"Error: file can't be opened\"; print $fh \"writing text...\\n\"; close($fh); ``` </b></details> <details> <summary>How can you read a file and print every line?</summary><br><b> ``` open(my $fh, '<', 'file_to_read.ext') or die \"Error: file can't be opened\"; my @file = <$fh>; foreach my $line (@file) { print $line; } ``` We can use the file handle without assigning it to an array: ``` open(my $fh, '<', 'file_to_read.ext') or die \"Error: file can't be opened\"; foreach my $line (<$fh>) { print $line; } ``` </b></details>", "metadata": {"source_file": "learning-materials/topics/perl/README.md", "section": "'>>' Append.", "language": "en", "created_at": "2025-07-19T19:22:02.135202"}}
{"text": "<details> <summary>Does Perl have support for OOP?</summary><br><b> From the official [docs](https://perldoc.perl.org/perlootut): \"By default, Perl's built-in OO system is very minimal, leaving you to do most of the work.\" </b></details> <details> <summary>What is the purpose of the bless function?</summary><br><b> The function os the `bless` function is used to turning a plain data structure into an object. </b></details> <details> <summary>How to create a Perl class? How can you call a method?</summary><br><b> - Let's create the package: `Example.pm` ``` package Example; sub new { my $class = shift; my $self = {}; bless $self, $class; return $self; } sub is_working { print \"Working!\"; } 1; ``` - Now we can instance the `Example` class and call `is_working` method: ``` my $e = new Example(); $e->is_working();", "metadata": {"source_file": "learning-materials/topics/perl/README.md", "section": "Perl OOP", "language": "en", "created_at": "2025-07-19T19:22:02.135344"}}
{"text": "``` </b></details> <details> <summary>Does Perl have inheritance? What is the `SUPER` keyword?</summary><br><b> Yes, Perl supports inheritance. We can read about it in the official [docs](https://perldoc.perl.org/perlobj#Inheritance). We also can read about `SUPER` keyword that is used to call a method from the parent class. It gives an example about how we can apply inheritance. </b></details> <details> <summary>Does Perl have polymorphism? What is method overriding?</summary><br><b> Yes, it has polymorphism. In fact method overriding is a way to apply it in Perl. Method overriding in simple words appears when we have a class with a method that already exist in a parent class. Example: ``` package A; sub new { return bless {}, shift; }; sub printMethod { print \"A\\n\"; }; package B; use parent -norequire, 'A'; sub new { return bless {}, shift; }; sub printMethod { print \"B\\n\"; }; my $a = A->new(); my $b = B->new(); A->new()->printMethod(); B->new()->printMethod();", "metadata": {"source_file": "learning-materials/topics/perl/README.md", "section": "Output: Working!", "language": "en", "created_at": "2025-07-19T19:22:02.135592"}}
{"text": "``` </b></details> <details> <summary>How can you call a method of an inherited class?</summary><br><b> ```", "metadata": {"source_file": "learning-materials/topics/perl/README.md", "section": "B", "language": "en", "created_at": "2025-07-19T19:22:02.135629"}}
{"text": "package A; sub new { return bless {}, shift; }; sub printA { print \"A\"; };", "metadata": {"source_file": "learning-materials/topics/perl/README.md", "section": "Class `A` with `printA` method.", "language": "en", "created_at": "2025-07-19T19:22:02.135649"}}
{"text": "package B; use parent -norequire, 'A'; sub new { return bless {}, shift; };", "metadata": {"source_file": "learning-materials/topics/perl/README.md", "section": "Class `B` that extends or use the parent class `A`.", "language": "en", "created_at": "2025-07-19T19:22:02.135666"}}
{"text": "my $b = B->new(); $b->printA(); ``` </b></details>", "metadata": {"source_file": "learning-materials/topics/perl/README.md", "section": "Instance class `B` allows call the inherited method", "language": "en", "created_at": "2025-07-19T19:22:02.135680"}}
{"text": "<details> <summary>How can we evaluate and capture an exception in Perl?</summary><br><b> From the official [eval docs](https://perldoc.perl.org/functions/eval): \"`eval` in all its forms is used to execute a little Perl program, trapping any errors encountered so they don't crash the calling program.\". e.g: ``` eval { die; }; if ($@) { print \"Error. Details: $@\"; } ``` If we execute this we get the next output: ``` Error. Details: Died at eval.pl line 2. ``` The `eval` (`try` in another programming languages) is trying to execute a code. This code fails (it's a die), and then the code continues into the `if` condition that evaluates `$@` error variable have something stored. This is like a `catch` in another programming languages. At this way we can handle errors. </b></details>", "metadata": {"source_file": "learning-materials/topics/perl/README.md", "section": "Perl Exception Handling", "language": "en", "created_at": "2025-07-19T19:22:02.135853"}}
{"text": "<details> <summary>What is Perl Open3?</summary><br><b> From the official [IPC::Open3 docs](https://perldoc.perl.org/IPC::Open3): \"IPC::Open3 - open a process for reading, writing, and error handling using open3()\". With `open3` we can have the full control of the STDIN, STDOUT, STDERR. It's usually used to execute commands. </b></details> <details> <summary>Using Open3: Create a file with the size of 15MB and check it's created successfully</summary><br><b> - Code: ``` use IPC::Open3; use Data::Dumper; sub execute_command { my @command_to_execute = @_; my ($stdin, $stdout, $stderr); eval { open3($stdin, $stdout, $stderr, @command_to_execute); }; if ($@) { print \"Error. Details: $@\"; } close($stdin); return $stdout; } my $file_name = 'perl_open3_test'; &execute_command('truncate', '-s', '15M', $file_name); my $result = &execute_command('stat', '-c', '%s', $file_name); print Dumper(<$result>); ``` - Result: ``` $ -> perl command.pl $VAR1 = '15728640 '; ``` </b></details>", "metadata": {"source_file": "learning-materials/topics/perl/README.md", "section": "Perl OS", "language": "en", "created_at": "2025-07-19T19:22:02.136016"}}
{"text": "<details> <summary>What is a Perl package? And a module?</summary><br><b> With a Perl package we are defining a namespace. A Perl module in one simple word can be defined as a `class`. When we create a `class` in Perl we use the `package` keyword. A module can be used with the `use` keyword. </b></details> <details> <summary>What is the difference between .pl and .pm extensions?</summary><br><b> There's no a real difference between a `.pm` and `.pl` extensions. Perl use `.pm` extensions just to difference it as a perl module (a class). `.pl` extensions are usually named for perl scripts without OOP classes. </b></details> <details> <summary>Why a Perl class (or module) should return something at the end of the file? Check the example.</summary><br><b> If we want to `use` a Perl module (`import` a class), this module should end in a value different than 0. This is necessary because if we try to import the class and it has a false value, we will not be able to use it. ``` package A; sub", "metadata": {"source_file": "learning-materials/topics/perl/README.md", "section": "Perl Packages & Modules", "language": "en", "created_at": "2025-07-19T19:22:02.136408"}}
{"text": "new { return bless {}, shift; }; sub printMethod { print \"A\\n\"; }; 1; ``` </b></details> <details> <summary>What is cpan? And cpanm?</summary><br><b> CPAN is the Comprehensive Perl Archive Network. CPANM From the official [App::cpanminus](https://metacpan.org/pod/App::cpanminus): \"App::cpanminus - get, unpack, build and install modules from CPAN\". [Find CPAN modules](https://metacpan.org/) </b></details> <details> <summary>How can you install cpanm and a Perl module?</summary><br><b> There are some different alternatives to install Perl modules. We will use `cpanm`. - Install `cpanm`: ``` $ cpan App::cpanminus ``` - Install the `Test` module with `cpanm`: ``` cpanm Test ``` Now we can test the `Test` installed module: ``` $ perl -M'Test::Simple tests => 1' -e 'ok( 1 + 1 == 2 );' 1..1 ok 1 ``` ``` $ perl -M'Test::Simple tests => 1' -e 'ok( 1 + 1 == 3 );' 1..1 not ok 1", "metadata": {"source_file": "learning-materials/topics/perl/README.md", "section": "Perl Packages & Modules", "language": "en", "created_at": "2025-07-19T19:22:02.136561"}}
{"text": "``` </b></details>", "metadata": {"source_file": "learning-materials/topics/perl/README.md", "section": "Looks like you failed 1 test of 1.", "language": "en", "created_at": "2025-07-19T19:22:02.136594"}}
{"text": "**Note**: Some of the exercises <b>cost $$$</b> and can't be performed using the free tier or resources **2nd Note**: The provided solutions are using the AWS console. It's recommended you use IaC technologies to solve the exercises (e.g., Terraform, Pulumi).<br> - [AWS](#aws) - [Exercises](#exercises) - [IAM](#iam) - [EC2](#ec2) - [S3](#s3) - [ELB](#elb) - [Auto Scaling Groups] (#auto-scaling-groups) - [VPC](#vpc) - [Databases](#databases) - [DNS](#dns) - [Containers](#containers) - [Lambda](#lambda) - [Elastic Beanstalk](#elastic-beanstalk) - [CodePipeline](#codepipeline) - [CDK](#cdk) - [Misc](#misc) - [Questions](#questions) - [Global Infrastructure](#global-infrastructure) - [IAM](#iam-1) - [EC2](#ec2-1) - [AMI](#ami) - [EBS](#ebs) - [Instance Store](#instance-store) - [EFS](#efs) - [Pricing Models](#pricing-models) - [Launch Template](#launch-template) - [ENI](#eni) - [Placement Groups](#placement-groups) - [VPC](#vpc-1) - [Default VPC](#default-vpc) - [Lambda](#lambda-1) -", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "AWS", "language": "en", "created_at": "2025-07-19T19:22:02.140884"}}
{"text": "[Containers](#containers-1) - [ECS](#ecs) - [Fargate](#fargate) - [S3](#s3-1) - [Basics](#basics) - [Buckets 101](#buckets-101) - [Objects](#objects) - [S3 Security](#s3-security) - [Misc](#misc-1) - [Disaster Recovery](#disaster-recovery) - [CloudFront](#cloudfront) - [ELB](#elb-1) - [NLB](#nlb) - [ALB](#alb) - [Auto Scaling Group](#auto-scaling-group) - [Security](#security) - [Databases](#databases-1) - [RDS](#rds) - [Aurora](#aurora) - [DynamoDB](#dynamodb) - [ElastiCache](#elasticache) - [RedShift](#redshift) - [Identify the Service](#identify-the-service) - [DNS (Route 53)](#dns-route-53) - [SQS](#sqs) - [SNS](#sns) - [Monitoring and Logging](#monitoring-and-logging) - [Billing and Support](#billing-and-support) - [AWS Organizations](#aws-organizations) - [Automation](#automation) - [Misc](#misc-2) - [High Availability](#high-availability) - [Production Operations and Migrations](#production-operations-and-migrations) - [Scenarios](#scenarios) - [Architecture", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "AWS", "language": "en", "created_at": "2025-07-19T19:22:02.140929"}}
{"text": "Design](#architecture-design) - [Misc](#misc-3)", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "AWS", "language": "en", "created_at": "2025-07-19T19:22:02.140949"}}
{"text": "|Name|Topic|Objective & Instructions|Solution|Comments| |--------|--------|------|----|----| | Create a User | IAM | [Exercise](exercises/create_user/exercise.md) | [Solution](exercises/create_user/solution.md) | | | Password Policy | IAM | [Exercise](exercises/password_policy_and_mfa/exercise.md) | [Solution](exercises/password_policy_and_mfa/solution.md) | | | Create a role | IAM | [Exercise](exercises/create_role/exercise.md) | [Solution](exercises/create_role/solution.md) | | | Credential Report | IAM | [Exercise](exercises/credential_report/exercise.md) | [Solution](exercises/credential_report/solution.md) | | | Access Advisor | IAM | [Exercise](exercises/access_advisor/exercise.md) | [Solution](exercises/access_advisor/solution.md) | |", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "IAM", "language": "en", "created_at": "2025-07-19T19:22:02.141006"}}
{"text": "|Name|Topic|Objective & Instructions|Solution|Comments| |--------|--------|------|----|----| | Launch EC2 web instance | EC2 | [Exercise](exercises/launch_ec2_web_instance/exercise.md) | [Solution](exercises/launch_ec2_web_instance/solution.md) | | | Security Groups | EC2 | [Exercise](exercises/security_groups/exercise.md) | [Solution](exercises/security_groups/solution.md) | | | IAM Roles | EC2, IAM | [Exercise](exercises/ec2_iam_roles/exercise.md) | [Solution](exercises/ec2_iam_roles/solution.md) | | | Spot Instances | EC2 | [Exercise](exercises/create_spot_instances/exercise.md) | [Solution](exercises/create_spot_instances/solution.md) | | | Elastic IP | EC2, Networking | [Exercise](exercises/elastic_ip/exercise.md) | [Solution](exercises/elastic_ip/solution.md) | | | Placement Groups Creation | EC2, Placement Groups | [Exercise](exercises/placement_groups/exercise.md) | [Solution](exercises/placement_groups/solution.md) | | | Elastic Network Interfaces | EC2, ENI |", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "EC2", "language": "en", "created_at": "2025-07-19T19:22:02.141143"}}
{"text": "[Exercise](exercises/elastic_network_interfaces/exercise.md) | [Solution](exercises/elastic_network_interfaces/solution.md) | | | Hibernate an Instance | EC2 | [Exercise](exercises/hibernate_instance.md) | [Solution](exercises/hibernate_instance/solution.md) | | | Volume Creation | EC2, EBS | [Exercise](exercises/ebs_volume_creation/exercise.md) | [Solution](exercises/ebs_volume_creation/solution.md) | | | Snapshots | EC2, EBS | [Exercise](exercises/snapshots/exercise.md) | [Solution](exercises/snapshots/solution.md) | | | Create an AMI | EC2, AMI | [Exercise](exercises/create_ami/exercise.md) | [Solution](exercises/create_ami/solution.md) | | | Create EFS | EC2, EFS | [Exercise](exercises/create_efs/exercise.md) | [Solution](exercises/create_efs/solution.md) | |", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "EC2", "language": "en", "created_at": "2025-07-19T19:22:02.141162"}}
{"text": "|Name|Topic|Objective & Instructions|Solution|Comments| |--------|--------|------|----|----| | Create buckets | S3 | [Exercise](exercises/s3/new_bucket/exercise.md) | [Solution](exercises/s3/new_bucket/solution.md) | Bucket Lifecycle Policy | S3, Lifecycle Policy | |", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "S3", "language": "en", "created_at": "2025-07-19T19:22:02.141188"}}
{"text": "|Name|Topic|Objective & Instructions|Solution|Comments| |--------|--------|------|----|----| | Application Load Balancer | ELB, ALB | [Exercise](exercises/app_load_balancer/exercise.md) | [Solution](exercises/app_load_balancer/solution.md) | | | Multiple Target Groups | ELB, ALB | [Exercise](exercises/alb_multiple_target_groups/exercise.md) | [Solution](exercises/alb_multiple_target_groups/solution.md) | | | Network Load Balancer | ELB, NLB | [Exercise](exercises/network_load_balancer/exercise.md) | [Solution](exercises/network_load_balancer/solution.md) | |", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "ELB", "language": "en", "created_at": "2025-07-19T19:22:02.141226"}}
{"text": "|Name|Topic|Objective & Instructions|Solution|Comments| |--------|--------|------|----|----| | Auto Scaling Groups Basics | ASG | [Exercise](exercises/auto_scaling_groups_basics/exercise.md) | [Solution](exercises/auto_scaling_groups_basics/solution.md) | | | Dynamic Scaling Policy | ASG, Policies | [Exercise](exercises/asg_dynamic_scaling_policy/exercise.md) | [Solution](exercises/asg_dynamic_scaling_policy/solution.md) | |", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "Auto Scaling Groups", "language": "en", "created_at": "2025-07-19T19:22:02.141261"}}
{"text": "|Name|Topic|Objective & Instructions|Solution|Comments| |--------|--------|------|----|----| | My First VPC | VPC | [Exercise](exercises/new_vpc/exercise.md) | [Solution](exercises/new_vpc/solution.md) | | | Subnets | VPC | [Exercise](exercises/subnets/exercise.md) | [Solution](exercises/subnets/solution.md) | |", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "VPC", "language": "en", "created_at": "2025-07-19T19:22:02.141438"}}
{"text": "|Name|Topic|Objective & Instructions|Solution|Comments| |--------|--------|------|----|----| | MySQL DB | RDS | [Exercise](exercises/mysql_db/exercise.md) | [Solution](exercises/mysql_db/solution.md) | | | Aurora DB | RDS | [Exercise](exercises/aurora_db/exercise.md) | [Solution](exercises/aurora_db/solution.md) | | | ElastiCache | ElastiCache | [Exercise](exercises/elasticache/exercise.md) | [Solution](exercises/elasticache/solution.md) | |", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "Databases", "language": "en", "created_at": "2025-07-19T19:22:02.141480"}}
{"text": "|Name|Topic|Objective & Instructions|Solution|Comments| |--------|--------|------|----|----| Register Domain | Route 53 | [Exercise](exercises/register_domain/exercise.md) | [Solution](exercises/register_domain/solution.md) | | Creating Records | Route 53 | [Exercise](exercises/creating_records/exercise.md) | [Solution](exercises/creating_records/solution.md) | | Health Checks | Route 53 | [Exercise](exercises/health_checks/exercise.md) | [Solution](exercises/health_checks/solution.md) | | Failover | Route 53 | [Exercise](exercises/route_53_failover/exercise.md) | [Solution](exercises/route_53_failover/solution.md) | |", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "DNS", "language": "en", "created_at": "2025-07-19T19:22:02.141524"}}
{"text": "|Name|Topic|Objective & Instructions|Solution|Comments| |--------|--------|------|----|----| | ECS Task | ECS, Fargate | [Exercise](exercises/ecs_task/exercise.md) | [Solution](exercises/ecs_task/solution.md) | |", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "Containers", "language": "en", "created_at": "2025-07-19T19:22:02.141545"}}
{"text": "|Name|Topic|Objective & Instructions|Solution|Comments| |--------|--------|------|----|----| | Hello Function | Lambda | [Exercise](exercises/hello_function/exercise.md) | [Solution](exercises/hello_function/solution.md) | | | URL Function | Lambda | [Exercise](exercises/url_function/exercise.md) | [Solution](exercises/url_function/solution.md) | | | Web App with DB | Lambda, DynamoDB | [Exercise](exercises/web_app_dynamodb/exercise.md) | [Solution](exercises/web_app_dynamodb/solution.md) | |", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "Lambda", "language": "en", "created_at": "2025-07-19T19:22:02.141579"}}
{"text": "|Name|Topic|Objective & Instructions|Solution|Comments| |--------|--------|------|----|----| | Simple Elastic Beanstalk Node.js app | Elastic Beanstalk | [Exercise](exercises/elastic_beanstalk_simple/exercise.md) | [Solution](exercises/elastic_beanstalk_simple/solution.md) | |", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "Elastic Beanstalk", "language": "en", "created_at": "2025-07-19T19:22:02.141601"}}
{"text": "|Name|Topic|Objective & Instructions|Solution|Comments| |--------|--------|------|----|----| | Basic CI with S3 | CodePipeline & S3 | [Exercise](exercises/basic_s3_ci/exercise.md) | [Solution](exercises/basic_s3_ci/solution.md) | |", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "CodePipeline", "language": "en", "created_at": "2025-07-19T19:22:02.141621"}}
{"text": "|Name|Topic|Objective & Instructions|Solution|Comments| |--------|--------|------|----|----| | Sample CDK | CDK | [Exercise](exercises/sample_cdk/exercise.md) | [Solution](exercises/sample_cdk/solution.md) | |", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "CDK", "language": "en", "created_at": "2025-07-19T19:22:02.141638"}}
{"text": "|Name|Topic|Objective & Instructions|Solution|Comments| |--------|--------|------|----|----| | Budget Setup | Budget | [Exercise](exercises/budget_setup/exercise.md) | [Solution](exercises/budget_setup/solution.md) | | | No Application :'( | Troubleshooting | [Exercise](exercises/no_application/exercise.md) | [Solution](exercises/no_application/solution.md) | |", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "Misc", "language": "en", "created_at": "2025-07-19T19:22:02.141661"}}
{"text": "<details> <summary>Explain the following * Availability zone * Region * Edge location</summary><br><b> AWS regions are data centers hosted across different geographical locations worldwide.<br> Within each region, there are multiple isolated locations known as Availability Zones. Each availability zone is one or more data-centers with redundant network and connectivity and power supply. Multiple availability zones ensure high availability in case one of them goes down.<br> Edge locations are basically content delivery network which caches data and insures lower latency and faster delivery to the users in any location. They are located in major cities in the world. </b></details> <details> <summary>True or False? Each AWS region is designed to be completely isolated from the other AWS regions </summary><br><b> True. </b></details> <details> <summary>True or False? Each region has a minimum number of 1 availability zones and the maximum is 4</summary><br><b> False. The minimum is 2 while", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "Global Infrastructure", "language": "en", "created_at": "2025-07-19T19:22:02.141982"}}
{"text": "the maximum is 6. </b></details> <details> <summary>What considerations to take when choosing an AWS region for running a new application?</summary><br><b> * Services Availability: not all service (and all their features) are available in every region * Reduced latency: deploy application in a region that is close to customers * Compliance: some countries have more strict rules and requirements such as making sure the data stays within the borders of the country or the region. In that case, only specific region can be used for running the application * Pricing: the pricing might not be consistent across regions so, the price for the same service in different regions might be different. </b></details>", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "Global Infrastructure", "language": "en", "created_at": "2025-07-19T19:22:02.142010"}}
{"text": "<details> <summary>What is IAM? What are some of its features?</summary><br><b> In short, it's used for managing users, groups, access policies & roles Full explanation can be found [here](https://aws.amazon.com/iam) </b></details> <details> <summary>True or False? IAM configuration is defined globally and not per region</summary><br><b> True </b></details> <details> <summary>True or False? When creating an AWS account, root account is created by default. This is the recommended account to use and share in your organization</summary><br><b> False. Instead of using the root account, you should be creating users and use them. </b></details> <details> <summary>True or False? Groups in AWS IAM, can contain only users and not other groups</summary><br><b> True </b></details> <details> <summary>True or False? Users in AWS IAM, can belong only to a single group</summary><br><b> False. Users can belong to multiple groups. </b></details> <details> <summary>What are some best practices regarding", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "IAM", "language": "en", "created_at": "2025-07-19T19:22:02.142784"}}
{"text": "IAM in AWS?</summary><br><b> * Delete root account access keys and don't use root account regularly * Create IAM user for any physical user. Don't share users. * Apply \"least privilege principle\": give users only the permissions they need, nothing more than that. * Set up MFA and consider enforcing using it * Make use of groups to assign permissions ( user -> group -> permissions ) </b></details> <details> <summary>What permissions does a new user have?</summary><br><b> Only a login access. </b></details> <details> <summary>True or False? If a user in AWS is using password for authenticating, he doesn't needs to enable MFA</summary><br><b> False(!). MFA is a great additional security layer to use for authentication. </b></details> <details> <summary>What ways are there to access AWS?</summary><br><b> * AWS Management Console * AWS CLI * AWS SDK </b></details> <details> <summary>What are Roles?</summary><br><b> [AWS docs](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html):", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "IAM", "language": "en", "created_at": "2025-07-19T19:22:02.142813"}}
{"text": "\"An IAM role is an IAM identity that you can create in your account that has specific permissions...it is an AWS identity with permission policies that determine what the identity can and cannot do in AWS.\" For example, you can make use of a role which allows EC2 service to access s3 buckets (read and write). </b></details> <details> <summary>What are Policies?</summary><br><b> Policies documents used to give permissions as to what a user, group or role are able to do. Their format is JSON. </b></details> <details> <summary>A user is unable to access an s3 bucket. What might be the problem?</summary><br><b> There can be several reasons for that. One of them is lack of policy. To solve that, the admin has to attach the user with a policy what allows him to access the s3 bucket. </b></details> <details> <summary>What should you use to: - Grant access between two services/resources? - Grant user access to resources/services?</summary><br><b> * Role * Policy </b></details> <details>", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "IAM", "language": "en", "created_at": "2025-07-19T19:22:02.142960"}}
{"text": "<summary>What statements AWS IAM policies are consist of?</summary><br><b> * Sid: identifier of the statement (optional) * Effect: allow or deny access * Action: list of actions (to deny or allow) * Resource: a list of resources to which the actions are applied * Principal: role or account or user to which to apply the policy * Condition: conditions to determine when the policy is applied (optional) </b></details> <details> <summary>Explain the following policy: ``` { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect:\": \"Allow\", \"Action\": \"*\", \"Resources\": \"*\" } ] } ``` </summary><br><b> This policy permits to perform any action on any resource. It happens to be the \"AdministratorAccess\" policy. </b></details> <details> <summary>What security tools AWS IAM provides?</summary><br><b> * IAM Credentials Report: lists all the account users and the status of their credentials * IAM Access Advisor: Shows service permissions granted to a user and information on when he accessed these services", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "IAM", "language": "en", "created_at": "2025-07-19T19:22:02.142993"}}
{"text": "the last time </b></details> <details> <summary>Which tool would you use to optimize user permissions by identifying which services he doesn't regularly (or at all) access?</summary><br><b> IAM Access Advisor </b></details> <details> <summary>What type of IAM object would you use to allow inter-service communication?</summary><br><b> Role </b></details>", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "IAM", "language": "en", "created_at": "2025-07-19T19:22:02.143013"}}
{"text": "<details> <summary>What is EC2?</summary><br><b> \"a web service that provides secure, resizable compute capacity in the cloud\". Read more [here](https://aws.amazon.com/ec2) </b></details> <details> <summary>True or False? EC2 is a regional service</summary><br><b> True. As opposed to IAM for example, which is a global service, EC2 is a regional service. </b></details> <details> <summary>What are some of the properties/configuration options of EC2 instances that can be set or modified?</summary><br><b> * OS (Linux, Windows) * RAM and CPU * Networking - IP, Card properties like speed * Storage Space - (EBS, EFS, EC2 Instance Store) * EC2 User Data * Security groups </b></details> <details> <summary>What would you use for customizing EC2 instances? As in software installation, OS configuration, etc.</summary><br><b> AMI. With AMI (Amazon Machine Image) you can customize EC2 instances by specifying which software to install, what OS changes should be applied, etc. </b></details>", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "EC2", "language": "en", "created_at": "2025-07-19T19:22:02.143188"}}
{"text": "<details> <summary>What is AMI?</summary><br><b> Amazon Machine Images is \"An Amazon Machine Image (AMI) provides the information required to launch an instance\". Read more [here](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html) </b></details> <details> <summary>What are the different sources for AMIs?</summary><br><b> * Personal AMIs - AMIs you create * AWS Marketplace for AMIs - AMIs made by others, mostly sold for some price * Public AMIs - Provided by AWS </b></details> <details> <summary>True or False? AMI are built for specific region</summary><br><b> True (but they can be copied from one region to another). </b></details> <details> <summary>Describe in high-level the process of creating AMIs</summary><br><b> 1. Start an EC2 instance 2. Customized the EC2 instance (install packages, change OS configuration, etc.) 3. Stop the instance (for avoiding data integrity issues) 4. Create EBS snapshot and build an AMI 5. To verify and test the AMI, launch an instance from", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "AMI", "language": "en", "created_at": "2025-07-19T19:22:02.143621"}}
{"text": "the AMI </b></details> <details> <summary>What is an instance type?</summary><br><b> \"the instance type that you specify determines the hardware of the host computer used for your instance\" Read more about instance types [here](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-types.html) </b></details> <details> <summary>Explain the instance type naming convention</summary><br><b> Let's take for example the following instance type: m5.large `m` is the instance class `5` is the generation `large` is the size of the instance (affects the spec properties like vCPUs and RAM) </b></details> <details> <summary>True or False? The following are instance types available for a user in AWS: * Compute optimized * Network optimized * Web optimized</summary><br><b> False. From the above list only compute optimized is available. </b></details> <details> <summary>Explain each of the following instance types: * \"Compute Optimized\" * \"Memory Optimized\" * \"Storage Optimized\"</summary><br><b>", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "AMI", "language": "en", "created_at": "2025-07-19T19:22:02.143650"}}
{"text": "Compute Optimized: * Used for compute-intensive tasks * It has high performance processors * Use cases vary: gaming serves, machine learning, batch processing, etc. Memory Optimized: * Used for processing large data sets in memory * Other use cases: high performance, databases, distributed cache stores Storage Optimized: * Used for storage intensive tasks - high read and write access to large data sets * Use cases: databases, OLTP system, distributing file systems </b></details> <details> <summary>What can you attach to an EC2 instance in order to store data?</summary><br><b> EBS </b></details>", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "AMI", "language": "en", "created_at": "2025-07-19T19:22:02.143671"}}
{"text": "<details> <summary>Explain Amazon EBS</summary><br><b> [AWS Docs](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AmazonEBS.html): \"provides block level storage volumes for use with EC2 instances. EBS volumes behave like raw, unformatted block devices.\" </b></details> <details> <summary>What happens to EBS volumes when the instance is terminated?</summary><br><b> By default, the root volume is marked for deletion, while other volumes will still remain.<br> You can control what will happen to every volume upon termination. </b></details> <details> <summary>What happens to the EC2 disk (EBS) when the instance is stopped?</summary><br><b> Disk is intact and can be used when the instance starts. </b></details> <details> <summary>True or False? EBS volumes are locked to a specific availability zone</summary><br><b> True </b></details> <details> <summary>Explain EBS Snapshots</summary><br><b> EBS snapshots used for making a backup of the EBS volume at point of time. </b></details>", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "EBS", "language": "en", "created_at": "2025-07-19T19:22:02.144095"}}
{"text": "<details> <summary>What are the use cases for using EBS snapshots?</summary><br><b> * Backups of the data * Moving the data between AZs </b></details> <details> <summary>Is it possible to attach the same EBS volume to multiple EC2 instances?</summary><br><b> Yes, with multi-attach it's possible to attach a single EBS volume to multiple instances. </b></details> <details> <summary>True or False? EBS is a network drive hence, it requires network connectivity</summary><br><b> True </b></details> <details> <summary>What EBS volume types are there?</summary><br><b> * HDD (st 1, sc 1): Low cost HDD volumes * SSD * io1, io2: Highest performance SSD * gp2, gp3: General purpose SSD </b></details> <details> <summary>If you need an EBS volume for low latency workloads, which volume type would you use?</summary><br><b> SSD - io1, io2 </b></details> <details> <summary>If you need an EBS volume for workloads that require good performance but the cost is also an important aspect for you, which volume", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "EBS", "language": "en", "created_at": "2025-07-19T19:22:02.144241"}}
{"text": "type would you use?</summary><br><b> SSD - gp2, gp3 </b></details> <details> <summary>If you need an EBS volume for high-throughput, which volume type would you use?</summary><br><b> SSD - io1, io2 </b></details> <details> <summary>If you need an EBS volume for infrequently data access, which volume type would you use?</summary><br><b> HDD - sc1 </b></details> <details> <summary>Which EBS volume types can be used as boot volumes for EC2 instances?</summary><br><b> SSD: gp2, gp3, io1, io2 </b></details> <details> <summary>True or False? In EBS gp2 volume type, IP will increase if the disk size increases</summary><br><b> True. </b></details>", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "EBS", "language": "en", "created_at": "2025-07-19T19:22:02.144270"}}
{"text": "<details> <summary>If you would like to have an hardware disk attached to your EC2 instead of a network one (EBS). What would you use?</summary><br><b> EC2 Instance Store. </b></details> <details> <summary>Explain EC2 Instance Store. Why would someone choose to use it over other options?</summary><br><b> EC2 instance store provides better I/O performances when compared to EBS.<br> It is mostly used for cache and temporary data purposes. </b></details> <details> <summary>Are there any disadvantages in using instance store over EBS?</summary><br><b> Yes, the data on instance store is lost when they are stopped. </b></details>", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "Instance Store", "language": "en", "created_at": "2025-07-19T19:22:02.144370"}}
{"text": "<details> <summary>What is Amazon EFS?</summary><br><b> [AWS Docs](https://aws.amazon.com/efs): \"Amazon Elastic File System (Amazon EFS) provides a simple, scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources.\" In simpler words, it's a network file system you can mount on one or more EC2 instances. </b></details> <details> <summary>True or False? EFS is locked into a single availability zone</summary><br><b> False. EFS can be mounted across multiple availability zones. </b></details> <details> <summary>What are some use cases for using EFS?</summary><br><b> * Data sharing (e.g. developers working on the same source control) * Web serving * Content management </b></details> <details> <summary>True or False? EFS only compatible with Linux based AMI</summary><br><b> True </b></details> <details> <summary>True or False? EFS requires the user to perform capacity planning as it doesn't scales automatically</summary><br><b> False. EFS", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "EFS", "language": "en", "created_at": "2025-07-19T19:22:02.144690"}}
{"text": "scales automatically and you pay-per-use. </b></details> <details> <summary>What EFS modes are there?</summary><br><b> * Performance mode * General purpose: used mainly for CMS, web serving, ... as it's optimal for latency sensitive applications * Max I/O: great for scaling to high levels of throughput and I/O operations per second * Throughput mode * Bursting: scale throughput based on FS size * Provisioned: fixed throughput </b></details> <details> <summary>Which EFS mode would you use if you need to perform media processing?</summary><br><b> Performance Mode (Max I/O): It provides high throughput and scales to operations per second. Mainly used for big data, media processing, etc. </b></details> <details> <summary>What is the default EFS mode?</summary><br><b> Performance Mode (General Purpose): Used for web serving, CMS, ... anything that is sensitive to latency. </b></details> <details> <summary>What EFS storage tiers are there?</summary><br><b> * Standard: frequently accessed", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "EFS", "language": "en", "created_at": "2025-07-19T19:22:02.144740"}}
{"text": "files * Infrequent access: lower prices to store files but it also costs to retrieve them </b></details>", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "EFS", "language": "en", "created_at": "2025-07-19T19:22:02.144766"}}
{"text": "<details> <summary>What EC2 pricing models are there?</summary><br><b> On Demand - pay a fixed rate by the hour/second with no commitment. You can provision and terminate it at any given time. Reserved - you get capacity reservation, basically purchase an instance for a fixed time of period. The longer, the cheaper. Spot - Enables you to bid whatever price you want for instances or pay the spot price. Dedicated Hosts - physical EC2 server dedicated for your use. </b></details> <details> <summary>True or False? Reserved instance has to be used for a minimum of 1 year</summary><br><b> True. </b></details> <details> <summary>Explain the following types of reserved instances: * Convertible Reserved Instances * Scheduled Reserved Instances</summary><br><b> * Convertible Reserved Instances: used for long running workloads but used when instance type might change during the period of time it's reserved * Scheduled Reserved Instances: when you need to reserve an instance for a long period but", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "Pricing Models", "language": "en", "created_at": "2025-07-19T19:22:02.146355"}}
{"text": "you don't need it continuously (so for example you need it only in the morning) </b></details> <details> <summary>True or False? In EC2 On Demand, you pay per hour when using Linux or Windows and per second (after first minute) when using any other operating system</summary><br><b> False. You pay per second (after the first minute) when using Windows or Linux and per hour for any other OS. </b></details> <details> <summary>You need an instance for short-term and the workload running on instance must not be interrupted. Which pricing model would you use?</summary><br><b> On Demand is good for short-term non-interrupted workloads (but it also has the highest cost). </b></details> <details> <summary>You need an instance for running an application for a period of 2 years continuously, without changing instance type. Which pricing model would you use?</summary><br><b> Reserved instances: they are cheaper than on-demand and the instance is yours for the chosen period of time. </b></details>", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "Pricing Models", "language": "en", "created_at": "2025-07-19T19:22:02.146385"}}
{"text": "<details> <summary>Which pricing model has potentially the biggest discount and what its advantage</summary><br><b> Spot instances provide the biggest discount but has the disadvantage of risking losing them due bigger bid price. </b></details> <details> <summary>You need an instance for two years, but only between 10:00-15:00 every day. Which pricing model would you use?</summary><br><b> Reserved instances from the \"Scheduled Reserved Instances\" type which allows you to reserve for specific time window (like 10:00-15:00 every day). </b></details> <details> <summary>You need an instance for running workloads. You don't care if they fail for a given moment as long as they run eventually. Which pricing model would you use?</summary><br><b> Spot instances. The discount potential is the highest compared to all other pricing models. The disadvantage is that you can lose the instance at any point so, you must run only workloads that you are fine with them failing suddenly. </b></details>", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "Pricing Models", "language": "en", "created_at": "2025-07-19T19:22:02.146406"}}
{"text": "<details> <summary>You need a physical server only for your use. Which pricing model are you going to use?</summary><br><b> EC2 Dedicated Host </b></details> <details> <summary>What are some of the differences between dedicated hosts and dedicated instances?</summary><br><b> In dedicated hosts you have per host billing, you have more visibility (sockets, cores, ...) and you can control where instance will be placed.<br> In dedicated instances the billing is per instance but you can't control placement and you don't have visibility of sockets, cores, ... </b></details> <details> <summary>For what use cases, EC2 dedicated hosts are useful for?</summary><br><b> * Compliance needs * When the software license is complex (Bring Your Own License) and doesn't support cloud or multi-tenants * Regulatory requirements </b></details> <details> <summary>What are Security Groups?</summary><br><b> \"A security group acts as a virtual firewall that controls the traffic for one or more instances\" More", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "Pricing Models", "language": "en", "created_at": "2025-07-19T19:22:02.146636"}}
{"text": "on this subject [here](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html) </b></details> <details> <summary>True or False? Security groups only contain deny rules</summary><br><b> False. Security groups only contain allow rules. </b></details> <details> <summary>True or False? One security group can be attached to multiple instances</summary><br><b> True </b></details> <details> <summary>True or False? Security groups are not locked down to a region and VPC (meaning you don't have to create a new one when switching regions)</summary><br><b> False. They are locked down to regions and VPC. </b></details> <details> <summary>True or False? By default, when using security groups, all inbound traffic to an EC2 instance is blocked and all outbound traffic is allowed</summary><br><b> True </b></details> <details> <summary>What is the advantage of referencing security groups from a given security group?</summary><br><b> Imagine you have an instance referencing two", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "Pricing Models", "language": "en", "created_at": "2025-07-19T19:22:02.146675"}}
{"text": "security groups, allowing to get inbound traffic from them.<br> Now imagine you have two instances, each using one of the security groups referenced in the instance we've just mentioned. This means you can get traffic from these two instances because they use security groups which referenced in the instance mentioned at the beginning. No need to use IPs. </b></details> <details> <summary>How to migrate an instance to another availability zone?</summary><br><b> </b></details> <details> <summary>What can you attach to an EC2 instance in order to store data?</summary><br><b> EBS </b></details> <details> <summary>What EC2 reserved instance types are there?</summary><br><b> Standard RI - most significant discount + suited for steady-state usage Convertible RI - discount + change attribute of RI + suited for steady-state usage Scheduled RI - launch within time windows you reserve Learn more about EC2 RI [here](https://aws.amazon.com/ec2/pricing/reserved-instances) </b></details> <details>", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "Pricing Models", "language": "en", "created_at": "2025-07-19T19:22:02.146696"}}
{"text": "<summary>For how long can reserved instances be reserved?</summary><br><b> 1 or 3 years. </b></details> <details> <summary>What allows you to control inbound and outbound instance traffic?</summary><br><b> Security Groups </b></details> <details> <summary>What bootstrapping means and how to use it in AWS EC2?</summary><br><b> Bootstrapping is about launching commands when a machine starts for the first time. In AWS EC2 this is done using the EC2 user data script. </b></details> <details> <summary>You get time out when trying reach your application which runs on an EC2 instance. Specify one reason why it would possibly happen</summary><br><b> Security group isn't configured properly. </b></details> <details> <summary>What is the AWS Instance Connect?</summary><br><b> [AWS](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Connect-using-EC2-Instance-Connect.html): \"Amazon EC2 Instance Connect provides a simple and secure way to connect to your Linux instances using Secure Shell (SSH).\"", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "Pricing Models", "language": "en", "created_at": "2025-07-19T19:22:02.146745"}}
{"text": "</b></details> <details> <summary>You try to run EC2 commands in an EC2 instance you've just created but it fails due to missing credentials. What would you do?</summary><br><b> DO NOT configure AWS credentials on the instance (this means anyone else in your account would be able to use and see your credentials).<br> The best practice is to attach an IAM role with sufficient permissions (like `IAMReadOnlyAccess`) </b></details> <details> <summary>True or False? Cancelling a Spot instance request terminates the instance</summary><br><b> False. When you cancel a Spot instance request, you are not terminating the instances created by it.<br> To terminate such instances, you must cancel the Spot instance request first. </b></details> <details> <summary>What are Spot Fleets?</summary><br><b> Set of Spot instances and if you would like, also on-demand instances. </b></details> <details> <summary>What strategies are there to allocate Spot instances?</summary><br><b> * lowestPrice: launch", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "Pricing Models", "language": "en", "created_at": "2025-07-19T19:22:02.146770"}}
{"text": "instances from the pool that has the lowest price * diversified: distributed across all pools * capacityOptimized: optimized based on the number of instances </b></details> <details> <summary>From networking perspective, what do you get by default when running an EC2 instance?</summary><br><b> A private IP and a public IP. </b></details> <details> <summary>Explain EC2 hibernate</summary><br><b> [AWS Docs](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Hibernate.html: \"Hibernation saves the contents from the instance memory (RAM) to your Amazon Elastic Block Store (Amazon EBS) root volume.\" </b></details> <details> <summary>True or False? Using EC2 hibernate option results in having faster instance boot</summary><br><b> True. This is because the operating system isn't restarted or stopped. </b></details> <details> <summary>What are some use cases for using EC2 hibernate option?</summary><br><b> * Save RAM state * Service with long time initialization * Keep long-running processes", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "Pricing Models", "language": "en", "created_at": "2025-07-19T19:22:02.146790"}}
{"text": "</b></details> <details> <summary>What are some limitations of EC2 hibernate option?</summary><br><b> * Instance RAM size is limited * Root volume must be encrypted EBS * Hibernation time is limited * Doesn't supports all instances types * No support for bare metal. Only On-Demand and Reserved instances * Doesn't supports all AMIs </b></details> <details> <summary>Explain what is EC2 Nitro</summary><br><b> * Next generation EC2 instances using new virtualization technology * Better EBS: 64,000 EBS IOPS * Better networking: HPC, IPv6 * Better security </b></details> <details> <summary>What CPU customization is available with EC2?</summary><br><b> * Modifying number of CPU cores (useful for high RAM and low CPU applications) * Modifying number of threads per cure (useful for HPC workloads) </b></details> <details> <summary>Explain EC2 Capacity Reservations</summary><br><b> * Allows you to ensure you have EC2 capacity when you need it * Usually combined with Reserved Instances and Saving", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "Pricing Models", "language": "en", "created_at": "2025-07-19T19:22:02.146808"}}
{"text": "Plans to achieve cost saving </b></details>", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "Pricing Models", "language": "en", "created_at": "2025-07-19T19:22:02.146928"}}
{"text": "<details> <summary>What is a launch template?</summary><br><b> [AWS Docs](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-launch-templates.html): \"You can create a launch template that contains the configuration information to launch an instance. You can use launch templates to store launch parameters so that you do not have to specify them every time you launch an instance\" </b></details> <details> <summary>What is the difference between Launch Configuration and Launch Template?</summary><br><b> Launch configuration is a legacy form of Launch Template that must be recreated every time you would like to update the configuration. In addition, launch template has the clear benefits of: * Provision both On-Demand and Spot instances * supporting multiple versions * support creating parameters subsets (used for reuse and inheritance) </b></details>", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "Launch Template", "language": "en", "created_at": "2025-07-19T19:22:02.147063"}}
{"text": "<details> <summary>Explain Elastic Network Interfaces (ENI)</summary><br><b> [AWS Docs](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html): \"An elastic network interface is a logical networking component in a VPC that represents a virtual network card.\" </b></details> <details> <summary>Name at least three attributes the Elastic Network Interfaces (ENI) can include</summary><br><b> 1. One public IPv4 address 2. Mac Address 3. A primary private IPv4 address (from the address range of your VPC) </b></details> <details> <summary>True or False? ENI are not bound to a specific availability zone</summary><br><b> False. ENI are bound to specific availability zone. </b></details> <details> <summary>True or False? ENI can be created independently of EC2 instances</summary><br><b> True. They can be attached later on and on the fly (for failover purposes). </b></details>", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "ENI", "language": "en", "created_at": "2025-07-19T19:22:02.147197"}}
{"text": "<details> <summary>What are \"Placement Groups\"?</summary><br><b> [AWS Docs](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html): \"When you launch a new EC2 instance, the EC2 service attempts to place the instance in such a way that all of your instances are spread out across underlying hardware to minimize correlated failures. You can use placement groups to influence the placement of a group of interdependent instances to meet the needs of your workload.\" </b></details> <details> <summary>What Placement Groups strategies are there?</summary><br><b> * Cluster: places instance close together in an AZ. * Spread: spreads the instance across the hardware * Partition: spreads the instances across different partitions (= different sets of hardware/racks) within an AZ </b></details> <details> <summary>For each of the following scenarios choose a placement group strategy: * High availability is top priority * Low latency between instances * Instances must be isolated", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "Placement Groups", "language": "en", "created_at": "2025-07-19T19:22:02.147591"}}
{"text": "from each other * Big Data applications that are partition aware * Big Data process that needs to end quickly</summary><br><b> * High availability is top priority - Spread * Low latency between instances - Cluster * Instances must be isolated from each other - Spread * Big Data applications that are partition aware - Partition * Big Data process that needs to end quickly - Cluster </b></details> <details> <summary>What are the cons and pros of the \"Cluster\" placement group strategy?</summary><br><b> Cons: if the hardware fails, all instances fail Pros: Low latency & high throughput network </b></details> <details> <summary>What are the cons and pros of the \"Spread\" placement group strategy?</summary><br><b> Cons: * Current limitation is 7 instances per AZ (per replacement group) Pros: * Maximized high availability (instances on different hardware, span across AZs) </b></details>", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "Placement Groups", "language": "en", "created_at": "2025-07-19T19:22:02.147621"}}
{"text": "<details> <summary>What is VPC?</summary><br><b> \"A logically isolated section of the AWS cloud where you can launch AWS resources in a virtual network that you define\" Read more about it [here](https://aws.amazon.com/vpc). </b></details> <details> <summary>True or False? VPC spans multiple regions</summary><br><b> False </b></details> <details> <summary>True or False? It's possible to have multiple VPCs in one region</summary><br><b> True. As of today, the soft limit is 5. </b></details> <details> <summary>True or False? Subnets belong to the same VPC, can be in different availability zones</summary><br><b> True. Just to clarify, a single subnet resides entirely in one AZ. </b></details> <details> <summary>You have noticed your VPC's subnets (which use x.x.x.x/20 CIDR) have 4096 available IP addresses although this CIDR should have 4096 addresses. What is the reason for that?</summary><br><b> AWS reserves 5 IP addresses in each subnet - first 4 and the last one, and so they aren't", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "VPC", "language": "en", "created_at": "2025-07-19T19:22:02.148608"}}
{"text": "available for use. </b></details> <details> <summary>What AWS uses the 5 reserved IP addresses for?</summary><br><b> x.x.x.0 - network address x.x.x.1 - VPC router x.x.x.2 - DNS mapping x.x.x.3 - future use x.x.x.255 - broadcast address </b></details> <details> <summary>What is an Internet Gateway?</summary><br><b> [AWS Docs](https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Internet_Gateway.html): \"component that allows communication between instances in your VPC and the internet\" In addition it's good to know that IGW is: * Highly available and redundant * Not porivding internet access by its own (you need route tables to be edited) * Created separately from VPC </b></details> <details> <summary>True or False? One or more VPCs can be attached to one Internet Gateway</summary><br><b> False. Only one VPC can be attached to one IGW and vice versa </b></details> <details> <summary>True or False? NACL allow or deny traffic on the subnet level</summary><br><b> True </b></details>", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "VPC", "language": "en", "created_at": "2025-07-19T19:22:02.148637"}}
{"text": "<details> <summary>What is VPC peering?</summary><br><b> [docs.aws](https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html): \"A VPC peering connection is a networking connection between two VPCs that enables you to route traffic between them using private IPv4 addresses or IPv6 addresses.\" </b></details> <details> <summary>True or False? Multiple Internet Gateways can be attached to one VPC</summary><br><b> False. Only one internet gateway can be attached to a single VPC. </b></details> <details> <summary>You've restarted your EC2 instance and the public IP has changed. How would you deal with it so it won't happen?</summary><br><b> Use Elastic IP which provides you a fixed IP address. </b></details> <details> <summary>When creating a new VPC, there is an option called \"Tenancy\". What is it used for?</summary><br><b> [AWS Docs](https://docs.aws.amazon.com/vpc/latest/userguide/create-vpc.html): `Tenancy` option defines if EC2 instances that you launch into the VPC will", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "VPC", "language": "en", "created_at": "2025-07-19T19:22:02.148658"}}
{"text": "run on hardware that's shared with other AWS accounts or on hardware that's dedicated for your use only. </b></details> <details> <summary>What is an Elastic IP address?</summary><br><b> [AWS Docs](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/elastic-ip-addresses-eip.html): \"An Elastic IP address is a static IPv4 address designed for dynamic cloud computing. An Elastic IP address is allocated to your AWS account, and is yours until you release it. By using an Elastic IP address, you can mask the failure of an instance or software by rapidly remapping the address to another instance in your account.\" </b></details> <details> <summary>Why would you use an Elastic IP address?</summary><br><b> Let's say you have an instance that you need to shutdown or perform some maintenance on. In that case, what you would want to do is to move the Elastic IP address to another instance that is operational, until you finish to perform the maintenance and then you can move it back to the original", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "VPC", "language": "en", "created_at": "2025-07-19T19:22:02.148677"}}
{"text": "instance (or keep it assigned to the second one). </b></details> <details> <summary>True or False? When stopping and starting an EC2 instance, its public IP changes</summary><br><b> True </b></details> <details> <summary>What are the best practices around Elastic IP?</summary><br><b> The best practice is actually not using them in the first place. It's more common to use a load balancer without a public IP or use a random public IP and register a DNS record to it </b></details> <details> <summary>True or False? An Elastic IP is free, as long it's not associated with an EC2 instance</summary><br><b> False. An Elastic IP is free of charge as long as **it is ** associated with an EC2 instance. This instance should be running and should have only one Elastic IP. </b></details> <details> <summary>True or False? Route Tables used to allow or deny traffic from the internet to AWS instances</summary><br><b> False. </b></details> <details> <summary>Explain Security Groups and Network", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "VPC", "language": "en", "created_at": "2025-07-19T19:22:02.148923"}}
{"text": "ACLs</summary><br><b> * NACL - security layer on the subnet level. * Security Group - security layer on the instance level. Read more about it [here](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html) and [here](https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html) </b></details> <details> <summary>What is AWS Direct Connect?</summary><br><b> Allows you to connect your corporate network to AWS network. </b></details> <details> <summary>What would you use if you need a fixed public IP for your EC2 instance?</summary><br><b> Elastic IP </b></details> <details> <summary>Kratos, your colleague, decided to use a subnet of /27 because he needs 29 IP addresses for EC2 instances. Is Kratos right?</summary><br><b> No. Since AWS reserves 5 IP addresses for every subnet, Kratos will have 32-5=27 addresses and this is less than what he needs (29). It's better if Kratos uses a subnet of size /26 but good luck telling him that. </b></details>", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "VPC", "language": "en", "created_at": "2025-07-19T19:22:02.148956"}}
{"text": "<details> <summary>True or False? By default, any new account has a default VPC</summary><br><b> True. </b></details> <details> <summary>True or False? Default VPC doesn't have internet connectivity and any launched EC2 will only have a private IP assigned</summary><br><b> False. The default VPC has internet connectivity and any launched EC2 instance gets a public IPv4 address. In addition, any launched EC2 instance gets a public and private DNS names. </b></details> <details> <summary>Which of the following is included with default VPC? * Internet gateway connected to the default VPC * A route to main route table that points all traffic to internet gateway * Default public subnet * Default /16 IPv4 CIDR block</summary><br><b> All of them :) </b></details>", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "Default VPC", "language": "en", "created_at": "2025-07-19T19:22:02.149092"}}
{"text": "<details> <summary>Explain what is AWS Lambda</summary><br><b> AWS definition: \"AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume.\" Read more on it [here](https://aws.amazon.com/lambda) </b></details> <details> <summary>True or False? In AWS Lambda, you are charged as long as a function exists, regardless of whether it's running or not</summary><br><b> False. Charges are being made when the function is executed for the time it takes to execute and compute resources it uses. </b></details> <details> <summary>Which of the following set of languages Lambda supports? - R, Swift, Rust, Kotlin - Python, Ruby, Go, Kotlin, Bash - Python, Ruby, PHP, PowerShell, C#, Perl - Python, Ruby, Go, Node.js, Groovy, C++ - Python, Ruby, Go, Node.js, PowerShell, C# </summary><br><b> - Python, Ruby, Go, Node.js, PowerShell, C# </b></details> <details> <summary>True or False? Basic lambda permissions allow you only to upload logs to Amazon", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "Lambda", "language": "en", "created_at": "2025-07-19T19:22:02.149556"}}
{"text": "CloudWatch Logs</summary><br><b> True </b></details> <details> <summary>What's one of the issues with the current architecture? <img src=\"images/lambda/aws_lambda_direct_access.png\"/> </summary><br><b> Users shouldn't access directly AWS Lambda directly. If you'd to like to expose your Lambda function to users a better approach would be to set up API Gateway endpoint between the users and the Lambda function. This not only provides enhanced security but also easier access for the user where he can use HTTP or HTTPS for accessing the function. </b></details> <details> <summary>Specify one or more use cases for using AWS Lambda</summary><br><b> - Uploading images to S3 and tagging them or inserting information on the images to a database - Uploading videos to S3 and edit them or add subtitles/captions to them and store the result in S3 - Use SNS and/or SQS to trigger functions based on notifications or messages received from these services. - Cron Jobs: Use Lambda together with", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "Lambda", "language": "en", "created_at": "2025-07-19T19:22:02.149578"}}
{"text": "CloudWatch events to schedule tasks/functions periodically. </b></details> <details> <summary>You run an architecture where you have a Lambda function that uploads images to S3 bucket and stores information on the images in DynamoDB. You would like to expose the function to users so they can invoke it. Your friend Carlos suggests you expose the credentials to the Lambda function. What's your take on that?</summary><br><b> That's a big no. You shouldn't let users direct access to your Lambda function. The way to go here and expose the Lambda function to users is to to an API Gateway endpoint. </b></details>", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "Lambda", "language": "en", "created_at": "2025-07-19T19:22:02.149598"}}
{"text": "<details> <summary>What is Amazon ECS?</summary><br><b> [AWS Docs](https://aws.amazon.com/ecs): \"Amazon Elastic Container Service (Amazon ECS) is a fully managed container orchestration service. Customers such as Duolingo, Samsung, GE, and Cook Pad use ECS to run their most sensitive and mission critical applications because of its security, reliability, and scalability.\" In simpler words, it allows you to launch containers on AWS.<br> While AWS takes care of starting/stopping containers, you need to provision and maintain the infrastructure where the containers are running (EC2 instances). </b></details> <details> <summary>What one should do in order to make EC2 instance part of an ECS cluster?</summary><br><b> Install ECS agent on it. Some AMIs have built-in configuration for that. </b></details> <details> <summary>What ECS launch types are there?</summary><br><b> * EC2 Instance * AWS Fargate </b></details> <details> <summary>What is Amazon ECR?</summary><br><b> [AWS", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "ECS", "language": "en", "created_at": "2025-07-19T19:22:02.149914"}}
{"text": "Docs](https://aws.amazon.com/ecr): \"Amazon Elastic Container Registry (ECR) is a fully-managed Docker container registry that makes it easy for developers to store, manage, and deploy Docker container images.\" </b></details> <details> <summary>What the role \"EC2 Instance Profile\" is used for in regards to ECS?</summary><br><b> EC2 Instance Profile used by ECS agent on an EC2 instance to: * Make API calls to ECS Service * Send logs to CloudWatch from the container * Use secrets defined in SSM Parameter Store or Secrets Manager * Pull container images from ECR (Registry) </b></details> <details> <summary>How to share data between containers (some from ECS and some from Fargate)?</summary><br><b> Using EFS is a good way to share data between containers and it works also between different AZs. </b></details>", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "ECS", "language": "en", "created_at": "2025-07-19T19:22:02.149942"}}
{"text": "<details> <summary>What is AWS Fargate?</summary><br><b> [Amazon Docs](https://aws.amazon.com/fargate): \"AWS Fargate is a serverless, pay-as-you-go compute engine that lets you focus on building applications without managing servers. AWS Fargate is compatible with both Amazon Elastic Container Service (ECS) and Amazon Elastic Kubernetes Service (EKS)\" In simpler words, AWS Fargate allows you launch containers on AWS without worrying about managing infrastructure. It runs containers based on the CPU and RAM you need. </b></details> <details> <summary>How AWS Fargate different from AWS ECS?</summary><br><b> In AWS ECS, you manage the infrastructure - you need to provision and configure the EC2 instances.<br> While in AWS Fargate, you don't provision or manage the infrastructure, you simply focus on launching Docker containers. You can think of it as the serverless version of AWS ECS. </b></details> <details> <summary>True or False? Fargate creates an ENI for every task it", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "Fargate", "language": "en", "created_at": "2025-07-19T19:22:02.150235"}}
{"text": "runs</summary><br><b> True. </b></details>", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "Fargate", "language": "en", "created_at": "2025-07-19T19:22:02.150264"}}
{"text": "<details> <summary>Explain what is AWS S3?</summary><br><b> - S3 is a object storage service which is fast, scalable and durable. S3 enables customers to upload, download or store any file or object that is up to 5 TB in size.<br> - S3 stands for: Simple Storage Service - As a user you don't have to worry about filesystems or disk space </b></details>", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "Basics", "language": "en", "created_at": "2025-07-19T19:22:02.150318"}}
{"text": "<details> <summary>What is a bucket?</summary><br><b> An S3 bucket is a resource which is similar to folders in a file system and allows storing objects, which consist of data. </b></details> <details> <summary>True or False? Buckets are defined globally</summary><br><b> False. They are defined at the region level. </b></details> <details> <summary>True or False? A bucket name must be globally unique</summary><br><b> True </b></details> <details> <summary>How to rename a bucket in S3?</summary><br><b> A S3 bucket name is immutable. That means it's not possible to change it, without removing and creating a new bucket. This is why the process for renaming a bucket is as follows: * Create a new bucket with the desired name * Move the data from the old bucket to it * Delete the old bucket With the AWS CLI that would be: ```sh", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "Buckets 101", "language": "en", "created_at": "2025-07-19T19:22:02.150481"}}
{"text": "aws s3 mb s3://[NEW_BUCKET_NAME]", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "Create new bucket", "language": "en", "created_at": "2025-07-19T19:22:02.150502"}}
{"text": "$ aws s3 sync s3://[OLD_BUCKET_NAME] s3://[NEW_BUCKET_NAME]", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "Sync the content from the old bucket to the new bucket", "language": "en", "created_at": "2025-07-19T19:22:02.150516"}}
{"text": "$ aws s3 rb --force s3://[OLD_BUCKET_NAME] ``` </b></details> <details> <summary>True or False? The max object size a user can upload in one go, is 5TB</summary><br><b> True </b></details> <details> <summary>Explain \"Multi-part upload\"</summary><br><b> [Amazon docs](https://docs.aws.amazon.com/AmazonS3/latest/userguide/mpuoverview.html): \"Multipart upload allows you to upload a single object as a set of parts. Each part is a contiguous portion of the object's data...In general, when your object size reaches 100 MB, you should consider using multipart uploads instead of uploading the object in a single operation.\" </b></details>", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "Remove old bucket", "language": "en", "created_at": "2025-07-19T19:22:02.150592"}}
{"text": "<details> <summary>Explain \"Object Versioning\"</summary><br><b> When enabled at a bucket level, versioning allows you to upload new version of files, overriding previous version and so be able to easily roll-back and protect your data from being permanently deleted. </b></details> <details> <summary>Explain the following: - Object Lifecycles - Object Sharing</summary><br><b> * Object Lifecycles - Transfer objects between storage classes based on defined rules of time periods * Object Sharing - Share objects via a URL link </b></details> <details> <summary>Explain Object Durability and Object Availability</summary><br><b> Object Durability: The percent over a one-year time period that a file will not be lost Object Availability: The percent over a one-year time period that a file will be accessible </b></details>", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "Objects", "language": "en", "created_at": "2025-07-19T19:22:02.150750"}}
{"text": "<details> <summary>True or False? Every new S3 bucket is public by default</summary><br><b> False. A newly created bucket is private unless it was configured to be public. </b></details> <details> <summary>What's a presigned URL?</summary><br><b> Since every newly created bucket is by default private it doesn't allows to share files with users. Even if the person who uploaded them tries to view them, it gets denied. A presigned URL is a way to bypass that and allow sharing the files with users by including the credentials (token) as part of the URL. It can be done for limited time. </b></details> <details> <summary>What security measures have you taken in context of S3?</summary><br><b> * Don't make a bucket public. * Enable encryption if it's disabled. * Define an access policy </b></details> <details> <summary>What encryption types supported by S3?</summary><br><b> * SSE-S3 * SSE-KMS * SSE-C </b></details> <details> <summary>Describe shortly how SSE-S3 (AES) encryption", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "S3 Security", "language": "en", "created_at": "2025-07-19T19:22:02.151216"}}
{"text": "works</summary><br><b> 1. You upload a file to S3 using HTTP (or HTTPS) and header 2. S3 uses the managed data key to encrypt it 3. S3 stores the encrypted object in the bucket </b></details> <details> <summary>True or False? In case of SSE-S3 (AES-256) encryption, you manage the key</summary><br><b> False. S3 manages the key and uses AES-256 algorithm for the encryption. </b></details> <details> <summary>Who or what manages the keys in the case of SSE-KMS encryption?</summary><br><b> The KMS service. </b></details> <details> <summary>Why would someone choose to use SSE-KMS instead of SSE-S3?</summary><br><b> SS3-KMS provides control over who has access to the keys and you can also enabled audit trail. </b></details> <details> <summary>True or False? In case of SSE-C encryption, both S3 and you manage the keys</summary><br><b> False. You manage the keys. It's customer provided keys. </b></details> <details> <summary>True or False? In case of SSE-C HTTPS must be used and encryption key", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "S3 Security", "language": "en", "created_at": "2025-07-19T19:22:02.151239"}}
{"text": "must be provided in headers for every HTTP request</summary><br><b> True. </b></details> <details> <summary>Describe shortly how SSE-C encryption works</summary><br><b> 1. User uploads a file to S3 using HTTPS while providing data key in the header 2. AWS S3 performs the encryption using the provided data key and encrypted object is stored in the bucket If a user would like to get the object, the same data key would have to be provided. </b></details> <details> <summary>With which string an header starts? * x-zmz * x-amz * x-ama </summary><br><b> x-amz </b></details>", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "S3 Security", "language": "en", "created_at": "2025-07-19T19:22:02.151258"}}
{"text": "<details> <summary>What is a storage class? What storage classes are there?</summary><br><b> Each object has a storage class assigned to, affecting its availability and durability. This also has effect on costs. Storage classes offered today: * Standard: * Used for general, all-purpose storage (mostly storage that needs to be accessed frequently) * The most expensive storage class * 11x9% durability * 2x9% availability * Default storage class * Standard-IA (Infrequent Access) * Long lived, infrequently accessed data but must be available the moment it's being accessed * 11x9% durability * 99.90% availability * One Zone-IA (Infrequent Access): * Long-lived, infrequently accessed, non-critical data * Less expensive than Standard and Standard-IA storage classes * 2x9% durability * 99.50% availability * Intelligent-Tiering: * Long-lived data with changing or unknown access patterns. Basically, In this class the data automatically moves to the class most suitable for you based on usage", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "Misc", "language": "en", "created_at": "2025-07-19T19:22:02.152040"}}
{"text": "patterns * Price depends on the used class * 11x9% durability * 99.90% availability * Glacier: Archive data with retrieval time ranging from minutes to hours * Glacier Deep Archive: Archive data that rarely, if ever, needs to be accessed with retrieval times in hours * Both Glacier and Glacier Deep Archive are: * The most cheap storage classes * have 9x9% durability More on storage classes [here](https://aws.amazon.com/s3/storage-classes) </b></details> <details> <summary>A customer would like to move data which is rarely accessed from standard storage class to the most cheapest class there is. Which storage class should be used? * One Zone-IA * Glacier Deep Archive * Intelligent-Tiering</summary><br><b> Glacier Deep Archive </b></details> <details> <summary>What Glacier retrieval options are available for the user?</summary><br><b> Expedited, Standard and Bulk </b></details> <details> <summary>True or False? Each AWS account can store up to 500 PetaByte of data. Any additional storage", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "Misc", "language": "en", "created_at": "2025-07-19T19:22:02.152073"}}
{"text": "will cost double</summary><br><b> False. Unlimited capacity. </b></details> <details> <summary>Explain what is Storage Gateway</summary><br><b> \"AWS Storage Gateway is a hybrid cloud storage service that gives you on-premises access to virtually unlimited cloud storage\". More on Storage Gateway [here](https://aws.amazon.com/storagegateway) </b></details> <details> <summary>Explain the following Storage Gateway deployments types * File Gateway * Volume Gateway * Tape Gateway</summary><br><b> Explained in detail [here](https://aws.amazon.com/storagegateway/faqs) </b></details> <details> <summary>What is the difference between stored volumes and cached volumes?</summary><br><b> Stored Volumes - Data is located at customer's data center and periodically backed up to AWS Cached Volumes - Data is stored in AWS cloud and cached at customer's data center for quick access </b></details> <details> <summary>What is \"Amazon S3 Transfer Acceleration\"?</summary><br><b> AWS definition: \"Amazon S3", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "Misc", "language": "en", "created_at": "2025-07-19T19:22:02.152094"}}
{"text": "Transfer Acceleration enables fast, easy, and secure transfers of files over long distances between your client and an S3 bucket\" Learn more [here](https://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration.html) </b></details> <details> <summary>Explain data consistency</summary><br><b> S3 Data Consistency provides strong read-after-write consistency for PUT and DELETE requests of objects in the S3 bucket in all AWS Regions. S3 always return latest file version. </b></details> <details> <summary>Can you host dynamic websites on S3? What about static websites?</summary><br><b> No. S3 support only statis hosts. On a static website, individual webpages include static content. They might also contain client-side scripts. By contrast, a dynamic website relies on server-side processing, including server-side scripts such as PHP, JSP, or ASP.NET. Amazon S3 does not support server-side scripting. </b></details>", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "Misc", "language": "en", "created_at": "2025-07-19T19:22:02.152113"}}
{"text": "<details> <summary>Explain what is CloudFront</summary><br><b> AWS definition: \"Amazon CloudFront is a fast content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to customers globally with low latency, high transfer speeds, all within a developer-friendly environment.\" More on CloudFront [here](https://aws.amazon.com/cloudfront) </b></details> <details> <summary>Explain the following * Origin * Edge location * Distribution</summary><br><b> </b></details> <details> <summary>What delivery methods available for the user with CDN?</summary><br><b> </b></details> <details> <summary>True or False?. Objects are cached for the life of TTL</summary><br><b> True </b></details> <details> <summary>What is AWS Snowball?</summary><br><b> A transport solution which was designed for transferring large amounts of data (petabyte-scale) into and out the AWS cloud. </b></details>", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "CloudFront", "language": "en", "created_at": "2025-07-19T19:22:02.152375"}}
{"text": "<details> <summary>What is ELB (Elastic Load Balancing)?</summary><br><b> [AWS Docs](https://aws.amazon.com/elasticloadbalancing): \"Elastic Load Balancing automatically distributes incoming application traffic across multiple targets, such as Amazon EC2 instances, containers, IP addresses, and Lambda functions.\" </b></details> <details> <summary>True or False? Elastic Load Balancer is a managed resource (= AWS takes care of it)</summary><br><b> True. AWS responsible for making sure ELB is operational and takes care of lifecycle operations like upgrades, maintenance and high availability. </b></details> <details> <summary>What types of AWS load balancers are there?</summary><br><b> * Classic Load Balancer (CLB): Mainly for TCP (layer 4) and HTTP, HTTPS (layer 7) * Application Load Balancer (ALB): Mainly for HTTP, HTTPS and WebSocket * Network Load Balancer (NLB): Mainly for TCP, TLS and UDP * Gateway Load Balancer (GWLB): Mainly for layer 3 operations (IP protocol) </b></details>", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "ELB", "language": "en", "created_at": "2025-07-19T19:22:02.153541"}}
{"text": "<details> <summary>What's a \"listener\" in regards to ELB?</summary><br><b> </b></details> <details> <summary>What's a \"target group\" in regards to ELB?</summary><br><b> </b></details> <details> <summary>Which load balancer would you use for services which use HTTP or HTTPS traffic?</summary><br><b> Application Load Balancer (ALB). </b></details> <details> <summary>What are some use cases for using Gateway Load Balancer?</summary><br><b> * Intrusion Detection * Firewall * Payload manipulation </b></details> <details> <summary>Explain \"health checks\" in the context of AWS ELB</summary><br><b> Health checks used by ELB to check whether EC2 instance(s) are properly working.<br> If health checks fail, ELB knows to not forward traffic to that specific EC2 instance where the health checks failed. </b></details> <details> <summary>True or False? AWS ELB health checks are done on a port and a route</summary><br><b> True. For example, port `2017` and endpoint `/health`. </b></details> <details>", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "ELB", "language": "en", "created_at": "2025-07-19T19:22:02.153575"}}
{"text": "<summary>What types of load balancers are supported in EC2 and what are they used for?</summary><br><b> * Application LB - layer 7 traffic<br> * Network LB - ultra-high performances or static IP address (layer 4)<br> * Classic LB - low costs, good for test or dev environments (retired by August 15, 2022)<br> * Gateway LB - transparent network gateway and and distributes traffic such as firewalls, intrusion detection and prevention systems, and deep packet inspection systems. (layer 3)<br> </b></details> <details> <summary>Which type of AWS load balancer is used in the following drawing?<br> <img src=\"../../images/aws/identify_load_balancer.png\"/> </summary><br><b> Application Load Balancer (routing based on different endpoints + HTTP is used). </b></details> <details> <summary>What are possible target groups for ALB (Application Load Balancer)?</summary><br><b> * EC2 tasks * ECS instances * Lambda functions * Private IP Addresses </b></details> <details> <summary>True or False? ALB can", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "ELB", "language": "en", "created_at": "2025-07-19T19:22:02.153596"}}
{"text": "route only to a single route group</summary><br><b> False. ALB can route to multiple target groups. </b></details> <details> <summary>If you wanted to analyze network traffic, you would use the `____ load balancer`</summary><br><b> Gateway Load Balancer </b></details> <details> <summary>Who has better latency? Application Load Balancer or Network Load Balancer?</summary><br><b> Network Load Balancer (~100 ms) as ALB has a latency of ~400 ms </b></details> <details> <summary>True or False? Network load balancer has one static IP per availability zone</summary><br><b> True. </b></details> <details> <summary>What are the supported target groups for network load balancer?</summary><br><b> * EC2 instance * IP addresses * Application Load Balancer </b></details> <details> <summary>What are the supported target groups for gateway load balancer?</summary><br><b> * EC2 instance * IP addresses (must be private IPs) </b></details> <details> <summary>Name one use case for using application load", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "ELB", "language": "en", "created_at": "2025-07-19T19:22:02.153845"}}
{"text": "balancer as a target group for network load balancer</summary><br><b> You might want to have a fixed IP address (NLB) and then forward HTTP traffic based on path, query, ... which is then done by ALB </b></details> <details> <summary>What are some use cases for using Network Load Balancer?</summary><br><b> * TCP, UDP traffic * Extreme performance </b></details> <details> <summary>True or False? Network load balancers operate in layer 4</summary><br><b> True. They forward TCP, UDP traffic. </b></details> <details> <summary>True or False? It's possible to enable sticky session for network load balancer so the same client is always redirected to the same instance</summary><br><b> False. This is only supported in Classic Load Balancer and Application Load Balancer. </b></details> <details> <summary>Explain Cross Zone Load Balancing</summary><br><b> With cross zone load balancing, traffic distributed evenly across all (registered) instances in all the availability zones. </b></details>", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "ELB", "language": "en", "created_at": "2025-07-19T19:22:02.153877"}}
{"text": "<details> <summary>True or False? For network load balancer, cross zone load balancing is always on and can't be disabled </summary><br><b> False. It's disabled by default </b></details> <details> <summary>True or False? In regards to cross zone load balancing, AWS charges you for inter AZ data in network load balancer but no in application load balancer</summary><br><b> True. It charges for inter AZ data in network load balancer, but not in application load balancer </b></details> <details> <summary>True or False? Both ALB and NLB support multiple listeners with multiple SSL certificates </summary><br><b> True </b></details> <details> <summary>Explain Deregistration Delay (or Connection Draining) in regards to ELB</summary><br><b> The period of time or process of \"draining\" instances from requests/traffic (basically let it complete all active connections but don't start new ones) so it can be de-registered eventually and ELB won't send requests/traffic to it anymore. </b></details>", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "ELB", "language": "en", "created_at": "2025-07-19T19:22:02.153897"}}
{"text": "<details> <summary>At what network level/layer a Network Load Balancer operates?</summary><br><b> Layer 4 </b></details>", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "NLB", "language": "en", "created_at": "2025-07-19T19:22:02.153925"}}
{"text": "<details> <summary>True or False? With ALB (Application Load Balancer) it's possible to do routing based on query string and/or headers</summary><br><b> True. </b></details> <details> <summary>True or False? For application load balancer, cross zone load balancing is always on and can't be disabled</summary><br><b> True </b></details>", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "ALB", "language": "en", "created_at": "2025-07-19T19:22:02.153963"}}
{"text": "<details> <summary>Explain Auto Scaling Group</summary><br><b> [Amazon Docs](https://docs.aws.amazon.com/autoscaling/ec2/userguide/AutoScalingGroup.html): \"An Auto Scaling group contains a collection of Amazon EC2 instances that are treated as a logical grouping for the purposes of automatic scaling and management. An Auto Scaling group also enables you to use Amazon EC2 Auto Scaling features such as health check replacements and scaling policies\" </b></details> <details> <summary>You have two instance running as part of ASG. You change the desired capacity to 1. What will be the outcome of this change?</summary><br><b> One of the instances will be terminated. </b></details> <details> <summary>How can you customize the trigger for the scaling in/out of an auto scaling group?</summary><br><b> One way is to use CloudWatch alarms where an alarm will monitor a metric and based on a certain value (or range) you can choose to scale-in or scale-out the ASG. </b></details> <details>", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "Auto Scaling Group", "language": "en", "created_at": "2025-07-19T19:22:02.154579"}}
{"text": "<summary>What are some metrics/rules used for auto scaling</summary><br><b> * Network In/Out * Number of requests on ELB per instance * Average CPU, RAM usage </b></details> <details> <summary>What is dynamic Scaling policy in regards to Auto Scaling Groups?</summary><br><b> A policy in which scaling will occur automatically based on different metrics. There are 3 types: 1. Target Tracking Scaling: scale when the baseline changes (e.g. CPU is over 60%) 2. Step Scaling: more granular scaling where you can choose different actions for different metrics values (e.g. when CPU less than 20%, remove one instance. When CPU is over 40%, add 3 instances) 3. Scheduled Actions: set in advance scaling for specific period of time (e.g. add instances on Monday between 10:00 am to 11:00 am) </b></details> <details> <summary>What is a predictive scaling policy in regards to Auto Scaling Groups?</summary><br><b> Scale by analyzing historical load and schedule scaling based on forecast load.", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "Auto Scaling Group", "language": "en", "created_at": "2025-07-19T19:22:02.154604"}}
{"text": "</b></details> <details> <summary>Explain scaling cooldowns in regards to Auto Scaling Groups</summary><br><b> During a scaling cooldown, ASG will not terminate or launch additional instances. The cooldown happens after scaling activity and the reason for this behaviour is that some metrics have to be collected and stabilize before another scaling operating can take place. </b></details> <details> <summary>Explain the default ASG termination policy</summary><br><b> 1. It finds the AZ which the most number of EC2 instances 2. If number of instances > 1, choose the one with oldest launch configuration, template and terminate it </b></details> <details> <summary>True or False? by default, ASG tries to balance the number of instances across AZ</summary><br><b> True, this is why when it terminates instances, it chooses the AZ with the most instances. </b></details> <details> <summary>Explain Lifecycle hooks in regards to Auto Scaling Groups</summary><br><b> Lifecycle hooks allows you", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "Auto Scaling Group", "language": "en", "created_at": "2025-07-19T19:22:02.154624"}}
{"text": "perform extra steps before the instance goes in service (During pending state) or before it terminates (during terminating state). </b></details> <details> <summary>If you use ASG and you would like to run extra steps before the instance goes in service, what will you use? </summary><br><b> Lifecycle hooks in pending state. </b></details> <details> <summary>Describe one way to test ASG actually works</summary><br><b> In Linux instances, you can install the 'stress' package and run stress to load the system for certain period of time and see if ASG kicks in by adding additional capacity (= more instances). For example: `sudo stress --cpu 100 --timeout 20` </b></details>", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "Auto Scaling Group", "language": "en", "created_at": "2025-07-19T19:22:02.154642"}}
{"text": "requirements for the infrastructure and the customer must provide their own control implementation within their use of AWS services\" Learn more about it [here](https://aws.amazon.com/compliance/shared-responsibility-model) </b></details> <details> <summary>What is the AWS compliance program?</summary><br><b> </b></details> <details> <summary>How to secure instances in AWS?</summary><br><b> * Instance IAM roles should have minimal permissions needed. You don't want an instance-level incident to become an account-level incident * Use \"AWS System Manager Session Manager\" for SSH * Using latest OS images with your instances </b></details> <details> <summary>What is AWS Artifact?</summary><br><b> AWS definition: \"AWS Artifact is your go-to, central resource for compliance-related information that matters to you. It provides on-demand access to AWS’ security and compliance reports and select online agreements.\" Read more about it [here](https://aws.amazon.com/artifact) </b></details>", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "Security", "language": "en", "created_at": "2025-07-19T19:22:02.155586"}}
{"text": "<details> <summary>What is AWS Inspector?</summary><br><b> AWS definition: \"Amazon Inspector is an automated security assessment service that helps improve the security and compliance of applications deployed on AWS. Amazon Inspector automatically assesses applications for exposure, vulnerabilities, and deviations from best practices.\"\" Learn more [here](https://aws.amazon.com/inspector) </b></details> <details> <summary>What is AWS Guarduty?</summary><br><b> AWS definition: \"Amazon GuardDuty is a threat detection service that continuously monitors for malicious activity and unauthorized behavior to protect your Amazon Web Services accounts, workloads, and data stored in Amazon S3\" <br> Monitor VPC Flow lows, DNS logs, CloudTrail S3 events and CloudTrail Mgmt events. </b></details> <details> <summary>What is AWS Shield?</summary><br><b> AWS definition: \"AWS Shield is a managed Distributed Denial of Service (DDoS) protection service that safeguards applications running on AWS.\"", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "Security", "language": "en", "created_at": "2025-07-19T19:22:02.155624"}}
{"text": "</b></details> <details> <summary>What is AWS WAF? Give an example of how it can used and describe what resources or services you can use it with</summary><br><b> </b></details> <details> <summary>What AWS VPN is used for?</summary><br><b> </b></details> <details> <summary>What is the difference between Site-to-Site VPN and Client VPN?</summary><br><b> </b></details> <details> <summary>What is AWS CloudHSM?</summary><br><b> Amazon definition: \"AWS CloudHSM is a cloud-based hardware security module (HSM) that enables you to easily generate and use your own encryption keys on the AWS Cloud.\" Learn more [here](https://aws.amazon.com/cloudhsm) </b></details> <details> <summary>True or False? AWS Inspector can perform both network and host assessments</summary><br><b> True </b></details> <details> <summary>What is AWS Key Management Service (KMS)?</summary><br><b> AWS definition: \"KMS makes it easy for you to create and manage cryptographic keys and control their use across a wide range of", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "Security", "language": "en", "created_at": "2025-07-19T19:22:02.155645"}}
{"text": "AWS services and in your applications.\" More on KMS [here](https://aws.amazon.com/kms) </b></details> <details> <summary>What is AWS Acceptable Use Policy?</summary><br><b> It describes prohibited uses of the web services offered by AWS. More on AWS Acceptable Use Policy [here](https://aws.amazon.com/aup) </b></details> <details> <summary>True or False? A user is not allowed to perform penetration testing on any of the AWS services</summary><br><b> False. On some services, like EC2, CloudFront and RDS, penetration testing is allowed. </b></details> <details> <summary>True or False? DDoS attack is an example of allowed penetration testing activity</summary><br><b> False. </b></details> <details> <summary>True or False? AWS Access Key is a type of MFA device used for AWS resources protection</summary><br><b> False. Security key is an example of an MFA device. </b></details> <details> <summary>What is Amazon Cognito?</summary><br><b> Amazon definition: \"Amazon Cognito handles user", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "Security", "language": "en", "created_at": "2025-07-19T19:22:02.155665"}}
{"text": "authentication and authorization for your web and mobile apps.\" Learn more [here](https://docs.aws.amazon.com/cognito/index.html) </b></details> <details> <summary>What is AWS ACM?</summary><br><b> Amazon definition: \"AWS Certificate Manager is a service that lets you easily provision, manage, and deploy public and private Secure Sockets Layer/Transport Layer Security (SSL/TLS) certificates for use with AWS services and your internal connected resources.\" Learn more [here](https://aws.amazon.com/certificate-manager) </b></details>", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "Security", "language": "en", "created_at": "2025-07-19T19:22:02.155683"}}
{"text": "<details> <summary>What is AWS RDS?</summary><br><b> * Relational Database Service * Managed DB service (you can't ssh the machine) * Supports multiple DBs: MySQL, Oracle, Aurora (AWS Proprietary), ... </b></details> <details> <summary>Why to use AWS RDS instead of launching an EC2 instance and install a database on it?</summary><br><b> AWS RDS is a managed service, that means it's automatically provisioned and patched for you. In addition, it provides you with continuous backup (and the ability to restore from any point of time), scaling capability (both horizontal and vertical), monitoring dashboard and read replicas. </b></details> <details> <summary>What do you know about RDS backups?</summary><br><b> * Automated backups * Full daily backup (done during maintenance window) * Transactions logs backup every 5 minutes * Retention can be increased and by default it's 7 days </b></details> <details> <summary>Explain AWS RDS Storage Auto Scaling</summary><br><b> * RDS storage can", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "RDS", "language": "en", "created_at": "2025-07-19T19:22:02.156681"}}
{"text": "automatically be increased upon lack in storage * The user needs to set \"Maximum Storage Threshold\" to have some limit on storage scaling * Use cases: applications with unpredictable workloads * Supports multiple RDS database engines </b></details> <details> <summary>Explain Amazon RDS Read Replicas</summary><br><b> [AWS Docs](https://aws.amazon.com/rds/features/read-replicas): \"Amazon RDS Read Replicas provide enhanced performance and durability for RDS database (DB) instances. They make it easy to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads.\" In simpler words, it allows you to scale your reads. </b></details> <details> <summary>True or False? RDS read replicas are supported within az, cross az and cross region</summary><br><b> True </b></details> <details> <summary>True or False? RDS read replicas are asynchronous</summary><br><b> True. This is done so the reads are consistent. </b></details> <details> <summary>True", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "RDS", "language": "en", "created_at": "2025-07-19T19:22:02.156738"}}
{"text": "or False? Amazon RDS supports MongoDB</summary><br><b> False. RDS is relational database and MongoDB is a NoSQL db. </b></details> <details> <summary>What are some use cases for using RDS read replicas?</summary><br><b> You have a main application which works against your database but you would like to add additional app, one used for logging, analytics, ... so you prefer it won't use the same database. In this case, you create a read replica instance and the second application works against that instance. </b></details> <details> <summary>Explain RDS Multi Availability Zone</summary><br><b> * RDS multi AZ used mainly for disaster recovery purposes * There is an RDS master instance and in another AZ an RDS standby instance * The data is synced synchronously between them * The user, application is accessing one DNS name and where there is a failure with the master instance, the DNS name moves to the standby instance, so the failover done automatically </b></details> <details>", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "RDS", "language": "en", "created_at": "2025-07-19T19:22:02.156765"}}
{"text": "<summary>True or False? Moving AWS RDS from single AZ to multi AZ is an operation with downtime (meaning there is a need to stop the DB)</summary><br><b> False. It's a zero downtime operation = no need to stop the database. </b></details> <details> <summary>How AWS RDS switches from single AZ to multi AZ?</summary><br><b> 1. Snapshot is taken by RDS 2. The snapshot is restored to another, standby, RDS instance 3. Synchronization is enabled between the two instances </b></details> <details> <summary>True or False? RDS encryption should be defined at launch time</summary><br><b> True </b></details> <details> <summary>True or False? in regards to RDS, replicas can be encrypted even if the master isn't encrypted</summary><br><b> False </b></details> <details> <summary>How to make RDS snapshots encrypted?</summary><br><b> * If RDS database is encrypted then, the snapshot itself is also encrypted * If RDS database isn't encrypted then, the snapshot itself isn't encrypted and then you can", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "RDS", "language": "en", "created_at": "2025-07-19T19:22:02.156947"}}
{"text": "copy the un-encrypted snapshot to created an encrypted copy </b></details> <details> <summary>How to encrypt an un-encrypted RDS instance?</summary><br><b> Create a copy of the un-encrypted instance -> copy the snapshot to create an encrypted copy -> restore the database from the encrypted snapshot -> migrate the application to work against the copied instance -> remove the original DB instance </b></details> <details> <summary>How IAM authentication works with RDS?</summary><br><b> For example: 1. EC2 instance uses IAM role to make an API call to get auth token 2. The token, with SSL encryption, is used for accessing the RDS instance Note: The token has a lifetime of 15 minutes </b></details> <details> <summary>True or False? In case of RDS (not Aurora), read replicas require you to change the SQL connection string</summary><br><b> True. Since read replicas add endpoints, each with its own DNS name, you need to modify your app to reference these new endpoints to balance the load read.", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "RDS", "language": "en", "created_at": "2025-07-19T19:22:02.156979"}}
{"text": "<details> <summary>What do you know about Amazon Aurora?</summary><br><b> * A MySQL & Postgresql based relational database. * Proprietary technology from AWS * The default database proposed for the user when using RDS for creating a database. * Storage automatically grows in increments of 10 GiB * HA native - failover in instant * Has better performances over MySQL and Postgres * Supports 15 replicas (while MySQL supports 5) </b></details> <details> <summary>True or False? Aurora stores 4 copies of your data across 2 availability zones</summary><br><b> False. It stores 6 copies across 3 availability zones </b></details> <details> <summary>True or False? Aurora support self healing where corrupted data replaced by doing peer-to-peer replication</summary><br><b> True </b></details> <details> <summary>True or False? Aurora storage is striped across 20 volumes</summary><br><b> False. 100 volumes. </b></details> <details> <summary>True or False? It's possible to scale Aurora", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "Aurora", "language": "en", "created_at": "2025-07-19T19:22:02.157243"}}
{"text": "replicas</summary><br><b> True. If your read replica instances exhaust their CPU, you can scale by adding more instances </b></details> <details> <summary>Explain Aurora Serverless. What use cases is it good for?</summary><br><b> * Aurora serverless is an automated database instantiation and it's auto scaled based on an actual usage * It's good mainly for infrequent or unpredictable workflows * You pay per second so it can eventually be more cost effective </b></details> <details> <summary>What is the use case for Aurora multi-master?</summary><br><b> Aurora multi-master is perfect for a use case where you want to have instant failover for write node. </b></details>", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "Aurora", "language": "en", "created_at": "2025-07-19T19:22:02.157263"}}
{"text": "<details> <summary>What is AWS DynamoDB?</summary><br><b> </b></details> <details> <summary>Explain \"Point-in-Time Recovery\" feature in DynamoDB</summary><br><b> Amazon definition: \"You can create on-demand backups of your Amazon DynamoDB tables, or you can enable continuous backups using point-in-time recovery. For more information about on-demand backups, see On-Demand Backup and Restore for DynamoDB.\" Learn more [here](https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/PointInTimeRecovery.html) </b></details> <details> <summary>Explain \"Global Tables\" in DynamoDB</summary><br><b> Amazon definition: \"A global table is a collection of one or more replica tables, all owned by a single AWS account.\" Learn more [here](https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/V2globaltables_HowItWorks.html) </b></details> <details> <summary>What is DynamoDB Accelerator?</summary><br><b> Amazon definition: \"Amazon DynamoDB Accelerator (DAX) is a fully managed, highly", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "DynamoDB", "language": "en", "created_at": "2025-07-19T19:22:02.157394"}}
{"text": "available, in-memory cache for DynamoDB that delivers up to a 10x performance improvement – from milliseconds to microseconds...\" Learn more [here](https://aws.amazon.com/dynamodb/dax) </b></details>", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "DynamoDB", "language": "en", "created_at": "2025-07-19T19:22:02.157414"}}
{"text": "<details> <summary>What is AWS ElastiCache? In what use case should it be used?</summary><br><b> Amazon Elasticache is a fully managed Redis or Memcached in-memory data store.<br> It's great for read-intensive workloads where the common data/queries are cached and apps/users access the cache instead of the primary database. </b></details> <details> <summary>Describe the workflow of an application using the cache in AWS</summary><br><b> 1. The application performs a query against the DB. There is a check to see if the data is in the cache 1. If it is, it's a \"cache hit\" and the data is retrieved from there 2. If it's not in there, it's a \"cache miss\" and the data is pulled from the database 1. The data is then also written to the cache (assuming it is often accessed) and next time the user queries for the same data, it might be retrieved from the cache (depends on how much time passed and whether this specific data was invalidated or not) </b></details> <details> <summary>How can you", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "ElastiCache", "language": "en", "created_at": "2025-07-19T19:22:02.157857"}}
{"text": "make an application stateless using ElastiCache?</summary><br><b> Let's say you have multiple instances running the same application and every time you use the application, it creates a user session.<br> This user session can be stored in ElastiCache so even if the user contacts a different instance of the application, the application can retrieve the session from the ElsatiCache. </b></details> <details> <summary>You need a highly available cache with backup and restore features. Which one would you use?</summary><br><b> ElastiCache Redis. </b></details> <details> <summary>You need a cache with read replicas that can be scaled and one support multi AZ. Which one would you use?</summary><br><b> ElastiCache Redis. </b></details> <details> <summary>You need a cache that supports sharding and built with multi-threaded architecture in mind. Which one would you use?</summary><br><b> ElastiCache Memcached </b></details> <details> <summary>True or False? ElastiCache doesn't supports IAM", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "ElastiCache", "language": "en", "created_at": "2025-07-19T19:22:02.157887"}}
{"text": "authentication</summary><br><b> True. </b></details> <details> <summary>What patterns are there for loading data into the cache?</summary><br><b> * Write Through: add or update data in the cache when the data is written to the DB * Lazy Loading: all the read data is cached * Session Store: store temporary session data in cache </b></details>", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "ElastiCache", "language": "en", "created_at": "2025-07-19T19:22:02.158076"}}
{"text": "<details> <summary>What is AWS Redshift and how is it different than RDS?</summary><br><b> cloud data warehouse </b></details> <details> <summary>What do you if you suspect AWS Redshift performs slowly?</summary><br><b> * You can confirm your suspicion by going to AWS Redshift console and see running queries graph. This should tell you if there are any long-running queries. * If confirmed, you can query for running queries and cancel the irrelevant queries * Check for connection leaks (query for running connections and include their IP) * Check for table locks and kill irrelevant locking sessions </b></details> <details> <summary>What is Amazon DocumentDB?</summary><br><b> Amazon definition: \"Amazon DocumentDB (with MongoDB compatibility) is a fast, scalable, highly available, and fully managed document database service that supports MongoDB workloads. As a document database, Amazon DocumentDB makes it easy to store, query, and index JSON data.\" Learn more", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "RedShift", "language": "en", "created_at": "2025-07-19T19:22:02.158275"}}
{"text": "[here](https://aws.amazon.com/documentdb) </b></details> <details> <summary>What \"AWS Database Migration Service\" is used for?</summary><br><b> </b></details> <details> <summary>What type of storage is used by Amazon RDS?</summary><br><b> EBS </b></details>", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "RedShift", "language": "en", "created_at": "2025-07-19T19:22:02.158296"}}
{"text": "<details> <summary>What would you use for automating code/software deployments?</summary><br><b> AWS CodeDeploy </b></details> <details> <summary>You would like to invoke a function every time you enter a URL in the browser. Which service would you use for that?</summary><br><b> AWS Lambda </b></details> <details> <summary>What would you use for easily creating similar AWS environments/resources for different customers?</summary><br><b> CloudFormation </b></details> <details> <summary>Using which service, can you add user sign-up, sign-in and access control to mobile and web apps?</summary><br><b> Cognito </b></details> <details> <summary>Which service would you use for building a website or web application?</summary><br><b> Lightsail </b></details> <details> <summary>Which tool would you use for choosing between Reserved instances or On-Demand instances?</summary><br><b> Cost Explorer </b></details> <details> <summary>What would you use to check how many unassociated Elastic IP", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "Identify the Service", "language": "en", "created_at": "2025-07-19T19:22:02.159082"}}
{"text": "address you have?</summary><br><b> Trusted Advisor </b></details> <details> <summary>Which service allows you to transfer large amounts (Petabytes) of data in and out of the AWS cloud?</summary><br><b> AWS Snowball </b></details> <details> <summary>Which service would you use if you need a data warehouse?</summary><br><b> AWS RedShift </b></details> <details> <summary>Which service provides a virtual network dedicated to your AWS account?</summary><br><b> VPC </b></details> <details> <summary>What you would use for having automated backups for an application that has MySQL database layer?</summary><br><b> Amazon Aurora </b></details> <details> <summary>What would you use to migrate on-premise database to AWS?</summary><br><b> AWS Database Migration Service (DMS) </b></details> <details> <summary>What would you use to check why certain EC2 instances were terminated?</summary><br><b> AWS CloudTrail </b></details> <details> <summary>What would you use for SQL database?</summary><br><b>", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "Identify the Service", "language": "en", "created_at": "2025-07-19T19:22:02.159113"}}
{"text": "AWS RDS </b></details> <details> <summary>What would you use for NoSQL database?</summary><br><b> AWS DynamoDB </b></details> <details> <summary>What would you use for adding image and video analysis to your application?</summary><br><b> AWS Rekognition </b></details> <details> <summary>Which service would you use for debugging and improving performances issues with your applications?</summary><br><b> AWS X-Ray </b></details> <details> <summary>Which service is used for sending notifications?</summary><br><b> SNS </b></details> <details> <summary>What would you use for running SQL queries interactively on S3?</summary><br><b> AWS Athena </b></details> <details> <summary>What would you use for preparing and combining data for analytics or ML?</summary><br><b> AWS Glue </b></details> <details> <summary>Which service would you use for monitoring malicious activity and unauthorized behavior in regards to AWS accounts and workloads?</summary><br><b> Amazon GuardDuty </b></details> <details>", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "Identify the Service", "language": "en", "created_at": "2025-07-19T19:22:02.159132"}}
{"text": "<summary>Which service would you use for centrally manage billing, control access, compliance, and security across multiple AWS accounts?</summary><br><b> AWS Organizations </b></details> <details> <summary>Which service would you use for web application protection?</summary><br><b> AWS WAF </b></details> <details> <summary>You would like to monitor some of your resources in the different services. Which service would you use for that?</summary><br><b> CloudWatch </b></details> <details> <summary>Which service would you use for performing security assessment?</summary><br><b> AWS Inspector </b></details> <details> <summary>Which service would you use for creating DNS record?</summary><br><b> Route 53 </b></details> <details> <summary>What would you use if you need a fully managed document database?</summary><br><b> Amazon DocumentDB </b></details> <details> <summary>Which service would you use to add access control (or sign-up, sign-in forms) to your web/mobile apps?</summary><br><b>", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "Identify the Service", "language": "en", "created_at": "2025-07-19T19:22:02.159150"}}
{"text": "AWS Cognito </b></details> <details> <summary>Which service is often referred to as \"used for decoupling applications\"?</summary><br><b> AWS SQS. Since it's a messaging queue so it allows applications to switch from synchronous communication to asynchronous one. </b></details> <details> <summary>Which service would you use if you need messaging queue?</summary><br><b> Simple Queue Service (SQS) </b></details> <details> <summary>Which service would you use if you need managed DDOS protection?</summary><br><b> AWS Shield </b></details> <details> <summary>Which service would you use if you need store frequently used data for low latency access?</summary><br><b> ElastiCache </b></details> <details> <summary>What would you use to transfer files over long distances between a client and an S3 bucket?</summary><br><b> Amazon S3 Transfer Acceleration </b></details> <details> <summary>Which services are involved in getting a custom string (based on the input) when inserting a URL in the", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "Identify the Service", "language": "en", "created_at": "2025-07-19T19:22:02.159167"}}
{"text": "browser?</summary><br><b> Lambda - to define a function that gets an input and returns a certain string<br> API Gateway - to define the URL trigger (= when you insert the URL, the function is invoked). </b></details> <details> <summary>Which service would you use for data or events streaming?</summary><br><b> Kinesis </b></details> <details> <summary>Which (free) tool would you use to get information on cost savings?</summary><br><b> Trusted Advisor </b></details> <details> <summary>You would like to have on-perm storage access to AWS storage. What would you use for that?</summary><br><b> Storage Gateway </b></details>", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "Identify the Service", "language": "en", "created_at": "2025-07-19T19:22:02.159185"}}
{"text": "<details> <summary>What is Route 53?</summary><br><b> [AWS Route 53](https://aws.amazon.com/route53): \"Amazon Route 53 is a highly available and scalable cloud Domain Name System (DNS) web service...\" Some of Route 53 features: * Register domains * DNS service - domain name translations * Health checks - verify your app is available * Not a feature but its SLA is 100% availability </b></details> <details> <summary>What it means that \"Route 53 is an Authoritative DNS\"?</summary><br><b> The customer can update DNS records </b></details> <details> <summary>What each Route 53 record contains?</summary><br><b> * Domain/subdomain name (e.g. blipblop.com) * Value (e.g. 201.7.202.2) * Record type (e.g. A, AAAA, MX) * TTL: amount of time the record is going to be cached * Routing Policy: how to respond to queries </b></details> <details> <summary>What DNS record types does Route 53 supports?</summary><br><b> * A * AAAA * CNAME * NS * DS * CAA * SOA * MX * TXT * SPF * SRV * NAPTR * PTR", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "DNS (Route 53)", "language": "en", "created_at": "2025-07-19T19:22:02.160630"}}
{"text": "</b></details> <details> <summary>What are hosted zones?</summary><br><b> A container that includes records for defining how to route traffic from a domain and its subdomains </b></details> <details> <summary>What types of hosted zones are there?</summary><br><b> * Public Hosted Zones - include records to specify how to route traffic on the internet * Private Hosted Zones - contain records that specify how you traffic within VPC(s) </b></details> <details> <summary>What is the difference between CNAME record and an Alias record?</summary><br><b> CNAME is used for mapping one hostname to any other hostname while Alias is used to map an hostname to an AWS resource. In addition, Alias work for both root domain (somedomain.com) and non-root domain, while CNAME works only with non-root domain (foo.somedomain.com) </b></details> <details> <summary>True or False? Alias record can be set up for an EC2 DNS name</summary><br><b> False </b></details> <details> <summary>True or False? Alias record", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "DNS (Route 53)", "language": "en", "created_at": "2025-07-19T19:22:02.160666"}}
{"text": "can be set up for an VPC interface endpoint</summary><br><b> True </b></details> <details> <summary>True or False? Alias record is only of type A or AAAA</summary><br><b> True </b></details> <details> <summary>What is a routing policy in regards to AWS Route 53?</summary><br><b> A routing policy routing defines how Route 53 responds to DNS queries. </b></details> <details> <summary>What Route 53 routing policies are there?</summary><br><b> * Simple * Geolocation * Failover * Latency based * Geoproximity * Multi-Value Answer * Weighted </b></details> <details> <summary>Suppose you need to route % of your traffic to a certain instance and the rest of the traffic, to another instance. Which routing policy would you choose?</summary><br><b> Weighted routing policy. </b></details> <details> <summary>Suppose you need to route traffic to a single source with Route 53, without any other requirements, which routing policy would you choose?</summary><br><b> The `simple` routing policy", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "DNS (Route 53)", "language": "en", "created_at": "2025-07-19T19:22:02.160687"}}
{"text": "</b></details> <details> <summary>Explain the geolocation routing policy</summary><br><b> * Routing based on user location * Location can be specified by continent, country or US state * It's recommended to have a default record in case there is no match on location </b></details> <details> <summary>What are some use cases for using geolocation routing policy?</summary><br><b> * Restrict content distribution * App localization * Load balancing </b></details> <details> <summary>Explain the geoproximity routing policy</summary><br><b> * Route based on the geographic location of resources * Shifting routing is done based on the `bias` value * Resources can be of AWS and non-AWS type * For non-AWS you have to specify latitude and longitude in addition to AWS region as done in AWS-based resources * To use it, you have to use Route 53 traffic flow </b></details> <details> <summary>What are some use cases for <code>weighted</code> routing policy?</summary><br><b> * Load balancing between", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "DNS (Route 53)", "language": "en", "created_at": "2025-07-19T19:22:02.160706"}}
{"text": "regions * Testing new applications versions </b></details> <details> <summary>True or False? Route 53 <code>simple</code> routing policy supports both single and multiple values</summary><br><b> True. If multiple values are returned from Route 53 then, the client chooses a single value to use. </b></details> <details> <summary>True or False? In <code>weighted</code> routing DNS records must have the same name but not the same type</summary><br><b> False. They must have the same name AND type. </b></details> <details> <summary>You would like to use a routing policy that will take latency into account and will route to the resource with the lowest latency. Which routing policy would you use?</summary><br><b> Latency-based routing policy. </b></details> <details> <summary>What happens when you set all records to weight 0 when using <code>Weighted</code> routing policy?</summary><br><b> All records are used equally. </b></details> <details> <summary>What Route 53 health checks are used", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "DNS (Route 53)", "language": "en", "created_at": "2025-07-19T19:22:02.160765"}}
{"text": "for?</summary><br><b> Automated DNS failover based on monitoring: * Another health check * endpoint (app, AWS resource, server) * CloudWatch alarms </b></details> <details> <summary>You would like to use a routing policy based on the resource location and be able to shift more traffic to some resources. Which one would you use?</summary><br><b> Geoproximity routing policy </b></details> <details> <summary>Explain Route 53 Traffic Flow feature</summary><br><b> It's a visual editor for managing complex routing decision trees. It allows you to simplify the process of managing records. Configuration can be saved (as Traffic Flow Policy) and applied to different domains/hosted zones. In addition, it supports versioning </b></details> <details> <summary>What are calculated health checks?</summary><br><b> When you combine the results of multiple health checks into a single health check. </b></details> <details> <summary>What is one possible use case for using calculated health", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "DNS (Route 53)", "language": "en", "created_at": "2025-07-19T19:22:02.160787"}}
{"text": "checks?</summary><br><b> Performing maintenance for a website without causing all the health checks to fail. </b></details> <details> <summary>You would like to use a routing policy based on the user location. Which one would you use?</summary><br><b> Geolocation routing policy. It's based on user location. Don't confuse it with latency-based routing policy. While shorter distance may result in lower latency, this is not the requirement in the question. </b></details> <details> <summary>True or False? Route 53 Multi Value is a substitute for those who want cheaper solution than ELB</summary><br><b> False. Route 53 Multi Value is not a substitute for ELB. It's focused on client-side load balancing as opposed to ELB. </b></details> <details> <summary>True or False? Domain registrar and DNS service is inherently the same thing</summary><br><b> False. DNS service can be Route 53 (where you manage DNS records) while the domain itself can be purchased from other sources that aren't Amazon", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "DNS (Route 53)", "language": "en", "created_at": "2025-07-19T19:22:02.160806"}}
{"text": "related (e.g. GoDadday). </b></details>", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "DNS (Route 53)", "language": "en", "created_at": "2025-07-19T19:22:02.160824"}}
{"text": "<details> <summary>What is Simple Queue Service (SQS)?</summary><br><b> AWS definition: \"Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications\". Learn more about it [here](https://aws.amazon.com/sqs) </b></details> <details> <summary>Explain \"producer\" and \"consumer\" in regards to messaging queue</summary><br><b> Producer is the application or in general, the source that sends messages to the queue. Consumer is the process or application that pulls the messages from the queue. </b></details> <details> <summary>What \"default retention of messages\" means?</summary><br><b> It refers to a retention period in which a message has to consumed/processed and deleted from the queue. As of today, the retention of a message is 4 days by default and the maximum allows is 14 days. </b></details> <details> <summary>What's the limitation on message size in SQS? * 128KB * 128MB", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "SQS", "language": "en", "created_at": "2025-07-19T19:22:02.161446"}}
{"text": "* 256KB * 256MB</summary><br><b> 256KB </b></details> <details> <summary>True or False? It's possible to have duplicated messages in the queue</summary><br><b> True. It's referred to as \"at least once delivery\". </b></details> <details> <summary>True or False? \"Consumers\" can be only EC2 instances</summary><br><b> False. They can be Lambda functions and even on-premise instances </b></details> <details> <summary>True or False? Processes/Applications use from the SDK the SendMessage API in order to send messages to the queue</summary><br><b> True. </b></details> <details> <summary>What it means \"best effort ordering\" in regards to SQS?</summary><br><b> It means messages in the queue can be out of order. </b></details> <details> <summary>What is \"Delay Queue\" in regards to SQS?</summary><br><b> It's the time in seconds to delay the delivery of new messages (when they reached the queue already). The limit as of today is 15 minutes. </b></details> <details> <summary>What is \"Visibility", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "SQS", "language": "en", "created_at": "2025-07-19T19:22:02.161478"}}
{"text": "Timeout?\"</summary><br><b> The time in seconds for a message to not be visible for consumers. The limit as of today is 12 hours </b></details> <details> <summary>Give an example of architecture or workflow that involves SQS and EC2 & S3</summary><br><b> A website that allows users to upload videos and adds subtitles to them: 1. First the user uploads the video through the web interface which uploads it to an S3 bucket 2. SQS gets notified with a message on the video location 3. EC2 instance (or Lambda function) starts to work on adding the subtitles 4. The video with the subtitles is uploaded to an S3 buckets 5. SQS gets notified of the result and specifically the video location </b></details> <details> <summary>What's MessageGroupID?</summary><br><b> </b></details>", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "SQS", "language": "en", "created_at": "2025-07-19T19:22:02.161498"}}
{"text": "<details> <summary>What is Simply Notification Service?</summary><br><b> AWS definition: \"a highly available, durable, secure, fully managed pub/sub messaging service that enables you to decouple microservices, distributed systems, and serverless applications.\" Read more about it [here](https://aws.amazon.com/sns) </b></details> <details> <summary>Explain the following in regards to SNS: - Topics - Subscribers - Publishers</summary><br><b> * Topics - used for grouping multiple endpoints * Subscribers - the endpoints where topics send messages to * Publishers - the provider of the message (event, person, ...) </b></details> <details> <summary>How SNS is different from SQS?</summary><br><b> SNS, as opposed to SQS, works in a publisher/subscriber model. Where's SQS works in Producer/Consumer model. SQS delivers the message to one consumer where's SNS will send a message to multiple subscribers. </b></details> <details> <summary>What's a Fan-Out pattern?</summary><br><b> A messaging", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "SNS", "language": "en", "created_at": "2025-07-19T19:22:02.161665"}}
{"text": "pattern where a single message is send to multiple destinations (often simultaneously). So one-to-many broadcast message. </b></details>", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "SNS", "language": "en", "created_at": "2025-07-19T19:22:02.161684"}}
{"text": "<details> <summary>What is AWS CloudWatch?</summary><br><b> AWS definition: \"Amazon CloudWatch is a monitoring and observability service...\" More on CloudWatch [here](https://aws.amazon.com/cloudwatch) </b></details> <details> <summary>What is AWS CloudTrail?</summary><br><b> AWS definition: \"AWS CloudTrail is a service that enables governance, compliance, operational auditing, and risk auditing of your AWS account.\" Read more on CloudTrail [here](https://aws.amazon.com/cloudtrail) </b></details>", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "Monitoring and Logging", "language": "en", "created_at": "2025-07-19T19:22:02.161758"}}
{"text": "<details> <summary>What are Service Control Policies and to what service they belong?</summary><br><b> AWS organizations service and the definition by Amazon: \"SCPs offer central control over the maximum available permissions for all accounts in your organization, allowing you to ensure your accounts stay within your organization’s access control guidelines.\" Learn more [here](https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scp.html) </b></details> <details> <summary>Explain AWS pricing model</summary><br><b> It mainly works on \"pay-as-you-go\" meaning you pay only for what are using and when you are using it. In s3 you pay for 1. How much data you are storing 2. Making requests (PUT, POST, ...) In EC2 it's based on the purchasing option (on-demand, spot, ...), instance type, AMI type and the region used. More on AWS pricing model [here](https://aws.amazon.com/pricing) </b></details> <details> <summary>How do you estimate AWS costs?</summary><br><b> * TCO", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "Billing and Support", "language": "en", "created_at": "2025-07-19T19:22:02.162302"}}
{"text": "calculator * AWS simple calculator * Cost Explorer * AWS Budgets * Cost Allocation Tags </b></details> <details> <summary>What basic support in AWS includes?</summary><br><b> * 24x7 customer service * Trusted Advisor * AWS personal Health Dashoard </b></details> <details> <summary>How are EC2 instances billed?</summary><br><b> </b></details> <details> <summary>What AWS Pricing Calculator is used for?</summary><br><b> </b></details> <details> <summary>What is Amazon Connect?</summary><br><b> Amazon definition: \"Amazon Connect is an easy to use omnichannel cloud contact center that helps companies provide superior customer service at a lower cost.\" Learn more [here](https://aws.amazon.com/connect) </b></details> <details> <summary>What are \"APN Consulting Partners\"?</summary><br><b> Amazon definition: \"APN Consulting Partners are professional services firms that help customers of all types and sizes design, architect, build, migrate, and manage their workloads and applications on AWS,", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "Billing and Support", "language": "en", "created_at": "2025-07-19T19:22:02.162328"}}
{"text": "accelerating their journey to the cloud.\" Learn more [here](https://aws.amazon.com/partners/consulting) </b></details> <details> <summary>Which of the following are AWS accounts types (and are sorted by order)? - Basic, Developer, Business, Enterprise - Newbie, Intermediate, Pro, Enterprise - Developer, Basic, Business, Enterprise - Beginner, Pro, Intermediate Enterprise </summary><br><b> - Basic, Developer, Business, Enterprise </b></details> <details> <summary>True or False? Region is a factor when it comes to EC2 costs/pricing</summary><br><b> True. You pay differently based on the chosen region. </b></details> <details> <summary>What is \"AWS Infrastructure Event Management\"?</summary><br><b> AWS Definition: \"AWS Infrastructure Event Management is a structured program available to Enterprise Support customers (and Business Support customers for an additional fee) that helps you plan for large-scale events such as product or application launches, infrastructure migrations, and", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "Billing and Support", "language": "en", "created_at": "2025-07-19T19:22:02.162357"}}
{"text": "marketing events.\" </b></details>", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "Billing and Support", "language": "en", "created_at": "2025-07-19T19:22:02.162669"}}
{"text": "<details> <summary>What is \"AWS Organizations\"?</summary><br><b> AWS definition: \"AWS Organizations helps you centrally govern your environment as you grow and scale your workloads on AWS.\" Read more on Organizations [here](https://aws.amazon.com/organizations) </b></details> <details> <summary>What's an OU in regards to AWS Organizations?'</summary><br><b> OU (Organizational Units) is a way to group multiple accounts together so you can treat them as a single unit. By default there is the \"Root\" OU created in AWS Organizations. Most of the time OUs are based on functions or common set of controls. </b></details>", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "AWS Organizations", "language": "en", "created_at": "2025-07-19T19:22:02.162794"}}
{"text": "<details> <summary>What is AWS CodeDeploy?</summary><br><b> Amazon definition: \"AWS CodeDeploy is a fully managed deployment service that automates software deployments to a variety of compute services such as Amazon EC2, AWS Fargate, AWS Lambda, and your on-premises servers.\" Learn more [here](https://aws.amazon.com/codedeploy) </b></details> <details> <summary>Explain what is CloudFormation</summary><br><b> AWS definition: \"AWS CloudFormation is a service that helps you model and set up your Amazon Web Services resources so that you can spend less time managing those resources and more time focusing on your applications that run in AWS. You create a template that describes all the AWS resources that you want (like Amazon EC2 instances or Amazon RDS DB instances), and CloudFormation takes care of provisioning and configuring those resources for you.\" </b></details> <details> <summary>What is AWS CDK?</summary><br><b> AWS definition: \"The AWS Cloud Development Kit (AWS CDK) is an", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "Automation", "language": "en", "created_at": "2025-07-19T19:22:02.163025"}}
{"text": "open-source software development framework to define cloud infrastructure as code and provision it through AWS CloudFormation. CDK gives the flexibility to use popular programming languages like TypeScript, JavaScript, Python, Java, C# and Go (in Developer Preview) to define your infrastructure, and AWS CDK provides a set of libraries for AWS services that abstract away the need to write raw CloudFormation templates. Learn more [here](https://aws.amazon.com/cdk) </b></details>", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "Automation", "language": "en", "created_at": "2025-07-19T19:22:02.163046"}}
{"text": "<details> <summary>Which AWS service you have experience with that you think is not very common?</summary><br><b> </b></details> <details> <summary>What is AWS CloudSearch?</summary><br><b> </b></details> <details> <summary>What is AWS Lightsail?</summary><br><b> AWS definition: \"Lightsail is an easy-to-use cloud platform that offers you everything needed to build an application or website, plus a cost-effective, monthly plan.\" </b></details> <details> <summary>What is AWS Rekognition?</summary><br><b> AWS definition: \"Amazon Rekognition makes it easy to add image and video analysis to your applications using proven, highly scalable, deep learning technology that requires no machine learning expertise to use.\" Learn more [here](https://aws.amazon.com/rekognition) </b></details> <details> <summary>What AWS Resource Groups used for?</summary><br><b> Amazon definition: \"You can use resource groups to organize your AWS resources. Resource groups make it easier to manage and automate tasks", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "Misc", "language": "en", "created_at": "2025-07-19T19:22:02.163979"}}
{"text": "on large numbers of resources at one time. \" Learn more [here](https://docs.aws.amazon.com/ARG/latest/userguide/welcome.html) </b></details> <details> <summary>What is AWS Global Accelerator?</summary><br><b> Amazon definition: \"AWS Global Accelerator is a service that improves the availability and performance of your applications with local or global users...\" Learn more [here](https://aws.amazon.com/global-accelerator) </b></details> <details> <summary>What is AWS Config?</summary><br><b> Amazon definition: \"AWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources.\" Learn more [here](https://aws.amazon.com/config) </b></details> <details> <summary>What is AWS X-Ray?</summary><br><b> AWS definition: \"AWS X-Ray helps developers analyze and debug production, distributed applications, such as those built using a microservices architecture.\" Learn more [here](https://aws.amazon.com/xray) </b></details> <details> <summary>What is AWS", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "Misc", "language": "en", "created_at": "2025-07-19T19:22:02.164008"}}
{"text": "OpsWorks?</summary><br><b> Amazon definition: \"AWS OpsWorks is a configuration management service that provides managed instances of Chef and Puppet.\" Learn more about it [here](https://aws.amazon.com/opsworks) </b></details> <details> <summary>What is AWS Snowmobile?</summary><br><b> \"AWS Snowmobile is an Exabyte-scale data transfer service used to move extremely large amounts of data to AWS.\" Learn more [here](https://aws.amazon.com/snowmobile) </b></details> <details> <summary>What is AWS Athena?</summary><br><b> \"Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL.\" Learn more about AWS Athena [here](https://aws.amazon.com/athena) </b></details> <details> <summary>What is Amazon Cloud Directory?</summary><br><b> Amazon definition: \"Amazon Cloud Directory is a highly available multi-tenant directory-based store in AWS. These directories scale automatically to hundreds of millions of objects as needed for applications.\"", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "Misc", "language": "en", "created_at": "2025-07-19T19:22:02.164028"}}
{"text": "Learn more [here](https://docs.aws.amazon.com/clouddirectory/latest/developerguide/what_is_cloud_directory.html) </b></details> <details> <summary>What is AWS Elastic Beanstalk?</summary><br><b> AWS definition: \"AWS Elastic Beanstalk is an easy-to-use service for deploying and scaling web applications and services...You can simply upload your code and Elastic Beanstalk automatically handles the deployment\" Learn more about it [here](https://aws.amazon.com/elasticbeanstalk) </b></details> <details> <summary>What is AWS SWF?</summary><br><b> Amazon definition: \"Amazon SWF helps developers build, run, and scale background jobs that have parallel or sequential steps. You can think of Amazon SWF as a fully-managed state tracker and task coordinator in the Cloud.\" Learn more on Amazon Simple Workflow Service [here](https://aws.amazon.com/swf) </b></details> <details> <summary>What is AWS EMR?</summary><br><b> AWS definition: \"big data platform for processing vast amounts of data using open", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "Misc", "language": "en", "created_at": "2025-07-19T19:22:02.164047"}}
{"text": "source tools such as Apache Spark, Apache Hive, Apache HBase, Apache Flink, Apache Hudi, and Presto.\" Learn more [here](https://aws.amazon.com/emr) </b></details> <details> <summary>What is AWS Quick Starts?</summary><br><b> AWS definition: \"Quick Starts are built by AWS solutions architects and partners to help you deploy popular technologies on AWS, based on AWS best practices for security and high availability.\" Read more [here](https://aws.amazon.com/quickstart) </b></details> <details> <summary>What is the Trusted Advisor?</summary><br><b> Amazon definition: \"AWS Trusted Advisor provides recommendations that help you follow AWS best practices. Trusted Advisor evaluates your account by using checks. These checks identify ways to optimize your AWS infrastructure, improve security and performance, reduce costs, and monitor service quotas.\" Learn more [here](https://aws.amazon.com/premiumsupport/technology/trusted-advisor/) </b></details> <details> <summary>What is AWS Service", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "Misc", "language": "en", "created_at": "2025-07-19T19:22:02.164065"}}
{"text": "Catalog?</summary><br><b> Amazon definition: \"AWS Service Catalog allows organizations to create and manage catalogs of IT services that are approved for use on AWS.\" Learn more [here](https://aws.amazon.com/servicecatalog) </b></details> <details> <summary>What is AWS CAF?</summary><br><b> Amazon definition: \"AWS Professional Services created the AWS Cloud Adoption Framework (AWS CAF) to help organizations design and travel an accelerated path to successful cloud adoption. \" Learn more [here](https://aws.amazon.com/professional-services/CAF) </b></details> <details> <summary>What is AWS Cloud9?</summary><br><b> AWS: \"AWS Cloud9 is a cloud-based integrated development environment (IDE) that lets you write, run, and debug your code with just a browser\" </b></details> <details> <summary>What is AWS CloudShell?</summary><br><b> AWS: \"AWS CloudShell is a browser-based shell that makes it easy to securely manage, explore, and interact with your AWS resources.\" </b></details> <details>", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "Misc", "language": "en", "created_at": "2025-07-19T19:22:02.164292"}}
{"text": "<summary>What is AWS Application Discovery Service?</summary><br><b> Amazon definition: \"AWS Application Discovery Service helps enterprise customers plan migration projects by gathering information about their on-premises data centers.\" Learn more [here](https://aws.amazon.com/application-discovery) </b></details> <details> <summary>What is the AWS well-architected framework and what pillars it's based on?</summary><br><b> AWS definition: \"The Well-Architected Framework has been developed to help cloud architects build secure, high-performing, resilient, and efficient infrastructure for their applications. Based on five pillars — operational excellence, security, reliability, performance efficiency, and cost optimization\" Learn more [here](https://aws.amazon.com/architecture/well-architected) </b></details> <details> <summary>What AWS services are serverless (or have the option to be serverless)?</summary><br><b> AWS Lambda AWS Athena </b></details>", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "Misc", "language": "en", "created_at": "2025-07-19T19:22:02.164324"}}
{"text": "<details> <summary>What high availability means from AWS perspective?</summary><br><b> * Application/Service is running in at least 2 availability zones * Application/Service should survive (= operate as usual) a data center disaster </b></details>", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "High Availability", "language": "en", "created_at": "2025-07-19T19:22:02.164373"}}
{"text": "<details> <summary>Describe in high-level how to upgrade a system on AWS with (near) zero downtime</summary><br><b> One way is through launching a new instance. In more detail: 1. Launch a new instance 2. Install all the updates and applications 3. Test the instance 4. If all tests passed successfully, you can start using the new instance and perform the switch with the old one, in one of various ways: 1. Go to route53 and update the record with the IP of the new instance 2. If you are using an Elastic IP then move it to the new instance ... </b></details> <details> <summary>You try to use an detached EBS volume from us-east-1b in us-east-1a, but it fails. What might be the reason?</summary><br><b> EBS volumes are locked to a specific availability zone. To use them in another availability zone, you need to take a snapshot and restore it in the destination availability zone. </b></details> <details> <summary>When you launch EC2 instances, it takes them time to boot due to commands you", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "Production Operations and Migrations", "language": "en", "created_at": "2025-07-19T19:22:02.164903"}}
{"text": "run with user data. How to improve instances boot time?</summary><br><b> Consider creating customized AMI with the commands from user data already executed there. This will allow you launch instance instantly. </b></details> <details> <summary>You try to mount EFS on your EC2 instance and it doesn't work (hangs...) What might be a possible reason?</summary><br><b> Security group isn't attached to your EFS or it lacks a rule to allow NFS traffic. </b></details> <details> <summary>How to migrate an EBS volume across availability zones?</summary><br><b> 1. Pause the application 2. Take a snapshot of the EBS volume 3. Restore the snapshot in another availability zone </b></details> <details> <summary>How to encrypt an unencrypted EBS volume attached to an EC2 instance?</summary><br><b> 1. Create EBS snapshot of the volume 2. Copy the snapshot and mark the \"Encrypt\" option 3. Create a new EBS volume out of the encrypted snapshot </b></details> <details> <summary>You've created a network", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "Production Operations and Migrations", "language": "en", "created_at": "2025-07-19T19:22:02.164932"}}
{"text": "load balancer but it doesn't work (you can't reach your app on your EC2 instance). What might be a possible reason?</summary><br><b> Missing security group or misconfigured one. For example, if you go to your instances in the AWS console you might see that the instances under your NLB are in \"unhealthy status\" and if you didn't create a dedicated security group for your NLB, that means that the security group used is the one attached to the EC2 instances. Go to the security group of your instance(s) and enable the traffic that NLB should forward (e.g. TCP on port 80). </b></details>", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "Production Operations and Migrations", "language": "en", "created_at": "2025-07-19T19:22:02.164953"}}
{"text": "<details> <summary>You have a load balancer running and behind it 5 web servers. Users complain that every time they move to a new page, they have to authenticate, instead of doing it once. How can you solve it?</summary><br><b> Enable sticky sessions. This way, the user keep working against the same instance, instead of being redirected to a different instance every request. </b></details> <details> <summary>You have a load balancer running and behind it 5 web servers. Users complain that some times when they try to use the application it doesn't works. You've found out that sometimes some of the instances crash. How would you deal with it?</summary><br><b> One possible way is to use health checks with the load balancer to ensure the instances are ready to be used before forwarding traffic to them. </b></details> <details> <summary>You run your application on 5 EC2 instances on one AZ and on 10 EC2 instances in another AZ. You distribute traffic between all of them using a network", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "Scenarios", "language": "en", "created_at": "2025-07-19T19:22:02.165596"}}
{"text": "load balancer, but it seems that instances in one AZ have higher CPU rates than the instances in the other AZ. What might be the issue and how to solve it?</summary><br><b> It's possible that traffic is distributed evenly between the AZs but that doesn't mean it's distributed equally across all instances evenly. To distribute it evenly between all the instances, you have to enable cross-zone load balancing. </b></details> <details> <summary>You are running an ALB that routes traffic using two hostnames: a.b.com and d.e.com. Is it possible to configure HTTPS for both of the hostnames?</summary><br><b> Yes, using SNI (Server Name Indication) each application can has its own SSL certificate (This is supported from 2017). </b></details> <details> <summary>You have set up read replicas to scale reads but users complain that when they update posts in forums, the posts are not being updated. What may cause this issue?</summary><br><b> Read Replicas use asynchronous replication so it's", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "Scenarios", "language": "en", "created_at": "2025-07-19T19:22:02.165618"}}
{"text": "possible users access a read replica instance that wasn't synced yet. </b></details> <details> <summary>You need a persistent shared storage between your containers that some are running in Fargate and some in ECS. What would you use?</summary><br><b> EFS. It allows us to have persistent multi-AZ shared storage for containers. </b></details> <details> <summary>You would like to run an AWS Fargate task every time a file is uploaded to a certain S3 bucket. How would you achieve that?</summary><br><b> Use Amazon EventBridge so every time a file is uploaded to an S3 bucket (event) it will run an ECS task. Such task should have an ECS Task Role so it can get the object from the S3 bucket (and possibly other permissions if it needs to update the DB for example). </b></details> <details> <summary>Your hosts scale down and then back up quite often. What's your take on that? </summary><br><b> Often circular scaling (scale down, up and vice versa) is not a sign that the threshold set for scaling", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "Scenarios", "language": "en", "created_at": "2025-07-19T19:22:02.165814"}}
{"text": "down and up are met quite often. In most cases that's a sign for you to adjust the threshold so scaling down doesn't happen as often. </b></details>", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "Scenarios", "language": "en", "created_at": "2025-07-19T19:22:02.165846"}}
{"text": "<details> <summary>You've been asked to design an architecture for high performance and low-latency application (millions of requests per second). Which load balancer would you use?</summary><br><b> Network Load Balancer </b></details> <details> <summary>What should you use for scaling reads?</summary><br><b> You can use an ElastiCache cluster or RDS Read Replicas. </b></details> <details> <summary>You have two applications who communicate synchronously. It worked fine until there suddenly a spike of traffic. What change you might apply in this case?</summary><br><b> More details are missing to determine for sure but it might be better to decouple the applications by introducing one of the following: * Queue model with SQS * Publisher/Subscriber model with SNS </b></details>", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "Architecture Design", "language": "en", "created_at": "2025-07-19T19:22:02.165966"}}
{"text": "<details> <summary>What's an ARN?</summary><br><b> ARN (Amazon Resources Names) are used for uniquely identifying different AWS resources. It is used when you would like to identify resource uniqely across all AWS infrastructures. </b></details>", "metadata": {"source_file": "learning-materials/topics/aws/README.md", "section": "Misc", "language": "en", "created_at": "2025-07-19T19:22:02.166001"}}
{"text": "Setup a cost budget in your AWS account based on your needs.", "metadata": {"source_file": "learning-materials/topics/aws/exercises/budget_setup/solution.md", "section": "Objectives", "language": "en", "created_at": "2025-07-19T19:22:02.166628"}}
{"text": "1. Go to \"Billing\" 2. Click on \"Budgets\" in the menu 3. Click on \"Create a budget\" 4. Choose \"Cost Budget\" and click on \"Next\" 5. Choose the values that work for you. For example, recurring monthly budget with a specific amount 6. Insert a budget name and Click on \"Next\" 7. Set up an alert but clicking on \"Add an alert threshold\" 1. Set a threshold (e.g. 75% of budgeted amount) 2. Set an email where a notification will be sent 8. Click on \"Next\" until you can click on \"Create a budget\"", "metadata": {"source_file": "learning-materials/topics/aws/exercises/budget_setup/solution.md", "section": "Solution", "language": "en", "created_at": "2025-07-19T19:22:02.166774"}}
{"text": "Explain what might be possible reasons for the following issues: 1. Getting \"time out\" when trying to reach an application running on EC2 instance 2. Getting \"connection refused\" error", "metadata": {"source_file": "learning-materials/topics/aws/exercises/no_application/solution.md", "section": "Objectives", "language": "en", "created_at": "2025-07-19T19:22:02.166963"}}
{"text": "1. 'Time out' Can be due to one of the following: * Security group doesn't allow access * No host (yes, I know. Not the first thing to check and yet...) * Operating system firewall blocking traffic 2. 'Connection refused' can happen due to one of the following: * Application didn't launch properly or has some issue (doesn't listens on the designated port) * Firewall replied with a reject instead of dropping the packets", "metadata": {"source_file": "learning-materials/topics/aws/exercises/no_application/solution.md", "section": "Solution", "language": "en", "created_at": "2025-07-19T19:22:02.167041"}}
{"text": "Two running EC2 instances", "metadata": {"source_file": "learning-materials/topics/aws/exercises/network_load_balancer/solution.md", "section": "Requirements", "language": "en", "created_at": "2025-07-19T19:22:02.167199"}}
{"text": "1. Create a network load balancer 1. healthy threshold: 3 2. unhealthy threshold: 3 3. interval: 10 seconds 4. Listener should be using TCP protocol on port 80", "metadata": {"source_file": "learning-materials/topics/aws/exercises/network_load_balancer/solution.md", "section": "Objectives", "language": "en", "created_at": "2025-07-19T19:22:02.167231"}}
{"text": "1. Go to EC2 service 2. Click in the left side menu on \"Load balancers\" under \"Load balancing\" 3. Click on \"Create load balancer\" 4. Choose \"Network Load Balancer\" 5. Insert a name for the LB 6. Choose AZs where you want the LB to operate 7. Choose a security group 8. Under \"Listeners and routing\" click on \"Create target group\" and choose \"Instances\" 1. Provide a name for the target group 2. Set healthy threshold to 3 3. Set unhealthy threshold to 3 4. Set interval to 10 seconds 5. Set protocol to TCP and port to 80 6. Click on \"Next\" and choose two instances you have 7. Click on \"Create target group\" 9. Refresh target groups and choose the one you've just created 10. Click on \"Create load balancer\" and wait for it to be provisioned", "metadata": {"source_file": "learning-materials/topics/aws/exercises/network_load_balancer/solution.md", "section": "Console", "language": "en", "created_at": "2025-07-19T19:22:02.167409"}}
{"text": "1. Create a MySQL database with the following properties * Instance type: db.t2.micro * gp2 storage * Storage Auto scaling should be enabled and threshold should be set to 500 GiB * Public access should be enabled * Port should be set to 3306 * DB name: 'db' * Backup retention: 10 days 2. Create read replica for the database you've created", "metadata": {"source_file": "learning-materials/topics/aws/exercises/mysql_db/solution.md", "section": "Objectives", "language": "en", "created_at": "2025-07-19T19:22:02.167699"}}
{"text": "1. Go to RDS service 2. Click on \"Databases\" in the left side menu and click on the \"Create database\" button 3. Choose \"standard create\" 4. Choose \"MySQL\" and the recommended version 5. Choose \"Production\" template 6. Specify DB instance identifier 7. Specify Credentials (master username and password) 8. Choose DB instance type: Burstable classes, db.t2.micro 9. Choose \"gp2\" as storage 10. Enable storage autoscalling: maximum storage threshold of 500 GiB 11. Choose \"Do not create a standby instance\" 12. Choose a default VPC and subnet 12. Check \"Yes\" for public access 13. Choose \"No preference\" for AZ 14. Database port should be 3306 15. For authentication, choose \"Password and IAM database authentication\" 16. Set initial database name as \"db\" 17. Increase backup retention period to 10 days 18. Click on \"Create database\" button 1. Go to the database under \"Databases\" in the left side menu 2. Click on \"Actions\" -> Create read replica 3. Click on \"Create read replica\"", "metadata": {"source_file": "learning-materials/topics/aws/exercises/mysql_db/solution.md", "section": "Console", "language": "en", "created_at": "2025-07-19T19:22:02.167961"}}
{"text": "A. Create two Spot instances using a Spot Request with the following properties: * Amazon Linux 2 AMI * 2 instances as target capacity (at any given point of time) while each one has 2 vCPUs and 3 GiB RAM B. Create a single Spot instance using Amazon Linux 2 and t2.micro", "metadata": {"source_file": "learning-materials/topics/aws/exercises/create_spot_instances/solution.md", "section": "Objectives", "language": "en", "created_at": "2025-07-19T19:22:02.168210"}}
{"text": "A. Create Spot Fleets: 1. Go to EC2 service 2. Click on \"Spot Requests\" 3. Click on \"Request Spot Instances\" button 4. Set the following values for parameters: * Amazon Linux 2 AMI * Total target capacity -> 2 * Check \"Maintain target capacity\" * vCPUs: 2 * Memory: 3 GiB RAM 5. Click on Launch B. Create a single Spot instance: 1. Go to EC2 service 2. Click on \"Instances\" 3. Click on \"Launch Instances\" 4. Choose \"Amazon Linux 2 AMI\" and click on \"Next\" 5. Choose t2.micro and click on \"Next: Configure Instance Details\" 6. Select \"Request Spot instances\" 7. Set Maximum price above current price 8. Click on \"Review and Launch\"", "metadata": {"source_file": "learning-materials/topics/aws/exercises/create_spot_instances/solution.md", "section": "Solution", "language": "en", "created_at": "2025-07-19T19:22:02.168505"}}
{"text": "Create a basic AWS Lambda function that will be triggered when you enter a URL in the browser", "metadata": {"source_file": "learning-materials/topics/aws/exercises/url_function/solution.md", "section": "URL Function", "language": "en", "created_at": "2025-07-19T19:22:02.168803"}}
{"text": "1. Go to Lambda console panel and click on `Create function` 1. Give the function a name like `urlFunction` 2. Select `Python3` runtime 3. Now to handle function's permissions, we can attach IAM role to our function either by setting a role or creating a new role. I selected \"Create a new role from AWS policy templates\" 4. In \"Policy Templates\" select \"Simple Microservice Permissions\" 1. Next, you should see a text editor where you will insert a code similar to the following", "metadata": {"source_file": "learning-materials/topics/aws/exercises/url_function/solution.md", "section": "Define a function", "language": "en", "created_at": "2025-07-19T19:22:02.168894"}}
{"text": "``` import json def lambda_handler(event, context): firstName = event['name'] return 'Hello ' + firstName ``` 2. Click on \"Create Function\"", "metadata": {"source_file": "learning-materials/topics/aws/exercises/url_function/solution.md", "section": "Function's code", "language": "en", "created_at": "2025-07-19T19:22:02.168923"}}
{"text": "1. Now let's test the function. Click on \"Test\". 2. Select \"Create new test event\" 3. Set the \"Event name\" to whatever you'd like. For example \"TestEvent\" 4. Provide keys to test ``` { \"name\": 'Spyro' } ``` 5. Click on \"Create\"", "metadata": {"source_file": "learning-materials/topics/aws/exercises/url_function/solution.md", "section": "Define a test", "language": "en", "created_at": "2025-07-19T19:22:02.168958"}}
{"text": "1. Choose the test event you've create (`TestEvent`) 2. Click on the `Test` button 3. You should see something similar to `Execution result: succeeded` 4. If you'll go to AWS CloudWatch, you should see a related log stream", "metadata": {"source_file": "learning-materials/topics/aws/exercises/url_function/solution.md", "section": "Test the function", "language": "en", "created_at": "2025-07-19T19:22:02.168992"}}
{"text": "We'll define a trigger in order to trigger the function when inserting the URL in the browser 1. Go to \"API Gateway console\" and click on \"New API Option\" 2. Insert the API name, description and click on \"Create\" 3. Click on Action -> Create Resource 4. Insert resource name and path (e.g. the path can be /hello) and click on \"Create Resource\" 5. Select the resource we've created and click on \"Create Method\" 6. For \"integration type\" choose \"Lambda Function\" and insert the lambda function name we've given to the function we previously created. Make sure to also use the same region 7. Confirm settings and any required permissions 8. Now click again on the resource and modify \"Body Mapping Templates\" so the template includes this: ``` { \"name\": \"$input.params('name')\" } ``` 9. Finally save and click on Actions -> Deploy API", "metadata": {"source_file": "learning-materials/topics/aws/exercises/url_function/solution.md", "section": "Define a trigger", "language": "en", "created_at": "2025-07-19T19:22:02.169168"}}
{"text": "1. In the API Gateway console, in stages menu, select the API we've created and click on the GET option 2. You'll see an invoke URL you can click on. You might have to modify it to include the input so it looks similar to this: `.../hello?name=mario` 3. You should see in your browser `Hello Mario`", "metadata": {"source_file": "learning-materials/topics/aws/exercises/url_function/solution.md", "section": "Running the function", "language": "en", "created_at": "2025-07-19T19:22:02.169223"}}
{"text": "Click [here to view to solution](solution.md)", "metadata": {"source_file": "learning-materials/topics/aws/exercises/url_function/exercise.md", "section": "Solution", "language": "en", "created_at": "2025-07-19T19:22:02.169303"}}
{"text": "Note: registering domain costs money. Don't do this exercise, unless you understand that you are going to register a domain and it's going to cost you money. 1. Register your own custom domain using AWS Route 53 2. What is the type of your domain? 3. How many records your domain has?", "metadata": {"source_file": "learning-materials/topics/aws/exercises/register_domain/solution.md", "section": "Objectives", "language": "en", "created_at": "2025-07-19T19:22:02.169458"}}
{"text": "1. Go to Route 53 service page 2. Click in the menu on \"Registered Domains\" under \"Domains\" 3. Click on \"Register Domain\" 4. Insert your domain 5. Check if it's available. If it is, add it to the cart Note: registering domain costs money. Don't click on \"continue\", unless you understand that you are going to register a domain and it's going to cost you money. 6. Click on \"Continue\" and fill in your contact information 7. Choose if you want to renew it in the future automatically. Accept the terms and click on \"Complete Order\" 8. Go to hosted zones and you should see there your newly registered domain 1. The domain type is \"Public\" 1. The domain has 2 DNS records: NS and SOA", "metadata": {"source_file": "learning-materials/topics/aws/exercises/register_domain/solution.md", "section": "Solution", "language": "en", "created_at": "2025-07-19T19:22:02.169609"}}
{"text": "At least one registered domain", "metadata": {"source_file": "learning-materials/topics/aws/exercises/creating_records/solution.md", "section": "Requirements", "language": "en", "created_at": "2025-07-19T19:22:02.169827"}}
{"text": "1. Create the following record for your domain: 1. Record name: foo 2. Record type: A 3. Set some IP in the value field 2. Verify from the shell that you are able to use the record you've created to lookup for the IP address by using the domain name", "metadata": {"source_file": "learning-materials/topics/aws/exercises/creating_records/solution.md", "section": "Objectives", "language": "en", "created_at": "2025-07-19T19:22:02.169877"}}
{"text": "1. Go to Route 53 service -> Hosted zones 2. Click on your domain name 3. Click on \"Create record\" 4. Insert \"foo\" in \"Record name\" 5. Set \"Record type\" to A 6. In \"Value\" insert \"201.7.20.22\" 7. Click on \"Create records\" 1. In your shell, type `nslookup foo.<YOUR DOMAIN>` or `dig foo.<YOUR NAME`", "metadata": {"source_file": "learning-materials/topics/aws/exercises/creating_records/solution.md", "section": "Solution", "language": "en", "created_at": "2025-07-19T19:22:02.169925"}}
{"text": "1. Create an instance that supports hibernation 2. Hibernate the instance 3. Start the instance 4. What way is there to prove that instance was hibernated from OS perspective?", "metadata": {"source_file": "learning-materials/topics/aws/exercises/hibernate_instance/solution.md", "section": "Objectives", "language": "en", "created_at": "2025-07-19T19:22:02.170104"}}
{"text": "1. Create an instance that supports hibernation 1. Go to EC2 service 2. Go to instances and create an instance 3. In \"Configure instance\" make sure to check \"Enable hibernation as an additional stop behavior\" 4. In \"Add storage\", make sure to encrypt EBS and make sure the size > instance RAM size (because hibernation saves the RAM state) 5. Review and Launch 2. Hibernate the instance 1. Go to the instance page 2. Click on \"Instance state\" -> \"Hibernate instance\" -> Hibernate 3. Instance state -> Start 4. Run the \"uptime\" command, which will display the amount of time the system was up", "metadata": {"source_file": "learning-materials/topics/aws/exercises/hibernate_instance/solution.md", "section": "Solution", "language": "en", "created_at": "2025-07-19T19:22:02.170216"}}
{"text": "Initialize a CDK project and set up files required to build a CDK project.", "metadata": {"source_file": "learning-materials/topics/aws/exercises/sample_cdk/solution.md", "section": "Exercise", "language": "en", "created_at": "2025-07-19T19:22:02.170562"}}
{"text": "1. Install CDK on your machine by running `npm install -g aws-cdk`. 2. Create a new directory named `sample` for your project and run `cdk init app --language typescript` to initialize a CDK project. You can choose language as csharp, fsharp, go, java, javascript, python or typescript. 3. You would see the following files created in your directory: 1. `cdk.json`, `tsconfig.json`, `package.json` - These are configuration files that are used to define some global settings for your CDK project. 2. `bin/sample.ts` - This is the entry point for your CDK project. This file is used to define the stack that you want to create. 3. `lib/sample-stack.ts` - This is the main file that will contain the code for your CDK project.", "metadata": {"source_file": "learning-materials/topics/aws/exercises/sample_cdk/solution.md", "section": "Initialize a CDK project", "language": "en", "created_at": "2025-07-19T19:22:02.170756"}}
{"text": "1. In `lib/sample-stack.ts` file, add the following code to create a lambda function: ```typescript import * as cdk from 'aws-cdk-lib'; import * as lambda from 'aws-cdk-lib/aws-lambda'; import { Construct } from 'constructs'; export class SampleStack extends cdk.Stack { constructor(scope: Construct, id: string, props?: cdk.StackProps) { super(scope, id, props); const hello = new lambda.Function(this, 'SampleLambda', { runtime: lambda.Runtime.NODEJS_14_X, code: lambda.Code.fromInline('exports.handler = async () => \"hello world\";'), handler: 'index.handler' }); } } ``` This will create a sample lambda function that returns \"hello world\" when invoked.", "metadata": {"source_file": "learning-materials/topics/aws/exercises/sample_cdk/solution.md", "section": "Create a Sample lambda function", "language": "en", "created_at": "2025-07-19T19:22:02.170861"}}
{"text": "Before you deploy your project. You need to bootstrap your project. This will create a CloudFormation stack that will be used to deploy your project. You can bootstrap your project by running `cdk bootstrap`. Learn more about bootstrapping [here](https://docs.aws.amazon.com/cdk/latest/guide/bootstrapping.html).", "metadata": {"source_file": "learning-materials/topics/aws/exercises/sample_cdk/solution.md", "section": "Bootstrap the CDK project", "language": "en", "created_at": "2025-07-19T19:22:02.170902"}}
{"text": "1. Run `npm install` to install all the dependencies for your project whenever you make changes. 2. Run `cdk synth` to synthesize the CloudFormation template for your project. You will see a new file called `cdk.out/CDKToolkit.template.json` that contains the CloudFormation template for your project. 3. Run `cdk diff` to see the changes that will be made to your AWS account. You will see a new stack called `SampleStack` that will create a lambda function and all the changes associated with it. 4. Run `cdk deploy` to deploy your project. You should see a new stack called created in your AWS account under CloudFormation. 5. Go to Lambda console and you will see a new lambda function called `SampleLambda` created in your account.", "metadata": {"source_file": "learning-materials/topics/aws/exercises/sample_cdk/solution.md", "section": "Deploy the Project", "language": "en", "created_at": "2025-07-19T19:22:02.171045"}}
{"text": "Create a basic AWS Lambda function that when given a name, will return \"Hello <NAME>\"", "metadata": {"source_file": "learning-materials/topics/aws/exercises/hello_function/solution.md", "section": "Exercise", "language": "en", "created_at": "2025-07-19T19:22:02.171234"}}
{"text": "1. Go to Lambda console panel and click on `Create function` 1. Give the function a name like `BasicFunction` 2. Select `Python3` runtime 3. Now to handle function's permissions, we can attach IAM role to our function either by setting a role or creating a new role. I selected \"Create a new role from AWS policy templates\" 4. In \"Policy Templates\" select \"Simple Microservice Permissions\" 1. Next, you should see a text editor where you will insert a code similar to the following", "metadata": {"source_file": "learning-materials/topics/aws/exercises/hello_function/solution.md", "section": "Define a function", "language": "en", "created_at": "2025-07-19T19:22:02.171327"}}
{"text": "One EC2 instance that you can get rid of :)", "metadata": {"source_file": "learning-materials/topics/aws/exercises/ebs_volume_creation/solution.md", "section": "Requirements", "language": "en", "created_at": "2025-07-19T19:22:02.171604"}}
{"text": "1. Create a volume in the same AZ as your instance, with the following properties: 1. gp2 volume type 2. 4 GiB size 2. Once created, attach it to your EC2 instance 3. Remove your EC2 instance. What happened to the EBS volumes attached to the EC2 instance?", "metadata": {"source_file": "learning-materials/topics/aws/exercises/ebs_volume_creation/solution.md", "section": "Objectives", "language": "en", "created_at": "2025-07-19T19:22:02.171653"}}
{"text": "1. Go to EC2 service 2. Click on \"Volumes\" under \"Elastic Block Store\" 3. Click on \"Create Volume\" 4. Select the following properties 1. gp2 volume type 2. 4 GiB size 3. The same AZ as your instance 5. Click on \"Create volume\" 6. Right click on the volume you've just created -> attach volume -> choose your EC2 instance and click on \"Attach\" 7. Terminate your instance 8. The default EBS volume (created when you launched the instance for the first time) will be deleted (unless you didn't check \"Delete on termination\"), but the volume you've created as part of this exercise, will remain Note: don't forget to remove the EBS volume you've created in this exercise", "metadata": {"source_file": "learning-materials/topics/aws/exercises/ebs_volume_creation/solution.md", "section": "Solution", "language": "en", "created_at": "2025-07-19T19:22:02.171825"}}
{"text": "Create a basic role to provide EC2 service with Full IAM access permissions.<br> In the end, run from the CLI (or CloudShell) the command to verify the role was created.", "metadata": {"source_file": "learning-materials/topics/aws/exercises/create_role/solution.md", "section": "Objectives", "language": "en", "created_at": "2025-07-19T19:22:02.172031"}}
{"text": "1. Go to AWS console -> IAM 2. Click in the left side menu on \"Access Manamgement\" -> Roles 3. Click on \"Create role\" 3. Choose \"AWS service\" as the type of trusted entity and then choose \"EC2\" as a use case. Click on \"Next\" 4. In permissions page, check \"IAMFullAccess\" and click on \"Next\" until you get to \"Review\" page 5. In the \"Review\" page, give the role a name (e.g. IAMFullAcessEC2), provide a short description and click on \"Create role\" 6. `aws iam list-roles` will list all the roles in the account, including the one we've just created.", "metadata": {"source_file": "learning-materials/topics/aws/exercises/create_role/solution.md", "section": "Solution", "language": "en", "created_at": "2025-07-19T19:22:02.172150"}}
{"text": "1. Create a new VPC 1. It should have a CIDR that supports using at least 60,000 hosts 2. It should be named \"exercise-vpc\"", "metadata": {"source_file": "learning-materials/topics/aws/exercises/new_vpc/solution.md", "section": "Objectives", "language": "en", "created_at": "2025-07-19T19:22:02.172433"}}
{"text": "1. Under \"Virtual Private Cloud\" click on \"Your VPCs\" 2. Click on \"Create VPC\" 3. Insert a name - \"exercise-vpc\" 4. Insert IPv4 CIDR block: 10.0.0.0/16 5. Keep \"Tenancy\" at Default 6. Click on \"Create VPC\"", "metadata": {"source_file": "learning-materials/topics/aws/exercises/new_vpc/solution.md", "section": "Console", "language": "en", "created_at": "2025-07-19T19:22:02.172472"}}
{"text": "Click [here](terraform/main.tf) to view the solution", "metadata": {"source_file": "learning-materials/topics/aws/exercises/new_vpc/solution.md", "section": "Terraform", "language": "en", "created_at": "2025-07-19T19:22:02.172490"}}
{"text": "Click [here](pulumi/__main__.py) to view the solution", "metadata": {"source_file": "learning-materials/topics/aws/exercises/new_vpc/solution.md", "section": "Pulumi - Python", "language": "en", "created_at": "2025-07-19T19:22:02.172643"}}
{"text": "To verify you've create the VPC, you can run: `aws ec2 describe-vpcs -filters Name=tag:Name,Values=exercise-vpc`", "metadata": {"source_file": "learning-materials/topics/aws/exercises/new_vpc/solution.md", "section": "Verify Solution", "language": "en", "created_at": "2025-07-19T19:22:02.172671"}}
{"text": "1. Running EC2 instance without any IAM roles (so you if you connect the instance and try to run AWS commands, it fails) 2. IAM role with \"IAMReadOnlyAccess\" policy", "metadata": {"source_file": "learning-materials/topics/aws/exercises/ec2_iam_roles/solution.md", "section": "Requirements", "language": "en", "created_at": "2025-07-19T19:22:02.172953"}}
{"text": "1. Attach a role (and if such role doesn't exists, create it) with \"IAMReadOnlyAccess\" policy to the EC2 instance 2. Verify you can run AWS commands in the instance", "metadata": {"source_file": "learning-materials/topics/aws/exercises/ec2_iam_roles/solution.md", "section": "Objectives", "language": "en", "created_at": "2025-07-19T19:22:02.172988"}}
{"text": "1. Go to EC2 service 2. Click on the instance to which you would like to attach the IAM role 3. Click on \"Actions\" -> \"Security\" -> \"Modify IAM Role\" 4. Choose the IAM role with \"IAMReadOnlyAccess\" policy and click on \"Save\" 5. Running AWS commands now in the instance should work fine (e.g. `aws iam list-users`)", "metadata": {"source_file": "learning-materials/topics/aws/exercises/ec2_iam_roles/solution.md", "section": "Console", "language": "en", "created_at": "2025-07-19T19:22:02.173039"}}
{"text": "* An EC2 instance with network interface", "metadata": {"source_file": "learning-materials/topics/aws/exercises/elastic_network_interfaces/solution.md", "section": "Requirements", "language": "en", "created_at": "2025-07-19T19:22:02.173204"}}
{"text": "A. Create a network interface and attach it to the EC2 instance that already has one network interface B. Explain why would anyone use two network interfaces", "metadata": {"source_file": "learning-materials/topics/aws/exercises/elastic_network_interfaces/solution.md", "section": "Objectives", "language": "en", "created_at": "2025-07-19T19:22:02.173233"}}
{"text": "A. 1. Go to EC2 service 2. Click on \"Network Interfaces\" under \"Network & Security\" 3. Click on \"Create network interface\" 4. Provide a description 5. Choose a subnet (one that is in the AZ as the instance) 6. Optionally attach a security group and click on \"Create network interface\" 7. Click on \"Actions\" -> \"Attach\" and choose the instance to attach it to 8. If you go now to \"Instances\" page you'll see your instance has two network interfaces B. 1. You can move the second network interface between instances. This allows us to create kind of a failover mechanism between the instances.", "metadata": {"source_file": "learning-materials/topics/aws/exercises/elastic_network_interfaces/solution.md", "section": "Solution", "language": "en", "created_at": "2025-07-19T19:22:02.173343"}}
{"text": "Zero EC2 instances running", "metadata": {"source_file": "learning-materials/topics/aws/exercises/auto_scaling_groups_basics/solution.md", "section": "Requirements", "language": "en", "created_at": "2025-07-19T19:22:02.173522"}}
{"text": "A. Create a scaling group for web servers with the following properties: * Amazon Linux 2 AMI * t2.micro as the instance type * user data: ``` yum install -y httpd systemctl start httpd systemctl enable httpd ``` B. Were new instances created since you created the auto scaling group? How many? Why? C. Change desired capacity to 2. Did it launch more instances? D. Change back the desired capacity to 1. What is the result of this action?", "metadata": {"source_file": "learning-materials/topics/aws/exercises/auto_scaling_groups_basics/solution.md", "section": "Objectives", "language": "en", "created_at": "2025-07-19T19:22:02.173599"}}
{"text": "A. 1. Go to EC2 service 2. Click on \"Auto Scaling Groups\" under \"Auto Scaling\" 3. Click on \"Create Auto Scaling Group\" 4. Insert a name 5. Click on \"Create a launch template\" 1. Insert a name and a version for the template 2. Select an AMI to use (Amazon Linux 2) 3. Select t2.micro instance type 4. Select a key pair 5. Attach a security group 6. Under \"Advanced\" insert the user data 7. Click on \"Create\" 6. Choose the launch template you've just created and click on \"Next\" 7. Choose \"Adhere to launch template\" 8. Choose in which AZs to launch and click on \"Next\" 9. Link it to ALB (if you don't have one, create it) 10. Mark ELB health check in addition to EC2. Click on \"Next\" until you reach the review page and click on \"Create auto scaling group\" B. One instance was launched to met the criteria of the auto scaling group we've created. The reason it launched only one is due to \"Desired capacity\" set to 1. C. Change it by going to your auto scaling group -> Details -> Edit -> \"2 desired", "metadata": {"source_file": "learning-materials/topics/aws/exercises/auto_scaling_groups_basics/solution.md", "section": "Console", "language": "en", "created_at": "2025-07-19T19:22:02.173945"}}
{"text": "capacity\". This should create another instance if only one is running D. Reducing desired capacity back to 1 will terminate one of the instances (assuming 2 are running).", "metadata": {"source_file": "learning-materials/topics/aws/exercises/auto_scaling_groups_basics/solution.md", "section": "Console", "language": "en", "created_at": "2025-07-19T19:22:02.173990"}}
{"text": "Launch one EC2 instance with the following requirements: 1. Amazon Linux 2 image 2. Instance type: pick up one that has 1 vCPUs and 1 GiB memory 3. Instance storage should be deleted upon the termination of the instance 4. When the instance starts, it should install: 1. Install the httpd package 2. Start the httpd service 3. Make sure the content of /var/www/html/index.html is `I made it! This is is awesome!` 5. It should have the tag: \"Type: web\" and the name of the instance should be \"web-1\" 6. HTTP traffic (port 80) should be accepted from anywhere", "metadata": {"source_file": "learning-materials/topics/aws/exercises/launch_ec2_web_instance/solution.md", "section": "Objectives", "language": "en", "created_at": "2025-07-19T19:22:02.174342"}}
{"text": "1. Choose a region close to you 2. Go to EC2 service 3. Click on \"Instances\" in the menu and click on \"Launch instances\" 4. Choose image: Amazon Linux 2 5. Choose instance type: t2.micro 6. Make sure \"Delete on Termination\" is checked in the storage section 7. Under the \"User data\" field the following: ``` yum update -y yum install -y httpd systemctl start httpd systemctl enable httpd echo \"<h1>I made it! This is is awesome!</h1>\" > /var/www/html/index.html ``` 8. Add tags with the following keys and values: * key \"Type\" and the value \"web\" * key \"Name\" and the value \"web-1\" 9. In the security group section, add a rule to accept HTTP traffic (TCP) on port 80 from anywhere 10. Click on \"Review\" and then click on \"Launch\" after reviewing. 11. If you don't have a key pair, create one and download it. 12. ### Solution using Terraform ``` provider \"aws\" { region = \"us-east-1\" // Or your desired region } resource \"aws_instance\" \"web_server\" { ami = \"ami-12345678\" // Replace with the correct", "metadata": {"source_file": "learning-materials/topics/aws/exercises/launch_ec2_web_instance/solution.md", "section": "Solution", "language": "en", "created_at": "2025-07-19T19:22:02.174812"}}
{"text": "AMI for Amazon Linux 2 instance_type = \"t2.micro\" // Or any instance type with 1 vCPU and 1 GiB memory tags = { Name = \"web-1\" Type = \"web\" } root_block_device { volume_size = 8 // Or any desired size delete_on_termination = true } provisioner \"remote-exec\" { inline = [ \"sudo yum update -y\", \"sudo yum install -y httpd\", \"sudo systemctl start httpd\", \"sudo bash -c 'echo \\\"I made it! This is awesome!\\\" > /var/www/html/index.html'\", \"sudo systemctl enable httpd\" ] connection { type = \"ssh\" user = \"ec2-user\" private_key = file(\"~/.ssh/your_private_key.pem\") // Replace with the path to your private key host = self.public_ip } } security_group_ids = [aws_security_group.web_sg.id] } resource \"aws_security_group\" \"web_sg\" { name = \"web_sg\" description = \"Security group for web server\" ingress { from_port = 80 to_port = 80 protocol = \"tcp\" cidr_blocks = [\"0.0.0.0/0\"] } } ```", "metadata": {"source_file": "learning-materials/topics/aws/exercises/launch_ec2_web_instance/solution.md", "section": "Solution", "language": "en", "created_at": "2025-07-19T19:22:02.174845"}}
{"text": "1. Create a new S3 bucket 2. Add to the bucket index.html file and make it a static website 3. Create a GitHub repo and put the index.html there 4. Make sure to connect your AWS account to GitHub 5. Create a CI pipeline in AWS to publish the updated index.html from GitHub every time someone makes a change to the repo, to a specific branch", "metadata": {"source_file": "learning-materials/topics/aws/exercises/basic_s3_ci/solution.md", "section": "Objectives", "language": "en", "created_at": "2025-07-19T19:22:02.175338"}}
{"text": "1. Go to S3 service in AWS console 2. Insert bucket name and choose region 3. Uncheck \"block public access\" to make it public 4. Click on \"Create bucket\"", "metadata": {"source_file": "learning-materials/topics/aws/exercises/basic_s3_ci/solution.md", "section": "Create S3 bucket", "language": "en", "created_at": "2025-07-19T19:22:02.175390"}}
{"text": "1. Navigate to the newly created bucket and click on \"properties\" tab 2. Click on \"Edit\" in \"Static Website Hosting\" section 3. Check \"Enable\" for \"Static web hosting\" 4. Set \"index.html\" as index document and \"error.html\" as error document.", "metadata": {"source_file": "learning-materials/topics/aws/exercises/basic_s3_ci/solution.md", "section": "Static website hosting", "language": "en", "created_at": "2025-07-19T19:22:02.175426"}}
{"text": "1. Click on \"Permissions\" tab in the newly created S3 bucket 2. Click on Bucket Policy -> Edit -> Policy Generator. Click on \"Generate Policy\" for \"GetObject\" 3. Copy the generated policy and go to Permissions tab and replace it with the current policy", "metadata": {"source_file": "learning-materials/topics/aws/exercises/basic_s3_ci/solution.md", "section": "S3 bucket permissions", "language": "en", "created_at": "2025-07-19T19:22:02.175464"}}
{"text": "1. Go to Developers Tools Console and create a new connection (GitHub)", "metadata": {"source_file": "learning-materials/topics/aws/exercises/basic_s3_ci/solution.md", "section": "GitHub Source", "language": "en", "created_at": "2025-07-19T19:22:02.175483"}}
{"text": "1. Go to CodePipeline in AWS console 2. Click on \"Create Pipeline\" -> Insert a pipeline name -> Click on Next 3. Choose the newly created source (GitHub) under sources 4. Select repository name and branch name 5. Select \"AWS CodeBuild\" as build provider 6. Select \"Managed Image\", \"standard\" runtime and \"new service role\" 7. In deploy stage choose the newly created S3 bucket and for deploy provider choose \"Amazon S3\" 8. Review the pipeline and click on \"Create pipeline\"", "metadata": {"source_file": "learning-materials/topics/aws/exercises/basic_s3_ci/solution.md", "section": "Create a CI pipeline", "language": "en", "created_at": "2025-07-19T19:22:02.175607"}}
{"text": "1. Clone the project from GitHub 2. Make changes to index.html and commit them (git commit -a) 3. Push the new change, verify that the newly created AWS pipeline was triggered and check the content of the site", "metadata": {"source_file": "learning-materials/topics/aws/exercises/basic_s3_ci/solution.md", "section": "Test the pipeline", "language": "en", "created_at": "2025-07-19T19:22:02.175650"}}
{"text": "1. Existing Auto Scaling Group with maximum capacity set to at least 3 2. One running EC2 instance with max of 4 CPUs", "metadata": {"source_file": "learning-materials/topics/aws/exercises/asg_dynamic_scaling_policy/solution.md", "section": "Requirements", "language": "en", "created_at": "2025-07-19T19:22:02.175907"}}
{"text": "1. Create a dynamic scaling policy with the following properties 1. Track average CPU utilization 2. Target value should be 70% 2. Increase the CPU utilization to at least 70% 1. Do you see change in number of instances? 1. Decrease CPU utilization to less than 70% 1. Do you see change in number of instances?", "metadata": {"source_file": "learning-materials/topics/aws/exercises/asg_dynamic_scaling_policy/solution.md", "section": "Objectives", "language": "en", "created_at": "2025-07-19T19:22:02.175962"}}
{"text": "1. Go to EC2 service -> Auto Scaling Groups and click on the tab \"Automating scaling\" 2. Choose \"Target tracking scaling\" under \"Policy Type\" 3. Set metric type to Average CPU utilization 4. Set target value to 70% and click on \"Create\" 1. If you are using Amazon Linux 2, you can stress the instance with the following: ``` sudo amazon-linux-extras install epel -y sudo yum install stress -y stress -c 4 # assuming you have 4 CPUs ``` 2. Yes, additional EC2 instance was added 1. Simply stop the stress command 2. Yes, one of the EC2 instances was terminated", "metadata": {"source_file": "learning-materials/topics/aws/exercises/asg_dynamic_scaling_policy/solution.md", "section": "Console", "language": "en", "created_at": "2025-07-19T19:22:02.176070"}}
{"text": "Go to the Access Advisor and answer the following questions regarding one of the users: 1. Are there services this user never accessed? 2. What was the last service the user has accessed? 3. What the Access Advisor is used/good for?", "metadata": {"source_file": "learning-materials/topics/aws/exercises/access_advisor/solution.md", "section": "Objectives", "language": "en", "created_at": "2025-07-19T19:22:02.176288"}}
{"text": "1. Go to AWS IAM service and click on \"Users\" under \"Access Management\" 2. Click on one of the users 3. Click on the \"Access Advisor\" tab 4. Check which service was last accessed and which was never accessed Access Advisor can be good to evaluate whether there are services the user is not accessing (as in never or not frequently). This can be help in deciding whether some permissions should be revoked or modified.", "metadata": {"source_file": "learning-materials/topics/aws/exercises/access_advisor/solution.md", "section": "Solution", "language": "en", "created_at": "2025-07-19T19:22:02.176371"}}
{"text": "1. Create the following buckets: 1. Private bucket 1. eu-west-2 region 2. Upload a single file to the bucket. Any file. 2. Public bucket 1. eu-west-1 region 2. Versioning should be enabled", "metadata": {"source_file": "learning-materials/topics/aws/exercises/s3/new_bucket/solution.md", "section": "Objectives", "language": "en", "created_at": "2025-07-19T19:22:02.176600"}}
{"text": "For the first bucket: 1. Go to S3 service in the AWS console. If not in buckets page, click on \"buckets\" in the left side menu 2. Click on \"Create bucket\" 3. Give a globally unique name for your bucket 4. Choose the region \"eu-west-2\" 5. Click on \"Create bucket\" 6. Click on the bucket name 7. Under \"objects\" click on \"Upload\" -> \"Add files\" -> Choose file to upload -> Click on \"Upload\" For the second bucket: 1. Go to S3 service in the AWS console. If not in buckets page, click on \"buckets\" in the left side menu 2. Click on \"Create bucket\" 3. Give a globally unique name for your bucket 4. Choose the region \"eu-west-1\" 5. Make sure to uncheck the box for \"Private bucket\" to make it public 6. Make sure to check the enable box for \"Bucket Versioning\" 7. Click on \"Create bucket\"", "metadata": {"source_file": "learning-materials/topics/aws/exercises/s3/new_bucket/solution.md", "section": "Console", "language": "en", "created_at": "2025-07-19T19:22:02.176834"}}
{"text": "* An EC2 instance with public IP (not elastic IP)", "metadata": {"source_file": "learning-materials/topics/aws/exercises/elastic_ip/solution.md", "section": "Requirements", "language": "en", "created_at": "2025-07-19T19:22:02.177087"}}
{"text": "1. Write down the public IP of your EC2 instance somewhere and stop & start the instance. Does the public IP address is the same? why? 2. Handle this situation so you have the same public IP even after stopping and starting the instance", "metadata": {"source_file": "learning-materials/topics/aws/exercises/elastic_ip/solution.md", "section": "Objectives", "language": "en", "created_at": "2025-07-19T19:22:02.177130"}}
{"text": "1. Go to EC2 service -> Instances 1. Write down current public IP address 2. Click on \"Instance state\" -> Stop instance -> Stop 3. Click on \"Instance state\" -> Start Instance 4. Yes, the public IP address has changed 2. Let's use an Elastic IP address 1. In EC2 service, under \"Network & Security\" click on \"Elastic IP\" 2. Click on the \"Allocate elastic IP address\" button 3. Make sure you select \"Amazon's pool of IPv4 addresses\" and click on \"Allocate\" 4. Click on \"Actions\" and then \"Associate Elastic IP address\" 1. Select \"instance\", choose your instance and provide its private IP address 2. Click on \"Associate\" 5. Now, if we go back to the instance page, we can see it is using the Elastic IP address as its public IP Note: to remove it, use \"disassociate\" option and don't forget to also release it so you won't be billed.", "metadata": {"source_file": "learning-materials/topics/aws/exercises/elastic_ip/solution.md", "section": "Solution", "language": "en", "created_at": "2025-07-19T19:22:02.177328"}}
{"text": "1. Create/Download a credential report 2. Answer the following questions based on the report: 1. Are there users with MFA not activated? 2. Are there users with password enabled that didn't 3. Explain the use case for using the credential report", "metadata": {"source_file": "learning-materials/topics/aws/exercises/credential_report/solution.md", "section": "Objectives", "language": "en", "created_at": "2025-07-19T19:22:02.177693"}}
{"text": "1. Go to the AWS IAM service 2. Under \"Access Reports\" click on \"Credential report\" 3. Click on \"Download Report\" and open it once it's downloaded 4. Answer the questions in this exercises by inspecting the report The credential report is useful to identify whether there any users who need assistance or attention in regards to their security. For example a user who didn't change his password for a long time and didn't activate MFA.", "metadata": {"source_file": "learning-materials/topics/aws/exercises/credential_report/solution.md", "section": "Solution", "language": "en", "created_at": "2025-07-19T19:22:02.177812"}}
{"text": "Implement the following architecture: <TODO>", "metadata": {"source_file": "learning-materials/topics/aws/exercises/web_app_lambda_dynamodb/exercise.md", "section": "Objectives", "language": "en", "created_at": "2025-07-19T19:22:02.177986"}}
{"text": "Two EC2 instances with a simple web application that shows the web page with the string \"Hey, it's a me, `<HOSTNAME>`!\"", "metadata": {"source_file": "learning-materials/topics/aws/exercises/app_load_balancer/solution.md", "section": "Requirements", "language": "en", "created_at": "2025-07-19T19:22:02.178133"}}
{"text": "1. Create an application load balancer for the two instances you have, with the following properties 1. healthy threshold: 3 2. unhealthy threshold: 3 3. interval: 10 seconds 2. Verify load balancer is working (= you get reply from both instances at different times)", "metadata": {"source_file": "learning-materials/topics/aws/exercises/app_load_balancer/solution.md", "section": "Objectives", "language": "en", "created_at": "2025-07-19T19:22:02.178177"}}
{"text": "1. Go to EC2 service 2. Click in the left side menu on \"Load balancers\" under \"Load balancing\" 3. Click on \"Create load balancer\" 4. Choose \"Application Load Balancer\" 5. Insert a name for the LB 6. Choose an AZ where you want the LB to operate 7. Choose a security group 8. Under \"Listeners and routing\" click on \"Create target group\" and choose \"Instances\" 1. Provide a name for the target group 2. Set healthy threshold to 3 3. Set unhealthy threshold to 3 4. Set interval to 10 seconds 5. Click on \"Next\" and choose the two of the instances you've created 6. Click on \"Create target group\" 9. Refresh target groups and choose the one you've just created 10. Click on \"Create load balancer\" and wait for it to be provisioned 11. Copy DNS address and paste it in the browser. If you refresh, you should see different message based on the instance where the traffic was routed to", "metadata": {"source_file": "learning-materials/topics/aws/exercises/app_load_balancer/solution.md", "section": "Console", "language": "en", "created_at": "2025-07-19T19:22:02.178403"}}
{"text": "Two EC2 instances with a simple web application that shows the web page with the string \"Hey, it's a me, `<HOSTNAME>`!\" One EC2 instance with a simple web application that shows the web page with the string \"Hey, it's only a test...\" under the endpoint /test", "metadata": {"source_file": "learning-materials/topics/aws/exercises/alb_multiple_target_groups/solution.md", "section": "Requirements", "language": "en", "created_at": "2025-07-19T19:22:02.178619"}}
{"text": "1. Create an application load balancer for the two instances you have, with the following properties 1. healthy threshold: 3 2. unhealthy threshold: 3 3. interval: 10 seconds 2. Create another target group for the third instance 1. Traffic should be forwarded to this group based on the \"/test\" path", "metadata": {"source_file": "learning-materials/topics/aws/exercises/alb_multiple_target_groups/solution.md", "section": "Objectives", "language": "en", "created_at": "2025-07-19T19:22:02.178668"}}
{"text": "1. Go to EC2 service 2. Click in the left side menu on \"Load balancers\" under \"Load balancing\" 3. Click on \"Create load balancer\" 4. Choose \"Application Load Balancer\" 5. Insert a name for the LB 6. Choose an AZ where you want the LB to operate 7. Choose a security group 8. Under \"Listeners and routing\" click on \"Create target group\" and choose \"Instances\" 1. Provide a name for the target group 2. Set healthy threshold to 3 3. Set unhealthy threshold to 3 4. Set interval to 10 seconds 5. Click on \"Next\" and choose two out of three instances you've created 6. Click on \"Create target group\" 9. Refresh target groups and choose the one you've just created 10. Click on \"Create load balancer\" and wait for it to be provisioned 11. In the left side menu click on \"Target Groups\" under \"Load Balancing\" 12. Click on \"Create target group\" 13. Set it with the same properties as previous target group but this time, add the third instance that you didn't include in the previous target group 14. Go", "metadata": {"source_file": "learning-materials/topics/aws/exercises/alb_multiple_target_groups/solution.md", "section": "Console", "language": "en", "created_at": "2025-07-19T19:22:02.179009"}}
{"text": "back to your ALB and under \"Listeners\" click on \"Edit rules\" under your current listener 1. Add a rule where if the path is \"/test\" then traffic should be forwarded to the second target group you've created 2. Click on \"Save\" 15. Test it by going to the browser, insert the address and add \"/test\" to the address", "metadata": {"source_file": "learning-materials/topics/aws/exercises/alb_multiple_target_groups/solution.md", "section": "Console", "language": "en", "created_at": "2025-07-19T19:22:02.179039"}}
{"text": "EBS Volume", "metadata": {"source_file": "learning-materials/topics/aws/exercises/snapshots/solution.md", "section": "Requirements", "language": "en", "created_at": "2025-07-19T19:22:02.179255"}}
{"text": "A. Create a snapshot of an EBS volume B. Verify the snapshot was created C. Move the data to another region D. Create a volume out of it in a different AZ", "metadata": {"source_file": "learning-materials/topics/aws/exercises/snapshots/solution.md", "section": "Objectives", "language": "en", "created_at": "2025-07-19T19:22:02.179295"}}
{"text": "A. 1. Go to EC2 service 2. Click on \"Volumes\" under \"Elastic Block Store\" 3. Right click on the chosen volume -> Create snapshot 4. Insert a description and click on \"Create Snapshot\" B. 1. Click on \"Snapshots\" under \"Elastic Block Store\" 2. You should see the snapshot you've created C. 1. Select the snapshot and click on Actions -> Copy 2. Select a region to where the snapshot will be copied D. 1. Select the snapshot and click on Actions -> Create volume 2. Choose a different AZ 3. Click on \"Create Volume\"", "metadata": {"source_file": "learning-materials/topics/aws/exercises/snapshots/solution.md", "section": "Solution", "language": "en", "created_at": "2025-07-19T19:22:02.179401"}}
{"text": "Note: DON'T perform this exercise unless you understand what you are doing and what is the outcome of applying these changes to your account", "metadata": {"source_file": "learning-materials/topics/aws/exercises/password_policy_and_mfa/solution.md", "section": "AWS IAM - Password Policy & MFA", "language": "en", "created_at": "2025-07-19T19:22:02.179654"}}
{"text": "1. Create password policy with the following settings: 1. At least minimum 8 characters 2. At least one number 3. Prevent password reuse 2. Then enable MFA for the account.", "metadata": {"source_file": "learning-materials/topics/aws/exercises/password_policy_and_mfa/solution.md", "section": "Objectives", "language": "en", "created_at": "2025-07-19T19:22:02.179690"}}
{"text": "Password Policy: 1. Go to IAM service in AWS 2. Click on \"Account settings\" under \"Access management\" 3. Click on \"Change password policy\" 1. Check \"Enforce minimum password length\" and set it to 8 characters 1. Check \"Require at least one number\" 1. Check \"Prevent password reuse\" 4. Click on \"Save changes\" MFA: 1. Click on the account name 2. Click on \"My Security Credentials\" 3. Expand \"Multi-factor authentication (MFA)\" and click on \"Activate MFA\" 4. Choose one of the devices 5. Follow the instructions to set it up and click on \"Assign MFA\" 6. ### Solution using Terraform: ``` resource \"aws_iam_account_password_policy\" \"strict\" { minimum_password_length = 8 require_numbers = true allow_users_to_change_password = true password_reuse_prevention = 1 } ``` **Note:** You cannot add MFA through terraform, you have to do it in the GUI.", "metadata": {"source_file": "learning-materials/topics/aws/exercises/password_policy_and_mfa/solution.md", "section": "Solution", "language": "en", "created_at": "2025-07-19T19:22:02.179897"}}
{"text": "One running EC2 instance", "metadata": {"source_file": "learning-materials/topics/aws/exercises/create_ami/solution.md", "section": "Requirements", "language": "en", "created_at": "2025-07-19T19:22:02.180279"}}
{"text": "1. Make some changes in the operating system of your instance (create files, modify files, ...) 2. Create an AMI image from running EC2 instance 3. Launch a new instance using the custom AMI you've created", "metadata": {"source_file": "learning-materials/topics/aws/exercises/create_ami/solution.md", "section": "Objectives", "language": "en", "created_at": "2025-07-19T19:22:02.180319"}}
{"text": "1. Connect to your EC2 instance (ssh, console, ...) 2. Make some changes in the operating system 3. Go to EC2 service 4. Right click on the instance where you made some changes -> Image and templates -> Create image 5. Give the image a name and click on \"Create image\" 6. Launch new instance and choose the image you've just created", "metadata": {"source_file": "learning-materials/topics/aws/exercises/create_ami/solution.md", "section": "Solution", "language": "en", "created_at": "2025-07-19T19:22:02.180390"}}
{"text": "As you probably know at this point, it's not recommended to work with the root account in AWS. For this reason you are going to create a new account which you'll use regularly as the admin account. 1. Create a user with password credentials 2. Add the newly created user to a group called \"admin\" and attach to it the policy called \"Administrator Access\" 3. Make sure the user has a tag called with the key `Role` and the value `DevOps`", "metadata": {"source_file": "learning-materials/topics/aws/exercises/create_user/solution.md", "section": "Objectives", "language": "en", "created_at": "2025-07-19T19:22:02.180626"}}
{"text": "1. Go to the AWS IAM service 2. Click on \"Users\" in the right side menu (right under \"Access Management\") 3. Click on the button \"Add users\" 4. Insert the user name (e.g. mario) 5. Select the credential type: \"Password\" 6. Set console password to custom and click on \"Next\" 7. Click on \"Add user to group\" 8. Insert \"admin\" as group name 9. Check the \"AdministratorAccess\" policy and click on \"Create group\" 10. Click on \"Next: Tags\" 11. Add a tag with the key `Role` and the value `DevOps` 12. Click on \"Review\" and then create on \"Create user\" 13. ### Solution using Terraform ``` resource \"aws_iam_group_membership\" \"team\" { name = \"tf-testing-group-membership\" users = [ aws_iam_user.newuser.name, ] group = aws_iam_group.admin.name } resource \"aws_iam_group_policy_attachment\" \"test-attach\" { group = aws_iam_group.admin.name policy_arn = \"arn:aws:iam::aws:policy/AdministratorAccess\" } resource \"aws_iam_group\" \"admin\" { name = \"admin\" } resource \"aws_iam_user\" \"newuser\" { name = \"newuser\"", "metadata": {"source_file": "learning-materials/topics/aws/exercises/create_user/solution.md", "section": "Solution", "language": "en", "created_at": "2025-07-19T19:22:02.180880"}}
{"text": "path = \"/system/\" tags = { Role = \"DevOps\" } } ```", "metadata": {"source_file": "learning-materials/topics/aws/exercises/create_user/solution.md", "section": "Solution", "language": "en", "created_at": "2025-07-19T19:22:02.180910"}}
{"text": "1. Create an Aurora database with the following properties * Edition: MySQL * Instance type: db.t3.small * A reader node in a different AZ * Public access should be enabled * Port should be set to 3306 * DB name: 'db' * Backup retention: 10 days 2. How many instances does your DB cluster has?", "metadata": {"source_file": "learning-materials/topics/aws/exercises/aurora_db/solution.md", "section": "Objectives", "language": "en", "created_at": "2025-07-19T19:22:02.181166"}}
{"text": "1. Go to RDS service 2. Click on \"Databases\" in the left side menu and click on the \"Create database\" button 3. Choose \"standard create\" 4. Choose \"Aurora DB\" 5. Choose \"MySQL\" edition and \"Provisioned\" as capacity type 6. Choose \"single-master\" 7. Specify Credentials (master username and password) 8. Choose DB instance type: Burstable classes, db.t3.small 9. Choose \"Create an Aurora Replica or Reader node in a different AZ\" 10. Choose a default VPC and subnet 11. Check \"Yes\" for public access 12. Database port should be 3306 13. For authentication, choose \"Password and IAM database authentication\" 14. Set initial database name as \"db\" 15. Increase backup retention period to 10 days 16. Click on \"Create database\" button 1. Two instances - one reader and one writer", "metadata": {"source_file": "learning-materials/topics/aws/exercises/aurora_db/solution.md", "section": "Console", "language": "en", "created_at": "2025-07-19T19:22:02.181323"}}
{"text": "1. Create ElastiCache Redis * Instance type should be \"cache.t2.micro\" * Replicas should be 0", "metadata": {"source_file": "learning-materials/topics/aws/exercises/elasticache/solution.md", "section": "Objectives", "language": "en", "created_at": "2025-07-19T19:22:02.181514"}}
{"text": "1. Go to ElastiCache service 2. Click on \"Get Started Now\" 3. Choose \"Redis\" 4. Insert a name and description 5. Choose \"cache.t2.micro\" an node type 6. Set number of replicas to 0 7. Create new subnet group 8. Click on \"Create\"", "metadata": {"source_file": "learning-materials/topics/aws/exercises/elasticache/solution.md", "section": "Console", "language": "en", "created_at": "2025-07-19T19:22:02.181557"}}
{"text": "For this exercise you'll need: 1. EC2 instance with web application 2. Security group inbound rules that allow HTTP traffic", "metadata": {"source_file": "learning-materials/topics/aws/exercises/security_groups/solution.md", "section": "Requirements", "language": "en", "created_at": "2025-07-19T19:22:02.181745"}}
{"text": "1. List the security groups you have in your account, in the region you are using 2. Remove the HTTP inbound traffic rule 3. Can you still access the application? What do you see/get? 4. Add back the rule 5. Can you access the application now?", "metadata": {"source_file": "learning-materials/topics/aws/exercises/security_groups/solution.md", "section": "Objectives", "language": "en", "created_at": "2025-07-19T19:22:02.181794"}}
{"text": "1. Go to EC2 service - > Click on \"Security Groups\" under \"Network & Security\" You should see at least one security group. One of them is called \"default\" 2. Click on the security group with HTTP rules and click on \"Edit inbound rules\". Remove the HTTP related rules and click on \"Save rules\" 3. No. There is a time out because we removed the rule allowing HTTP traffic. 4. Click on the security group -> edit inbound rules and add the following rule: * Type: HTTP * Port range: 80 * Source: Anywhere -> 0.0.0.0/0 5. yes", "metadata": {"source_file": "learning-materials/topics/aws/exercises/security_groups/solution.md", "section": "Console", "language": "en", "created_at": "2025-07-19T19:22:02.181897"}}
{"text": "1. `aws ec2 describe-security-groups` -> by default, there is one security group called \"default\", in a new account 2. Remove the rule: ``` aws ec2 revoke-security-group-ingress \\ --group-name someHTTPSecurityGroup --protocol tcp \\ --port 80 \\ --cidr 0.0.0.0/0 ``` 3. No. There is a time out because we removed the rule allowing HTTP traffic. 4. Add the rule we remove: ``` aws ec2 authorize-security-group-ingress \\ --group-name someHTTPSecurityGroup --protocol tcp \\ --port 80 \\ --cidr 0.0.0.0/0 ``` 5. yes", "metadata": {"source_file": "learning-materials/topics/aws/exercises/security_groups/solution.md", "section": "CLI", "language": "en", "created_at": "2025-07-19T19:22:02.181975"}}
{"text": "A. Create a placement group. It should be one with a low latency network. Make sure to launch an instance as part of this placement group. B. Create another placement group. This time high availability is a priority", "metadata": {"source_file": "learning-materials/topics/aws/exercises/placement_groups/solution.md", "section": "Objectives", "language": "en", "created_at": "2025-07-19T19:22:02.182198"}}
{"text": "A. 1. Go to EC2 service 2. Click on \"Placement Groups\" under \"Network & Security\" 3. Click on \"Create placement group\" 4. Give it a name and choose the \"Cluster\" placement strategy because the requirement is low latency network 5. Click on \"Create group\" 6. Go to \"Instances\" and click on \"Launch an instance\". Choose any properties you would like, just make sure to check \"Add instance to placement group\" and choose the placement group you've created B. 1. Go to EC2 service 2. Click on \"Placement Groups\" under \"Network & Security\" 3. Click on \"Create placement group\" 4. Give it a name and choose the \"Spread\" placement strategy because the requirement is high availability as top priority 5. Click on \"Create group\"", "metadata": {"source_file": "learning-materials/topics/aws/exercises/placement_groups/solution.md", "section": "Solution", "language": "en", "created_at": "2025-07-19T19:22:02.182345"}}
{"text": "A running EC2 web instance with an health check defined for it in Route 53", "metadata": {"source_file": "learning-materials/topics/aws/exercises/route_53_failover/solution.md", "section": "Requirements", "language": "en", "created_at": "2025-07-19T19:22:02.182773"}}
{"text": "1. Create a failover record that will failover to another record if an health check isn't passing 1. Make sure TTL is 30 2. Associate the failover record with the health check you have", "metadata": {"source_file": "learning-materials/topics/aws/exercises/route_53_failover/solution.md", "section": "Objectives", "language": "en", "created_at": "2025-07-19T19:22:02.182814"}}
{"text": "1. Go to Route 53 service 2. Click on \"Hosted Zones\" in the left-side menu 3. Click on your hosted zone 4. Click on \"Created record\" 5. Insert \"failover\" in record name and set record type to A 6. Insert the IP of your instance 7. Set the routing policy to failover 8. Set TTL to 30 9. Associate with an health check 10. Add another record with the same properties as the previous one 11. Click on \"Create records\" 12. Go to your EC2 instance and edit its security group to remove the HTTP rules 13. Use your web app and if you print the hotsname of your instance then you will notice, a failover was performed and a different EC2 instance is used", "metadata": {"source_file": "learning-materials/topics/aws/exercises/route_53_failover/solution.md", "section": "Console", "language": "en", "created_at": "2025-07-19T19:22:02.182960"}}
{"text": "Note: this costs money", "metadata": {"source_file": "learning-materials/topics/aws/exercises/ecs_task/solution.md", "section": "AWS Containers - Run Tasks", "language": "en", "created_at": "2025-07-19T19:22:02.183135"}}
{"text": "Create a task in ECS to launch in Fargate. The task itself can be a sample app.", "metadata": {"source_file": "learning-materials/topics/aws/exercises/ecs_task/solution.md", "section": "Objectives", "language": "en", "created_at": "2025-07-19T19:22:02.183160"}}
{"text": "1. Go to Elastic Container Service page 2. Click on \"Get Started\" 3. Choose \"sample-app\" 4. Verify it's using Farget and not ECS (EC2 Instance) and click on \"Next\" 5. Select \"None\" in Load balancer type and click on \"Next\" 6. Insert cluster name (e.g. my_cluster) and click on \"Next\" 7. Review everything and click on \"Create\" 8. Wait for everything to complete 1. Go to clusters page and check the status of the task (it will take a couple of seconds/minutes before changing to \"Running\") 1. Click on the task and you'll see the launch type is Fargate", "metadata": {"source_file": "learning-materials/topics/aws/exercises/ecs_task/solution.md", "section": "Console", "language": "en", "created_at": "2025-07-19T19:22:02.183262"}}
{"text": "1. Single newly created VPC 2. Region with more than two availability zones", "metadata": {"source_file": "learning-materials/topics/aws/exercises/subnets/solution.md", "section": "Requirements", "language": "en", "created_at": "2025-07-19T19:22:02.183437"}}
{"text": "1. Create a subnet in your newly created VPC 1. CIDR: 10.0.0.0/24 1. Name: NewSubnet1 2. Create additional subnet 1. CIDR: 10.0.1.0/24 2. Name: NewSubnet2 3. Different AZ compared to previous subnet 3. Create additional subnet 4. CIDR: 10.0.2.0/24 5. Name: NewSubnet3 6. Different AZ compared to previous subnets", "metadata": {"source_file": "learning-materials/topics/aws/exercises/subnets/solution.md", "section": "Objectives", "language": "en", "created_at": "2025-07-19T19:22:02.183483"}}
{"text": "1. Click on \"Subnets\" under \"Virtual Private Cloud\" 2. Make sure you filter by your newly created VPC (to not see the subnets in all other VPCs). You can do this in the left side menu 3. Click on \"Create subnet\" 4. Choose your newly created VPC 5. Set the subnet name to \"NewSubnet1\" 6. Choose AZ 7. Set CIDR to 10.0.0.0/24 8. Click on \"Add new subnet\" 9. Set the subnet name to \"NewSubnet2\" 10. Choose a different AZ 11. Set CIDR to 10.0.1.0/24 12. Click on \"Add new subnet\" 13. Set the subnet name to \"NewSubnet3\" 14. Choose a different AZ 15. Set CIDR to 10.0.2.0/24", "metadata": {"source_file": "learning-materials/topics/aws/exercises/subnets/solution.md", "section": "Console", "language": "en", "created_at": "2025-07-19T19:22:02.183655"}}
{"text": "1. Create a subnet in your newly created VPC 1. CIDR: 10.0.0.0/24 2. Name: NewSubnet1 2. Create additional subnet 1. CIDR: 10.0.1.0/24 2. Name: NewSubnet2 3. Different AZ compared to previous subnet 3. Create additional subnet 1. CIDR: 10.0.2.0/24 2. Name: NewSubnet3 3. Different AZ compared to previous subnets", "metadata": {"source_file": "learning-materials/topics/aws/exercises/subnets/exercise.md", "section": "Objectives", "language": "en", "created_at": "2025-07-19T19:22:02.183825"}}
{"text": "3 web instances in different AZs.", "metadata": {"source_file": "learning-materials/topics/aws/exercises/health_checks/solution.md", "section": "Requirements", "language": "en", "created_at": "2025-07-19T19:22:02.183994"}}
{"text": "1. For each instance create a health checks with the following properties: 1. Name it after the AZ where the instance resides 2. Failure threshold should be 5 2. Edit the security group of one of your instances and remove HTTP rules. 1. Did it change the status of the health check?", "metadata": {"source_file": "learning-materials/topics/aws/exercises/health_checks/solution.md", "section": "Objectives", "language": "en", "created_at": "2025-07-19T19:22:02.184042"}}
{"text": "1. Go to Route 53 2. Click on \"Health Checks\" in the left-side menu 3. Click on \"Create health check\" 4. Insert the name: us-east-2 5. What to monitor: endpoint 6. Insert the IP address of the instance 7. Insert the endpoint /health if your web instance supports that endpoint 8. In advanced configuration, set Failure threshold to 5 9. Click on \"next\" and then on \"Create health check\" 10. Repeat steps 1-9 for the other two instances you have 1. Go to security group of one of your instances 2. Click on \"Actions\" -> Edit inbound rules -> Delete HTTP based rules 3. Go back to health checks page and after a couple of seconds you should see that the status becomes \"unhealthy\"", "metadata": {"source_file": "learning-materials/topics/aws/exercises/health_checks/solution.md", "section": "Console", "language": "en", "created_at": "2025-07-19T19:22:02.184186"}}
{"text": "Two EC2 instances in different availability zones", "metadata": {"source_file": "learning-materials/topics/aws/exercises/create_efs/solution.md", "section": "Requirements", "language": "en", "created_at": "2025-07-19T19:22:02.184393"}}
{"text": "1. Create an EFS with the following properties 1. Set lifecycle management to 60 days 2. The mode should match a use case of scaling to high levels of throughput and I/O operations per second 2. Mount the EFS in both of your EC2 instances", "metadata": {"source_file": "learning-materials/topics/aws/exercises/create_efs/solution.md", "section": "Objectives", "language": "en", "created_at": "2025-07-19T19:22:02.184436"}}
{"text": "1. Go to EFS console 2. Click on \"Create file system\" 3. Create on \"customize\" 1. Set lifecycle management to \"60 days since last access\" 2. Set Performance mode to \"MAX I/O\" due to the requirement of \"Scaling to high levels of throughput\" 3. Click on \"Next\" 4. Choose security group to attach (if you don't have any, create one and make sure it has a rule to allow NFS traffic) and click on \"Next\" until you are able to review and create it 5. SSH into your EC2 instances 1. Run `sudo yum install -y amazon-efs-utils` 2. Run `mkdir efs` 3. If you go to your EFS page and click on \"Attach\", you can see what ways are there to mount your EFS on your instancess 1. The command to mount the EFS should be similar to `sudo mount -t efs -o tls <EFS name>:/ efs` - copy and paste it in your ec2 instance's OS", "metadata": {"source_file": "learning-materials/topics/aws/exercises/create_efs/solution.md", "section": "Solution", "language": "en", "created_at": "2025-07-19T19:22:02.184644"}}
{"text": "1. make sure the node.js application has a _npm start_ command specified in the __package.json__ file like the following example ``` { \"name\": \"application-name\", \"version\": \"0.0.1\", \"private\": true, \"scripts\": { \"start\": \"node app\" }, \"dependencies\": { \"express\": \"3.1.0\", \"jade\": \"*\", \"mysql\": \"*\", \"async\": \"*\", \"node-uuid\": \"*\" } ``` 2. zip the application, and make sure to not zip the parent folder, only the files together, like: ``` \\Parent - (exclude the folder itself from the the zip) - file1 - (include in zip) - subfolder1 (include in zip) - file2 (include in zip) - file3 (include in zip) ```", "metadata": {"source_file": "learning-materials/topics/aws/exercises/elastic_beanstalk_simple/solution.md", "section": "Prerequisites", "language": "en", "created_at": "2025-07-19T19:22:02.184966"}}
{"text": "1. Create a \"New Environment\" 2. Select Environment => _Web Server Environment_ 3. Fill the Create a web server environment section a. Fill the \"Application Name\" 4. Fill the Environment information section a. Fill the \"Environment Name\" b. Domain - \"Leave for autogenerated value\" 5. Platform a. Choose Platform => _node.js_ 6. Application Code => upload the Zipped Code from your local computer 7. Create Environment 8. Wait for the environment to come up 9. Check the website a. Navigate to the _Applications_ tab, b. select the recently created node.js app c. click on the URL - highlighted", "metadata": {"source_file": "learning-materials/topics/aws/exercises/elastic_beanstalk_simple/solution.md", "section": "Solution", "language": "en", "created_at": "2025-07-19T19:22:02.185219"}}
{"text": "[Elastic Beanstalk / Node.js getting started](https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/nodejs-getstarted.html)", "metadata": {"source_file": "learning-materials/topics/aws/exercises/elastic_beanstalk_simple/solution.md", "section": "Documentation", "language": "en", "created_at": "2025-07-19T19:22:02.185252"}}
{"text": "1. Having a running node.js application on AWS Elastic Beanstalk platform", "metadata": {"source_file": "learning-materials/topics/aws/exercises/elastic_beanstalk_simple/exercise.md", "section": "Requirements", "language": "en", "created_at": "2025-07-19T19:22:02.185343"}}
{"text": "1. Create an AWS Elastic Beanstalk application with the basic properties a. No ALB, No Database, Just use the default platform settings", "metadata": {"source_file": "learning-materials/topics/aws/exercises/elastic_beanstalk_simple/exercise.md", "section": "Objectives", "language": "en", "created_at": "2025-07-19T19:22:02.185379"}}
{"text": "1. Having ALB attached in place 2. Having custom domain name in place 3. Having automated pipelines in place 4. Having blue-green deployment in place 5. Writing the Node.js application", "metadata": {"source_file": "learning-materials/topics/aws/exercises/elastic_beanstalk_simple/exercise.md", "section": "Out of scope", "language": "en", "created_at": "2025-07-19T19:22:02.185405"}}
{"text": "Learn how to work with Git Branches", "metadata": {"source_file": "learning-materials/topics/git/branch_01.md", "section": "Objective", "language": "en", "created_at": "2025-07-19T19:22:02.185508"}}
{"text": "1. Pick up a Git repository (or create a new one) with at least one commit 2. Create a new branch called \"dev\" 3. Modify one of the files in the repository 4. Create a new commit 5. Verify the commit you created is only in \"dev\" branch", "metadata": {"source_file": "learning-materials/topics/git/branch_01.md", "section": "Instructions", "language": "en", "created_at": "2025-07-19T19:22:02.185552"}}
{"text": "Answer the following: 1. Why branches are useful? Give an example of one real-world scenario for using branches", "metadata": {"source_file": "learning-materials/topics/git/branch_01.md", "section": "After you complete the exercise", "language": "en", "created_at": "2025-07-19T19:22:02.185573"}}
{"text": "| Name | Topic | Objective & Instructions | Solution | Comments | | ----------------- | ------ | -------------------------------- | ------------------------------------------- | -------- | | My first Commit | Commit | [Exercise](commit_01.md) | [Solution](solutions/commit_01_solution.md) | | | Time to Branch | Branch | [Exercise](branch_01.md) | [Solution](solutions/branch_01_solution.md) | | | Squashing Commits | Commit | [Exercise](squashing_commits.md) | [Solution](solutions/squashing_commits.md) | |", "metadata": {"source_file": "learning-materials/topics/git/README.md", "section": "Exercises", "language": "en", "created_at": "2025-07-19T19:22:02.185895"}}
{"text": "<details> <summary>How do you know if a certain directory is a git repository?</summary><br><b> You can check if there is a \".git\" directory. </b> </details> <details> <summary>Explain the following: <code>git directory</code>, <code>working directory</code> and <code>staging area</code></summary><br> <b> This answer taken from [git-scm.com](https://git-scm.com/book/en/v1/Getting-Started-Git-Basics#_the_three_states) \"The Git directory is where Git stores the meta-data and object database for your project. This is the most important part of Git, and it is what is copied when you clone a repository from another computer. The working directory is a single checkout of one version of the project. These files are pulled out of the compressed database in the Git directory and placed on disk for you to use or modify. The staging area is a simple file, generally contained in your Git directory, that stores information about what will go into your next commit. It’s sometimes referred to as the", "metadata": {"source_file": "learning-materials/topics/git/README.md", "section": "Git Basics", "language": "en", "created_at": "2025-07-19T19:22:02.186625"}}
{"text": "index, but it’s becoming standard to refer to it as the staging area.\" </b> </details> <details> <summary>What is the difference between <code>git pull</code> and <code>git fetch</code>?</summary><br><b> Shortly, git pull = git fetch + git merge When you run git pull, it gets all the changes from the remote or central repository and attaches it to your corresponding branch in your local repository. git fetch gets all the changes from the remote repository, stores the changes in a separate branch in your local repository </b></details> <details> <summary>How to check if a file is tracked and if not, then track it?</summary><br><b> There are different ways to check whether a file is tracked or not: - `git ls-files <file>` -> exit code of 0 means it's tracked - `git blame <file>` ... </b> </details> <details> <summary>Explain what the file <code>gitignore</code> is used for</summary><br><b> The purpose of <code>gitignore</code> files is to ensure that certain files not tracked by Git", "metadata": {"source_file": "learning-materials/topics/git/README.md", "section": "Git Basics", "language": "en", "created_at": "2025-07-19T19:22:02.186654"}}
{"text": "remain untracked. To stop tracking a file that is currently tracked, use git rm --cached. </b> </details> <details> <summary>How can you see which changes have done before committing them?</summary><br><b> `git diff` </b></details> <details> <summary>What <code>git status</code> does?</summary><br><b> `git status` helps you to understand the tracking status of files in your repository. Focusing on working directory and staging area - you can learn which changes were made in the working directory, which changes are in the staging area and in general, whether files are being tracked or not. </b></details> <details> <summary>You've created new files in your repository. How to make sure Git tracks them?</summary><br><b> `git add FILES` </b> </details>", "metadata": {"source_file": "learning-materials/topics/git/README.md", "section": "Git Basics", "language": "en", "created_at": "2025-07-19T19:22:02.186678"}}
{"text": "<details> <summary>You have files in your repository you don't want Git to ever track them. What should you be doing to avoid ever tracking them?</summary><br><b> Add them to the file `.gitignore`. This will make sure these files are never added to staging area. </b></details> <details> <summary>A development team in your organization is using a monorepo and it's became quite big, including hundred thousands of files. They say running many git operations is taking a lot of time to run (like git status for example). Why does that happen and what can you do in order to help them?</summary><br><b> Many Git operations are related to filesystem state. `git status` for example will run diffs to compare HEAD commit to index and another diff to compare index to working directory. As part of these diffs, it would need to run quite a lot of `lstat()` system calls. When running on hundred thousands of files, it can take seconds if not minutes. One thing to do about it, would be to use the", "metadata": {"source_file": "learning-materials/topics/git/README.md", "section": "Scenarios", "language": "en", "created_at": "2025-07-19T19:22:02.187357"}}
{"text": "built-in `fsmonitor` (filesystem monitor) of Git. With fsmonitor (which integrated with Watchman), Git spawn a daemon that will watch for any changes continuously in the working directory of your repository and will cache them . This way, when you run `git status` instead of scanning the working directory, you are using a cached state of your index. <p align=\"center\"> <img src=\"images/design/development/git_fsmonitor.png\"/> </p> Next, you can try to enable `feature.manyFile` with `git config feature.manyFiles true`. This does two things: 1. Sets `index.version = 4` which enables path-prefix compression in the index 2. Sets `core.untrackedCache=true` which by default is set to `keep`. The untracked cache is quite important concept. What it does is to record the mtime of all the files and directories in the working directory. This way, when time comes to iterate over all the files and directories, it can skip those whom mtime wasn't updated. Before enabling it, you might want to run `git", "metadata": {"source_file": "learning-materials/topics/git/README.md", "section": "Scenarios", "language": "en", "created_at": "2025-07-19T19:22:02.187553"}}
{"text": "update-index --test-untracked-cache` to test it out and make sure mtime operational on your system. Git also has the built-in `git-maintainence` command which optimizes Git repository so it's faster to run commands like `git add` or `git fatch` and also, the git repository takes less disk space. It's recommended to run this command periodically (e.g. each day). In addition, track only what is used/modified by developers - some repositories may include generated files that are required for the project to run properly (or support certain accessibility options), but not actually being modified by any way by the developers. In that case, tracking them is futile. In order to avoid populating those file in the working directory, one can use the `sparse checkout` feature of Git. Finally, with certain build systems, you can know which files are being used/relevant exactly based on the component of the project that the developer is focusing on. This, together with the `sparse checkout` can lead", "metadata": {"source_file": "learning-materials/topics/git/README.md", "section": "Scenarios", "language": "en", "created_at": "2025-07-19T19:22:02.187587"}}
{"text": "to a situation where only a small subset of the files are being populated in the working directory. Making commands like `git add`, `git status`, etc. really quick </b></details>", "metadata": {"source_file": "learning-materials/topics/git/README.md", "section": "Scenarios", "language": "en", "created_at": "2025-07-19T19:22:02.187607"}}
{"text": "<details> <summary>What's is the branch strategy (flow) you know?</summary><br><b> - Git flow - GitHub flow - Trunk based development - GitLab flow [Explanation](https://www.bmc.com/blogs/devops-branching-strategies/#:~:text=What%20is%20a%20branching%20strategy,used%20in%20the%20development%20process). </b></details> <details> <summary>True or False? A branch is basically a simple pointer or reference to the head of certain line of work</summary><br><b> True </b></details> <details> <summary>You have two branches - main and devel. How do you make sure devel is in sync with main?</summary><br><b> <code> ``` git checkout main git pull git checkout devel git merge main ``` </code> </b></details> <details> <summary>Describe shortly what happens behind the scenes when you run <code>git branch <BRANCH></code></summary><br><b> Git runs update-ref to add the SHA-1 of the last commit of the branch you're on into the new branch you would like to create </b></details> <details> <summary>When you", "metadata": {"source_file": "learning-materials/topics/git/README.md", "section": "Branches", "language": "en", "created_at": "2025-07-19T19:22:02.187838"}}
{"text": "run <code>git branch <BRANCH></code> how does Git know the SHA-1 of the last commit?</summary><br><b> Using the HEAD file: `.git/HEAD` </b></details> <details> <summary>What <code>unstaged</code> means in regards to Git?</summary><br><b> A file that is in the working directory but is not in the HEAD nor in the staging area is referred to as \"unstaged\". </b></details> <details> <summary>True or False? when you <code>git checkout some_branch</code>, Git updates .git/HEAD to <code>/refs/heads/some_branch</code></summary><br><b> True </b></details>", "metadata": {"source_file": "learning-materials/topics/git/README.md", "section": "Branches", "language": "en", "created_at": "2025-07-19T19:22:02.187865"}}
{"text": "<details> <summary>You have two branches - main and devel. How do you merge devel into main?</summary><br><b> ``` git checkout main git merge devel git push origin main ``` </b></details> <details> <summary>How to resolve git merge conflicts?</summary><br><b> <p> First, you open the files which are in conflict and identify what are the conflicts. Next, based on what is accepted in your company or team, you either discuss with your colleagues on the conflicts or resolve them by yourself After resolving the conflicts, you add the files with `git add <file_name>` Finally, you run `git rebase --continue` </p> </b></details> <details> <summary>What merge strategies are you familiar with?</summary><br><b> Mentioning two or three should be enough and it's probably good to mention that 'recursive' is the default one. recursive resolve ours theirs This page explains it the best: https://git-scm.com/docs/merge-strategies </b></details> <details> <summary>Explain Git octopus", "metadata": {"source_file": "learning-materials/topics/git/README.md", "section": "Merge", "language": "en", "created_at": "2025-07-19T19:22:02.188120"}}
{"text": "merge</summary><br><b> Probably good to mention that it's: - It's good for cases of merging more than one branch (and also the default of such use cases) - It's primarily meant for bundling topic branches together This is a great article about Octopus merge: http://www.freblogg.com/2016/12/git-octopus-merge.html </b></details> <details> <summary>What is the difference between <code>git reset</code> and <code>git revert</code>?</summary><br><b> <p> `git revert` creates a new commit which undoes the changes from last commit. `git reset` depends on the usage, can modify the index or change the commit which the branch head is currently pointing at. </p> </b></details>", "metadata": {"source_file": "learning-materials/topics/git/README.md", "section": "Merge", "language": "en", "created_at": "2025-07-19T19:22:02.188140"}}
{"text": "<details> <summary>You would like to move forth commit to the top. How would you achieve that?</summary><br><b> Using the `git rebase` command </b></details> <details> <summary>In what situations are you using <code>git rebase</code>?</summary><br><b> Suppose a team is working on a `feature` branch that is coming from the `main` branch of the repo. At a point, where the feature development is done, and finally we wish to merge the feature branch into the main branch without keeping the history of the commits made in the feature branch, a `git rebase` will be helpful. </b></details> <details> <summary>How do you revert a specific file to previous commit?</summary><br><b> ``` git checkout HEAD~1 -- /path/of/the/file ``` </b></details> <details> <summary>How to squash last two commits?</summary><br><b> </b></details> <details> <summary>What is the <code>.git</code> directory? What can you find there?</summary><br><b> The <code>.git</code> folder contains all the information that is", "metadata": {"source_file": "learning-materials/topics/git/README.md", "section": "Rebase", "language": "en", "created_at": "2025-07-19T19:22:02.188624"}}
{"text": "necessary for your project in version control and all the information about commits, remote repository address, etc. All of them are present in this folder. It also contains a log that stores your commit history so that you can roll back to history. This info copied from [https://stackoverflow.com/questions/29217859/what-is-the-git-folder](https://stackoverflow.com/questions/29217859/what-is-the-git-folder) </b></details> <details> <summary>What are some Git anti-patterns? Things that you shouldn't do</summary><br><b> - Not waiting too long between commits - Not removing the .git directory :) </b></details> <details> <summary>How do you remove a remote branch?</summary><br><b> You delete a remote branch with this syntax: git push origin :[branch_name] </b></details> <details> <summary>Are you familiar with gitattributes? When would you use it?</summary><br><b> gitattributes allow you to define attributes per pathname or path pattern.<br> You can use it for example to control endlines", "metadata": {"source_file": "learning-materials/topics/git/README.md", "section": "Rebase", "language": "en", "created_at": "2025-07-19T19:22:02.188646"}}
{"text": "in files. In Windows and Unix based systems, you have different characters for new lines (\\r\\n and \\n accordingly). So using gitattributes we can align it for both Windows and Unix with `* text=auto` in .gitattributes for anyone working with git. This is way, if you use the Git project in Windows you'll get \\r\\n and if you are using Unix or Linux, you'll get \\n. </b></details> <details> <summary>How do you discard local file changes? (before commit)</summary><br><b> `git checkout -- <file_name>` </b></details> <details> <summary>How do you discard local commits?</summary><br><b> `git reset HEAD~1` for removing last commit If you would like to also discard the changes you `git reset --hard`` </b></details> <details> <summary>True or False? To remove a file from git but not from the filesystem, one should use <code>git rm </code></summary><br><b> False. If you would like to keep a file on your filesystem, use `git reset <file_name>` </b></details>", "metadata": {"source_file": "learning-materials/topics/git/README.md", "section": "Rebase", "language": "en", "created_at": "2025-07-19T19:22:02.188816"}}
{"text": "<details> <summary>How to list the current git references in a given repository? </summary><br><b> `find .git/refs/` </b></details>", "metadata": {"source_file": "learning-materials/topics/git/README.md", "section": "References", "language": "en", "created_at": "2025-07-19T19:22:02.188857"}}
{"text": "<details> <summary>What git diff does?</summary><br><b> git diff can compare between two commits, two files, a tree and the staging area, etc. </b></details> <details> <summary>Which one is faster? <code>git diff-index HEAD</code> or <code>git diff HEAD</code> </summary><br><b> `git diff-index` is faster but to be fair, it's because it does less. `git diff index` won't look at the content, only metadata like timestamps. </b></details> <details> <summary>By which other Git commands does git diff used?</summary><br><b> The diff mechanism used by `git status` to perform a comparison and let the user know which files are being tracked </b></details>", "metadata": {"source_file": "learning-materials/topics/git/README.md", "section": "Git Diff", "language": "en", "created_at": "2025-07-19T19:22:02.188954"}}
{"text": "<details> <summary>Describe how `git status` works</summary><br><b> Shortly, it runs `git diff` twice: 1. Compare between HEAD to staging area 2. Compare staging area to working directory </b></details> <details> <summary>If <code>git status</code> has to run diff on all the files in the HEAD commit to those in staging area/index and another one on staging area/index and working directory, how is it fairly fast? </summary><br><b> One reason is about the structure of the index, commits, etc. - Every file in a commit is stored in tree object - The index is then a flattened structure of tree objects - All files in the index have pre-computed hashes - The diff operation then, is comparing the hashes Another reason is caching - Index caches information on working directory - When Git has the information for certain file cached, there is no need to look at the working directory file </b></details>", "metadata": {"source_file": "learning-materials/topics/git/README.md", "section": "Git Internal", "language": "en", "created_at": "2025-07-19T19:22:02.189140"}}
{"text": "Learn how to commit changes in Git repositories", "metadata": {"source_file": "learning-materials/topics/git/commit_01.md", "section": "Objective", "language": "en", "created_at": "2025-07-19T19:22:02.190881"}}
{"text": "1. Create a new directory 2. Make it a git repository 3. Create a new file called `file` with the content \"hello commit\" 4. Commit your new file 5. Run a git command to verify your commit was recorded", "metadata": {"source_file": "learning-materials/topics/git/commit_01.md", "section": "Instructions", "language": "en", "created_at": "2025-07-19T19:22:02.190942"}}
{"text": "Answer the following: * What are the benefits of commits? * Is there another way to verify a commit was created?", "metadata": {"source_file": "learning-materials/topics/git/commit_01.md", "section": "After you complete the exercise", "language": "en", "created_at": "2025-07-19T19:22:02.190967"}}
{"text": "Learn how to squash commits", "metadata": {"source_file": "learning-materials/topics/git/squashing_commits.md", "section": "Objective", "language": "en", "created_at": "2025-07-19T19:22:02.191067"}}
{"text": "1. In a git repository, create a new file with the content \"Mario\" and create a new commit 2. Make change to the content of the file you just created so the content is \"Mario & Luigi\" and create another commit 3. Verify you have two separate commits 4. Squash the latest two commits into one commit", "metadata": {"source_file": "learning-materials/topics/git/squashing_commits.md", "section": "Instructions", "language": "en", "created_at": "2025-07-19T19:22:02.191119"}}
{"text": "Answer the following: * What is the reason for squashing commits? * Is it possible to squash more than 2 commits?", "metadata": {"source_file": "learning-materials/topics/git/squashing_commits.md", "section": "After you complete the exercise", "language": "en", "created_at": "2025-07-19T19:22:02.191143"}}
{"text": "``` mkdir my_repo && cd my_repo git init echo \"hello_commit\" > file git add file git commit -a -m \"It's my first commit. Exciting!\" git log ```", "metadata": {"source_file": "learning-materials/topics/git/solutions/commit_01_solution.md", "section": "Git Commit 01 - Solution", "language": "en", "created_at": "2025-07-19T19:22:02.191262"}}
{"text": "``` cd some_repository echo \"master branch\" > file1 git add file1 git commit -a -m \"added file1\" git checkout -b dev echo \"dev branch\" > file2 git add file2 git commit -a -m \"added file2\" ``` Verify: ``` git log (you should see two commits) git checkout master git log (you should see one commit) ```", "metadata": {"source_file": "learning-materials/topics/git/solutions/branch_01_solution.md", "section": "Branch 01 - Solution", "language": "en", "created_at": "2025-07-19T19:22:02.191373"}}
{"text": "1. In a git repository, create a new file with the content \"Mario\" and commit the change: ``` echo \"Mario\" > new_file git add new_file git commit -m \"New file\" ``` 2. Make a change to the content of the file you just created so it becomes \"Mario & Luigi,\" then create another commit: ``` echo \"Mario & Luigi\" > new_file git commit -a -m \"Added Luigi\" ``` 3. Verify you have two separate commits by running: ``` git log ``` 4. Squash the two commits you've created into one commit: ``` git rebase -i HEAD~2 ``` You should see something similar to: ``` pick 5412076 New file pick 4016808 Added Luigi ``` Change `pick` to `squash`: ``` pick 5412076 New file squash 4016808 Added Luigi ``` Save it and provide a commit message for the squashed commit. > **Note**: If running `git rebase -i HEAD~2` returns a fatal error (e.g., \"invalid upstream 'HEAD~2'\"), that usually means your second commit is actually the root commit and there's no valid parent before it. In that case, you can either: > * Use `git", "metadata": {"source_file": "learning-materials/topics/git/solutions/squashing_commits.md", "section": "Git - Squashing Commits - Solution", "language": "en", "created_at": "2025-07-19T19:22:02.191820"}}
{"text": "rebase -i --root` to allow rewriting the root commit, **or** > * Create an initial commit before these two commits so that `HEAD~2` points to valid commits.", "metadata": {"source_file": "learning-materials/topics/git/solutions/squashing_commits.md", "section": "Git - Squashing Commits - Solution", "language": "en", "created_at": "2025-07-19T19:22:02.191853"}}
{"text": "**Answer the following:** * **What is the reason for squashing commits?** History becomes cleaner and it's easier to track changes without many small commits like \"removed a character,\" for example. * **Is it possible to squash more than 2 commits?** Yes.", "metadata": {"source_file": "learning-materials/topics/git/solutions/squashing_commits.md", "section": "After you complete the exercise", "language": "en", "created_at": "2025-07-19T19:22:02.191890"}}
{"text": "Learn about image layers", "metadata": {"source_file": "learning-materials/topics/containers/image_layers.md", "section": "Objective", "language": "en", "created_at": "2025-07-19T19:22:02.192157"}}
{"text": "Make sure Docker is installed on your system and the service is started ```", "metadata": {"source_file": "learning-materials/topics/containers/image_layers.md", "section": "Requirements", "language": "en", "created_at": "2025-07-19T19:22:02.192184"}}
{"text": "rpm -qa | grep docker systemctl status docker ```", "metadata": {"source_file": "learning-materials/topics/containers/image_layers.md", "section": "Fedora/RHEL/CentOS", "language": "en", "created_at": "2025-07-19T19:22:02.192200"}}
{"text": "1. Write a Dockefile. Any Dockefile! :) (just make sure it's a valid one) 2. Build an image using the Dockerfile you've wrote 3. Which of the instructions you've used, created new layers and which added image metadata? 4. What ways are there to confirm your answer to the last question? 5. Can you reduce the size of the image you've created?", "metadata": {"source_file": "learning-materials/topics/containers/image_layers.md", "section": "Instructions", "language": "en", "created_at": "2025-07-19T19:22:02.192253"}}
{"text": "- [Containers](#containers) - [Exercises](#exercises) - [Running Containers](#running-containers) - [Images](#images) - [Misc](#misc) - [Questions](#questions) - [Containers 101](#containers-101) - [Commands Commands](#commands-commands) - [Images](#images-1) - [Registry](#registry) - [Tags](#tags) - [Containerfile](#containerfile) - [Storage](#storage) - [Architecture](#architecture) - [Docker Architecture](#docker-architecture) - [Docker Compose](#docker-compose) - [Networking](#networking) - [Docker Networking](#docker-networking) - [Security](#security) - [Docker in Production](#docker-in-production) - [OCI](#oci) - [Scenarios](#scenarios)", "metadata": {"source_file": "learning-materials/topics/containers/README.md", "section": "Containers", "language": "en", "created_at": "2025-07-19T19:22:02.193002"}}
{"text": "<a name=\"exercises-running-containers\"></a>", "metadata": {"source_file": "learning-materials/topics/containers/README.md", "section": "Exercises", "language": "en", "created_at": "2025-07-19T19:22:02.193033"}}
{"text": "|Name|Topic|Objective & Instructions|Solution|Comments| |--------|--------|------|----|----| |Running Containers|Basics|[Exercise](running_containers.md)|[Solution](solutions/running_containers.md) |Containerized Web Server|Applications|[Exercise](containerized_web_server.md)|[Solution](solutions/containerized_web_server.md) |Containerized Database|Applications|[Exercise](containerized_db.md)|[Solution](solutions/containerized_db.md) |Containerized Database with Persistent Storage|Applications|[Exercise](containerized_db_persistent_storage.md)|[Solution](solutions/containerized_db_persistent_storage.md) <a name=\"exercises-containers-images\"></a>", "metadata": {"source_file": "learning-materials/topics/containers/README.md", "section": "Running Containers", "language": "en", "created_at": "2025-07-19T19:22:02.193057"}}
{"text": "|Name|Topic|Objective & Instructions|Solution|Comments| |--------|--------|------|----|----| |Working with Images|Image|[Exercise](working_with_images.md)|[Solution](solutions/working_with_images.md) |Sharing Images (without a registry)|Images|[Exercise](sharing_images.md)|[Solution](solutions/sharing_images.md) |Creating images on the fly|Images|[Exercise](commit_image.md)|[Solution](solutions/commit_image.md) |My First Containerfile|Containerfile|[Exercise](write_containerfile_run_container.md)| <a name=\"exercises-containers-misc\"></a>", "metadata": {"source_file": "learning-materials/topics/containers/README.md", "section": "Images", "language": "en", "created_at": "2025-07-19T19:22:02.193085"}}
{"text": "|Name|Topic|Objective & Instructions|Solution|Comments| |--------|--------|------|----|----| |Run, Forest, Run!|Restart Policies|[Exercise](run_forest_run.md)|[Solution](solutions/run_forest_run.md) |Layer by Layer|Image Layers|[Exercise](image_layers.md)|[Solution](solutions/image_layers.md) |Containerize an application | Containerization |[Exercise](containerize_app.md)|[Solution](solutions/containerize_app.md) |Multi-Stage Builds|Multi-Stage Builds|[Exercise](multi_stage_builds.md)|[Solution](solutions/multi_stage_builds.md)", "metadata": {"source_file": "learning-materials/topics/containers/README.md", "section": "Misc", "language": "en", "created_at": "2025-07-19T19:22:02.193109"}}
{"text": "<a name=\"questions-containers-101\"></a>", "metadata": {"source_file": "learning-materials/topics/containers/README.md", "section": "Questions", "language": "en", "created_at": "2025-07-19T19:22:02.193124"}}
{"text": "<details> <summary>What is a Container?</summary><br><b> This can be tricky to answer since there are many ways to create a containers: - Docker - systemd-nspawn - LXC If to focus on OCI (Open Container Initiative) based containers, it offers the following [definition](https://github.com/opencontainers/runtime-spec/blob/master/glossary.md#container): \"An environment for executing processes with configurable isolation and resource limitations. For example, namespaces, resource limits, and mounts are all part of the container environment.\" </b></details> <details> <summary>Why containers are needed? What is their goal?</summary><br><b> OCI provides a good [explanation](https://github.com/opencontainers/runtime-spec/blob/master/principles.md#the-5-principles-of-standard-containers): \"Define a unit of software delivery called a Standard Container. The goal of a Standard Container is to encapsulate a software component and all its dependencies in a format that is self-describing and", "metadata": {"source_file": "learning-materials/topics/containers/README.md", "section": "Containers 101", "language": "en", "created_at": "2025-07-19T19:22:02.193835"}}
{"text": "portable, so that any compliant runtime can run it without extra dependencies, regardless of the underlying machine and the contents of the container.\" </b></details> <details> <summary>What is a container image?</summary><br><b> * An image of a container contains the application, its dependencies and the operating system where the application is executed.<br> * It's a collection of read-only layers. These layers are loosely coupled * Each layer is assembled out of one or more files </b></details> <details> <summary>How are containers different from virtual machines (VMs)?</summary><br><b> The primary difference between containers and VMs is that containers allow you to virtualize multiple workloads on a single operating system while in the case of VMs, the hardware is being virtualized to run multiple machines each with its own guest OS. You can also think about it as containers are for OS-level virtualization while VMs are for hardware virtualization. * Containers don't require an", "metadata": {"source_file": "learning-materials/topics/containers/README.md", "section": "Containers 101", "language": "en", "created_at": "2025-07-19T19:22:02.193864"}}
{"text": "entire guest operating system as VMs. Containers share the system's kernel as opposed to VMs. They isolate themselves via the use of kernel's features such as namespaces and cgroups * It usually takes a few seconds to set up a container as opposed to VMs which can take minutes or at least more time than containers as there is an entire OS to boot and initialize as opposed to containers which has share of the underlying OS * Virtual machines considered to be more secured than containers * VMs portability considered to be limited when compared to containers </b></details> <details> <summary>In which scenarios would you use containers and in which you would prefer to use VMs?</summary><br><b> You should choose VMs when: * You need run an application which requires all the resources and functionalities of an OS * You need full isolation and security You should choose containers when: * You need a lightweight solution * Running multiple versions or instances of a single application", "metadata": {"source_file": "learning-materials/topics/containers/README.md", "section": "Containers 101", "language": "en", "created_at": "2025-07-19T19:22:02.193884"}}
{"text": "</b></details> <details> <summary>Describe the process of containerizing an application</summary><br><b> 1. Write a Containerfile/Dockerfile that includes your app (including the commands to run it) and its dependencies 2. Build the image using the Containerfile/Dockefile you wrote 3. You might want to push the image to a registry 4. Run the container using the image you've built </b></details> <details> <summary>What are some of the advantages in using containers? you can compare to other options like VMs</summary><br><b> * Reusable: container can be used by multiple different users for different usages - production vs. staging, development, testing, etc. * Lightweight: containers are fairly lightweight which means deployments can be done quickly since you don't need to install a full OS (as in VMs for example) * Isolation: Containers are isolated environments, usually changes made to the OS won't affect the containers and vice-versa </b></details> <a", "metadata": {"source_file": "learning-materials/topics/containers/README.md", "section": "Containers 101", "language": "en", "created_at": "2025-07-19T19:22:02.194042"}}
{"text": "name=\"questions-common-commands\"></a>", "metadata": {"source_file": "learning-materials/topics/containers/README.md", "section": "Containers 101", "language": "en", "created_at": "2025-07-19T19:22:02.194073"}}
{"text": "Note: I've used `Podman` in the answers, but other containers engines can be used as well (e.g. Docker) <details> <summary>How to run a container?</summary><br><b> `podman run ubuntu` </b></details> <details> <summary>Why after running <code>podman container run ubuntu</code> the output of <code>podman container ls</code> is empty?</summary><br><b> Because the container immediately exits after running the ubuntu image. This is completely normal and expected as containers designed to run a service or a app and exit when they are done running it. To see the container you can run `podman ps -a` If you want the container to keep running, you can run a command like `sleep 100` which will run for 100 seconds or you can attach to terminal of the container with a command similar: `podman container run -it ubuntu /bin/bash` </b></details> <details> <summary>How to list all the containers on the local host?</summary><br><b> `podman container ls` </b></details> <details> <summary>How to attach", "metadata": {"source_file": "learning-materials/topics/containers/README.md", "section": "Commands Commands", "language": "en", "created_at": "2025-07-19T19:22:02.194766"}}
{"text": "your shell to a terminal of a running container?</summary><br><b> `podman container exec -it [container id/name] bash` This can be done in advance while running the container: `podman container run -it [image:tag] /bin/bash` </b></details> <details> <summary>True or False? You can remove a running container if it doesn't running anything</summary><br><b> False. You have to stop the container before removing it. </b></details> <details> <summary>How to stop and remove a container?</summary><br><b> `podman container stop <container id/name> && podman container rm <container id/name>` </b></details> <details> <summary>What happens when you run <code>docker container run ubuntu</code>?</summary><br><b> 1. Docker client posts the command to the API server running as part of the Docker daemon 2. Docker daemon checks if a local image exists 1. If it exists, it will use it 2. If doesn't exists, it will go to the remote registry (Docker Hub by default) and pull the image locally 3. containerd", "metadata": {"source_file": "learning-materials/topics/containers/README.md", "section": "Commands Commands", "language": "en", "created_at": "2025-07-19T19:22:02.194795"}}
{"text": "and runc are instructed (by the daemon) to create and start the container </b></details> <details> <summary>How to run a container in the background?</summary><br><b> With the -d flag. It will run in the background and will not attach it to the terminal. `docker container run -d httpd` or `podman container run -d httpd` </b></details> <details> <summary>If you'll run <code>sleep 100</code> inside a container, will you see it when listing all the processes of the host on which the container runs? Why?</summary><br><b> </b></details> <details> <summary>True or False? If image <code>httpd-service</code> has an entry point for running the httpd service then, the following will run the container and eventually the httpd service <code>podman run httpd-service ls</code></summary><br><b> False. Running that command will override the entry point so the httpd service won't run and instead podman will run the `ls` command. </b></details> <details> <summary>True or False? Running <code>podman", "metadata": {"source_file": "learning-materials/topics/containers/README.md", "section": "Commands Commands", "language": "en", "created_at": "2025-07-19T19:22:02.194815"}}
{"text": "restart CONTAINER_NAME</code> kills the main process inside the container and runs it again from scratch</summary><br><b> False. `podman restart` creates an entirely new container with the same ID while reusing the filesystem and state of the original container. </b></details> <details> <summary>You would like to run a web server inside a container but, be able to access it from the localhost. Demonstrate how to do that</summary><br><b> ``` podman run -d --name apache1 -p 8080:8080 registry.redhat.io/rhel8/httpd-24 curl 127.0.0.1:8080 ``` </b></details> <details> <summary>After running a container, it stopped. <code>podman ps</code> shows nothing. How can you show its details?</summary><br><b> `podman ps -a` will shows also the details of a stopped container. </b></details> <details> <summary>How to list all the image tags for a given container image?</summary><br><b> `podman search --list-tags IMAGE_NAME` </b></details> <a name=\"questions-images\"></a>", "metadata": {"source_file": "learning-materials/topics/containers/README.md", "section": "Commands Commands", "language": "en", "created_at": "2025-07-19T19:22:02.194834"}}
{"text": "<details> <summary>Why container images are relatively small?</summary><br><b> * Most of the images don't contain Kernel. They share and access the one used by the host on which they are running * Containers intended to run specific application in most cases. This means they hold only what the application needs in order to run </b></details> <details> <summary>You are interested in running a container with snake game application. How can you search for such image and check if it exists?</summary><br><b> `podman search snake-game`. Surprisingly, there are a couple of matches :) ``` INDEX NAME DESCRIPTION STARS docker.io docker.io/dyego/snake-game 0 docker.io docker.io/ainizetap/snake-game 0 docker.io docker.io/islamifauzi/snake-games 0 docker.io docker.io/harish1551/snake-game 0 docker.io docker.io/spkane/snake-game A console based snake game in a container 0 docker.io docker.io/rahulgadre/snake-game This repository contains all the files to ru... 0 ``` </b></details> <details>", "metadata": {"source_file": "learning-materials/topics/containers/README.md", "section": "Images", "language": "en", "created_at": "2025-07-19T19:22:02.196915"}}
{"text": "<summary>How to list the container images on certain host?</summary><br><b> ``` CONTAINER_BINARY=podman $CONTAINER_BINARY images ``` Note: you can also use `$CONTAINER_RUNTIME image ls` </b></details> <details> <summary>How to download/pull a container image without actually running a container?</summary><br><b> ``` CONTAINER_BINARY=podman $CONTAINER_BINARY pull rhel ``` </b></details> <details> <summary>True or False? It's not possible to remove an image if a certain container is using it</summary><br><b> True. You should stop and remove the container before trying to remove the image it uses. </b></details> <details> <summary>True or False? If a tag isn't specified when pulling an image, the 'latest' tag is being used</summary><br><b> True </b></details> <details> <summary>True or False? Using the 'latest' tag when pulling an image means, you are pulling the most recently published image</summary><br><b> False. While this might be true in some cases, it's not guaranteed that you'll", "metadata": {"source_file": "learning-materials/topics/containers/README.md", "section": "Images", "language": "en", "created_at": "2025-07-19T19:22:02.196947"}}
{"text": "pull the latest published image when using the 'latest' tag.<br> For example, in some images, 'edge' tag is used for the most recently published images. </b></details> <details> <summary>Where pulled images are stored?</summary><br><b> Depends on the container technology being used. For example, in case of Docker, images are stored in `/var/lib/docker/` </b></details> <details> <summary>Explain container image layers</summary><br><b> - The layers of an image is where all the content is stored - code, files, etc. - Each layer is independent - Each layer has an ID that is an hash based on its content - The layers (as the image) are immutable which means a change to one of the layers can be easily identified </b></details> <details> <summary>True or False? Changing the content of any of the image layers will cause the hash content of the image to change</summary><br><b> True. These hashes are content based and since images (and their layers) are immutable, any change will cause the hashes", "metadata": {"source_file": "learning-materials/topics/containers/README.md", "section": "Images", "language": "en", "created_at": "2025-07-19T19:22:02.197091"}}
{"text": "to change. </b></details> <details> <summary>How to list the layers of an image?</summary><br><b> In case of Docker, you can use `docker image inspect <name>` </b></details> <details> <summary>True or False? In most cases, container images contain their own kernel</summary><br><b> False. They share and access the one used by the host on which they are running. </b></details> <details> <summary>True or False? A single container image can have multiple tags</summary><br><b> True. When listing images, you might be able to see two images with the same ID but different tags. </b></details> <details> <summary>What is a dangling image?</summary><br><b> It's an image without tags attached to it. One way to reach this situation is by building an image with exact same name and tag as another already existing image. It can be still referenced by using its full SHA. </b></details> <details> <summary>How to see changes done to a given image over time?</summary><br><b> In the case of Docker, you", "metadata": {"source_file": "learning-materials/topics/containers/README.md", "section": "Images", "language": "en", "created_at": "2025-07-19T19:22:02.197122"}}
{"text": "could use `docker history <name>` </b></details> <details> <summary>What `podman commit` does?. When will you use it?</summary><br><b> Creates a new image from a running container. Users can apply extra changes to be saved in the new image version. Most of the time the user case for using `podman commit` would be to apply changes allowing to better debug the container. Not so much for creating a new image since commit adds additional overhead of potential logs and processes, not required for running the application in the container. This eventually makes images created by `podman commit` bigger due to the additional data stored there. </b></details> <details> <summary>True or False? Multiple images can share layers</summary><br><b> True.<br> One evidence for that can be found in pulling images. Sometimes when you pull an image, you'll see a line similar to the following:<br> `fa20momervif17: already exists` This is because it recognizes such layer already exists on the host, so there", "metadata": {"source_file": "learning-materials/topics/containers/README.md", "section": "Images", "language": "en", "created_at": "2025-07-19T19:22:02.197143"}}
{"text": "is no need to pull the same layer twice. </b></details> <details> <summary>What is the digest of an image? What problem does it solves?</summary><br><b> Tags are mutable. This is mean that we can have two different images with the same name and the same tag. It can be very confusing to see two images with the same name and the same tag in your environment. How would you know if they are truly the same or are they different?<br> This is where \"digests` come handy. A digest is a content-addressable identifier. It isn't mutable as tags. Its value is predictable and this is how you can tell if two images are the same content wise and not merely by looking at the name and the tag of the images. </b></details> <details> <summary>True or False? A single image can support multiple architectures (Linux x64, Windows x64, ...)</summary><br><b> True. </b></details> <details> <summary>What is a distribution hash in regards to layers?</summary><br><b> - Layers are compressed when pushed or pulled -", "metadata": {"source_file": "learning-materials/topics/containers/README.md", "section": "Images", "language": "en", "created_at": "2025-07-19T19:22:02.197162"}}
{"text": "distribution hash is the hash of the compressed layer - the distribution hash used when pulling or pushing images for verification (making sure no one tempered with image or layers) - It's also used for avoiding ID collisions (a case where two images have exactly the same generated ID) </b></details> <details> <summary>How multi-architecture images work? Explain by describing what happens when an image is pulled</summary><br><b> 1. A client makes a call to the registry to use a specific image (using an image name and optionally a tag) 2. A manifest list is parsed (assuming it exists) to check if the architecture of the client is supported and available as a manifest 3. If it is supported (a manifest for the architecture is available) the relevant manifest is parsed to obtain the IDs of the layers 4. Each layer is then pulled using the obtained IDs from the previous step </b></details> <details> <summary>How to check which architectures a certain container image", "metadata": {"source_file": "learning-materials/topics/containers/README.md", "section": "Images", "language": "en", "created_at": "2025-07-19T19:22:02.197181"}}
{"text": "supports?</summary><br><b> `docker manifest inspect <name>` </b></details> <details> <summary>How to check what a certain container image will execute once we'll run a container based on that image?</summary><br><b> Look for \"Cmd\" or \"Entrypoint\" fields in the output of `docker image inspec <image name>` </b></details> <details> <summary>How to view the instructions that were used to build image?</summary><br><b> `docker image history <image name>:<tag>` </b></details> <details> <summary>How <code>docker image build</code> works?</summary><br><b> 1. Docker spins up a temporary container 2. Runs a single instruction in the temporary container 3. Stores the result as a new image layer 4. Remove the temporary container 5. Repeat for every instruction </b></details> <details> <summary>What is the role of cache in image builds?</summary><br><b> When you build an image for the first time, the different layers are being cached. So, while the first build of the image might take time, any other", "metadata": {"source_file": "learning-materials/topics/containers/README.md", "section": "Images", "language": "en", "created_at": "2025-07-19T19:22:02.197199"}}
{"text": "build of the same image (given that Containerfile/Dockerfile didn't change or the content used by the instructions) will be instant thanks to the caching mechanism used. In little bit more details, it works this way: 1. The first instruction (FROM) will check if base image already exists on the host before pulling it 2. For the next instruction, it will check in the build cache if an existing layer was built from the same base image + if it used the same instruction 1. If it finds such layer, it skips the instruction and links the existing layer and it keeps using the cache. 2. If it doesn't find a matching layer, it builds the layer and the cache is invalidated. Note: in some cases (like COPY and ADD instructions) the instruction might stay the same but if the content of what being copied is changed then the cache is invalidated. The way this check is done is by comparing the checksum of each file that is being copied. </b></details> <details> <summary>How to remove an image from the", "metadata": {"source_file": "learning-materials/topics/containers/README.md", "section": "Images", "language": "en", "created_at": "2025-07-19T19:22:02.197217"}}
{"text": "host?</summary><br><b> `podman rmi IMAGE` It will fail if some containers are using it. You can then use `--force` flag for that but generally, it's better if you inspect the containers using the image before doing so. To delete all images: `podman rmi -a` </b></details> <details> <summary>What ways are there to reduce container images size?</summary><br><b> * Reduce number of instructions - in some case you may be able to join layers by installing multiple packages with one instructions for example or using `&&` to concatenate RUN instructions * Using smaller images - in some cases you might be using images that contain more than what is needed for your application to run. It is good to get overview of some images and see whether you can use smaller images that you are usually using. * Cleanup after running commands - some commands, like packages installation, create some metadata or cache that you might not need for running the application. It's important to clean up after such", "metadata": {"source_file": "learning-materials/topics/containers/README.md", "section": "Images", "language": "en", "created_at": "2025-07-19T19:22:02.197337"}}
{"text": "commands to reduce the image size * For Docker images, you can use multi-stage builds </b></details> <details> <summary>What are the pros and cons of squashing images?</summary><br><b> Pros: * Smaller image * Reducing number of layers (especially if the image has lot of layers) Cons: * No sharing of the image layers * Push and pull can take more time (because no matching layers found on target) </b></details> <details> <summary>You would like to share an image with another developer, but without using a registry. How would you do it?</summary><br><b> ```", "metadata": {"source_file": "learning-materials/topics/containers/README.md", "section": "Images", "language": "en", "created_at": "2025-07-19T19:22:02.197373"}}
{"text": "podman save -o some_image.tar IMAGE rsync some_image.tar SOME_HOST", "metadata": {"source_file": "learning-materials/topics/containers/README.md", "section": "On the local host", "language": "en", "created_at": "2025-07-19T19:22:02.197399"}}
{"text": "podman load -i some_image.tar ``` </b></details> <details> <summary>True or False? Once a container is stopped and removed, its image removed as well from the host</summary><br><b> False. The image will still be available for use by potential containers in the future.<br> To remove the container, run `podman rmi IMAGE` </b></details> <details> <summary>How to view the instructions that were used to build image?</summary><br><b> `docker image history <image name>:<tag>` </b></details> <details> <summary>How to find out which files were added to the container image filesystem?</summary><br><b> `podman diff IMAGE_NAME` </b></details> <details> <summary>True or False? <code>podman diff</code> works only on the container filesystem and not mounted files</summary><br><b> True. For mounted files you can use `podman inspec CONTAINER_NAMD/ID` </b></details> <details> <summary>How the centralized location, where images are stored, is called?</summary><br><b> Registry </b></details>", "metadata": {"source_file": "learning-materials/topics/containers/README.md", "section": "On the remote host", "language": "en", "created_at": "2025-07-19T19:22:02.197546"}}
{"text": "<details> <summary>What is a Registry?</summary><br><b> - A registry is a service which stores container images and allows users to pull specified images to run containers. - There are public registries (everyone can access them) and private (accessed only internally in the organization or specific network) </b></details> <details> <summary>A registry contains one or more <code>____</code> which in turn contain one or more <code>____</code></summary><br><b> A registry contains one or more repositories which in turn contain one or more images. </b></details> <details> <summary>How to find out which registry do you use by default from your environment?</summary><br><b> Depends on the containers technology you are using. For example, in case of Docker, it can be done with `docker info` ``` > docker info Registry: https://index.docker.io/v1 ``` </b></details> <details> <summary>How to configure registries with the containers engine you are using?</summary><br><b> For podman, registries can", "metadata": {"source_file": "learning-materials/topics/containers/README.md", "section": "Registry", "language": "en", "created_at": "2025-07-19T19:22:02.197929"}}
{"text": "be configured in `/etc/containers/registries.conf` this way: ``` [registries.search] registries = [\"quay.io\"] ``` </b></details> <details> <summary>How to retrieve the latest ubuntu image?</summary><br><b> `podman image pull ubuntu:latest` </b></details> <details> <summary>How to push an image to a registry?</summary><br><b> `podman push IMAGE` You can specify a specific registry: `podman push IMAGE REGISTRY_ADDRESS` </b></details> <details> <summary>What are some best practices in regards to Container Images?</summary><br><b> - Use tags. Using `latest` is quite common (which can mean latest build or latest release) - tag like `3.1` can be used to reference the latest release/tag of the image like `3.1.6` - Don't use `commit` for creating new official images as they include the overhead of logs and processes and usually end up with bigger images - For sharing the image, use a registry (either a public or a private one, depends on your needs) </b></details> <details> <summary>What ways", "metadata": {"source_file": "learning-materials/topics/containers/README.md", "section": "Registry", "language": "en", "created_at": "2025-07-19T19:22:02.197957"}}
{"text": "are there for creating new images?</summary><br><b> 1. Create a Containerfile/Dockerfile and build an image out of it 2. Using `podman commit` on a running container after making changes to it </b></details>", "metadata": {"source_file": "learning-materials/topics/containers/README.md", "section": "Registry", "language": "en", "created_at": "2025-07-19T19:22:02.197976"}}
{"text": "<details> <summary>What are image tags? Why is it recommended to use tags when supporting multiple releases/versions of a project?</summary><br><b> Image tags are used to distinguish between multiple versions of the same software or project. Let's say you developed a project called \"FluffyUnicorn\" and the current release is `1.0`. You are about to release `1.1` but you still want to keep `1.0` as stable release for anyone who is interested in it. What would you do? If your answer is create another, separate new image, then you probably want to rethink the idea and just create a new image tag for the new release. In addition, it's important to note that container registries support tags. So when pulling an image, you can specify a specific tag of that image. </b></details> <details> <summary>How to tag an image?</summary><br><b> `podman tag IMAGE:TAG` for example: `podman tag FluffyUnicorn:latest` </b></details> <details> <summary>True or False? Once created, it's impossible to remove a", "metadata": {"source_file": "learning-materials/topics/containers/README.md", "section": "Tags", "language": "en", "created_at": "2025-07-19T19:22:02.198192"}}
{"text": "tag for a certain image</summary><br><b> False. You can run `podman rmi IMAGE:TAG`. </b></details> <details> <summary>True or False? Multiple tags can reference the same image</summary><br><b> True. </b></details>", "metadata": {"source_file": "learning-materials/topics/containers/README.md", "section": "Tags", "language": "en", "created_at": "2025-07-19T19:22:02.198212"}}
{"text": "<details> <summary>What is a Containerfile/Dockerfile?</summary><br><b> Different container engines (e.g. Docker, Podman) can build images automatically by reading the instructions from a Containerfile/Dockerfile. A Containerfile/Dockerfile is a text file that contains all the instructions for building an image which containers can use. </b></details> <details> <summary>What instruction exists in every Containerfile/Dockefile and what does it do?</summary><br><b> In every Containerfile/Dockerfile, you can find the instruction `FROM <image name>` which is also the first instruction (at least most of the time. You can put ARG before).<br> It specifies the base layer of the image to be used. Every other instruction is a layer on top of that base image. </b></details> <details> <summary>List five different instructions that are available for use in a Containerfile/Dockerfile</summary><br><b> * WORKDIR: sets the working directory inside the image filesystems for all the instructions", "metadata": {"source_file": "learning-materials/topics/containers/README.md", "section": "Containerfile", "language": "en", "created_at": "2025-07-19T19:22:02.199584"}}
{"text": "following it * EXPOSE: exposes the specified port (it doesn't adds a new layer, rather documented as image metadata) * ENTRYPOINT: specifies the startup commands to run when a container is started from the image * ENV: sets an environment variable to the given value * USER: sets the user (and optionally the user group) to use while running the image </b></details> <details> <summary>What are some of the best practices regarding Containerfiles/Dockerfiles that you are following?</summary><br><b> * Include only the packages you are going to use. Nothing else. * Specify a tag in FROM instruction. Not using a tag means you'll always pull the latest, which changes over time and might result in unexpected result. * Do not use environment variables to share secrets * Use images from official repositories * Keep images small! - you want them only to include what is required for the application to run successfully. Nothing else. * If are using the apt package manager, you might want to use", "metadata": {"source_file": "learning-materials/topics/containers/README.md", "section": "Containerfile", "language": "en", "created_at": "2025-07-19T19:22:02.199800"}}
{"text": "'no-install-recommends' with `apt-get install` to install only main dependencies (instead of suggested, recommended packages) </b></details> <details> <summary>What is the \"build context\"?</summary><br><b> [Docker docs](https://docs.docker.com/engine/reference/commandline/build): \"A build’s context is the set of files located in the specified PATH or URL\" </b></details> <details> <summary>What is the difference between ADD and COPY in Containerfile/Dockerfile?</summary><br><b> COPY takes in a source and destination. It lets you copy in a file or directory from the build context into the Docker image itself.<br> ADD lets you do the same, but it also supports two other sources. You can use a URL instead of a file or directory from the build context. In addition, you can extract a tar file from the source directly into the destination. </b></details> <details> <summary>What is the difference between CMD and RUN in Containerfile/Dockerfile?</summary><br><b> RUN lets you execute commands", "metadata": {"source_file": "learning-materials/topics/containers/README.md", "section": "Containerfile", "language": "en", "created_at": "2025-07-19T19:22:02.199834"}}
{"text": "inside of your Docker image. These commands get executed once at build time and get written into your Docker image as a new layer. CMD is the command the container executes by default when you launch the built image. A Containerfile/Dockerfile can only have one CMD. You could say that CMD is a Docker run-time operation, meaning it’s not something that gets executed at build time. It happens when you run an image. A running image is called a container. </b></details> <details> <summary>How to create a new image using a Containerfile/Dockerfile?</summary><br><b> The following command is executed from within the directory where Dockefile resides: `docker image build -t some_app:latest .` `podman image build -t some_app:latest .` </b></details> <details> <summary>Do you perform any checks or testing on your Containerfiles/Dockerfiles?</summary><br><b> One option is to use [hadolint](https://github.com/hadolint/hadolint) project which is a linter based on Containerfile/Dockerfile best", "metadata": {"source_file": "learning-materials/topics/containers/README.md", "section": "Containerfile", "language": "en", "created_at": "2025-07-19T19:22:02.199859"}}
{"text": "practices. </b></details> <details> <summary>Which instructions in Containerfile/Dockerfile create new layers?</summary><br><b> Instructions such as FROM, COPY and RUN, create new image layers instead of just adding metadata. </b></details> <details> <summary>Which instructions in Containerfile/Dockerfile create image metadata and don't create new layers?</summary><br><b> Instructions such as ENTRYPOINT, ENV, EXPOSE, create image metadata and they don't create new layers. </b></details> <details> <summary>Is it possible to identify which instruction create a new layer from the output of <code>podman image history</code>?</summary><br><b> </b></details> <details> <summary>True or False? Each Containerfile instruction runs in an independent container using an image built from every previous layer/entry</summary><br><b> True </b></details> <details> <summary>What's the difference between these two forms: ``` ENTRYPOINT [\"cmd\", \"param0\", \"param1\"] CMD [\"param0\"] ENTRYPOINT cmd param0", "metadata": {"source_file": "learning-materials/topics/containers/README.md", "section": "Containerfile", "language": "en", "created_at": "2025-07-19T19:22:02.199881"}}
{"text": "param1 CMD param0 ``` </summary><br><b> The first form is also referred as \"Exec form\" and the second one is referred as \"Shell form\".<br> The second one (Shell form) wraps the commands in `/bin/sh -c` hence creates a shell process for it. While using either Exec form or Shell form might be fine, it's the mixing that can lead to unexpected results.<br> Consider: ``` ENTRYPOINT [\"ls\"] CMD /tmp ``` That would results in running `ls /bin/sh -c /tmp` </b></details> <details> <summary>Containerfile/Dockerfile can contain more than one ENTRYPOINT instruction and one CMD instruction</summary><br><b> True but in case of ENTRYPOINT and CMD only the last instruction takes effect. </b></details> <details> <summary>What happens when CMD instruction is defined but not an ENTRYPOINT instruction in a Containerfile/Dockerfile?</summary><br><b> The ENTRYPOINT from the base image is being used in such case. </b></details> <details> <summary>In the case of running <code>podman run -it IMAGE ls</code> the", "metadata": {"source_file": "learning-materials/topics/containers/README.md", "section": "Containerfile", "language": "en", "created_at": "2025-07-19T19:22:02.199899"}}
{"text": "<code>ls</code> overrides the <code>___</code> instruction</summary><br><b> CMD </b></details> <a name=\"questions-containers-storage\"></a>", "metadata": {"source_file": "learning-materials/topics/containers/README.md", "section": "Containerfile", "language": "en", "created_at": "2025-07-19T19:22:02.199918"}}
{"text": "<details> <summary>Container storage is said to be ephemeral. What does it mean?</summary><br><b> It means the contents of the container and the data generated by it, is gone when the container is removed. </b></details> <details> <summary>True or False? Applications running on containers, should use the container storage to store persistent data</summary><br><b> False. Containers are not built to store persistent data and even if it's possible with some implementations, it might not perform well in case of applications with intensive I/O operations. </b></details> <details> <summary>You stopped a running container but, it still uses the storage in case you ever resume it. How to reclaim the storage of a container?</summary><br><b> In order to reclaim the storage of a container, you have to remove it. </b></details> <details> <summary>How to create a new volume?</summary><br><b> ``` CONTAINER_BINARY=podman $CONTAINER_BINARY volume create some_volume ``` </b></details> <details>", "metadata": {"source_file": "learning-materials/topics/containers/README.md", "section": "Storage", "language": "en", "created_at": "2025-07-19T19:22:02.200130"}}
{"text": "<summary>How to mount a directory from the host to a container?</summary><br><b> ``` CONTAINER_BINARY=podman mkdir /tmp/dir_on_the_host $CONTAINER_BINARY run -v /tmp/dir_on_the_host:/tmp/dir_on_the_container IMAGE_NAME ``` In some systems you'll have also to adjust security on the host itself: ``` podman unshare chown -R UID:GUID /tmp/dir_on_the_host sudo semanage fcontext -a -t container_file_t '/tmp/dir_on_the_host(/.*)?' sudo restorecon -Rv /tmp/dir_on_the_host ``` </b></details> <a name=\"questions-containerfile\"></a> <a name=\"questions-architecture\"></a>", "metadata": {"source_file": "learning-materials/topics/containers/README.md", "section": "Storage", "language": "en", "created_at": "2025-07-19T19:22:02.200150"}}
{"text": "<details> <summary>How container achieve isolation from the rest of the system?</summary><br><b> Through the use of namespaces and cgroups. Linux kernel has several types of namespaces: - Process ID namespaces: these namespaces include independent set of process IDs - Mount namespaces: Isolation and control of mountpoints - Network namespaces: Isolates system networking resources such as routing table, interfaces, ARP table, etc. - UTS namespaces: Isolate host and domains - IPC namespaces: Isolates interprocess communications - User namespaces: Isolate user and group IDs - Time namespaces: Isolates time machine </b></details> <details> <summary>What Linux kernel features does containers use?</summary><br><b> * cgroups (Control Groups): used for limiting the amount of resources a certain groups of processes (and their children of course) use. This way, a group of processes isn't consuming all host resources and other groups can run and use part of the resources as well * namespaces:", "metadata": {"source_file": "learning-materials/topics/containers/README.md", "section": "Architecture", "language": "en", "created_at": "2025-07-19T19:22:02.200788"}}
{"text": "same as cgroups, namespaces isolate some of the system resources so it's available only for processes in the namespace. Differently from cgroups the focus with namespaces is on resources like mount points, IPC, network, ... and not about memory and CPU as in cgroups * SElinux: the access control mechanism used to protect processes. Unfortunately to this date many users don't actually understand SElinux and some turn it off but nonetheless, it's a very important security feature of the Linux kernel, used by container as well * Seccomp: similarly to SElinux, it's also a security mechanism, but its focus is on limiting the processes in regards to using system calls and file descriptors </b></details> <details> <summary>Describe in detail what happens when you run `podman/docker run hello-world`?</summary><br><b> Docker/Podman CLI passes your request to Docker daemon. Docker/Podman daemon downloads the image from Docker Hub Docker/Podman daemon creates a new container by using the image it", "metadata": {"source_file": "learning-materials/topics/containers/README.md", "section": "Architecture", "language": "en", "created_at": "2025-07-19T19:22:02.201000"}}
{"text": "downloaded Docker/Podman daemon redirects output from container to Docker CLI which redirects it to the standard output </b></details> <details> <summary>Describe difference between cgroups and namespaces</summary><br><b> cgroup: Control Groups provide a mechanism for aggregating/partitioning sets of tasks, and all their future children, into hierarchical groups with specialized behavior. namespace: wraps a global system resource in an abstraction that makes it appear to the processes within the namespace that they have their own isolated instance of the global resource. In short: Cgroups = limits how much you can use; namespaces = limits what you can see (and therefore use) Cgroups involve resource metering and limiting: memory CPU block I/O network Namespaces provide processes with their own view of the system Multiple namespaces: pid,net, mnt, uts, ipc, user </b></details> <details> <summary>Which of the following are Linux features that containers use? * cspaces * namegroups *", "metadata": {"source_file": "learning-materials/topics/containers/README.md", "section": "Architecture", "language": "en", "created_at": "2025-07-19T19:22:02.201033"}}
{"text": "namespaces * cgroups * ELlinux * SElinux</summary><br><b> * namespaces * cgroups * SElinux </b></details> <details> <summary>True or False? Containers have ephemeral storage layer</summary><br><b> True. The ephemeral storage layer is added on top of the base image layer and is exclusive to the running container. This way, containers created from the same base image, don't share the same storage. </b></details> <a name=\"questions-docker-architecture\"></a>", "metadata": {"source_file": "learning-materials/topics/containers/README.md", "section": "Architecture", "language": "en", "created_at": "2025-07-19T19:22:02.201053"}}
{"text": "<details> <summary>Which components/layers compose the Docker technology?</summary><br><b> 1. Runtime - responsible for starting and stopping containers 2. Daemon - implements the Docker API and takes care of managing images (including builds), authentication, security, networking, etc. 3. Orchestrator </b></details> <details> <summary>What components are part of the Docker engine?</summary><br><b> - Docker daemon - containerd - runc </b></details> <details> <summary>What is the low-level runtime?</summary><br><b> - The low level runtime is called runc - It manages every container running on Docker host - Its purpose is to interact with the underlying OS to start and stop containers - Its reference implementation is of the OCI (Open Containers Initiative) container-runtime-spec - It's a small CLI wrapper for libcontainer </b></details> <details> <summary>What is the high-level runtime?</summary><br><b> - The high level runtime is called containerd - It was developed by Docker Inc and", "metadata": {"source_file": "learning-materials/topics/containers/README.md", "section": "Docker Architecture", "language": "en", "created_at": "2025-07-19T19:22:02.202612"}}
{"text": "at some point donated to CNCF - It manages the whole lifecycle of a container - start, stop, remove and pause - It take care of setting up network interfaces, volume, pushing and pulling images, ... - It manages the lower level runtime (runc) instances - It's used both by Docker and Kubernetes as a container runtime - It sits between Docker daemon and runc at the OCI layer Note: running `ps -ef | grep -i containerd` on a system with Docker installed and running, you should see a process of containerd </b></details> <details> <summary>True or False? The docker daemon (dockerd) performs lower-level tasks compared to containerd</summary><br><b> False. The Docker daemon performs higher-level tasks compared to containerd.<br> It's responsible for managing networks, volumes, images, ... </b></details> <details> <summary>Describe in detail what happens when you run `docker pull image:tag`?</summary><br><b> Docker CLI passes your request to Docker daemon. Dockerd Logs shows the process", "metadata": {"source_file": "learning-materials/topics/containers/README.md", "section": "Docker Architecture", "language": "en", "created_at": "2025-07-19T19:22:02.202647"}}
{"text": "docker.io/library/busybox:latest resolved to a manifestList object with 9 entries; looking for a unknown/amd64 match found match for linux/amd64 with media type application/vnd.docker.distribution.manifest.v2+json, digest sha256:400ee2ed939df769d4681023810d2e4fb9479b8401d97003c710d0e20f7c49c6 pulling blob \\\"sha256:61c5ed1cbdf8e801f3b73d906c61261ad916b2532d6756e7c4fbcacb975299fb Downloaded 61c5ed1cbdf8 to tempfile /var/lib/docker/tmp/GetImageBlob909736690 Applying tar in /var/lib/docker/overlay2/507df36fe373108f19df4b22a07d10de7800f33c9613acb139827ba2645444f7/diff\" storage-driver=overlay2 Applied tar sha256:514c3a3e64d4ebf15f482c9e8909d130bcd53bcc452f0225b0a04744de7b8c43 to 507df36fe373108f19df4b22a07d10de7800f33c9613acb139827ba2645444f7, size: 1223534 </b></details> <details> <summary>Describe in detail what happens when you run a container</summary><br><b> 1. The Docker client converts the run command into an API payload 2. It then POST the payload to the API endpoint exposed by the", "metadata": {"source_file": "learning-materials/topics/containers/README.md", "section": "Docker Architecture", "language": "en", "created_at": "2025-07-19T19:22:02.202668"}}
{"text": "Docker daemon 3. When the daemon receives the command to create a new container, it makes a call to containerd via gRPC 4. containerd converts the required image into an OCI bundle and tells runc to use that bundle for creating the container 5. runc interfaces with the OS kernel to pull together the different constructs (namespace, cgroups, etc.) used for creating the container 6. Container process is started as a child-process of runc 7. Once it starts, runc exists </b></details> <details> <summary>True or False? Killing the Docker daemon will kill all the running containers</summary><br><b> False. While this was true at some point, today the container runtime isn't part of the daemon (it's part of containerd and runc) so stopping or killing the daemon will not affect running containers. </b></details> <details> <summary>True or False? containerd forks a new instance runc for every container it creates</summary><br><b> True </b></details> <details> <summary>True or False? Running a", "metadata": {"source_file": "learning-materials/topics/containers/README.md", "section": "Docker Architecture", "language": "en", "created_at": "2025-07-19T19:22:02.202687"}}
{"text": "dozen of containers will result in having a dozen of runc processes</summary><br><b> False. Once a container is created, the parent runc process exists. </b></details> <details> <summary>What is shim in regards to Docker?</summary><br><b> shim is the process that becomes the container's parent when runc process exists. It's responsible for: - Reporting exit code back to the Docker daemon - Making sure the container doesn't terminate if the daemon is being restarted. It does so by keeping the stdout and stdin open </b></details> <details> <summary>How would you transfer data from one container into another?</summary><br><b> </b></details> <details> <summary>What happens to data of the container when a container exists?</summary><br><b> </b></details> <details> <summary>How do you remove old, non running, containers?</summary><br><b> 1. To remove one or more Docker images use the docker container rm command followed by the ID of the containers you want to remove. 2. The docker system", "metadata": {"source_file": "learning-materials/topics/containers/README.md", "section": "Docker Architecture", "language": "en", "created_at": "2025-07-19T19:22:02.202706"}}
{"text": "prune command will remove all stopped containers, all dangling images, and all unused networks 3. docker rm $(docker ps -a -q) - This command will delete all stopped containers. The command docker ps -a -q will return all existing container IDs and pass them to the rm command which will delete them. Any running containers will not be deleted. </b></details> <details> <summary>How the Docker client communicates with the daemon?</summary><br><b> Via the local socket at `/var/run/docker.sock` </b></details> <details> <summary>Explain Docker interlock</summary><br><b> </b></details> <details> <summary>What is Docker Repository?</summary><br><b> </b></details> <details> <summary>Explain image layers</summary><br><b> A Docker image is built up from a series of layers. Each layer represents an instruction in the image’s Containerfile/Dockerfile. Each layer except the very last one is read-only. Each layer is only a set of differences from the layer before it. The layers are stacked on top of", "metadata": {"source_file": "learning-materials/topics/containers/README.md", "section": "Docker Architecture", "language": "en", "created_at": "2025-07-19T19:22:02.203000"}}
{"text": "each other. When you create a new container, you add a new writable layer on top of the underlying layers. This layer is often called the “container layer”. All changes made to the running container, such as writing new files, modifying existing files, and deleting files, are written to this thin writable container layer. The major difference between a container and an image is the top writable layer. All writes to the container that add new or modify existing data are stored in this writable layer. When the container is deleted, the writable layer is also deleted. The underlying image remains unchanged. Because each container has its own writable container layer, and all changes are stored in this container layer, multiple containers can share access to the same underlying image and yet have their own data state. </b></details> <details> <summary>What best practices are you familiar related to working with containers?</summary><br><b> </b></details> <details> <summary>How do you", "metadata": {"source_file": "learning-materials/topics/containers/README.md", "section": "Docker Architecture", "language": "en", "created_at": "2025-07-19T19:22:02.203038"}}
{"text": "manage persistent storage in Docker?</summary><br><b> </b></details> <details> <summary>How can you connect from the inside of your container to the localhost of your host, where the container runs?</summary><br><b> </b></details> <details> <summary>How do you copy files from Docker container to the host and vice versa?</summary><br><b> </b></details> <a name=\"questions-docker-compose\"></a>", "metadata": {"source_file": "learning-materials/topics/containers/README.md", "section": "Docker Architecture", "language": "en", "created_at": "2025-07-19T19:22:02.203060"}}
{"text": "<details> <summary>Explain what is Docker compose and what is it used for</summary><br><b> Compose is a tool for defining and running multi-container Docker applications. With Compose, you use a YAML file to configure your application’s services. Then, with a single command, you create and start all the services from your configuration. For example, you can use it to set up ELK stack where the services are: elasticsearch, logstash and kibana. Each running in its own container.<br> In general, it's useful for running applications which composed out of several different services. It let's you manage it as one deployed app, instead of different multiple separate services. </b></details> <details> <summary>Describe the process of using Docker Compose</summary><br><br> * Define the services you would like to run together in a docker-compose.yml file * Run `docker-compose up` to run the services </b></details> <details> <summary>Explain Multi-stage builds</summary><br><b> Multi-stages builds", "metadata": {"source_file": "learning-materials/topics/containers/README.md", "section": "Docker Compose", "language": "en", "created_at": "2025-07-19T19:22:02.203745"}}
{"text": "allow you to produce smaller container images by splitting the build process into multiple stages. As an example, imagine you have one Containerfile/Dockerfile where you first build the application and then run it. The whole build process of the application might be using packages and libraries you don't really need for running the application later. Moreover, the build process might produce different artifacts which not all are needed for running the application. How do you deal with that? Sure, one option is to add more instructions to remove all the unnecessary stuff but, there are a couple of issues with this approach: 1. You need to know what to remove exactly and that might be not as straightforward as you think 2. You add new layers which are not really needed A better solution might be to use multi-stage builds where one stage (the build process) is passing the relevant artifacts/outputs to the stage that runs the application. </b></details> <details> <summary>True or False? In", "metadata": {"source_file": "learning-materials/topics/containers/README.md", "section": "Docker Compose", "language": "en", "created_at": "2025-07-19T19:22:02.203779"}}
{"text": "multi-stage builds, artifacts can be copied between stages</summary><br><b> True. This allows us to eventually produce smaller images. </b></details> <details> <summary>What <code>.dockerignore</code> is used for?</summary><br><b> By default, Docker uses everything (all the files and directories) in the directory you use as build context.<br> `.dockerignore` used for excluding files and directories from the build context </b></details> <a name=\"questions-networking\"></a>", "metadata": {"source_file": "learning-materials/topics/containers/README.md", "section": "Docker Compose", "language": "en", "created_at": "2025-07-19T19:22:02.203800"}}
{"text": "<details> <summary>What container network standards or architectures are you familiar with?</summary><br><b> CNM (Container Network Model): * Requires distrubited key value store (like etcd for example) for storing the network configuration * Used by Docker CNI (Container Network Interface): * Network configuration should be in JSON format </b></details> <a name=\"questions-docker-networking\"></a>", "metadata": {"source_file": "learning-materials/topics/containers/README.md", "section": "Networking", "language": "en", "created_at": "2025-07-19T19:22:02.203848"}}
{"text": "<details> <summary>What network specification Docker is using and how its implementation is called?</summary><br><b> Docker is using the CNM (Container Network Model) design specification.<br> The implementation of CNM specification by Docker is called \"libnetwork\". It's written in Go. </b></details> <details> <summary>Explain the following blocks in regards to CNM: * Networks * Endpoints * Sandboxes</summary><br><b> * Networks: software implementation of an switch. They used for grouping and isolating a collection of endpoints. * Endpoints: Virtual network interfaces. Used for making connections. * Sandboxes: Isolated network stack (interfaces, routing tables, ports, ...) </b></details> <details> <summary>True or False? If you would like to connect a container to multiple networks, you need multiple endpoints</summary><br><b> True. An endpoint can connect only to a single network. </b></details> <details> <summary>What are some features of libnetwork?</summary><br><b> * Native service", "metadata": {"source_file": "learning-materials/topics/containers/README.md", "section": "Docker Networking", "language": "en", "created_at": "2025-07-19T19:22:02.204012"}}
{"text": "discovery * ingress-based load balancer * network control plane and management plane </b></details> <a name=\"questions-security\"></a>", "metadata": {"source_file": "learning-materials/topics/containers/README.md", "section": "Docker Networking", "language": "en", "created_at": "2025-07-19T19:22:02.204032"}}
{"text": "<details> <summary>What security best practices are there regarding containers?</summary><br><b> * Install only the necessary packages in the container * Don't run containers as root when possible * Don't mount the Docker daemon unix socket into any of the containers * Set volumes and container's filesystem to read only * DO NOT run containers with `--privilged` flag </b></details> <details> <summary>A container can cause a kernel panic and bring down the whole host. What preventive actions can you apply to avoid this specific situation?</summary><br><b> * Install only the necessary packages in the container * Set volumes and container's filesystem to read only * DO NOT run containers with `--privilged` flag </b></details> <a name=\"questions-docker-in-production\"></a>", "metadata": {"source_file": "learning-materials/topics/containers/README.md", "section": "Security", "language": "en", "created_at": "2025-07-19T19:22:02.204150"}}
{"text": "<details> <summary>What are some best practices you following in regards to using containers in production?</summary><br><b> Images: * Use images from official repositories * Include only the packages you are going to use. Nothing else. * Specify a tag in FROM instruction. Not using a tag means you'll always pull the latest, which changes over time and might result in unexpected result. * Do not use environment variables to share secrets * Keep images small! - you want them only to include what is required for the application to run successfully. Nothing else. Components: * Secured connection between components (e.g. client and server) </b></details> <details> <summary>True or False? It's recommended for production environments that Docker client and server will communicate over network using HTTP socket</summary><br><b> False. Communication between client and server shouldn't be done over HTTP since it's insecure. It's better to enforce the daemon to only accept network connection", "metadata": {"source_file": "learning-materials/topics/containers/README.md", "section": "Docker in Production", "language": "en", "created_at": "2025-07-19T19:22:02.205034"}}
{"text": "that are secured with TLS.<br> Basically, the Docker daemon will only accept secured connections with certificates from trusted CA. </b></details> <details> <summary>What forms of self-healing options available for Docker containers?</summary><br><b> Restart Policies. It allows you to automatically restart containers after certain events. </b></details> <details> <summary>What restart policies are you familiar with?</summary><br><b> * always: restart the container when it's stopped (not with `docker container stop`) * unless-stopped: restart the container unless it was in stopped status * no: don't restart the container at any point (default policy) * on-failure: restart the container when it exists due to an error (= exit code different than zero) </b></details> <a name=\"questions-rootless-containers\"></a> <details> <summary>Explain Rootless Containers</summary><br><b> Historically, user needed root privileges to run containers. One of the most basic security recommendations is to", "metadata": {"source_file": "learning-materials/topics/containers/README.md", "section": "Docker in Production", "language": "en", "created_at": "2025-07-19T19:22:02.205068"}}
{"text": "provide users with minimum privileges for what they need. For containers it's been the situation for a long time and still for running some containers today from docker.io, you'll need to have root privileges. </b></details> <details> <summary>Are there disadvantages in running rootless containers?</summary><br><b> Yes, the full list can be found [here](https://github.com/containers/podman/blob/main/rootless.md). Some worth to mention: - No binding to ports smaller than 1024 - No images sharing CRI-O or other rootful users - No support running on NFS or parallel filesystem homerdirs - Some commands don't work (mount, podman stats, checkpoint, restore, ...) </b></details> <details> <summary>Give one example of rootless containers are more safe from security perspective</summary><br><b> In rootless containers, user namespace appears to be running as root but it doesn't, it's executed with regular user privileges. If an attacker manages to get out of the user space to the host with the", "metadata": {"source_file": "learning-materials/topics/containers/README.md", "section": "Docker in Production", "language": "en", "created_at": "2025-07-19T19:22:02.205088"}}
{"text": "same privileges, there's not much he can do because it's not root privileges as opposed to containers that run with root privileges. </b></details> <details> <summary>When running a container, usually a virtual ethernet device is created. To do so, root privileges are required. How is it then managed in rootless containers?</summary><br><b> Networking is usually managed by Slirp in rootless containers. Slirp creates a tap device which is also the default route and it creates it in the network namespace of the container. This device's file descriptor passed to the parent who runs it in the default namespace and the default namespace connected to the internet. This enables communication externally and internally. </b></details> <details> <summary>When running a container, usually a layered file system is created, but it requires root privileges. How is it then managed in rootless containers?</summary><br><b> New drivers were created to allow creating filesystems in a user namespaces.", "metadata": {"source_file": "learning-materials/topics/containers/README.md", "section": "Docker in Production", "language": "en", "created_at": "2025-07-19T19:22:02.205107"}}
{"text": "Drivers like the FUSE-OverlayFS. </b></details> <a name=\"questions-oci\"></a>", "metadata": {"source_file": "learning-materials/topics/containers/README.md", "section": "Docker in Production", "language": "en", "created_at": "2025-07-19T19:22:02.205125"}}
{"text": "<details> <summary>What is the OCI?</summary><br><b> OCI (Open Container Initiative) is an open governance established in 2015 to standardize container creation - mostly image format and runtime. At that time there were a number of parties involved and the most prominent one was Docker. Specifications published by OCI: - [image-spec](https://github.com/opencontainers/image-spec) - [runtime-spec](https://github.com/opencontainers/runtime-spec) </b></details> <details> <summary>Which operations OCI based containers must support?</summary><br><b> Create, Kill, Delete, Start and Query State. </b></details> <a name=\"questions-containers-scenarios\"></a>", "metadata": {"source_file": "learning-materials/topics/containers/README.md", "section": "OCI", "language": "en", "created_at": "2025-07-19T19:22:02.205190"}}
{"text": "<details> <summary>There is a running container that has a certain issue. You would like to share an image of that container with your team members, with certain environment variables set for debugging purposes. How would you do it?</summary><br><b> `podman commit` can be a good choice for that. You can create a new image of the running container (with the issue) and share that new image with your team members.<br> What you probably want to avoid using: - Using something as `podman save/load` as it applies on an image, not a running container (so you'll share the image but the issue might not be reproduced when your team members run a container using it) - Modifying Containerfile/Dockerfile as you don't really want to add environment variables meant for debugging to the source from which you usually build images </b></details> <details> <summary>You and your team work on the same project, but different versions of it. For each version, the team creates a new, separate image. What would", "metadata": {"source_file": "learning-materials/topics/containers/README.md", "section": "Scenarios", "language": "en", "created_at": "2025-07-19T19:22:02.205446"}}
{"text": "you suggest the team to change in such case?</summary><br><b> Use tags. You can distinguish between different releases of a project using image tags. There is no need to create an entire separate image for version/release of a project. </b></details>", "metadata": {"source_file": "learning-materials/topics/containers/README.md", "section": "Scenarios", "language": "en", "created_at": "2025-07-19T19:22:02.205468"}}
{"text": "Learn how to run, stop and remove containers", "metadata": {"source_file": "learning-materials/topics/containers/running_containers.md", "section": "Objective", "language": "en", "created_at": "2025-07-19T19:22:02.205590"}}
{"text": "Make sure Podman or Docker (or any other containers engine) is installed on your system", "metadata": {"source_file": "learning-materials/topics/containers/running_containers.md", "section": "Requirements", "language": "en", "created_at": "2025-07-19T19:22:02.205614"}}
{"text": "1. Run a container using the latest nginx image 2. List the containers to make sure the container is running 3. Run another container but this time use ubuntu latest and attach to the terminal of the container 4. List again the containers. How many containers are running? 5. Stop the containers 6. Remove the containers", "metadata": {"source_file": "learning-materials/topics/containers/running_containers.md", "section": "Instructions", "language": "en", "created_at": "2025-07-19T19:22:02.205662"}}
{"text": "1. Create an image: * Use centos or ubuntu as the base image * Install apache web server * Deploy any web application you want * Add https support (using HAProxy as reverse-proxy) 2. Once you wrote the Containerfile and created an image, run the container and test the application. Describe how did you test it and provide output 3. Describe one or more weakness of your Containerfile. Is it ready to be used in production?", "metadata": {"source_file": "learning-materials/topics/containers/write_containerfile_run_container.md", "section": "Objectives", "language": "en", "created_at": "2025-07-19T19:22:02.205957"}}
{"text": "Learn about multi-stage builds", "metadata": {"source_file": "learning-materials/topics/containers/multi_stage_builds.md", "section": "Objective", "language": "en", "created_at": "2025-07-19T19:22:02.206051"}}
{"text": "1. Without actually building an image or running any container, use the following Dockerfile and convert it to use multi-stage: ``` FROM nginx RUN apt-get update \\ && apt-get install -y curl python build-essential \\ && apt-get install -y nodejs \\ && apt-get clean -y RUN mkdir -p /my_app ADD ./config/nginx/docker.conf /etc/nginx/nginx.conf ADD ./config/nginx/k8s.conf /etc/nginx/nginx.conf.k8s ADD app/ /my_cool_app WORKDIR /my_cool_app RUN npm install -g ember-cli RUN npm install -g bower RUN apt-get update && apt-get install -y git \\ && npm install \\ && bower install \\ RUN ember build — environment=prod CMD [ “/root/nginx-app.sh”, “nginx”, “-g”, “daemon off;” ] ``` 2. What are the benefits of using multi-stage builds?", "metadata": {"source_file": "learning-materials/topics/containers/multi_stage_builds.md", "section": "Instructions", "language": "en", "created_at": "2025-07-19T19:22:02.206217"}}
{"text": "Have at least one image locally (run `podman image ls` to confirm).<br> If you don't have images locally, run simply `podman pull nginx:alpine`.", "metadata": {"source_file": "learning-materials/topics/containers/commit_image.md", "section": "Requirements", "language": "en", "created_at": "2025-07-19T19:22:02.206313"}}
{"text": "1. Run a container using a web server image (e.g. httpd, nginx, ...) - Bind container's port 80 to local port 80 - Run it in detached mode - Name should nginx_container 2. Verify the web server runs and accessible 3. Create an HTML file with the following content and copy it to the container to the container to path where it will be accessed as an index file ``` <html> <head> <title>It's a me</title> </head> <body> <h1>Mario</h1> </body> ``` 4. Create an image out of the running container and call it \"nginx_mario\" 5. Tag the container with \"mario\" tag 6. Remove the original container (container_nginx) and verify it was removed 7. Create a new container out of the image you've created (the same way as the original container) 8. Run `curl 127.0.0.1:80`. What do you see? 9. Run `podman diff` on the new image. Explain the output", "metadata": {"source_file": "learning-materials/topics/containers/commit_image.md", "section": "Objectives", "language": "en", "created_at": "2025-07-19T19:22:02.206512"}}
{"text": "Click [here to view the solution](solutions/commit_image.md)", "metadata": {"source_file": "learning-materials/topics/containers/commit_image.md", "section": "Solution", "language": "en", "created_at": "2025-07-19T19:22:02.206537"}}
{"text": "1. Run a container with a database of any type of you prefer (MySql, PostgreSQL, Mongo, etc.) 2. Verify the container is running 3. Access the container and create a new table (or collection, depends on which DB type you chose) for students 4. Insert a row (or document) of a student 5. Verify the row/document was added Click [here for the solution](solutions/containerized_db.md)", "metadata": {"source_file": "learning-materials/topics/containers/containerized_db.md", "section": "Containerized DB", "language": "en", "created_at": "2025-07-19T19:22:02.206649"}}
{"text": "Learn what restart policies do and how to use them", "metadata": {"source_file": "learning-materials/topics/containers/run_forest_run.md", "section": "Objective", "language": "en", "created_at": "2025-07-19T19:22:02.206766"}}
{"text": "1. Run a container with the following properties: * image: alpine * name: forest * restart policy: always * command to execute: sleep 15 2. Run `docker container ls` - Is the container running? What about after 15 seconds, is it still running? why? 3. How then can we stop the container from running? 4. Remove the container you've created 5. Run the same container again but this time with `sleep 600` and verify it runs 6. Restart the Docker service. Is the container still running? why? 8. Update the policy to `unless-stopped` 9. Stop the container 10. Restart the Docker service. Is the container running? why?", "metadata": {"source_file": "learning-materials/topics/containers/run_forest_run.md", "section": "Instructions", "language": "en", "created_at": "2025-07-19T19:22:02.206896"}}
{"text": "Learn how to work with containers images", "metadata": {"source_file": "learning-materials/topics/containers/working_with_images.md", "section": "Objective", "language": "en", "created_at": "2025-07-19T19:22:02.206978"}}
{"text": "1. List the containers images in your environment 2. Pull the latest ubuntu image 3. Run a container with the image you just pulled 4. Remove the image. Did it work? 5. Do whatever is needed in order to remove the image", "metadata": {"source_file": "learning-materials/topics/containers/working_with_images.md", "section": "Instructions", "language": "en", "created_at": "2025-07-19T19:22:02.207023"}}
{"text": "1. Run a containerized web server in the background and bind its port (8080) to a local port 2. Verify the port (8080) is bound 3. Reach the webserver from your local host 4. Now run the same web application but bound it to the local port 8080 Click [here for the solution](solutions/containerized_web_server.md)", "metadata": {"source_file": "learning-materials/topics/containers/containerized_web_server.md", "section": "Containerized Web Server", "language": "en", "created_at": "2025-07-19T19:22:02.207113"}}
{"text": "1. Run a container with a database of any type of you prefer (MySql, PostgreSQL, Mongo, etc.) 1. Use a mount point on the host for the database instead of using the container storage for that 2. Explain why using the host storage instead of the container one might be a better choice 2. Verify the container is running", "metadata": {"source_file": "learning-materials/topics/containers/containerized_db_persistent_storage.md", "section": "Containerized DB with Persistent Storage", "language": "en", "created_at": "2025-07-19T19:22:02.207209"}}
{"text": "Have at least one image locally (run `podman image ls` to confirm).<br> If you don't have images locally, run simply `podman pull httpd`.", "metadata": {"source_file": "learning-materials/topics/containers/sharing_images.md", "section": "Requirements", "language": "en", "created_at": "2025-07-19T19:22:02.207286"}}
{"text": "1. Choose an image and create an archive out of it 2. Check the archive size. Is it different than the image size? If yes, what's the difference? If not, why? 3. Copy the generated archive to a remote host 4. Load the image 5. Verify it was loaded and exists on the remote host", "metadata": {"source_file": "learning-materials/topics/containers/sharing_images.md", "section": "Objectives", "language": "en", "created_at": "2025-07-19T19:22:02.207334"}}
{"text": "Click [here to view the solution](solutions/sharing_images.md)", "metadata": {"source_file": "learning-materials/topics/containers/sharing_images.md", "section": "Solution", "language": "en", "created_at": "2025-07-19T19:22:02.207358"}}
{"text": "1. Write a Dockefile. Any Dockefile! :) (just make sure it's a valid one) ``` FROM ubuntu EXPOSE 212 ENV foo=bar WORKDIR /tmp RUN dd if=/dev/zero of=some_file bs=1024 count=0 seek=1024 RUN dd if=/dev/zero of=some_file bs=1024 count=0 seek=1024 RUN dd if=/dev/zero of=some_file bs=1024 count=0 seek=1024 ``` 2. Build an image using the Dockerfile you've wrote `docker image build -t super_cool_app:latest .` 3. Which of the instructions you've used, created new layers and which added image metadata? ``` FROM, RUN -> new layer EXPOSE, ENV, WORKDIR -> metadata ``` 4. What ways are there to confirm your answer to the last question? You can run `docker image history super_cool_app`. It will show you each instruction and its size. Usually instructions that create new layers has non-zero size, but this is not something you can rely on by itself since, some run commands can have size of zero in `docker image history` output (e.g. `ls -l`). You can also use `docker image inspect super_cool_appl`", "metadata": {"source_file": "learning-materials/topics/containers/solutions/image_layers.md", "section": "Instructions", "language": "en", "created_at": "2025-07-19T19:22:02.207973"}}
{"text": "and see if in the output, under \"RootFS\", there are the number of layers that matches the instructions that should create new layers. 5. Can you reduce the size of the image you've created? yes, for example, use all the RUN instructions as a single RUN instruction this way: `RUN dd if=/dev/zero of=some_file bs=1024 count=0 seek=1024 && dd if=/dev/zero of=some_file bs=1024 count=0 seek=1024 && dd if=/dev/zero of=some_file bs=1024 count=0 seek=1024` The change in size might not be dramatic in this case, but in some cases it will make a big impact on the image size.", "metadata": {"source_file": "learning-materials/topics/containers/solutions/image_layers.md", "section": "Instructions", "language": "en", "created_at": "2025-07-19T19:22:02.208129"}}
{"text": "1. Run a container using the latest nginx image - `podman container run nginx:latest` 2. List the containers to make sure the container is running - `podman container ls` 3. Run another container but this time use ubuntu latest and attach to the terminal of the container - `podman container run -it ubuntu:latest /bin/bash` 4. List again the containers. How many containers are running? - `podman container ls` -> 2 5. Stop the containers - WARNING: the following will stop all the containers on the host: `podman stop $(podman container ls -q)` or for each container `podman stop [container id/name]` 6. Remove the containers - WARNING: the following will remove other containers as well if such are running: `podman rm $(podman container ls -q -a)` or for each container `podman rm [container id/name]`", "metadata": {"source_file": "learning-materials/topics/containers/solutions/running_containers.md", "section": "Instructions", "language": "en", "created_at": "2025-07-19T19:22:02.208404"}}
{"text": "1. One possible solution (the emphasize is on passing the app from the first stage): ``` FROM node:6 RUN mkdir -p /my_cool_app RUN npm install -g ember-cli RUN npm install -g bower WORKDIR /my_cool_app RUN npm install ADD app/ /my_cool_app RUN bower install RUN ember build — environment=prod FROM nginx RUN mkdir -p /my_cool_app ADD ./config/nginx/docker.conf /etc/nginx/nginx.conf ADD ./config/nginx/k8s.conf /etc/nginx/nginx.conf.k8s", "metadata": {"source_file": "learning-materials/topics/containers/solutions/multi_stage_builds.md", "section": "Solution", "language": "en", "created_at": "2025-07-19T19:22:02.208764"}}
{"text": "COPY — from=0 /my_cool_app/dist /my_cool_app/dist WORKDIR /my_cool_app CMD [ “/root/nginx-app.sh”, “nginx”, “-g”, “daemon off;” ] ``` 2. Multi-stages builds allow you to produce smaller container images by splitting the build process into multiple stages as we did above. The app image doesn't contain anything related to the build process except the actual app.", "metadata": {"source_file": "learning-materials/topics/containers/solutions/multi_stage_builds.md", "section": "Copy build artifacts from the first stage", "language": "en", "created_at": "2025-07-19T19:22:02.208855"}}
{"text": "podman run --name nginx_container -d -p 80:80 nginx:alpine", "metadata": {"source_file": "learning-materials/topics/containers/solutions/commit_image.md", "section": "Run the container", "language": "en", "created_at": "2025-07-19T19:22:02.209177"}}
{"text": "curl 127.0.0.1:80 # <!DOCTYPE html> # <html> # <head> # <title>Welcome to nginx!</title>", "metadata": {"source_file": "learning-materials/topics/containers/solutions/commit_image.md", "section": "Verify web server is running", "language": "en", "created_at": "2025-07-19T19:22:02.209201"}}
{"text": "cat <<EOT >>index.html <html> <head> <title>It's a me</title> </head> <body> <h1>Mario</h1> </body> EOT", "metadata": {"source_file": "learning-materials/topics/containers/solutions/commit_image.md", "section": "Create index.html file", "language": "en", "created_at": "2025-07-19T19:22:02.209221"}}
{"text": "podman cp index.html nginx_container:/usr/share/nginx/html/index.html", "metadata": {"source_file": "learning-materials/topics/containers/solutions/commit_image.md", "section": "Copy index.html to the container", "language": "en", "created_at": "2025-07-19T19:22:02.209235"}}
{"text": "podman commit nginx_container nginx_mario", "metadata": {"source_file": "learning-materials/topics/containers/solutions/commit_image.md", "section": "Create a new image out of the running container", "language": "en", "created_at": "2025-07-19T19:22:02.209247"}}
{"text": "podman image ls", "metadata": {"source_file": "learning-materials/topics/containers/solutions/commit_image.md", "section": "Tag the image", "language": "en", "created_at": "2025-07-19T19:22:02.209259"}}
{"text": "podman tag dc7ed2343521 mario", "metadata": {"source_file": "learning-materials/topics/containers/solutions/commit_image.md", "section": "localhost/nginx_mario     latest   dc7ed2343521   52 seconds ago   25 MB", "language": "en", "created_at": "2025-07-19T19:22:02.209271"}}
{"text": "podman stop nginx_container podman rm nginx_container podman ps -a # no container 'nginx_container'", "metadata": {"source_file": "learning-materials/topics/containers/solutions/commit_image.md", "section": "Remove the container", "language": "en", "created_at": "2025-07-19T19:22:02.209287"}}
{"text": "podman run -d -p 80:80 nginx_mario", "metadata": {"source_file": "learning-materials/topics/containers/solutions/commit_image.md", "section": "Create a container out of the image", "language": "en", "created_at": "2025-07-19T19:22:02.209301"}}
{"text": "curl 127.0.0.1:80", "metadata": {"source_file": "learning-materials/topics/containers/solutions/commit_image.md", "section": "Check the container created from the new image", "language": "en", "created_at": "2025-07-19T19:22:02.209312"}}
{"text": "podman diff nginx_mario C /etc C /etc/nginx/conf.d C /etc/nginx/conf.d/default.conf A /run/nginx.pid C /usr/share/nginx/html C /usr/share/nginx/html/index.html C /var/cache/nginx C /var C /var/cache A /var/cache/nginx/client_temp A /var/cache/nginx/fastcgi_temp A /var/cache/nginx/proxy_temp A /var/cache/nginx/scgi_temp A /var/cache/nginx/uwsgi_temp", "metadata": {"source_file": "learning-materials/topics/containers/solutions/commit_image.md", "section": "Run diff", "language": "en", "created_at": "2025-07-19T19:22:02.209339"}}
{"text": "1. Run a container with a database of any type of you prefer (MySql, PostgreSQL, Mongo, etc.) 2. Verify the container is running 3. Access the container and create a new table (or collection, depends on which DB type you chose) for students 4. Insert a row (or document) of a student 5. Verify the row/document was added", "metadata": {"source_file": "learning-materials/topics/containers/solutions/containerized_db.md", "section": "Containerized DB", "language": "en", "created_at": "2025-07-19T19:22:02.209461"}}
{"text": "podman run --name mysql -e MYSQL_USER=mario -e MYSQL_PASSWORD=tooManyMushrooms -e MYSQL_DATABASE=university -e MYSQL_ROOT_PASSWORD=MushroomsPizza -d mysql", "metadata": {"source_file": "learning-materials/topics/containers/solutions/containerized_db.md", "section": "Run the container", "language": "en", "created_at": "2025-07-19T19:22:02.209486"}}
{"text": "podman ps", "metadata": {"source_file": "learning-materials/topics/containers/solutions/containerized_db.md", "section": "Verify it's running", "language": "en", "created_at": "2025-07-19T19:22:02.209499"}}
{"text": "podman exec -it mysql /bin/bash mysql -u root use university; CREATE TABLE Students (id int NOT NULL, name varchar(255) DEFAULT NULL, PRIMARY KEY (id)); insert into Projects (id, name) values (1,'Luigi'); select * from Students; ```", "metadata": {"source_file": "learning-materials/topics/containers/solutions/containerized_db.md", "section": "Add student row to the database", "language": "en", "created_at": "2025-07-19T19:22:02.209528"}}
{"text": "1. Run a container with the following properties: * image: alpine * name: forest * restart policy: always * command to execute: sleep 15 `docker run --restart always --name forest alpine sleep 15` 2. Run `docker container ls` - Is the container running? What about after 15 seconds, is it still running? why? It runs even after it completes to run `sleep 15` because the restart policy is \"always\". This means that Docker will keep restarting the **same** container even after it exists. 3. How then can we stop the container from running? The restart policy doesn't apply when the container is stopped with the command `docker container stop` 4. Remove the container you've created ``` docker container stop forest docker container rm forest ``` 5. Run the same container again but this time with `sleep 600` and verify it runs ``` docker run --restart always --name forest alpine sleep 600 docker container ls ``` 6. Restart the Docker service. Is the container still running? why? ``` sudo", "metadata": {"source_file": "learning-materials/topics/containers/solutions/run_forest_run.md", "section": "Instructions", "language": "en", "created_at": "2025-07-19T19:22:02.210004"}}
{"text": "systemctl restart docker ``` Yes, it's still running due to the restart policy `always` which means Docker will always bring up the container after it exists or stopped (not with the stop command). 8. Update the policy to `unless-stopped` `docker update --restart unless-stopped forest` 9. Stop the container `docker container stop forest` 10. Restart the Docker service. Is the container running? why? ``` sudo systemctl restart docker ``` No, the container is not running. This is because we changed the policy to `unless-stopped` which will run the container unless it was in stopped status. Since before the restart we stopped the container, Docker didn't continue running it after the restart.", "metadata": {"source_file": "learning-materials/topics/containers/solutions/run_forest_run.md", "section": "Instructions", "language": "en", "created_at": "2025-07-19T19:22:02.210164"}}
{"text": "Make sure Podman, Docker (or any other containers engine) is installed on your system", "metadata": {"source_file": "learning-materials/topics/containers/solutions/working_with_images.md", "section": "Requirements", "language": "en", "created_at": "2025-07-19T19:22:02.210279"}}
{"text": "1. List the containers images in your environment - `podman image ls` 2. Pull the latest ubuntu image - `podman image pull ubuntu:latest` 3. Run a container with the image you just pulled - `podman container run -it ubuntu:latest /bin/bash` 4. Remove the image. Did it work? - No. There is a running container which is using the image we try to remove 5. Do whatever is needed in order to remove the image - `podman rm <container_id>; podman image rm ubuntu`", "metadata": {"source_file": "learning-materials/topics/containers/solutions/working_with_images.md", "section": "Instructions", "language": "en", "created_at": "2025-07-19T19:22:02.210370"}}
{"text": "1. Run a containerized web server in the background and bind its port (8080) to a local port 2. Verify the port (8080) is bound 3. Reach the webserver from your local host 4. Now run the same web application but bound it to the local port 8080", "metadata": {"source_file": "learning-materials/topics/containers/solutions/containerized_web_server.md", "section": "Containerized Web Server", "language": "en", "created_at": "2025-07-19T19:22:02.210473"}}
{"text": "``` $ podman run -d -p 8080 httpd # run the container and bind the port 8080 to a local port $ podman port -l 8080 # show to which local port the port 8080 on the container, binds to 0.0.0.0:41203 $ curl http://0.0.0.0:41203 # use the port from the output of the previous command !DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.1//EN\" \"http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd\"> <html xmlns=\"http://www.w3.org/1999/xhtml\" xml:lang=\"en\"> <head> <title>Test Page for the HTTP Server on Red Hat Enterprise Linux</title> <meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\" /> $ podman run -d -p 8080:8080 httpd ```", "metadata": {"source_file": "learning-materials/topics/containers/solutions/containerized_web_server.md", "section": "Solution", "language": "en", "created_at": "2025-07-19T19:22:02.210566"}}
{"text": "mkdir -pv ~/local/mysql sudo semanage fcontext -a -t container_file_t '/home/USERNAME/local/mysql(/.*)?' sudo restorecon -R /home/USERNAME/local/mysql", "metadata": {"source_file": "learning-materials/topics/containers/solutions/containerized_db_persistent_storage.md", "section": "Create the directory for the DB on host", "language": "en", "created_at": "2025-07-19T19:22:02.210684"}}
{"text": "podman run --name mysql -e MYSQL_USER=mario -e MYSQL_PASSWORD=tooManyMushrooms -e MYSQL_DATABASE=university -e MYSQL_ROOT_PASSWORD=MushroomsPizza -d mysql -v /home/USERNAME/local/mysql:/var/lib/mysql/db", "metadata": {"source_file": "learning-materials/topics/containers/solutions/containerized_db_persistent_storage.md", "section": "Run the container", "language": "en", "created_at": "2025-07-19T19:22:02.210737"}}
{"text": "podman ps ``` It's better to use the storage host because in case the container ever gets removed (or storage reclaimed) you have the DB data still available.", "metadata": {"source_file": "learning-materials/topics/containers/solutions/containerized_db_persistent_storage.md", "section": "Verify it's running", "language": "en", "created_at": "2025-07-19T19:22:02.210777"}}
{"text": "podman save -o httpd.tar httpd", "metadata": {"source_file": "learning-materials/topics/containers/solutions/sharing_images.md", "section": "Save image as an archive", "language": "en", "created_at": "2025-07-19T19:22:02.210909"}}
{"text": "du -sh httpd.tar # output: 143MB podman image ls | grep httpd # output: 149MB", "metadata": {"source_file": "learning-materials/topics/containers/solutions/sharing_images.md", "section": "Check archive and image sizes", "language": "en", "created_at": "2025-07-19T19:22:02.210931"}}
{"text": "rsync -azc httpd.tar USER@REMOTE_HOST_FQDN:/tmp/", "metadata": {"source_file": "learning-materials/topics/containers/solutions/sharing_images.md", "section": "Copy the archive to a remote host", "language": "en", "created_at": "2025-07-19T19:22:02.210945"}}
{"text": "podman load -i /tmp/httpd.tar", "metadata": {"source_file": "learning-materials/topics/containers/solutions/sharing_images.md", "section": "Load the image", "language": "en", "created_at": "2025-07-19T19:22:02.210958"}}
{"text": "podman image ls ```", "metadata": {"source_file": "learning-materials/topics/containers/solutions/sharing_images.md", "section": "Verify it exists on the system after loading", "language": "en", "created_at": "2025-07-19T19:22:02.210970"}}
{"text": "<details> <summary>Explain what is Grafana</summary><br><b> [Grafana Docs](https://grafana.com/docs/grafana/latest/introduction): \"Grafana is a complete observability stack that allows you to monitor and analyze metrics, logs and traces. It allows you to query, visualize, alert on and understand your data no matter where it is stored. Create, explore, and share beautiful dashboards with your team and foster a data driven culture.\" </b></details> <details> <summary>What is Grafana Cloud?</summary><br><b> [Grafana cloud](https://grafana.com/products/cloud/) is an edition of Grafana that is offered as a service through the cloud. The observabilty stack is set up, administered and maintained by Grafana Labs and offers both free and paid options. You can also send data from existing data sources e.g. Promethetus, Loki and visualise existing time series data. </b></details> <details> <summary>What is Grafana Enterprise?</summary><br><b> [Grafana", "metadata": {"source_file": "learning-materials/topics/grafana/README.md", "section": "Grafana", "language": "en", "created_at": "2025-07-19T19:22:02.212427"}}
{"text": "Enterprise](https://grafana.com/docs/grafana/latest/enterprise/#enterprise-plugins) is a commercial edition of Grafana offered with enterprise features such as _Enterprise datasource_ plugins and built-in collaboration features. The edition includes full-time support and training from the Grafana team. </b></details> <details> <summary>What is the default HTTP port of Grafana?</summary><br><b> [Grafana getting started](https://grafana.com/docs/grafana/latest/getting-started/getting-started/): Grafana runs on port 3000 by default. </b></details> <details> <summary>Explain how we can enforce HTTPS</summary><br><b> [Grafana community](https://grafana.com/docs/grafana/latest/getting-started/getting-started/): Set the protocol to _https_ on the Configuration settings, Grafana will then expect clients to send requests using the HTTPS protocol. Any client that uses HTTP will receive an SSL/TLS error. </b></details> <details> <summary>How can we install plugins for Grafana?</summary><br><b>", "metadata": {"source_file": "learning-materials/topics/grafana/README.md", "section": "Grafana", "language": "en", "created_at": "2025-07-19T19:22:02.212464"}}
{"text": "[Grafana getting started](https://grafana.com/docs/grafana/latest/plugins/installation/): Navigate to the [Grafana plugins page](https://grafana.com/grafana/plugins/), find the desired plugin and click on it, then click the installation tab. There are two ways to install depending on where your Grafana server is running: - Cloud: On the **For** field of the installation tab, select the name of the organization you want to install the plugin on (unless you are only part of one), then click **install plugin**. Grafana cloud will automatically install the plugin to your Grafana instance, you may need to log out and back in to see the plugin. - Local grafana: You can use the Grafana CLI which lets you list available plugins and install them. ``` grafana-cli plugins list-remote grafana-cli plugins install <plugin-id> ``` You can also install a packaged plugin by downloading the asset from the installation tab, then extract the archive into the plugin directory. The path to the plugin", "metadata": {"source_file": "learning-materials/topics/grafana/README.md", "section": "Grafana", "language": "en", "created_at": "2025-07-19T19:22:02.212484"}}
{"text": "directory can be seen in the configuration file ``` unzip my-plugin-0.2.0.zip -d YOUR_PLUGIN_DIR/my-plugin ``` </b></details> <details> <summary>Explain what a 'Data source' is</summary><br><b> [Grafana Docs](https://grafana.com/docs/grafana/latest/datasources/): A data source is a storage backend that acts as a source of data for Grafana. Some popular data sources are Prometheus, InfluxDB, Loki, AWS cloudwatch. </b></details> <details> <summary>What is the \"Default configuration\"?</summary><br><b> [Grafana docs](https://grafana.com/docs/grafana/latest/administration/configuration/): The default configuration contains settings that Grafana use by default. The location depends on the OS environment, note that $WORKING_DIR refers to the working directory of Grafana. - Windows: ```$WORKING_DIR/conf/defaults.ini``` - Linux: ```/etc/grafana/grafana.ini``` - macOS: ```/usr/local/etc/grafana/grafana.ini``` </b></details> <details> <summary>Explain how we can add Custom configuration to", "metadata": {"source_file": "learning-materials/topics/grafana/README.md", "section": "Grafana", "language": "en", "created_at": "2025-07-19T19:22:02.212679"}}
{"text": "Grafana</summary><br><b> [Grafana docs](https://grafana.com/docs/grafana/latest/administration/configuration/): The custom configuration can be configured, either by modifying the custom configuration file or by adding environment variables that overrides default configuration. The configuration varies depending on the OS: - Windows: There is a file ```sample.ini``` in the same directory as the defaults.ini file, copy sample.ini and name it ```custom.ini```. Uncomment the settings you want to override. - Linux: Edit the configuration file at ```/etc/grafana/grafana.ini``` - macOS: Add a configuration file named ```custom.ini``` in the conf folder, if you installed Grafana using Homebrew then you can manually edit the ```conf/defaults.ini``` - Docker: You can override existing configuration in Grafana with environmental variables. An example is setting the Grafana instance name: ```E.g. export GF_DEFAULT_INSTANCE_NAME=my-instance``` </b></details> <details> <summary>Which external", "metadata": {"source_file": "learning-materials/topics/grafana/README.md", "section": "Grafana", "language": "en", "created_at": "2025-07-19T19:22:02.212736"}}
{"text": "authentication is supported out-of-the-box?</summary><br><b> [Grafana docs](https://grafana.com/docs/grafana/latest/auth/overview/): Grafana Auth is the built-in authentication system with password authentication enabled by default. </b></details> <details> <summary>How can we import a dashboard to a Grafana instance?</summary><br><b> [Grafana getting started](https://grafana.com/docs/grafana/latest/dashboards/export-import/): Grafana dashboards can be imported through the Grafana UI. Click on the + icon in the sidebar and then click import. You can import a dashboard through the following options: - Uploading a dashboard JSON file, which is exported from the Grafana UI or fetched through the [HTTPS API](https://grafana.com/docs/grafana/latest/http_api/dashboard/#create-update-dashboard ) - Paste a Grafana dashboard URL which is found at [grafana Dashboards](https://grafana.com/grafana/dashboards/), or a dashboard unique id into the text area. - Paste raw Dashboard JSON text into the", "metadata": {"source_file": "learning-materials/topics/grafana/README.md", "section": "Grafana", "language": "en", "created_at": "2025-07-19T19:22:02.212764"}}
{"text": "panel area. Click load afterwards. </b></details> <details> <summary>What is the data format for the dashboard?</summary><br><b> [Grafana docs](https://grafana.com/docs/grafana/latest/dashboards/json-model/): Grafana dashboards are represented in JSON files as objects, they store metadata about a dashboard e.g. dashboard properties, panel metadata and variables. </b></details> <details> <summary>Explain the steps to share your dashboard with your team</summary><br><b> [Grafana docs](https://grafana.com/docs/grafana/latest/sharing/share-dashboard/): Go to the homepage of your grafana Instance. Click on the share icon in the top navigation, from there three tabs are visible with the link tab shown. - Direct link: Click copy, send the link to a Grafana user, note that the user needs authorization to view the link. This is done by adding the user to a team. - Public Snapshot: Click on local snapshot to publish a snapshot to your local Grafana instance, or Publish to snapshots.raintank.io", "metadata": {"source_file": "learning-materials/topics/grafana/README.md", "section": "Grafana", "language": "en", "created_at": "2025-07-19T19:22:02.212783"}}
{"text": "which is a free service for publishing dashboard snapshots to an external Grafana instance You can configure snapshots to expire after a certain time and the timeout value to collect dashboard metrics </b></details> <details> <summary>How can you organise your dashboards and users in Grafana?</summary><br><b> [Grafana docs](https://grafana.com/blog/2022/03/14/how-to-best-organize-your-teams-and-resources-in-grafana/ ): The recommended way by Grafana labs is to create Folders for grouping dashboards, library panels and alerts. Users can be organised through Teams which grants permissions to members of a group. - [Folders](https://grafana.com/docs/grafana/latest/dashboards/dashboard_folders/): Click the + icon in the sidebar, then click \"Create folder\". In the create folder page, fill an unique name for the folder and click \"Create\" - [Teams](https://grafana.com/tutorials/create-users-and-teams/) You need to be the server admin in order to create Teams. 1. Click the server admin (shield)", "metadata": {"source_file": "learning-materials/topics/grafana/README.md", "section": "Grafana", "language": "en", "created_at": "2025-07-19T19:22:02.212801"}}
{"text": "icon in the sidebar, then in the Users tab, click New user. 2. Enter the user details e.g. name, E-mail, Username and Password. The password can be changed later by the user 3. Click Create to create the user account. </b></details> <details> <summary>Explain the steps to create an 'Alert'</summary><br><b> [Grafana docs](https://grafana.com/docs/grafana/latest/alerting/old-alerting/create-alerts/): \"Navigate to the panel you want to add or edit an alert rule for, click the title, and then click Edit. On the Alert tab, click Create Alert. If an alert already exists for this panel, then you can just edit the fields on the Alert tab. Fill out the fields. Descriptions are listed below in Alert rule fields. When you have finished writing your rule, click Save in the upper right corner to save alert rule and the dashboard. (Optional but recommended) Click Test rule to make sure the rule returns the results you expect\" </b></details>", "metadata": {"source_file": "learning-materials/topics/grafana/README.md", "section": "Grafana", "language": "en", "created_at": "2025-07-19T19:22:02.212820"}}
{"text": "<details> <summary>What is Circle CI?</summary><br><b> [Circle CI](https://circleci.com): \"CircleCI is a continuous integration and continuous delivery platform that can be used to implement DevOps practices.\" </b></details> <details> <summary>What are some benefits of Circle CI?</summary><br><b> [Circle CI Docs](https://circleci.com/docs/about-circleci): \"SSH into any job to debug your build issues. Set up parallelism in your .circleci/config.yml file to run jobs faster. Configure caching with two simple keys to reuse data from previous jobs in your workflow. Configure self-hosted runners for unique platform support. Access Arm resources for the machine executor. Use orbs, reusable packages of configuration, to integrate with third parties. Use pre-built Docker images in a variety of languages. Use the API to retrieve information about jobs and workflows. Use the CLI to access advanced tools locally. Get flaky test detection with test insights.\" </b></details> <details>", "metadata": {"source_file": "learning-materials/topics/circleci/README.md", "section": "Circle CI 101", "language": "en", "created_at": "2025-07-19T19:22:02.213213"}}
{"text": "<summary>Explain the following: * Pipeline * Workflow * Jobs * Steps </summary><br><b> * Pipeline: the entire CI/CD configuration (.circleci/config.yaml) * Workflow: primarily used when there is more than one job in the configuration to orchestrate the workflows * Jobs: One or more steps to execute as part of the CI/CD process * Steps: The actual commands to execute </b></details> <details> <summary>What is an Orb?</summary><br><b> [Circle CI Docs](https://circleci.com/developer/orbs): \"Orbs are shareable packages of CircleCI configuration you can use to simplify your builds\" They can come from the public registry or defined privately as part of an organization. </b></details>", "metadata": {"source_file": "learning-materials/topics/circleci/README.md", "section": "Circle CI 101", "language": "en", "created_at": "2025-07-19T19:22:02.213239"}}
{"text": "<details> <summary>Where (in what location in the project) Circle CI pipelines are defined?</summary><br><b> `.circleci/config.yml` </b></details> <details> <summary>Explain the following configuration file ``` version: 2.1 jobs: say-hello: docker: - image: cimg/base:stable steps: - checkout - run: name: \"Say hello\" command: \"echo Hello, World!\" workflows: say-hello-workflow: jobs: - say-hello ``` </summary><br><b> This configuration file will set up one job that will checkout the code of the project will run the command `echo Hello, World!`. It will run in a container using the image `cimg/base:stable`. </b></details>", "metadata": {"source_file": "learning-materials/topics/circleci/README.md", "section": "Circle CI Hands-On 101", "language": "en", "created_at": "2025-07-19T19:22:02.213460"}}
{"text": "- [Chaos Engineering](#chaos-engineering) - [Chaos Engineering Questions](#chaos-engineering-questions) - [Basics](#basics)", "metadata": {"source_file": "learning-materials/topics/chaos_engineering/README.md", "section": "Chaos Engineering", "language": "en", "created_at": "2025-07-19T19:22:02.213610"}}
{"text": "<details> <summary>What is Chaos Engineering?</summary><br><b> [Wikipedia](https://en.wikipedia.org/wiki/Chaos_engineering): \"Chaos Engineering is the discipline of experimenting on a system in order to build confidence in the system's capability to withstand turbulent conditions in production.\" [TechTarget](https://www.techtarget.com/searchitoperations/definition/chaos-engineering): \"Chaos engineering is the process of testing a distributed computing system to ensure that it can withstand unexpected disruptions.\" </b></details> <details> <summary>What's a typical Chaos Engineering workflow?</summary><br><b> According to [Gremlin](gremlin.com) there are three steps: 1. Planning an experiment where you design and choose a scenario in which your system should fail to operate properly 2. You execute the smallest possible experiment to test your theory 3. If nothing goes wrong, you scale your experiment and make the blast radius bigger. If your system breaks, you better understand why and", "metadata": {"source_file": "learning-materials/topics/chaos_engineering/README.md", "section": "Basics", "language": "en", "created_at": "2025-07-19T19:22:02.213859"}}
{"text": "start dealing with it The process then repeats itself either with same scenario or a new one. </b></details> <details> <summary>Cite a few tools used to operate Chaos exercises</summary><br><b> - AWS Fault Injection Simulator: inject failures in AWS resources - Azure Chaos Studio: inject failures in Azure resources - Chaos Monkey: one of the most famous tools to orchestrate Chaos on diverse Cloud providers - Litmus - A Framework for Kubernetes - Chaos Mesh: for Cloud Kubernetes platforms See an extensive list [here](https://github.com/dastergon/awesome-chaos-engineering) </b></details>", "metadata": {"source_file": "learning-materials/topics/chaos_engineering/README.md", "section": "Basics", "language": "en", "created_at": "2025-07-19T19:22:02.213888"}}
{"text": "<details> <summary>What is DevOps?</summary><br><b> The definition of DevOps from selected companies: **Amazon**: \"DevOps is the combination of cultural philosophies, practices, and tools that increases an organization’s ability to deliver applications and services at high velocity: evolving and improving products at a faster pace than organizations using traditional software development and infrastructure management processes. This speed enables organizations to better serve their customers and compete more effectively in the market.\" **Microsoft**: \"DevOps is the union of people, process, and products to enable continuous delivery of value to our end users. The contraction of “Dev” and “Ops” refers to replacing siloed Development and Operations to create multidisciplinary teams that now work together with shared and efficient practices and tools. Essential DevOps practices include agile planning, continuous integration, continuous delivery, and monitoring of applications.\" **Red", "metadata": {"source_file": "learning-materials/topics/devops/README.md", "section": "General", "language": "en", "created_at": "2025-07-19T19:22:02.215301"}}
{"text": "Hat**: \"DevOps describes approaches to speeding up the processes by which an idea (like a new software feature, a request for enhancement, or a bug fix) goes from development to deployment in a production environment where it can provide value to the user. These approaches require that development teams and operations teams communicate frequently and approach their work with empathy for their teammates. Scalability and flexible provisioning are also necessary. With DevOps, those that need power the most, get it—through self service and automation. Developers, usually coding in a standard development environment, work closely with IT operations to speed software builds, tests, and releases—without sacrificing reliability.\" **Google**: \"...The organizational and cultural movement that aims to increase software delivery velocity, improve service reliability, and build shared ownership among software stakeholders\" </b></details> <details> <summary>What are the benefits of DevOps? What can", "metadata": {"source_file": "learning-materials/topics/devops/README.md", "section": "General", "language": "en", "created_at": "2025-07-19T19:22:02.215343"}}
{"text": "it help us to achieve?</summary><br><b> * Collaboration * Improved delivery * Security * Speed * Scale * Reliability </b></details> <details> <summary>What are the anti-patterns of DevOps?</summary><br><b> A couple of examples: * One person is in charge of specific tasks. For example there is only one person who is allowed to merge the code of everyone else into the repository. * Treating production differently from development environment. For example, not implementing security in development environment * Not allowing someone to push to production on Friday ;) </b></details> <details> <summary>How would you describe a successful DevOps engineer or a team?</summary><br><b> The answer can focus on: * Collaboration * Communication * Set up and improve workflows and processes (related to testing, delivery, ...) * Dealing with issues Things to think about: * What DevOps teams or engineers should NOT focus on or do? * Do DevOps teams or engineers have to be innovative or practice", "metadata": {"source_file": "learning-materials/topics/devops/README.md", "section": "General", "language": "en", "created_at": "2025-07-19T19:22:02.215375"}}
{"text": "innovation as part of their role? </b></details> <details> <summary>One of your team members suggests to set a goal of \"deploying at least 20 times a day\" in regards to CD. What is your take on that?</summary><br><b> A couple of thoughts: 1. Why is it an important goal? Is it affecting the business somehow? One of the KPIs? In other words, does it matters? 2. This might introduce risks such as losing quality in favor of quantity 3. You might want to set a possibly better goal such as \"be able to deploy whenever we need to deploy\" </b></details>", "metadata": {"source_file": "learning-materials/topics/devops/README.md", "section": "General", "language": "en", "created_at": "2025-07-19T19:22:02.215395"}}
{"text": "<details> <summary>What do you take into consideration when choosing a tool/technology?</summary><br><b> A few ideas to think about: * mature/stable vs. cutting edge * community size * architecture aspects - agent vs. agentless, master vs. masterless, etc. * learning curve </b></details> <details> <summary>Can you describe which tool or platform you chose to use in some of the following areas and how? * CI/CD * Provisioning infrastructure * Configuration Management * Monitoring & alerting * Logging * Code review * Code coverage * Issue Tracking * Containers and Containers Orchestration * Tests</summary><br><b> This is a more practical version of the previous question where you might be asked additional specific questions on the technology you chose * CI/CD - Jenkins, Circle CI, Travis, Drone, Argo CD, Zuul * Provisioning infrastructure - Terraform, CloudFormation * Configuration Management - Ansible, Puppet, Chef * Monitoring & alerting - Prometheus, Nagios * Logging - Logstash,", "metadata": {"source_file": "learning-materials/topics/devops/README.md", "section": "Tooling", "language": "en", "created_at": "2025-07-19T19:22:02.215872"}}
{"text": "Graylog, Fluentd * Code review - Gerrit, Review Board * Code coverage - Cobertura, Clover, JaCoCo * Issue tracking - Jira, Bugzilla * Containers and Containers Orchestration - Docker, Podman, Kubernetes, Nomad * Tests - Robot, Serenity, Gauge </b></details> <details> <summary>A team member of yours, suggests to replace the current CI/CD platform used by the organization with a new one. How would you reply?</summary><br><b> Things to think about: * What we gain from doing so? Are there new features in the new platform? Does the new platform deals with some of the limitations presented in the current platform? * What this suggestion is based on? In other words, did he/she tried out the new platform? Was there extensive technical research? * What does the switch from one platform to another will require from the organization? For example, training users who use the platform? How much time the team has to invest in such move? </b></details>", "metadata": {"source_file": "learning-materials/topics/devops/README.md", "section": "Tooling", "language": "en", "created_at": "2025-07-19T19:22:02.216032"}}
{"text": "<details> <summary>What is Version Control?</summary><br><b> * Version control is the system of tracking and managing changes to software code. * It helps software teams to manage changes to source code over time. * Version control also helps developers move faster and allows software teams to preserve efficiency and agility as the team scales to include more developers. </b></details> <details> <summary>What is a commit?</summary><br><b> * In Git, a commit is a snapshot of your repo at a specific point in time. * The git commit command will save all staged changes, along with a brief description from the user, in a “commit” to the local repository. </b></details> <details> <summary>What is a merge?</summary><br><b> * Merging is Git's way of putting a forked history back together again. The git merge command lets you take the independent lines of development created by git branch and integrate them into a single branch. </b></details> <details> <summary>What is a merge", "metadata": {"source_file": "learning-materials/topics/devops/README.md", "section": "Version Control", "language": "en", "created_at": "2025-07-19T19:22:02.216637"}}
{"text": "conflict?</summary><br><b> * A merge conflict is an event that occurs when Git is unable to automatically resolve differences in code between two commits. When all the changes in the code occur on different lines or in different files, Git will successfully merge commits without your help. </b></details> <details> <summary>What best practices are you familiar with regarding version control?</summary><br><b> * Use a descriptive commit message * Make each commit a logical unit * Incorporate others' changes frequently * Share your changes frequently * Coordinate with your co-workers * Don't commit generated files * Don't commit binary files </b></details> <details> <summary>Would you prefer a \"configuration->deployment\" model or \"deployment->configuration\"? Why?</summary><br><b> Both have advantages and disadvantages. With \"configuration->deployment\" model for example, where you build one image to be used by multiple deployments, there is less chance of deployments being different from", "metadata": {"source_file": "learning-materials/topics/devops/README.md", "section": "Version Control", "language": "en", "created_at": "2025-07-19T19:22:02.216665"}}
{"text": "one another, so it has a clear advantage of a consistent environment. </b></details> <details> <summary>Explain mutable vs. immutable infrastructure</summary><br><b> In mutable infrastructure paradigm, changes are applied on top of the existing infrastructure and over time the infrastructure builds up a history of changes. Ansible, Puppet and Chef are examples of tools which follow mutable infrastructure paradigm. In immutable infrastructure paradigm, every change is actually a new infrastructure. So a change to a server will result in a new server instead of updating it. Terraform is an example of technology which follows the immutable infrastructure paradigm. </b></details>", "metadata": {"source_file": "learning-materials/topics/devops/README.md", "section": "Version Control", "language": "en", "created_at": "2025-07-19T19:22:02.216685"}}
{"text": "<details> <summary>Explain \"Software Distribution\"</summary><br><b> Read [this](https://venam.nixers.net/blog/unix/2020/03/29/distro-pkgs.html) fantastic article on the topic. From the article: \"Thus, software distribution is about the mechanism and the community that takes the burden and decisions to build an assemblage of coherent software that can be shipped.\" </b></details> <details> <summary>Why are there multiple software distributions? What differences they can have?</summary><br><b> Different distributions can focus on different things like: focus on different environments (server vs. mobile vs. desktop), support specific hardware, specialize in different domains (security, multimedia, ...), etc. Basically, different aspects of the software and what it supports, get different priority in each distribution. </b></details> <details> <summary>What is a Software Repository?</summary><br><b> Wikipedia: \"A software repository, or “repo” for short, is a storage location for software", "metadata": {"source_file": "learning-materials/topics/devops/README.md", "section": "Software Distribution", "language": "en", "created_at": "2025-07-19T19:22:02.217632"}}
{"text": "packages. Often a table of contents is stored, as well as metadata.\" Read more [here](https://en.wikipedia.org/wiki/Software_repository) </b></details> <details> <summary>What ways are there to distribute software? What are the advantages and disadvantages of each method?</summary><br><b> * Source - Maintain build script within version control system so that user can build your app after cloning repository. Advantage: User can quickly checkout different versions of application. Disadvantage: requires build tools installed on users machine. * Archive - collect all your app files into one archive (e.g. tar) and deliver it to the user. Advantage: User gets everything he needs in one file. Disadvantage: Requires repeating the same procedure when updating, not good if there are a lot of dependencies. * Package - depends on the OS, you can use your OS package format (e.g. in RHEL/Fefodra it's RPM) to deliver your software with a way to install, uninstall and update it using the standard", "metadata": {"source_file": "learning-materials/topics/devops/README.md", "section": "Software Distribution", "language": "en", "created_at": "2025-07-19T19:22:02.217663"}}
{"text": "packager commands. Advantages: Package manager takes care of support for installation, uninstallation, updating and dependency management. Disadvantage: Requires managing package repository. * Images - Either VM or container images where your package is included with everything it needs in order to run successfully. Advantage: everything is preinstalled, it has high degree of environment isolation. Disadvantage: Requires knowledge of building and optimizing images. </b></details> <details> <summary>Are you familiar with \"The Cathedral and the Bazaar models\"? Explain each of the models</summary><br><b> * Cathedral - source code released when software is released * Bazaar - source code is always available publicly (e.g. Linux Kernel) </b></details> <details> <summary>What is caching? How does it work? Why is it important?</summary><br><b> Caching is fast access to frequently used resources which are computationally expensive or IO intensive and do not change often. There can be several", "metadata": {"source_file": "learning-materials/topics/devops/README.md", "section": "Software Distribution", "language": "en", "created_at": "2025-07-19T19:22:02.217685"}}
{"text": "layers of cache that can start from CPU caches to distributed cache systems. Common ones are in memory caching and distributed caching. <br/> Caches are typically data structures that contains some data, such as a hashtable or dictionary. However, any data structure can provide caching capabilities, like set, sorted set, sorted dictionary etc. While, caching is used in many applications, they can create subtle bugs if not implemented correctly or used correctly. For example,cache invalidation, expiration or updating is usually quite challenging and hard. </b></details> <details> <summary>Explain stateless vs. stateful</summary><br><b> Stateless applications don't store any data in the host which makes it ideal for horizontal scaling and microservices. Stateful applications depend on the storage to save state and data, typically databases are stateful applications. </b></details> <details> <summary>What is Reliability? How does it fit DevOps?</summary><br><b> Reliability, when used in", "metadata": {"source_file": "learning-materials/topics/devops/README.md", "section": "Software Distribution", "language": "en", "created_at": "2025-07-19T19:22:02.217705"}}
{"text": "DevOps context, is the ability of a system to recover from infrastructure failure or disruption. Part of it is also being able to scale based on your organization or team demands. </b></details> <details> <summary>What does \"Availability\" mean? What means are there to track Availability of a service?</summary><br><b> </b></details> <details> <summary>Why isn't 100% availability a target? Why do most companies or teams set it to be 99%.X?</summary><br><b> </b></details> <details> <summary>Describe the workflow of setting up some type of web server (Apache, IIS, Tomcat, ...)</summary><br><b> </b></details> <details> <summary>How does a web server work?</summary><br><b> <a href=\"https://developer.mozilla.org/en-US/docs/Learn/Common_questions/What_is_a_web_server\" title=\"Click here to redirect to MDN official page\" style=\"background-color:#FFFFFF;color:#000000;text-decoration:none\">According to MDN Web Docs -</a> We can understand web servers using two view points, which is: (i) Hardware", "metadata": {"source_file": "learning-materials/topics/devops/README.md", "section": "Software Distribution", "language": "en", "created_at": "2025-07-19T19:22:02.217919"}}
{"text": "(ii) Software (i) A web server is nothing but a remote computer which stores website's component files(HTML,CSS and Javascript files) and web server's software.A web server connects to the Internet and supports physical data interchange with other devices connected to the web. (ii) On the software side, a web server includes several parts that control how web users access hosted files. At a minimum, this is an HTTP server. An HTTP server is software that understands URLs (web addresses) and HTTP (the protocol your browser uses to view webpages). An HTTP server can be accessed through the domain names of the websites it stores, and it delivers the content of these hosted websites to the end user's device.", "metadata": {"source_file": "learning-materials/topics/devops/README.md", "section": "Software Distribution", "language": "en", "created_at": "2025-07-19T19:22:02.217950"}}
{"text": "Whenever a browser needs a file that is hosted on a web server, the browser requests the page from the web server and the web server responds with that page. This communication between web browser and web server happens in the following ways: (1) User enters the domain name in the browser,and the browser then search for the IP address of the entered name. It can be done in 2 ways- -By searching in its cache. -By requesting one or more DNS (Domain Name System) Servers. (2) After knowing the IP Address, the browser requests the file via HTTP and the request reaches the correct (hardware) web server. (3) The (software) HTTP server accepts the request, finds the requested document, and sends it back to the browser, also through HTTP. (If the server doesn't find the requested document, it returns a 404 response instead.) (4) The Browser finally gets the webpages and displays it, or displays the error message. </b></details> <details> <summary>Explain \"Open Source\"</summary><br><b>", "metadata": {"source_file": "learning-materials/topics/devops/README.md", "section": "How communication between web server and web browsers established:", "language": "en", "created_at": "2025-07-19T19:22:02.219025"}}
{"text": "</b></details> <details> <summary>Describe the architecture of service/app/project/... you designed and/or implemented</summary><br><b> </b></details> <details> <summary>What types of tests are you familiar with?</summary><br><b> Styling, unit, functional, API, integration, smoke, scenario, ... You should be able to explain those that you mention. </b></details> <details> <summary>You need to install periodically a package (unless it's already exists) on different operating systems (Ubuntu, RHEL, ...). How would you do it?</summary><br><b> There are multiple ways to answer this question (there is no right and wrong here): * Simple cron job * Pipeline with configuration management technology (such Puppet, Ansible, Chef, etc.) ... </b></details> <details> <summary>What is Chaos Engineering?</summary><br><b> Wikipedia: \"Chaos engineering is the discipline of experimenting on a software system in production in order to build confidence in the system's capability to withstand turbulent and", "metadata": {"source_file": "learning-materials/topics/devops/README.md", "section": "How communication between web server and web browsers established:", "language": "en", "created_at": "2025-07-19T19:22:02.219058"}}
{"text": "unexpected conditions\" Read about Chaos Engineering [here](https://en.wikipedia.org/wiki/Chaos_engineering) </b></details> <details> <summary>What is \"infrastructure as code\"? What implementation of IAC are you familiar with?</summary><br><b> IAC (infrastructure as code) is a declarative approach of defining infrastructure or architecture of a system. Some implementations are ARM templates for Azure and Terraform that can work across multiple cloud providers. </b></details> <details> <summary>What benefits does infrastructure-as-code have?</summary><br><b> - fully automated process of provisioning, modifying and deleting your infrastructure - version control for your infrastructure which allows you to quickly rollback to previous versions - validate infrastructure quality and stability with automated tests and code reviews - makes infrastructure tasks less repetitive </b></details> <details> <summary>How do you manage build artifacts?</summary><br><b> Build artifacts are usually stored", "metadata": {"source_file": "learning-materials/topics/devops/README.md", "section": "How communication between web server and web browsers established:", "language": "en", "created_at": "2025-07-19T19:22:02.219078"}}
{"text": "in a repository. They can be used in release pipelines for deployment purposes. Usually there is retention period on the build artifacts. </b></details> <details> <summary>What Continuous Integration solution are you using/prefer and why?</summary><br><b> </b></details> <details> <summary>What deployment strategies are you familiar with or have used?</summary><br><b> There are several deployment strategies: * Rolling * Blue green deployment * Canary releases * Recreate strategy </b></details> <details> <summary>You joined a team where everyone developing one project and the practice is to run tests locally on their workstation and push it to the repository if the tests passed. What is the problem with the process as it is now and how to improve it?</summary><br><b> </b></details> <details> <summary>Explain test-driven development (TDD)</summary><br><b> </b></details> <details> <summary>Explain agile software development</summary><br><b> </b></details> <details> <summary>What do you", "metadata": {"source_file": "learning-materials/topics/devops/README.md", "section": "How communication between web server and web browsers established:", "language": "en", "created_at": "2025-07-19T19:22:02.219097"}}
{"text": "think about the following sentence?: \"Implementing or practicing DevOps leads to more secure software\"</summary><br><b> </b></details> <details> <summary>Do you know what is a \"post-mortem meeting\"? What is your opinion on that?</summary><br><b> </b></details> <details> <summary>What is a configuration drift? What problems is it causing?</summary><br><b> Configuration drift happens when in an environment of servers with the exact same configuration and software, a certain server or servers are being applied with updates or configuration which other servers don't get and over time these servers become slightly different than all others. This situation might lead to bugs which hard to identify and reproduce. </b></details> <details> <summary>How to deal with a configuration drift?</summary><br><b> Configuration drift can be avoided with desired state configuration (DSC) implementation. Desired state configuration can be a declarative file that defined how a system should be. There are", "metadata": {"source_file": "learning-materials/topics/devops/README.md", "section": "How communication between web server and web browsers established:", "language": "en", "created_at": "2025-07-19T19:22:02.219115"}}
{"text": "tools to enforce desired state such a terraform or azure dsc. There are incremental or complete strategies. </b></details> <details> <summary>Explain Declarative and Procedural styles. The technologies you are familiar with (or using) are using procedural or declarative style?</summary><br><b> Declarative - You write code that specifies the desired end state<br> Procedural - You describe the steps to get to the desired end state Declarative Tools - Terraform, Puppet, CloudFormation, Ansible<br> Procedural Tools - Chef To better emphasize the difference, consider creating two virtual instances/servers. In declarative style, you would specify two servers and the tool will figure out how to reach that state. In procedural style, you need to specify the steps to reach the end state of two instances/servers - for example, create a loop and in each iteration of the loop create one instance (running the loop twice of course). </b></details> <details> <summary>Do you have experience with", "metadata": {"source_file": "learning-materials/topics/devops/README.md", "section": "How communication between web server and web browsers established:", "language": "en", "created_at": "2025-07-19T19:22:02.219293"}}
{"text": "testing cross-projects changes? (aka cross-dependency)</summary><br><b> Note: cross-dependency is when you have two or more changes to separate projects and you would like to test them in mutual build instead of testing each change separately. </b></details> <details> <summary>Have you contributed to an open source project? Tell me about this experience</summary><br><b> </b></details> <details> <summary>What is Distributed Tracing?</summary><br><b> </b></details>", "metadata": {"source_file": "learning-materials/topics/devops/README.md", "section": "How communication between web server and web browsers established:", "language": "en", "created_at": "2025-07-19T19:22:02.219324"}}
{"text": "<details> <summary>What is GitOps?</summary><br><b> GitLab: \"GitOps is an operational framework that takes DevOps best practices used for application development such as version control, collaboration, compliance, and CI/CD tooling, and applies them to infrastructure automation\". Read more [here](https://about.gitlab.com/topics/gitops) </b></details> <details> <summary>What are some of the advantages of applying GitOps?</summary><br><b> * It introduces limited/granular access to infrastructure * It makes it easier to trace who makes changes to infrastructure </b></details> <details> <summary>When a repository refereed to as \"GitOps Repository\" what does it means?</summary><br><b> A repository that doesn't holds the application source code, but the configuration, infra, ... files that required to test and deploy the application. </b></details> <details> <summary>What are some practical implementations or practices of GitOp?</summary><br><b> * Store Infra files in a version control", "metadata": {"source_file": "learning-materials/topics/devops/README.md", "section": "GitOps", "language": "en", "created_at": "2025-07-19T19:22:02.219765"}}
{"text": "repository (like Git) * Apply review/approval process for changes </b></details> <details> <summary>Two engineers in your team argue on where to put the configuration and infra related files of a certain application. One of them suggests to put it in the same repo as the application repository and the other one suggests to put to put it in its own separate repository. What's your take on that?</summary><br><b> One might say we need more details as to what these configuration and infra files look like exactly and how complex the application and its CI/CD pipeline(s), but in general, most of the time you will want to put configuration and infra related files in their own separate repository and not in the repository of the application for multiple reasons: * Every change submitted to the configuration, shouldn't trigger the CI/CD of the application, it should be testing out and applying the modified configuration, not the application itself * When you mix application code with", "metadata": {"source_file": "learning-materials/topics/devops/README.md", "section": "GitOps", "language": "en", "created_at": "2025-07-19T19:22:02.219801"}}
{"text": "configuration and infra related files </b></details>", "metadata": {"source_file": "learning-materials/topics/devops/README.md", "section": "GitOps", "language": "en", "created_at": "2025-07-19T19:22:02.219821"}}
{"text": "<details> <summary>What are the differences between SRE and DevOps?</summary><br><b> Google: \"One could view DevOps as a generalization of several core SRE principles to a wider range of organizations, management structures, and personnel.\" Read more about it [here](https://sre.google/sre-book/introduction) </b></details> <details> <summary>What SRE team is responsible for?</summary><br><b> Google: \"the SRE team is responsible for availability, latency, performance, efficiency, change management, monitoring, emergency response, and capacity planning of their services\" Read more about it [here](https://sre.google/sre-book/introduction) </b></details> <details> <summary>What is an error budget?</summary><br><b> Atlassian: \"An error budget is the maximum amount of time that a technical system can fail without contractual consequences.\" Read more about it [here](https://www.atlassian.com/incident-management/kpis/error-budget) </b></details> <details> <summary>What do you think about the", "metadata": {"source_file": "learning-materials/topics/devops/README.md", "section": "SRE", "language": "en", "created_at": "2025-07-19T19:22:02.220422"}}
{"text": "following statement: \"100% is the only right availability target for a system\"</summary><br><b> Wrong. No system can guarantee 100% availability as no system is safe from experiencing zero downtime. Many systems and services will fall somewhere between 99% and 100% uptime (or at least this is how most systems and services should be). </b></details> <details> <summary>What are MTTF (mean time to failure) and MTTR (mean time to repair)? What these metrics help us to evaluate?</summary><br><b> * MTTF (mean time to failure) other known as uptime, can be defined as how long the system runs before if fails. * MTTR (mean time to recover) on the other hand, is the amount of time it takes to repair a broken system. * MTBF (mean time between failures) is the amount of time between failures of the system. </b></details> <details> <summary>What is the role of monitoring in SRE?</summary><br><b> Google: \"Monitoring is one of the primary means by which service owners keep track of a system’s health", "metadata": {"source_file": "learning-materials/topics/devops/README.md", "section": "SRE", "language": "en", "created_at": "2025-07-19T19:22:02.220445"}}
{"text": "and availability\" Read more about it [here](https://sre.google/sre-book/introduction) </b></details> <details> <summary>What are the two main SRE KPIs</summary><br><b> Service Level Indicators (SLI) and Service Level Objectives (SLO). </b></details> <details> <summary>What is Toil?</summary><br><b> Google: Toil is the kind of work tied to running a production service that tends to be manual, repetitive, automatable, tactical, devoid of enduring value, and that scales linearly as a service grows Read more about it [here](https://sre.google/sre-book/eliminating-toil/) </b></details> <details> <summary>What is a postmortem ? </summary><br><b> The postmortem is a process that should take place following an incident. It’s purpose is to identify the root cause of an incident and the actions that should be taken to avoid this kind of incidents from happening again. </b></details> <details> <summary>What is the core value often put forward when talking about postmortem?</summary><br><b>", "metadata": {"source_file": "learning-materials/topics/devops/README.md", "section": "SRE", "language": "en", "created_at": "2025-07-19T19:22:02.220527"}}
{"text": "Blamelessness. Postmortems need to be blameless and this value should be remided at the beginning of every postmortem. This is the best way to ensure that people are playing the game to find the root cause and not trying to hide their possible faults.</b></details>", "metadata": {"source_file": "learning-materials/topics/devops/README.md", "section": "SRE", "language": "en", "created_at": "2025-07-19T19:22:02.220558"}}
{"text": "1. Clone an open source project you would like to containerize. A couple of suggestions: ``` https://github.com/bregman-arie/node-hello-world https://github.com/bregman-arie/flask-hello-world ``` 2. Write a Dockerfile you'll use for building an image of the application (you can use any base image you would like) 3. Build an image using the Dockerfile you've just wrote 4. Verify the image exists 5. [Optional] Push the image you've just built to a registry 6. Run the application 7. Verify the app is running", "metadata": {"source_file": "learning-materials/topics/devops/containerize_app.md", "section": "Containerize an Application", "language": "en", "created_at": "2025-07-19T19:22:02.220979"}}
{"text": "Set up an highly available \"Hello World\" application with the following instructions: * Use a containerized Load Balancer * Provision two virtual machines (this is where the app will run) * The page, when visited, should show \"Hello World! I'm host X\" - X should be the name of the virtual machine", "metadata": {"source_file": "learning-materials/topics/devops/ha_hello_world.md", "section": "Highly Available \"Hello World\"", "language": "en", "created_at": "2025-07-19T19:22:02.221091"}}
{"text": "1. Clone an open source project you would like to containerize. A couple of suggestions: ``` https://github.com/bregman-arie/node-hello-world https://github.com/bregman-arie/flask-hello-world ``` `git clone https://github.com/bregman-arie/node-hello-world` 2. Write a Dockerfile you'll use for building an image of the application (you can use any base image you would like) ``` FROM alpine LABEL maintainer=\"your name/email\" RUN apk add --update nodejs npm COPY . /src WORKDIR /src RUN npm install EXPOSE 3000 ENTRYPOINT [\"node\", \"./app.js\"] ``` 3. Build an image using the Dockerfile you've just wrote `docker image build -t web_app:latest .` 4. Verify the image exists `docker image ls` 5. [Optional] Push the image you've just built to a registry ``` docker login docker image tag web_app:latest <your username>/web_app:latest", "metadata": {"source_file": "learning-materials/topics/devops/solutions/containerize_app.md", "section": "Containerize an Application", "language": "en", "created_at": "2025-07-19T19:22:02.221467"}}
{"text": "docker image push <your username>/web_app:latest ``` 6. Run the application ``` docker container run -d -p 80:3000 web_app:latest ``` 7. Verify the app is running ``` docker container ls docker logs <container ID/name>", "metadata": {"source_file": "learning-materials/topics/devops/solutions/containerize_app.md", "section": "Verify with \"docker image ls\"", "language": "en", "created_at": "2025-07-19T19:22:02.221515"}}
{"text": "1. Provision two VMs", "metadata": {"source_file": "learning-materials/topics/devops/solutions/ha_hello_world.md", "section": "Solution", "language": "en", "created_at": "2025-07-19T19:22:02.221621"}}
{"text": "To Tell about the basic questions asked in node to me in many interviews 1. What is Nodejs ? 2. How many threads does nodejs have ? 3. How do nodejs work ? 4. Is nodejs Single Threaded Or Multi Threaded ? 5. what is node cluster ? 6. Does parent process depends on the child preocess ? 7. How many types of module do nodejs have ? 8. Why nodejs ? 9. What is npm ? 10. Difference between pacakage.json and pacakage-lock.json ? 11. What is the difference betwwen creating a server with http and a framework ? 12. What do you mean by non-blocking ? 13. What is event loop ? 14. What is event driven ?", "metadata": {"source_file": "learning-materials/topics/node/node_questions_basic.md", "section": "OBJECTIVE", "language": "en", "created_at": "2025-07-19T19:22:02.221877"}}
{"text": "1. Node.js is an open-source, cross-platform JavaScript runtime environment that allows developers to build server-side and networking applications. 2. Nodejs is a single threaded langauage . It handles one operation at a time. 3. Node.js works by executing JavaScript code in a runtime environment outside of a web browser. 4. Node.js works by executing JavaScript code in a runtime environment outside of a web browser. mainly used for performance and scalabilty od the project 5. Parent process manages the child process but not depend on the clid process to run parralel 6. Three Modules mainly 1. Core Module - fs , require 2. Local modules - like function created by us and exported or imported from one file to another 3. Third Party module - like npm pacakages whcih we install to do a specific kind of work 7. NPM (Node Pacakage Manager) used for installing, managing, and sharing JavaScript packages and dependencies. 8. Difference between pacakage.json and pacakage-lock.json", "metadata": {"source_file": "learning-materials/topics/node/solutions/node_questions_basic_ans.md", "section": "ANSWERS", "language": "en", "created_at": "2025-07-19T19:22:02.222206"}}
{"text": "1.pacakage.json - contains the metadata and the dependendies of a project 2.pacakage-lock.json - lock the version of the installed dependencies", "metadata": {"source_file": "learning-materials/topics/node/solutions/node_questions_basic_ans.md", "section": "ANSWERS", "language": "en", "created_at": "2025-07-19T19:22:02.222233"}}
{"text": "- [Azure](#azure) - [Questions](#questions) - [Azure 101](#azure-101) - [Azure Resource Manager](#azure-resource-manager) - [Compute](#compute) - [Network](#network) - [Storage](#storage) - [Security](#security)", "metadata": {"source_file": "learning-materials/topics/azure/README.md", "section": "Azure", "language": "en", "created_at": "2025-07-19T19:22:02.222457"}}
{"text": "<details> <summary>What is Azure Portal?</summary><br><b> [Microsoft Docs](https://docs.microsoft.com/en-us/learn/modules/intro-to-azure-fundamentals/what-is-microsoft-azure): \"The Azure portal is a web-based, unified console that provides an alternative to command-line tools. With the Azure portal, you can manage your Azure subscription by using a graphical user interface.\" </b></details> <details> <summary>What is Azure Marketplace?</summary><br><b> [Microsoft Docs](https://docs.microsoft.com/en-us/learn/modules/intro-to-azure-fundamentals/what-is-microsoft-azure): \"Azure marketplace helps connect users with Microsoft partners, independent software vendors, and startups that are offering their solutions and services, which are optimized to run on Azure.\" </b></details> <details> <summary>Explain availability sets and availability zones</summary><br><b> An availability set is a logical grouping of VMs that allows Azure to understand how your application is built to provide redundancy", "metadata": {"source_file": "learning-materials/topics/azure/README.md", "section": "Azure 101", "language": "en", "created_at": "2025-07-19T19:22:02.222682"}}
{"text": "and availability. It is recommended that two or more VMs are created within an availability set to provide for a highly available application and to meet the 99.95% Azure SLA. </b></details> <details> <summary>What is Azure Policy?</summary><br><b> [Microsoft Learn](https://learn.microsoft.com/en-us/azure/governance/policy/overview): \"Azure Policy helps to enforce organizational standards and to assess compliance at-scale. Through its compliance dashboard, it provides an aggregated view to evaluate the overall state of the environment, with the ability to drill down to the per-resource, per-policy granularity. It also helps to bring your resources to compliance through bulk remediation for existing resources and automatic remediation for new resources.\" </b></details> <details> <summary>Explain Azure managed disks</summary><br><b> </b></details>", "metadata": {"source_file": "learning-materials/topics/azure/README.md", "section": "Azure 101", "language": "en", "created_at": "2025-07-19T19:22:02.222704"}}
{"text": "<details> <summary>Explain what's Azure Resource Manager</summary><br><b> From [Azure docs](https://docs.microsoft.com/en-us/azure/azure-resource-manager/management/overview): \"Azure Resource Manager is the deployment and management service for Azure. It provides a management layer that enables you to create, update, and delete resources in your Azure account. You use management features, like access control, locks, and tags, to secure and organize your resources after deployment.\" </b></details> <details> <summary>What are the ARM template's sections ?</summary><br><b> [Microsoft Learn](https://learn.microsoft.com/en-us/azure/azure-resource-manager/templates/overview): The template has the following sections: Parameters - Provide values during deployment that allow the same template to be used with different environments. Variables - Define values that are reused in your templates. They can be constructed from parameter values. User-defined functions - Create customized functions that", "metadata": {"source_file": "learning-materials/topics/azure/README.md", "section": "Azure Resource Manager", "language": "en", "created_at": "2025-07-19T19:22:02.222925"}}
{"text": "simplify your template. Resources - Specify the resources to deploy. Outputs - Return values from the deployed resources. </b></details> <details> <summary>What's an Azure Resource Group?</summary><br><b> From [Azure docs](https://docs.microsoft.com/en-us/azure/azure-resource-manager/management/manage-resource-groups-portal): \"A resource group is a container that holds related resources for an Azure solution. The resource group can include all the resources for the solution, or only those resources that you want to manage as a group.\" </b></details>", "metadata": {"source_file": "learning-materials/topics/azure/README.md", "section": "Azure Resource Manager", "language": "en", "created_at": "2025-07-19T19:22:02.223070"}}
{"text": "<details> <summary>What Azure compute services are you familiar with?</summary><br><b> * Azure Virtual Machines * Azure Batch * Azure Service Fabric * Azure Container Instances * Azure Virtual Machine Scale Sets </b></details> <details> <summary>What \"Azure Virtual Machines\" service is used for?</summary><br><b> Azure VMs support Windows and Linux OS. They can be used for hosting web servers, applications, backups, Databases, they can also be used as jump server or azure self-hosted agent for building and deploying apps. </b></details> <details> <summary>What \"Azure Virtual Machine Scale Sets\" service is used for?</summary><br><b> Scaling Linux or Windows virtual machines; it lets you create and manage a group of load balanced VMs. The number of VM instances can automatically increase or decrease in response to demand or a defined schedule. </b></details> <details> <summary>What \"Azure Functions\" service is used for?</summary><br><b> Azure Functions is the serverless compute service of", "metadata": {"source_file": "learning-materials/topics/azure/README.md", "section": "Compute", "language": "en", "created_at": "2025-07-19T19:22:02.223330"}}
{"text": "Azure. </b></details> <details> <summary>What \"Durable Azure Function\" are?</summary> <br> [Microsoft Learn](https://docs.microsoft.com/en-us/learn/modules/intro-to-azure-fundamentals/what-is-microsoft-azure): Durable Functions is an extension of Azure Functions that lets you write stateful functions in a serverless compute environment. </details> <details> <summary>What \"Azure Container Instances\" service is used for?</summary><br><b> Running containerized applications (without the need to provision virtual machines). </b></details> <details> <summary>What \"Azure Batch\" service is used for?</summary><br><b> Running parallel and high-performance computing applications </b></details> <details> <summary>What \"Azure Service Fabric\" service is used for?</summary><br><b> </b></details> <details> <summary>What \"Azure Kubernetes\" service is used for?</summary><br><b> </b></details>", "metadata": {"source_file": "learning-materials/topics/azure/README.md", "section": "Compute", "language": "en", "created_at": "2025-07-19T19:22:02.223360"}}
{"text": "<details> <summary>What Azure network services are you familiar with?</summary><br><b> </b></details> <details> <summary>Explain VNet peering</summary><br><b> VNet peering enables connecting virtual networks. This means that you can route traffic between resources of the connected VNets privately through IPv4 addresses. Connecting VNets within the same region is known as regional VNet Peering, however connecting VNets across Azure regions is known as global VNet Peering. </b></details> <details> <summary>What's an Azure region?</summary><br><b> An Azure region is a set of datacenters deployed within an interval-defined and connected through a dedicated regional low-latency network. </b></details> <details> <summary>What is the N-tier architecture?</summary><br><b> N-tier architecture divides an application into logical layers and physical tiers. Each layer has a specific responsibility. Tiers are physically separated, running on separate machines. An N-tier application can have a", "metadata": {"source_file": "learning-materials/topics/azure/README.md", "section": "Network", "language": "en", "created_at": "2025-07-19T19:22:02.223621"}}
{"text": "closed layer architecture or an open layer architecture. N-tier architectures are typically implemented as infrastructure-as-service (IaaS) applications, with each tier running on a separate set of VMs </b></details>", "metadata": {"source_file": "learning-materials/topics/azure/README.md", "section": "Network", "language": "en", "created_at": "2025-07-19T19:22:02.223677"}}
{"text": "<details> <summary>What Azure storage services are you familiar with?</summary><br><b> </b></details> <details> <summary>What storage options Azure supports?</summary><br><b> </b></details>", "metadata": {"source_file": "learning-materials/topics/azure/README.md", "section": "Storage", "language": "en", "created_at": "2025-07-19T19:22:02.223743"}}
{"text": "<details> <summary>What is the Azure Security Center? What are some of its features?</summary><br><b> It's a monitoring service that provides threat protection across all of the services in Azure. More specifically, it: * Provides security recommendations based on your usage * Monitors security settings and continuously all the services * Analyzes and identifies potential inbound attacks * Detects and blocks malware using machine learning </b></details> <details> <summary>What is Azure Active Directory?</summary><br><b> Azure AD is a cloud-based identity service. You can use it as a standalone service or integrate it with existing Active Directory service you already running. </b></details> <details> <summary>What is Azure Advanced Threat Protection?</summary><br><b> </b></details> <details> <summary>What components are part of Azure ATP?</summary><br><b> </b></details> <details> <summary>Where logs are stored in Azure Monitor?</summary><br><b> </b></details> <details> <summary>Explain", "metadata": {"source_file": "learning-materials/topics/azure/README.md", "section": "Security", "language": "en", "created_at": "2025-07-19T19:22:02.223931"}}
{"text": "Azure Site Recovery</summary><br><b> </b></details> <details> <summary>Explain what the advisor does</summary><br><b> </b></details> <details> <summary>Which protocols are available for configuring health probe</summary><br><b> </b></details> <details> <summary>Explain Azure Active</summary><br><b> </b></details> <details> <summary>What is a subscription? What types of subscriptions are there?</summary><br><b> </b></details> <details> <summary>Explain what is a blob storage service</summary><br><b> </b></details>", "metadata": {"source_file": "learning-materials/topics/azure/README.md", "section": "Security", "language": "en", "created_at": "2025-07-19T19:22:02.223952"}}
{"text": "1. Read input from the user until you get empty string 2. For each of the lines you read, count the number of characters and print it", "metadata": {"source_file": "learning-materials/topics/shell/count_chars.md", "section": "Objectives", "language": "en", "created_at": "2025-07-19T19:22:02.224109"}}
{"text": "1. You must use a while loop 2. Assume at least three lines of input", "metadata": {"source_file": "learning-materials/topics/shell/count_chars.md", "section": "Constraints", "language": "en", "created_at": "2025-07-19T19:22:02.224134"}}
{"text": "* Write a script that will print \"Got it: <argument value>\" in case of one argument * In case no arguments were provided, it will print \"Usage: ./<program name> <argument>\" * In case of more than one argument, print \"hey hey...too many!\"", "metadata": {"source_file": "learning-materials/topics/shell/num_of_args.md", "section": "Objectives", "language": "en", "created_at": "2025-07-19T19:22:02.224220"}}
{"text": "|Name|Topic|Objective & Instructions|Solution|Comments| |--------|--------|------|----|----| |Hello World|Variables|[Exercise](hello_world.md)|[Solution](solutions/hello_world.md) | Basic |Basic date|Variables|[Exercise](basic_date.md)|[Solution](solutions/basic_date.md) | Basic |Great Day|Variables|[Exercise](great_day.md)|[Solution](solutions/great_day.md) | Basic |Factors|Arithmetic|[Exercise](factors.md)|[Solution](solutions/factors.md) | Basic |Argument Check|Conditionals|[Exercise](argument_check.md)|[Solution](solutions/argument_check.md) | Basic |Files Size|For Loops|[Exercise](files_size.md)|[Solution](solutions/files_size.md) | Basic |Count Chars|Input + While Loops|[Exercise](count_chars.md)|[Solution](solutions/count_chars.md) | Basic |Sum|Functions|[Exercise](sum.md)|[Solution](solutions/sum.md) | Basic |Number of Arguments|Case Statement|[Exercise](num_of_args.md)|[Solution](solutions/num_of_args.md) | Basic |Empty", "metadata": {"source_file": "learning-materials/topics/shell/README.md", "section": "Shell Scripting Exercises", "language": "en", "created_at": "2025-07-19T19:22:02.224459"}}
{"text": "Files|Misc|[Exercise](empty_files.md)|[Solution](solutions/empty_files.md) | Basic |Directories Comparison|Misc|[Exercise](directories_comparison.md)|[Solution](solutions/directories_comparison.md) | Basic |It's alive!|Misc|[Exercise](host_status.md)|[Solution](solutions/host_status.md) | Intermediate", "metadata": {"source_file": "learning-materials/topics/shell/README.md", "section": "Shell Scripting Exercises", "language": "en", "created_at": "2025-07-19T19:22:02.224660"}}
{"text": "<details> <summary>What does this line in shell scripts means?: <code>#!/bin/bash</code></summary><br><b> `#!/bin/bash` is She-bang /bin/bash is the most common shell used as default shell for user login of the linux system. The shell’s name is an acronym for Bourne-again shell. Bash can execute the vast majority of scripts and thus is widely used because it has more features, is well developed and better syntax. </b></details> <details> <summary>True or False? When a certain command/line fails in a shell script, the shell script, by default, will exit and stop running</summary><br><b> Depends on the language and settings used. If the script is a bash script then this statement is true. When a script written in Bash fails to run a certain command it will keep running and will execute all other commands mentioned after the command which failed. Most of the time we might actually want the opposite to happen. In order to make Bash exist when a specific command fails, use 'set -e' in your", "metadata": {"source_file": "learning-materials/topics/shell/README.md", "section": "Shell Scripting - Self Assessment", "language": "en", "created_at": "2025-07-19T19:22:02.225308"}}
{"text": "script. </b></details> <details> <summary>What do you tend to include in every script you write?</summary><br><b> Few example: * Comments on how to run it and/or what it does * If a shell script, adding \"set -e\" since I want the script to exit if a certain command failed You can have an entirely different answer. It's based only on your experience and preferences. </b></details> <details> <summary>Today we have tools and technologies like Ansible, Puppet, Chef, ... Why would someone still use shell scripting?</summary><br><b> * Speed * Flexibility * The module we need doesn't exist (perhaps a weak point because most CM technologies allow to use what is known as \"shell\" module) * We are delivering the scripts to customers who don't have access to the public network and don't necessarily have Ansible installed on their systems. </b></details>", "metadata": {"source_file": "learning-materials/topics/shell/README.md", "section": "Shell Scripting - Self Assessment", "language": "en", "created_at": "2025-07-19T19:22:02.225343"}}
{"text": "<details> <summary>How to define a variable with the value \"Hello World\"?</summary><br><b> `HW=\"Hello World` </b></details> <details> <summary>How to define a variable with the value of the current date?</summary><br><b> `DATE=$(date)` </b></details> <details> <summary>How to print the first argument passed to a script?</summary><br><b> `echo $1` </b></details> <details> <summary>Write a script to print \"yay\" unless an argument was passed and then print that argument</summary><br><b> ``` echo \"${1:-yay}\" ``` </b></details> <details> <summary>What would be the output of the following script? ```", "metadata": {"source_file": "learning-materials/topics/shell/README.md", "section": "Shell Scripting - Variables", "language": "en", "created_at": "2025-07-19T19:22:02.225432"}}
{"text": "NINJA_TURTLE=Donatello function the_best_ninja_turtle { local NINJA_TURTLE=Michelangelo echo $NINJA_TURTLE } NINJA_TURTLE=Raphael the_best_ninja_turtle ``` </summary><br><b> Michelangelo </b></details> <details> <summary>Explain what would be the result of each command: * <code>echo $0</code> * <code>echo $?</code> * <code>echo $$</code> * <code>echo $#</code></summary><br><b> </b></details> <details> <summary>What is <code>$@</code>?</summary><br><b> </b></details> <details> <summary>What is difference between <code>$@</code> and <code>$*</code>?</summary><br><b> `$@` is an array of all the arguments passed to the script `$*` is a single string of all the arguments passed to the script </b></details> <details> <summary>How do you get input from the user in shell scripts?</summary><br><b> Using the keyword <code>read</code> so for example <code>read x</code> will wait for user input and will store it in the variable x. </b></details> <details> <summary>How to compare variables", "metadata": {"source_file": "learning-materials/topics/shell/README.md", "section": "!/usr/bin/env bash", "language": "en", "created_at": "2025-07-19T19:22:02.225576"}}
{"text": "length?</summary><br><b> ``` if [ ${#1} -ne ${#2} ]; then ... ``` </b></details>", "metadata": {"source_file": "learning-materials/topics/shell/README.md", "section": "!/usr/bin/env bash", "language": "en", "created_at": "2025-07-19T19:22:02.225595"}}
{"text": "<details> <summary>Explain conditionals and demonstrate how to use them</summary><br><b> </b></details> <details> <summary>In shell scripting, how to negate a conditional?</summary><br><b> </b></details> <details> <summary>In shell scripting, how to check if a given argument is a number?</summary><br><b> ``` regex='^[0-9]+$' if [[ ${var//*.} =~ $regex ]]; then ... ``` </b></details>", "metadata": {"source_file": "learning-materials/topics/shell/README.md", "section": "Shell Scripting - Conditionals", "language": "en", "created_at": "2025-07-19T19:22:02.225635"}}
{"text": "<details> <summary>How to perform arithmetic operations on numbers?</summary><br><b> One way: `$(( 1 + 2 ))` Another way: `expr 1 + 2` </b></details> <details> <summary>How to perform arithmetic operations on numbers?</summary><br><b> </b></details> <details> <summary>How to check if a given number has 4 as a factor?</summary><br><b> `if [ $(($1 % 4)) -eq 0 ]; then` </b></details>", "metadata": {"source_file": "learning-materials/topics/shell/README.md", "section": "Shell Scripting - Arithmetic Operations", "language": "en", "created_at": "2025-07-19T19:22:02.225683"}}
{"text": "<details> <summary>What is a loop? What types of loops are you familiar with?</summary><br><b> </b></details> <details> <summary>Demonstrate how to use loops</summary><br><b> </b></details>", "metadata": {"source_file": "learning-materials/topics/shell/README.md", "section": "Shell Scripting - Loops", "language": "en", "created_at": "2025-07-19T19:22:02.225706"}}
{"text": "<details> <summary>How do you debug shell scripts?</summary><br><b> Answer depends on the language you are using for writing your scripts. If Bash is used for example then: * Adding -x to the script I'm running in Bash * Old good way of adding echo statements If Python, then using pdb is very useful. </b></details> <details> <summary>Running the following bash script, we don't get 2 as a result, why? ``` x = 2 echo $x ``` </summary><br><b> Should be `x=2` </b></details>", "metadata": {"source_file": "learning-materials/topics/shell/README.md", "section": "Shell Scripting - Troubleshooting", "language": "en", "created_at": "2025-07-19T19:22:02.225814"}}
{"text": "<details> <summary>How to extract everything after the last dot in a string?</summary><br><b> `${var//*.}` </b></details> <details> <summary>How to extract everything before the last dot in a string?</summary><br><b> ${var%.*} </b></details>", "metadata": {"source_file": "learning-materials/topics/shell/README.md", "section": "Shell Scripting - Substring", "language": "en", "created_at": "2025-07-19T19:22:02.225844"}}
{"text": "<details> <summary>Generate 8 digit random number</summary><br><b> shuf -i 9999999-99999999 -n 1 </b></details> <details> <summary>Can you give an example to some Bash best practices?</summary><br><b> </b></details> <details> <summary>What is the ternary operator? How do you use it in bash?</summary><br><b> A short way of using if/else. An example: [[ $a = 1 ]] && b=\"yes, equal\" || b=\"nope\" </b></details> <details> <summary>What does the following code do and when would you use it? <code>diff <(ls /tmp) <(ls /var/tmp)</code> </summary><br> It is called 'process substitution'. It provides a way to pass the output of a command to another command when using a pipe <code>|</code> is not possible. It can be used when a command does not support <code>STDIN</code> or you need the output of multiple commands. https://superuser.com/a/1060002/167769 </details> <details> <summary>What are you using for testing shell scripts?</summary><br><b> bats </b></details>", "metadata": {"source_file": "learning-materials/topics/shell/README.md", "section": "Shell Scripting - Misc", "language": "en", "created_at": "2025-07-19T19:22:02.226005"}}
{"text": "1. Write a script to remove all the empty files in a given directory (including nested directories)", "metadata": {"source_file": "learning-materials/topics/shell/empty_files.md", "section": "Objectives", "language": "en", "created_at": "2025-07-19T19:22:02.226227"}}
{"text": "You should include everything mentioned here in one shell script 1. Print the first argument passed to the script 2. Print the number of arguments passed to the script 3.", "metadata": {"source_file": "learning-materials/topics/shell/print_arguments.md", "section": "Objectives", "language": "en", "created_at": "2025-07-19T19:22:02.226314"}}
{"text": "1. You are given two directories as arguments and the output should be any difference between the two directories", "metadata": {"source_file": "learning-materials/topics/shell/directories_comparison.md", "section": "Objectives", "language": "en", "created_at": "2025-07-19T19:22:02.226391"}}
{"text": "1. Print the name and size of every file and directory in current path Note: use at least one for loop!", "metadata": {"source_file": "learning-materials/topics/shell/files_size.md", "section": "Objectives", "language": "en", "created_at": "2025-07-19T19:22:02.226459"}}
{"text": "1. Write a script to determine whether a given host is down or up", "metadata": {"source_file": "learning-materials/topics/shell/host_status.md", "section": "Objectives", "language": "en", "created_at": "2025-07-19T19:22:02.226519"}}
{"text": "1. Define a variable with the string 'Hello World' 2. Print the value of the variable you've defined and redirect the output to the file \"amazing_output.txt\"", "metadata": {"source_file": "learning-materials/topics/shell/hello_world.md", "section": "Objectives", "language": "en", "created_at": "2025-07-19T19:22:02.226587"}}
{"text": "Write a script that when given a number, will: * Check if the number has 2 as factor, if yes it will print \"one factor\" * Check if the number has 3 as factor, if yes it will print \"one factor...actually two!\" * If none of them (2 and 3) is a factor, print the number itself", "metadata": {"source_file": "learning-materials/topics/shell/factors.md", "section": "Objectives", "language": "en", "created_at": "2025-07-19T19:22:02.226680"}}
{"text": "1. Write a script that will print \"Today is a great day!\" unless it's given a day name and then it should print \"Today is <given day>\" Note: no need to check whether the given argument is actually a valid day", "metadata": {"source_file": "learning-materials/topics/shell/great_day.md", "section": "Objectives", "language": "en", "created_at": "2025-07-19T19:22:02.226800"}}
{"text": "Note: assume the script is executed with an argument 1. Write a script that will check if a given argument is the string \"pizza\" 1. If it's the string \"pizza\" print \"with pineapple?\" 2. If it's not the string \"pizza\" print \"I want pizza!\"", "metadata": {"source_file": "learning-materials/topics/shell/argument_check.md", "section": "Objectives", "language": "en", "created_at": "2025-07-19T19:22:02.226894"}}
{"text": "1. Write a script that gets two numbers and prints their sum 3. Make sure the input is valid (= you got two numbers from the user) 2. Test the script by running and passing it two numbers as arguments", "metadata": {"source_file": "learning-materials/topics/shell/sum.md", "section": "Objectives", "language": "en", "created_at": "2025-07-19T19:22:02.226984"}}
{"text": "1. Use functions", "metadata": {"source_file": "learning-materials/topics/shell/sum.md", "section": "Constraints", "language": "en", "created_at": "2025-07-19T19:22:02.227002"}}
{"text": "1. Write a script that will put the current date in a file called \"the_date.txt\"", "metadata": {"source_file": "learning-materials/topics/shell/basic_date.md", "section": "Objectives", "language": "en", "created_at": "2025-07-19T19:22:02.227060"}}
{"text": "echo -n \"Please insert your input: \" while read line; do echo -n \"$line\" | wc -c echo -n \"Please insert your input: \" done ```", "metadata": {"source_file": "learning-materials/topics/shell/solutions/count_chars.md", "section": "!/usr/bin/env bash", "language": "en", "created_at": "2025-07-19T19:22:02.227208"}}
{"text": "set -eu main() { case $# in 0) printf \"%s\" \"Usage: ./<program name> <argument>\"; return 1 ;; 1) printf \"%s\" \"Got it: $1\"; return 0 ;; *) return 1 ;; esac } main \"$@\" ```", "metadata": {"source_file": "learning-materials/topics/shell/solutions/num_of_args.md", "section": "!/usr/bin/env bash", "language": "en", "created_at": "2025-07-19T19:22:02.227320"}}
{"text": "for x in * do if [ -s $x ] then continue else rm -rf $x fi done ```", "metadata": {"source_file": "learning-materials/topics/shell/solutions/empty_files.md", "section": "! /bin/bash", "language": "en", "created_at": "2025-07-19T19:22:02.227411"}}
{"text": "Suppose the name of the bash script is ```dirdiff.sh``` ```", "metadata": {"source_file": "learning-materials/topics/shell/solutions/directories_comparison.md", "section": "Solution 1", "language": "en", "created_at": "2025-07-19T19:22:02.227547"}}
{"text": "if test $# -ne 2 then echo -e \"USAGE: ./dirdiff.sh directory1 directory2\" exit 1 fi", "metadata": {"source_file": "learning-materials/topics/shell/solutions/directories_comparison.md", "section": "!/bin/bash", "language": "en", "created_at": "2025-07-19T19:22:02.227575"}}
{"text": "if test `ls -1 $1 | sort | md5sum | awk -F \" \" '{print $1}'` == `ls -1 $2 | sort | md5sum | awk -F \" \" '{print $1}'` then echo -e \"No difference between the 2 directories\" exit 0 fi diff -q $1 $2 ```", "metadata": {"source_file": "learning-materials/topics/shell/solutions/directories_comparison.md", "section": "If both the checksums same, then both directories are same", "language": "en", "created_at": "2025-07-19T19:22:02.227617"}}
{"text": "With gnu find, you can use diff to compare directories recursively. ```shell diff --recursive directory1 directory2 ```", "metadata": {"source_file": "learning-materials/topics/shell/solutions/directories_comparison.md", "section": "Solution 2", "language": "en", "created_at": "2025-07-19T19:22:02.227638"}}
{"text": "for i in $(ls -S1); do echo $i: $(du -sh \"$i\" | cut -f1) done ```", "metadata": {"source_file": "learning-materials/topics/shell/solutions/files_size.md", "section": "!/usr/bin/env bash", "language": "en", "created_at": "2025-07-19T19:22:02.227757"}}
{"text": "SERVERIP=<IP Address> NOTIFYEMAIL=test@example.com ping -c 3 $SERVERIP > /dev/null 2>&1 if [ $? -ne 0 ] then # Use mailer here: mailx -s \"Server $SERVERIP is down\" -t \"$NOTIFYEMAIL\" < /dev/null fi ```", "metadata": {"source_file": "learning-materials/topics/shell/solutions/host_status.md", "section": "!/usr/bin/env bash", "language": "en", "created_at": "2025-07-19T19:22:02.227858"}}
{"text": "HW_STR=\"Hello World\" echo $HW_STR > amazing_output.txt ```", "metadata": {"source_file": "learning-materials/topics/shell/solutions/hello_world.md", "section": "!/usr/bin/env bash", "language": "en", "created_at": "2025-07-19T19:22:02.227939"}}
{"text": "(( $1 % 2 )) || res=\"one factor\" (( $1 % 3 )) || res+=\"...actually two!\" echo ${res:-$1} ```", "metadata": {"source_file": "learning-materials/topics/shell/solutions/factors.md", "section": "!/usr/bin/env bash", "language": "en", "created_at": "2025-07-19T19:22:02.228047"}}
{"text": "echo \"Today is ${1:-a great day!}\" ```", "metadata": {"source_file": "learning-materials/topics/shell/solutions/great_day.md", "section": "!/usr/bin/env bash", "language": "en", "created_at": "2025-07-19T19:22:02.228134"}}
{"text": "Note: assume the script is executed with an argument 1. Write a script that will check if a given argument is the string \"pizza\" 2. If it's the string \"pizza\" print \"with pineapple?\" 3. If it's not the string \"pizza\" print \"I want pizza!\"", "metadata": {"source_file": "learning-materials/topics/shell/solutions/argument_check.md", "section": "Objectives", "language": "en", "created_at": "2025-07-19T19:22:02.228218"}}
{"text": "[[ ${1} == \"pizza\" ]] && echo \"with pineapple?\" || echo \"I want pizza!\" ```", "metadata": {"source_file": "learning-materials/topics/shell/solutions/argument_check.md", "section": "!/usr/bin/env bash", "language": "en", "created_at": "2025-07-19T19:22:02.228242"}}
{"text": "re='^[0-9]+$' if ! [[ $1 =~ $re && $2 =~ $re ]]; then echo \"Oh no...I need two numbers\" exit 2 fi function sum { echo $(( $1 + $2 )) } sum $1 $2 ```", "metadata": {"source_file": "learning-materials/topics/shell/solutions/sum.md", "section": "!/usr/bin/env bash", "language": "en", "created_at": "2025-07-19T19:22:02.228510"}}
{"text": "echo $(date) > the_date.txt ```", "metadata": {"source_file": "learning-materials/topics/shell/solutions/basic_date.md", "section": "!/usr/bin/env bash", "language": "en", "created_at": "2025-07-19T19:22:02.228588"}}
{"text": "Design a database table for a message board system. It should include the following information: * Personal details * Who saw the message and when * Replies * Tagged people in the message * Message categories Notes: * No SQL is needed * You should include: table names, field names, data types and mention the foreign keys used.", "metadata": {"source_file": "learning-materials/topics/databases/table_for_message_board_system.md", "section": "Instructions", "language": "en", "created_at": "2025-07-19T19:22:02.228751"}}
{"text": "- [Databases](#databases) - [Exercises](#exercises) - [Questions](#questions) - [SQL](#sql) - [Time Series](#time-series)", "metadata": {"source_file": "learning-materials/topics/databases/README.md", "section": "Databases", "language": "en", "created_at": "2025-07-19T19:22:02.228910"}}
{"text": "|Name|Topic|Objective & Instructions|Solution|Comments| |--------|--------|------|----|----| | Message Board Tables | Relational DB Tables | [Exercise](topics/databases/table_for_message_board_system.md) | [Solution](topics/databases/solutions/table_for_message_board_system.md)", "metadata": {"source_file": "learning-materials/topics/databases/README.md", "section": "Exercises", "language": "en", "created_at": "2025-07-19T19:22:02.228935"}}
{"text": "<details> <summary>What type of databases are you familiar with?</summary><br><b> Relational (SQL) NoSQL Time series </b></details>", "metadata": {"source_file": "learning-materials/topics/databases/README.md", "section": "Questions", "language": "en", "created_at": "2025-07-19T19:22:02.228954"}}
{"text": "<details> <summary>What is a relational database?</summary><br><b> * Data Storage: system to store data in tables * SQL: programming language to manage relational databases * Data Definition Language: a standard syntax to create, alter and delete tables </b></details> <details> <summary>What does it mean when a database is ACID compliant?</summary><br> ACID stands for Atomicity, Consistency, Isolation, Durability. In order to be ACID compliant, the database must meet each of the four criteria **Atomicity** - When a change occurs to the database, it should either succeed or fail as a whole. For example, if you were to update a table, the update should completely execute. If it only partially executes, the update is considered failed as a whole, and will not go through - the DB will revert back to it's original state before the update occurred. It should also be mentioned that Atomicity ensures that each transaction is completed as it's own stand alone \"unit\" - if any part fails, the", "metadata": {"source_file": "learning-materials/topics/databases/README.md", "section": "SQL", "language": "en", "created_at": "2025-07-19T19:22:02.230163"}}
{"text": "whole statement fails. **Consistency** - any change made to the database should bring it from one valid state into the next. For example, if you make a change to the DB, it shouldn't corrupt it. Consistency is upheld by checks and constraints that are pre-defined in the DB. For example, if you tried to change a value from a string to an int when the column should be of datatype string, a consistent DB would not allow this transaction to go through, and the action would not be executed **Isolation** - this ensures that a database will never be seen \"mid-update\" - as multiple transactions are running at the same time, it should still leave the DB in the same state as if the transactions were being run sequentially. For example, let's say that 20 other people were making changes to the database at the same time. At the time you executed your query, 15 of the 20 changes had gone through, but 5 were still in progress. You should only see the 15 changes that had completed - you wouldn't see", "metadata": {"source_file": "learning-materials/topics/databases/README.md", "section": "SQL", "language": "en", "created_at": "2025-07-19T19:22:02.230194"}}
{"text": "the database mid-update as the change goes through. **Durability** - Once a change is committed, it will remain committed regardless of what happens (power failure, system crash, etc.). This means that all completed transactions must be recorded in non-volatile memory. Note that SQL is by nature ACID compliant. Certain NoSQL DB's can be ACID compliant depending on how they operate, but as a general rule of thumb, NoSQL DB's are not considered ACID compliant </details> <details> <summary>What is sharding?</summary><br><b> Sharding is a horizontal partitioning. Are you able to explain what is it good for? </b></details> <details> <summary>You find out your database became a bottleneck and users experience issues accessing data. How can you deal with such situation?</summary><br><b> Not much information provided as to why it became a bottleneck and what is current architecture, so one general approach could be<br> to reduce the load on your database by moving frequently-accessed data to", "metadata": {"source_file": "learning-materials/topics/databases/README.md", "section": "SQL", "language": "en", "created_at": "2025-07-19T19:22:02.230216"}}
{"text": "in-memory structure. </b></details> <details> <summary>What is a connection pool?</summary><br><b> Connection Pool is a cache of database connections and the reason it's used is to avoid an overhead of establishing a connection for every query done to a database. </b></details> <details> <summary>What is a connection leak?</summary><br><b> A connection leak is a situation where database connection isn't closed after being created and is no longer needed. </b></details> <details> <summary>What is Table Lock?</summary><br><b> </b></details> <details> <summary>Your database performs slowly than usual. More specifically, your queries are taking a lot of time. What would you do?</summary><br><b> * Query for running queries and cancel the irrelevant queries * Check for connection leaks (query for running connections and include their IP) * Check for table locks and kill irrelevant locking sessions </b></details> <details> <summary>What is a Data Warehouse?</summary><br><b> \"A data warehouse", "metadata": {"source_file": "learning-materials/topics/databases/README.md", "section": "SQL", "language": "en", "created_at": "2025-07-19T19:22:02.230236"}}
{"text": "is a subject-oriented, integrated, time-variant and non-volatile collection of data in support of organisation's decision-making process\" </b></details> <details> <summary>Explain what is a time-series database</summary><br><b> </b></details> <details> <summary>What is OLTP (Online transaction processing)?</summary><br><b> </b></details> <details> <summary>What is OLAP (Online Analytical Processing)?</summary><br><b> </b></details> <details> <summary>What is an index in a database?</summary><br><b> A database index is a data structure that improves the speed of operations in a table. Indexes can be created using one or more columns, providing the basis for both rapid random lookups and efficient ordering of access to records. </b></details> <details> <summary>What data types are there in relational databases?</summary><br><b> </b></details> <details> <summary>Explain Normalization</summary><br><b> Data that is used multiple times in a database should be stored once and referenced with", "metadata": {"source_file": "learning-materials/topics/databases/README.md", "section": "SQL", "language": "en", "created_at": "2025-07-19T19:22:02.230255"}}
{"text": "a foreign key.<br> This has the clear benefit of ease of maintenance where you need to change a value only in a single place to change it everywhere. </b></details> <details> <summary>Explain Primary Key and Foreign Key</summary><br><b> Primary Key: each row in every table should a unique identifier that represents the row.<br> Foreign Key: a reference to another table's primary key. This allows you to join table together to retrieve all the information you need without duplicating data. </b></details> <details> <summary>What types of data tables have you used?</summary><br><b> * Primary data table: main data you care about * Details table: includes a foreign key and has one to many relationship * Lookup values table: can be one table per lookup or a table containing all the lookups and has one to many relationship * Multi reference table </b></details> <details> <summary>What is ORM? What benefits it provides in regards to relational databases usage?</summary><br><b>", "metadata": {"source_file": "learning-materials/topics/databases/README.md", "section": "SQL", "language": "en", "created_at": "2025-07-19T19:22:02.230273"}}
{"text": "[Wikipedia](https://en.wikipedia.org/wiki/Object%E2%80%93relational_mapping): \"is a programming technique for converting data between incompatible type systems using object-oriented programming languages\" In regards to the relational databases: * Database as code * Database abstraction * Encapsulates SQL complexity * Enables code review process * Enables usage as a native OOP structure </b></details> <details> <summary>What is DDL?</summary><br><b> [Wikipedia](https://en.wikipedia.org/wiki/Data_definition_language): \"In the context of SQL, data definition or data description language (DDL) is a syntax for creating and modifying database objects such as tables, indices, and users.\" </b></details>", "metadata": {"source_file": "learning-materials/topics/databases/README.md", "section": "SQL", "language": "en", "created_at": "2025-07-19T19:22:02.230418"}}
{"text": "<details> <summary>What is Time Series database?</summary><br><b> A database designed specifically for time series based data. It comes with multiple optimizations: <TODO>: complete this :) </b></details>", "metadata": {"source_file": "learning-materials/topics/databases/README.md", "section": "Time Series", "language": "en", "created_at": "2025-07-19T19:22:02.230460"}}
{"text": "Note: This is just one possible design 2nd Note: PK = primary key, FK = Foreign key ----- People ----- ID int PK FirstName varchar(255) LastName varchar(255) DOB date Gender varchar(1) Phone varchar(10) | \\ | \\ | \\ v \\ \\ --- Messages --- v ID int PK MessageBoardID FK --- MessageTags --- --- MessageBoards --- PeopleID int FK ID int PK ID int PK ----> MsgDate datetime ---> MessageID FK Board text Message text PeopleID int Fk MessageID (FK) ^ | | | |______|", "metadata": {"source_file": "learning-materials/topics/databases/solutions/table_for_message_board_system.md", "section": "Solution", "language": "en", "created_at": "2025-07-19T19:22:02.230744"}}
{"text": "A completely free application for testing your knowledge on Linux. Disclaimer: developed by repository owner <a href=\"https://play.google.com/store/apps/details?id=com.codingshell.linuxmaster\"><img src=\"../../images/linux_master.jpeg\"/></a> - [Linux](#linux) - [Linux Master Application](#linux-master-application) - [Linux Exercises](#linux-exercises) - [Basics](#basics) - [Misc](#misc) - [Linux Questions](#linux-questions) - [Linux 101](#linux-101) - [I/O Redirection](#io-redirection) - [Filesystem Hierarchy Standard](#filesystem-hierarchy-standard) - [Permissions](#permissions) - [Scenarios](#scenarios) - [Systemd](#systemd) - [Troubleshooting and Debugging](#troubleshooting-and-debugging) - [Scenarios](#scenarios-1) - [Kernel](#kernel) - [SSH](#ssh) - [Globbing & Wildcards](#globbing--wildcards) - [Boot Process](#boot-process) - [Disk and Filesystem](#disk-and-filesystem) - [Performance Analysis](#performance-analysis) - [Processes](#processes) - [Security](#security) -", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "Linux Master Application", "language": "en", "created_at": "2025-07-19T19:22:02.233078"}}
{"text": "[Networking](#networking) - [DNS](#dns) - [Packaging](#packaging) - [DNF](#dnf) - [Applications and Services](#applications-and-services) - [Users and Groups](#users-and-groups) - [Hardware](#hardware) - [Namespaces](#namespaces) - [Virtualization](#virtualization) - [AWK](#awk) - [System Calls](#system-calls) - [Filesystem & Files](#filesystem--files) - [Advanced Networking](#advanced-networking) - [Memory](#memory) - [Distributions](#distributions) - [Sed](#sed) - [Misc](#misc-1)", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "Linux Master Application", "language": "en", "created_at": "2025-07-19T19:22:02.233120"}}
{"text": "|Name|Topic|Objective & Instructions|Solution|Comments| |--------|--------|------|----|----| | Navigation | cd, pwd | [Exercise](exercises/navigation/README.md) | [Solution](exercises/navigation/solution.md) | Create and Destroy | touch, rm, mkdir | [Exercise](exercises/create_remove/README.md) | [Solution](exercises/create_remove/solution.md) | Copy Time | touch, cp, ls | [Exercise](exercises/copy/README.md) | [Solution](exercises/copy/solution.md)", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "Basics", "language": "en", "created_at": "2025-07-19T19:22:02.233161"}}
{"text": "|Name|Topic|Objective & Instructions|Solution|Comments| |--------|--------|------|----|----| | Unique Count | | [Exercise](exercises/uniqe_count/README.md) | [Solution](exercises/uniqe_count/solution.md)", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "Misc", "language": "en", "created_at": "2025-07-19T19:22:02.233182"}}
{"text": "<details> <summary>What is Linux?</summary><br><b> [Wikipedia](https://en.wikipedia.org/wiki/Linux): \"Linux is a family of open-source Unix-like operating systems based on the Linux kernel, an operating system kernel first released on September 17, 1991, by Linus Torvalds. Linux is typically packaged in a Linux distribution.\" [Red Hat](https://www.redhat.com/en/topics/linux/what-is-linux): \"Linux® is an open source operating system (OS). An operating system is the software that directly manages a system’s hardware and resources, like CPU, memory, and storage. The OS sits between applications and hardware and makes the connections between all of your software and the physical resources that do the work.\" </b></details> <details> <summary>Explain what each of the following commands does and give an example on how to use it: * touch * ls * rm * cat * cp * mkdir * pwd * cd </summary><br><b> * touch - update file's timestamp. More commonly used for creating files * ls - listing files and", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "Linux 101", "language": "en", "created_at": "2025-07-19T19:22:02.234990"}}
{"text": "directories * rm - remove files and directories * cat - create, view and concatenate files * cp - copy files and directories * mkdir - create directories * pwd - print current working directory (= at what path the user currently located) * cd - change directory </b></details> <details> <summary>What each of the following commands does? * cd / * cd ~ * cd * cd .. * cd . * cd - </summary><br><b> * cd / -> change to the root directory * cd ~ -> change to your home directory * cd -> change to your home directory * cd .. -> change to the directory above your current i.e parent directory * cd . -> change to the directory you currently in * cd - -> change to the last visited path </b></details> <details> <summary>Some of the commands in the previous question can be run with the -r/-R flag. What does it do? Give an example to when you would use it</summary><br><b> The -r (or -R in some commands) flag allows the user to run a certain command recursively. For example, listing all the files under", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "Linux 101", "language": "en", "created_at": "2025-07-19T19:22:02.235027"}}
{"text": "the following tree is possible when done recursively (`ls -R`): /dir1/ dir2/ file1 file2 dir3/ file3 To list all the files, one can run `ls -R /dir1` </b></details> <details> <summary>Explain each field in the output of `ls -l` command</summary><br><b> It shows a detailed list of files in a long format. From the left: * file permissions, number of links, owner name, owner group, file size, timestamp of last modification and directory/file name </b></details> <details> <summary>What are hidden files/directories? How to list them?</summary><br><b> These are files directly not displayed after performing a standard ls direct listing. An example of these files are .bashrc which are used to execute some scripts. Some also store configuration about services on your host like .KUBECONFIG. The command used to list them is, `ls -a` </b></details> <details> <summary>What do > and < do in terms of input and output for programs?</summary><br><b> They take in input (<) and output for a given file", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "Linux 101", "language": "en", "created_at": "2025-07-19T19:22:02.235049"}}
{"text": "(>) using stdin and stdout. `myProgram < input.txt > executionOutput.txt` </b></details> <details> <summary>Explain what each of the following commands does and give an example on how to use it: * sed * grep * cut * awk </summary><br><b> - sed: a stream editor. Can be used for various purposes like replacing a word in a file: `sed -i s/salad/burger/g` - grep: a search tool. Used to search, count or match a text in a file: - searching for any line that contains a word in a file: `grep 'word' file.md` - or displaying the total number of times a string appears in a file: `grep -c 'This is a string' file.md` - cut: a tool for cutting out selected portions of each line of a file: - syntax: `cut OPTION [FILE]` - cutting first two bytes from a word in a file: `cut -b 1-2 file.md`, output: `wo` - awk: a programming language that is mainly used for text processing and data extraction. It can be used to manipulate and modify text in a file: - syntax: awk [OPTIONS] [FILTER] [FILE] extracting a", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "Linux 101", "language": "en", "created_at": "2025-07-19T19:22:02.235201"}}
{"text": "specific field from a CSV file: awk -F ',' '{print $1}' file.csv, output: first field of each line in the file </b></details> <details> <summary>How to rename the name of a file or a directory?</summary><br><b> Using the `mv` command. </b></details> <details> <summary>Specify which command would you use (and how) for each of the following scenarios * Remove a directory with files * Display the content of a file * Provides access to the file /tmp/x for everyone * Change working directory to user home directory * Replace every occurrence of the word \"good\" with \"great\" in the file /tmp/y</summary><br><b> - `rm -rf dir` - `cat or less` - `chmod 777 /tmp/x` - `cd ~` - `sed -i s/good/great/g /tmp/y` </b></details> <details> <summary>How can you check what is the path of a certain command?</summary><br><b> * whereis * which </b></details> <details> <summary>What is the difference between these two commands? Will it result in the same output? ``` echo hello world echo \"hello world\" ```", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "Linux 101", "language": "en", "created_at": "2025-07-19T19:22:02.235233"}}
{"text": "</summary><br><b> The echo command receives two separate arguments in the first execution and in the second execution it gets one argument which is the string \"hello world\". The output will be the same. </b></details> <details> <summary>Explain piping. How do you perform piping?</summary><br><b> Using a pipe in Linux, allows you to send the output of one command to the input of another command. For example: `cat /etc/services | wc -l` </b></details> <details> <summary>Fix the following commands: * sed \"s/1/2/g' /tmp/myFile * find . -iname \\*.yaml -exec sed -i \"s/1/2/g\" {} ; </summary><br><b> ``` sed 's/1/2/g' /tmp/myFile # sed \"s/1/2/g\" is also fine find . -iname \"*.yaml\" -exec sed -i \"s/1/2/g\" {} \\; ``` </b></details> <details> <summary>How to check which commands you executed in the past?</summary><br><b> history command or .bash_history file * also can use up arrow key to access or to show the recent commands you type </b></details> <details> <summary>Running the command", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "Linux 101", "language": "en", "created_at": "2025-07-19T19:22:02.235254"}}
{"text": "<code>df</code> you get \"command not found\". What could be wrong and how to fix it?</summary><br><b> </b> <p><b> Most likely the default/generated $PATH was somehow modified or overridden thus not containing <code>/bin/</code> where df would normally go. This issue could also happen if bash_profile or any configuration file of your interpreter was wrongly modified, causing erratics behaviours. You would solve this by fixing your $PATH variable: As to fix it there are several options: 1. Manually adding what you need to your $PATH <code>PATH=\"$PATH\":/user/bin:/..etc</code> 2. You have your weird env variables backed up. 3. You would look for your distro default $PATH variable, copy paste using method #1 Note: There are many ways of getting errors like this: if bash_profile or any configuration file of your interpreter was wrongly modified; causing erratics behaviours, permissions issues, bad compiled software (if you compiled it by yourself)... there is no answer that will be true 100%", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "Linux 101", "language": "en", "created_at": "2025-07-19T19:22:02.235274"}}
{"text": "of the time. </b> </p> </details> <details> <summary>How do you schedule tasks periodically?</summary><br><b> You can use the commands <code>cron</code> and <code>at</code>. With cron, tasks are scheduled using the following format: <code>*/30 * * * * bash myscript.sh</code> Executes the script every 30 minutes. <minute> <hour> <day of month> <month> <day of week> <command to execute> The tasks are stored in a cron file, you can write in it using <code>crontab -e</code> Alternatively if you are using a distro with systemd it's recommended to use systemd timers. </b></details> <a name=\"questions-linux-redirection\"></a>", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "Linux 101", "language": "en", "created_at": "2025-07-19T19:22:02.235293"}}
{"text": "<details> <summary>Explain Linux I/O redirection</summary><br><b> In Linux, IO redirection is a way of changing the default input/output behavior of a command or program. It allows you to redirect input and output from/to different sources/destinations, such as files, devices, and other commands. Here are some common examples of IO redirection: * Redirecting Standard Output (stdout): <code>ls > filelist.txt</code> * Redirecting Standard Error (stderr): <code>ls /some/nonexistent/directory 2> error.txt</code> * Appending to a file: <code>echo \"hello\" >> myfile.txt</code> * Redirecting Input (stdin): <code>sort < unsorted.txt</code> * Using Pipes: Pipes (\"|\"): <code>ls | grep \"\\.txt$\"</code> </b></details> <details> <summary>Demonstrate Linux output redirection</summary><br><b> <code>ls > ls_output.txt</code> </b></details> <details> <summary>Demonstrate Linux stderr output redirection</summary><br><b> <code>yippiekaiyay 2> ls_output.txt</code> </b></details> <details>", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "I/O Redirection", "language": "en", "created_at": "2025-07-19T19:22:02.235461"}}
{"text": "<summary>Demonstrate Linux stderr to stdout redirection</summary><br><b> <code>yippiekaiyay &> file</code> </b></details> <details> <summary>What is the result of running the following command? <code>yippiekaiyay 1>&2 die_hard</code></code></summary><br><b> An output similar to: `yippikaiyay: command not found...`<br> The file `die_hard` will not be created </b></details> <a name=\"questions-linux-fhs\"></a>", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "I/O Redirection", "language": "en", "created_at": "2025-07-19T19:22:02.235482"}}
{"text": "<details> <summary>In Linux FHS (Filesystem Hierarchy Standard) what is the <code>/</code>?</summary><br><b> The root of the filesystem. The beginning of the tree. </b></details> <details> <summary>What is stored in each of the following paths? - /bin, /sbin, /usr/bin and /usr/sbin - /etc - /home - /var - /tmp</summary><br><b> * binaries * configuration files * home directories of the different users * files that tend to change and be modified like logs * temporary files </b></details> <details> <summary>What is special about the /tmp directory when compared to other directories?</summary><br><b> `/tmp` folder is cleaned automatically, usually upon reboot. </b></details> <details> <summary>What kind of information one can find in /proc?</summary><br><b> It contains useful information about the processes that are currently running, it is regarded as control and information center for kernel. </b></details> <details> <summary>What makes /proc different from other", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "Filesystem Hierarchy Standard", "language": "en", "created_at": "2025-07-19T19:22:02.235802"}}
{"text": "filesystems?</summary><br><b> /proc is a special virtual filesystem in Unix-like operating systems, including Linux, that provides information about processes and system resources. </b></details> <details> <summary>True or False? only root can create files in /proc</summary><br><b> False. No one can create file in /proc directly (certain operations can lead to files being created in /proc by the kernel). </b></details> <details> <summary>What can be found in /proc/cmdline?</summary><br><b> The command passed to the boot loader to run the kernel </b></details> <details> <summary>In which path can you find the system devices (e.g. block storage)?</summary><br><b> /dev </b></details> <a name=\"questions-linux-permissions\"></a>", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "Filesystem Hierarchy Standard", "language": "en", "created_at": "2025-07-19T19:22:02.235946"}}
{"text": "<details> <summary>How to change the permissions of a file?</summary><br><b> Using the `chmod` command. </b></details> <details> <summary>What does the following permissions mean?: * 777 * 644 * 750</summary><br><b> <pre> 777 - You give the owner, group and other: Execute (1), Write (2) and Read (4); 4+2+1 = 7. 644 - Owner has Read (4), Write (2), 4+2 = 6; Group and Other have Read (4). 750 - Owner has x+r+w, Group has Read (4) and Execute (1); 4+1 = 5. Other have no permissions. </pre> </b></details> <details> <summary>What this command does? <code>chmod +x some_file</code></summary><br><b> It adds execute permissions to all sets i.e user, group and others </b></details> <details> <summary>Explain what is setgid and setuid</summary><br><b> * setuid is a linux file permission that permits a user to run a file or program with the permissions of the owner of that file. This is possible by elevation of current user privileges. * setgid is a process when executed will run as the group that", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "Permissions", "language": "en", "created_at": "2025-07-19T19:22:02.236490"}}
{"text": "owns the file. </b></details> <details> <summary>What is the purpose of sticky bit?</summary><br><b> Its a bit that only allows the owner or the root user to delete or modify the file. </b></details> <details> <summary>What the following commands do? - chmod - chown - chgrp</summary><br><b> * chmod - changes access permissions to files system objects * chown - changes the owner of file system files and directories * chgrp - changes the group associated with a file system object </b></details> <details> <summary>What is sudo? How do you set it up?</summary><br><b> sudo is a command-line utility in Unix-like operating systems that allows users to run programs with the privileges of another user, usually the superuser (root). It stands for \"superuser do. The sudo program is installed by default in almost all Linux distributions. If you need to install sudo in Debian/Ubuntu, use the command apt-get install sudo </b></details> <details> <summary>True or False? In order to install packages", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "Permissions", "language": "en", "created_at": "2025-07-19T19:22:02.236515"}}
{"text": "on the system one must be the root user or use the sudo command</summary><br><b> True </b></details> <details> <summary>Explain what are ACLs. For what use cases would you recommend to use them?</summary><br><b> </b></details> <details> <summary>You try to create a file but it fails. Name at least three different reason as to why it could happen</summary><br><b> * No more disk space * No more inodes * No permissions </b></details> <details> <summary>A user accidentally executed the following <code>chmod -x $(which chmod)</code>. How to fix it?</summary><br><b> Using `sudo setfacl -m u::rx /usr/bin/chmod` will set the execute permissions on `chmod` for all the users. Post this, the `chmod` binary can be used as usual. </b></details> <a name=\"questions-linux-scenarios\"></a>", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "Permissions", "language": "en", "created_at": "2025-07-19T19:22:02.236534"}}
{"text": "<details> <summary>You would like to copy a file to a remote Linux host. How would you do?</summary><br><b> There are multiple ways to transfer files between hosts. Personal opinion: use `rsync` </b></details> <details> <summary>How to generate a random string?</summary><br><b> One way is to run the following: `cat /proc/sys/kernel/random/uuid` </b></details> <details> <summary>How to generate a random string of 7 characters?</summary><br><b> `mkpasswd -l 7` </b></details> <a name=\"questions-linux-systemd\"></a>", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "Scenarios", "language": "en", "created_at": "2025-07-19T19:22:02.236597"}}
{"text": "<details> <summary>What is systemd?</summary><br> <b> Systemd is a daemon (System 'd', d stands for daemon). A daemon is a program that runs in the background without direct control of the user, although the user can at any time talk to the daemon. systemd has many features such as user processes control/tracking, snapshot support, inhibitor locks.. If we visualize the unix/linux system in layers, systemd would fall directly after the linux kernel.<br> Hardware -> Kernel -> <u>Daemons</u>, System Libraries, Server Display. </b> </details> <details> <summary>How to start or stop a service?</summary><br><b> To start a service: `systemctl start <service name>` To stop a service: `systemctl stop <service name>` </b></details> <details> <summary>How to check the status of a service?</summary><br><b> `systemctl status <service name>` </b></details> <details> <summary>On a system which uses systemd, how would you display the logs?</summary><br><b> <code>journalctl</code> </b></details>", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "Systemd", "language": "en", "created_at": "2025-07-19T19:22:02.236799"}}
{"text": "<details> <summary>Describe how to make a certain process/app a service</summary><br><b> </b></details>", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "Systemd", "language": "en", "created_at": "2025-07-19T19:22:02.236826"}}
{"text": "<details> <summary>Where system logs are located?</summary><br><b> /var/log </b></details> <details> <summary>How to follow file's content as it being appended without opening the file every time?</summary><br><b> tail -f <file_name> </b></details> <details> <summary>What are you using for troubleshooting and debugging <b>network</b> issues?</summary><br><b> <code>dstat -t</code> is great for identifying network and disk issues. <code>netstat -tnlaup</code> can be used to see which processes are running on which ports. <code>lsof -i -P</code> can be used for the same purpose as netstat. <code>ngrep -d any metafilter</code> for matching regex against payloads of packets. <code>tcpdump</code> for capturing packets <code>wireshark</code> same concept as tcpdump but with GUI (optional). </b></details> <details> <summary>What are you using for troubleshooting and debugging <b>disk & file system</b> issues?</summary><br><b> <code>dstat -t</code> is great for identifying network and disk", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "Troubleshooting and Debugging", "language": "en", "created_at": "2025-07-19T19:22:02.237143"}}
{"text": "issues. <code>opensnoop</code> can be used to see which files are being opened on the system (in real time). </b></details> <details> <summary>What are you using for troubleshooting and debugging <b>process</b> issues?</summary><br><b> <code>strace</code> is great for understanding what your program does. It prints every system call your program executed. </b></details> <details> <summary>What are you using for debugging CPU related issues?</summary><br><b> <code>top</code> will show you how much CPU percentage each process consumes <code>perf</code> is a great choice for sampling profiler and in general, figuring out what your CPU cycles are \"wasted\" on <code>flamegraphs</code> is great for CPU consumption visualization (http://www.brendangregg.com/flamegraphs.html) </b></details> <details> <summary>You get a call from someone claiming \"my system is SLOW\". What do you do?</summary><br><b> * Check with `top` for anything unusual * Run `dstat -t` to check if it's related to disk or", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "Troubleshooting and Debugging", "language": "en", "created_at": "2025-07-19T19:22:02.237163"}}
{"text": "network. * Check if it's network related with `sar` * Check I/O stats with `iostat` </b></details> <details> <summary>Explain iostat output</summary><br><b> </b></details> <details> <summary>How to debug binaries?</summary><br><b> </b></details> <details> <summary>What is the difference between CPU load and utilization?</summary><br><b> </b></details> <details> <summary>How you measure time execution of a program?</summary><br><b> </b></details>", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "Troubleshooting and Debugging", "language": "en", "created_at": "2025-07-19T19:22:02.237388"}}
{"text": "<details> <summary>You have a process writing to a file. You don't know which process exactly, you just know the path of the file. You would like to kill the process as it's no longer needed. How would you achieve it?</summary><br><b> 1. Run `lsof <FILE_PATH>` 2. Use the pid (process ID) from the lsof command and run `kill <PID>` </b></details>", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "Scenarios", "language": "en", "created_at": "2025-07-19T19:22:02.237456"}}
{"text": "<details> <summary>What is a kernel, and what does it do?</summary><br><b> The kernel is part of the operating system and is responsible for tasks like: * Allocating memory * Schedule processes * Control CPU </b></details> <details> <summary>How do you find out which Kernel version your system is using?</summary><br><b> `uname -a` command </b></details> <details> <summary>What is a Linux kernel module and how do you load a new module?</summary><br><b> A Linux kernel module is a piece of code that can be dynamically loaded into the kernel to extend its functionality. These modules are typically used to add support for hardware devices, filesystems, or system calls. The kernel itself is monolithic, but with modules, its capabilities can be extended without having to reboot the system or recompile the entire kernel. </b></details> <details> <summary>Explain user space vs. kernel space</summary><br><b> The operating system executes the kernel in protected memory to prevent anyone from", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "Kernel", "language": "en", "created_at": "2025-07-19T19:22:02.238103"}}
{"text": "changing (and risking it crashing). This is what is known as \"Kernel space\". \"User space\" is where users executes their commands or applications. It's important to create this separation since we can't rely on user applications to not tamper with the kernel, causing it to crash. Applications can access system resources and indirectly the kernel space by making what is called \"system calls\". </b></details> <details> <summary>In what phases of kernel lifecycle, can you change its configuration?</summary><br><b> * Build time (when it's compiled) * Boot time (when it starts) * Runtime (once it's already running) </b></details> <details> <summary>Where can you find kernel's configuration?</summary><br><b> Usually it will reside in `/boot/config-<kernel version>.<os release>.<arch>` </b></details> <details> <summary>Where can you find the file that contains the command passed to the boot loader to run the kernel?</summary><br><b> `/proc/cmdline` </b></details> <details> <summary>How to list", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "Kernel", "language": "en", "created_at": "2025-07-19T19:22:02.238132"}}
{"text": "kernel's runtime parameters?</summary><br><b> `sysctl -a` </b></details> <details> <summary>Will running <code>sysctl -a</code> as a regular user vs. root, produce different result?</summary><br><b> Yes, you might notice that in most systems, when running `systctl -a` with root, you'll get more runtime parameters compared to executing the same command with a regular user. </b></details> <details> <summary>You would like to enable IPv4 forwarding in the kernel, how would you do it?</summary><br><b> `sudo sysctl net.ipv4.ip_forward=1` To make it persistent (applied after reboot for example): insert `net.ipv4.ip_forward = 1` into `/etc/sysctl.conf` Another way to is to run `echo 1 | sudo tee /proc/sys/net/ipv4/ip_forward` </b></details> <details> <summary>How <code>sysctl</code> applies the changes to kernel's runtime parameters the moment you run sysctl command?</summary><br><b> If you `strace` the sysctl command you can see it does it by changing the file under /proc/sys/... In the past", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "Kernel", "language": "en", "created_at": "2025-07-19T19:22:02.238152"}}
{"text": "it was done with sysctl system call, but it was deprecated at some point. </b></details> <details> <summary>How changes to kernel runtime parameters persist? (applied even after reboot to the system for example)</summary><br><b> There is a service called `systemd-sysctl` that takes the content of /etc/sysctl.conf and applies it. This is how changes persist, even after reboot, when they are written in /etc/sysctl.conf </b></details> <details> <summary>Are the changes you make to kernel parameters in a container, affects also the kernel parameters of the host on which the container runs?</summary><br><b> No. Containers have their own /proc filesystem so any change to kernel parameters inside a container, are not affecting the host or other containers running on that host. </b></details> <a name=\"questions-linux-ssh\"></a>", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "Kernel", "language": "en", "created_at": "2025-07-19T19:22:02.238171"}}
{"text": "<details> <summary>What is SSH? How to check if a Linux server is running SSH?</summary><br><b> [Wikipedia Definition](https://en.wikipedia.org/wiki/SSH_(Secure_Shell)): \"SSH or Secure Shell is a cryptographic network protocol for operating network services securely over an unsecured network.\" [Hostinger.com Definition](https://www.hostinger.com/tutorials/ssh-tutorial-how-does-ssh-work): \"SSH, or Secure Shell, is a remote administration protocol that allows users to control and modify their remote servers over the Internet.\" An SSH server will have SSH daemon running. Depends on the distribution, you should be able to check whether the service is running (e.g. systemctl status sshd). </b></details> <details> <summary>Why SSH is considered better than telnet?</summary><br><b> Telnet also allows you to connect to a remote host but as opposed to SSH where the communication is encrypted, in telnet, the data is sent in clear text, so it doesn't considered to be secured because anyone on the", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "SSH", "language": "en", "created_at": "2025-07-19T19:22:02.238773"}}
{"text": "network can see what exactly is sent, including passwords. </b></details> <details> <summary>What is stored in <code>~/.ssh/known_hosts</code>?</summary><br><b> The file stores the key fingerprints for the clients connecting to the SSH server. This fingerprint creates a trust between the client and the server for future SSH connections. </b></details> <details> <summary>You try to ssh to a server and you get \"Host key verification failed\". What does it mean?</summary><br><b> It means that the key of the remote host was changed and doesn't match the one that stored on the machine (in ~/.ssh/known_hosts). </b></details> <details> <summary>What is the difference between SSH and SSL?</summary><br><b> </b></details> <details> <summary>What <code>ssh-keygen</code> is used for?</summary><br><b> <code>ssh-keygen</code> is a tool to generate an authentication key pair for SSH, that consists of a private and a public key. It supports a number of algorithms to generate authentication keys : - dsa", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "SSH", "language": "en", "created_at": "2025-07-19T19:22:02.238808"}}
{"text": "- ecdsa - ecdsa-sk - ed25519 - ed25519-sk - rsa (default) One can also specify number of bits in key. Command below generates an SSH key pair with RSA 4096-bits : ``` $ ssh-keygen -t rsa -b 4096 ``` The output looks like this: ``` Generating public/private rsa key pair. Enter file in which to save the key (/home/user/.ssh/id_rsa): Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /home/user/.ssh/id_rsa Your public key has been saved in /home/user/.ssh/id_rsa.pub The key fingerprint is: SHA256:f5MOGnhzYfC0ZCHvbSXXiRiNVYETjxpHcXD5xSojx+M user@mac-book-pro The key's randomart image is: +---[RSA 4096]----+ | . ..+***o| | o o++*o+| | . =+.++++| | B.oX+. .| | S *=o+ | | . o oE. | | . + + + | | . = + . | | . . | +----[SHA256]-----+ ``` One can check how many bits an SSH key has with : ``` $ ssh-keygen -l -f /home/user/.ssh/id_rsa ``` Output should look like this : ``` 4096 SHA256:f5MOGnhzYfC0ZCHvbSXXiRiNVYETjxpHcXD5xSojx+M", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "SSH", "language": "en", "created_at": "2025-07-19T19:22:02.238828"}}
{"text": "user@mac-book-pro (RSA) ``` It shows the key is RSA 4096-bits. `-l` and `-f` parameters usage explanation : ``` -l Show the fingerprint of the key file. -f filename Filename of the key file. ``` Learn more : [How can I tell how many bits my ssh key is? - Superuser](https://superuser.com/a/139311) </b></details> <details> <summary>What is SSH port forwarding?</summary><br><b> </b></details> <a name=\"questions-linux-wildcards\"></a>", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "SSH", "language": "en", "created_at": "2025-07-19T19:22:02.239047"}}
{"text": "<details> <summary>What is Globbing?</summary><br><b> </b></details> <details> <summary>What are wildcards? Can you give an example of how to use them?</summary><br><b> </b></details> <details> <summary>Explain what will <code>ls [XYZ]</code> match</summary><br><b> </b></details> <details> <summary>Explain what will <code>ls [^XYZ]</code> match</summary><br><b> </b></details> <details> <summary>Explain what will <code>ls [0-5]</code> match</summary><br><b> </b></details> <details> <summary>What each of the following matches - ? - *</summary><br><b> * The ? matches any single character * The * matches zero or more characters </b></details> <details> <summary>What do we grep for in each of the following commands?: * <code>grep '[0-9]\\{1,3\\}\\.[0-9]\\{1,3\\}\\.[0-9]\\{1,3\\}\\.[0-9]\\{1,3\\}' some_file</code> * <code>grep -E \"error|failure\" some_file</code> * <code>grep '[0-9]$' some_file</code> </summary><br><b> 1. An IP address 2. The word \"error\" or \"failure\" 3. Lines which end with a number", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "Globbing & Wildcards", "language": "en", "created_at": "2025-07-19T19:22:02.239317"}}
{"text": "</b></details> <details> <summary>Which line numbers will be printed when running `grep '\\baaa\\b'` on the following content: aaa bbb ccc.aaa aaaaaa</summary><br><b> lines 1 and 3. </b></details> <details> <summary>What is the difference single and double quotes?</summary><br><b> </b></details> <details> <summary>What is escaping? What escape character is used for escaping?</summary><br><b> </b></details> <details> <summary>What is an exit code? What exit codes are you familiar with?</summary><br><b> An exit code (or return code) represents the code returned by a child process to its parent process. 0 is an exit code which represents success while anything higher than 1 represents error. Each number has different meaning, based on how the application was developed. I consider this as a good blog post to read more about it: https://shapeshed.com/unix-exit-codes </b></details> <a name=\"questions-linux-boot\"></a>", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "Globbing & Wildcards", "language": "en", "created_at": "2025-07-19T19:22:02.239339"}}
{"text": "<details> <summary>Tell me everything you know about the Linux boot process</summary><br><b> Another way to ask this: what happens from the moment you turned on the server until you get a prompt </b></details> <details> <summary>What is GRUB2?</summary><br><b> </b></details> <details> <summary>What is Secure Boot?</summary><br><b> </b></details> <details> <summary>What can you find in /boot?</summary><br><b> </b></details> <a name=\"questions-linux-disk-fs\"></a>", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "Boot Process", "language": "en", "created_at": "2025-07-19T19:22:02.239398"}}
{"text": "<details> <summary>What's an inode?</summary><br><b> For each file (and directory) in Linux there is an inode, a data structure which stores meta data related to the file like its size, owner, permissions, etc. </b></details> <details> <summary>Which of the following is not included in inode: * Link count * File size * File name * File timestamp</summary><br><b> File name (it's part of the directory file) </b></details> <details> <summary>How to check which disks are currently mounted?</summary><br><b> Run `mount` </b></details> <details> <summary>You run the <code>mount</code> command but you get no output. How would you check what mounts you have on your system?</summary><br><b> `cat /proc/mounts` </b></details> <details> <summary>What is the difference between a soft link and hard link?</summary><br><b> Hard link is the same file, using the same inode. Soft link is a shortcut to another file, using a different inode. </b></details> <details> <summary>True or False? You can create an", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "Disk and Filesystem", "language": "en", "created_at": "2025-07-19T19:22:02.239981"}}
{"text": "hard link for a directory</summary><br><b> False </b></details> <details> <summary>True or False? You can create a soft link between different filesystems</summary><br><b> True </b></details> <details> <summary>True or False? Directories always have by minimum 2 links</summary><br><b> True. </b></details> <details> <summary>What happens when you delete the original file in case of soft link and hard link?</summary><br><b> </b></details> <details> <summary>Can you check what type of filesystem is used in /home?</summary><br><b> There are many answers for this question. One way is running `df -T` </b></details> <details> <summary>What is a swap partition? What is it used for?</summary><br><b> </b></details> <details> <summary>How to create a - new empty file - a file with text (without using text editor) - a file with given size</summary><br><b> * touch new_file.txt * cat > new_file [enter] submit text; ctrl + d to exit insert mode * truncate -s <size> new_file.txt </b></details>", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "Disk and Filesystem", "language": "en", "created_at": "2025-07-19T19:22:02.240010"}}
{"text": "<details> <summary>You are trying to create a new file but you get \"File system is full\". You check with df for free space and you see you used only 20% of the space. What could be the problem?</summary><br><b> </b></details> <details> <summary>How would you check what is the size of a certain directory?</summary><br><b> `du -sh` </b></details> <details> <summary>What is LVM?</summary><br><b> </b></details> <details> <summary>Explain the following in regards to LVM: * PV * VG * LV</summary><br><b> </b></details> <details> <summary>What is NFS? What is it used for?</summary><br><b> </b></details> <details> <summary>What RAID is used for? Can you explain the differences between RAID 0, 1, 5 and 10?</summary><br><b> </b></details> <details> <summary>Describe the process of extending a filesystem disk space</summary><br><b> </b></details> <details> <summary>What is lazy umount?</summary><br><b> </b></details> <details> <summary>What is tmpfs?</summary><br><b> </b></details> <details>", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "Disk and Filesystem", "language": "en", "created_at": "2025-07-19T19:22:02.240030"}}
{"text": "<summary>What is stored in each of the following logs? * /var/log/messages * /var/log/boot.log</summary><br><b> </b></details> <details> <summary>True or False? both /tmp and /var/tmp cleared upon system boot</summary><br><b> False. /tmp is cleared upon system boot while /var/tmp is cleared every a couple of days or not cleared at all (depends on distro). </b></details> <a name=\"questions-linux-performance-analysis\"></a>", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "Disk and Filesystem", "language": "en", "created_at": "2025-07-19T19:22:02.240049"}}
{"text": "<details> <summary>How to check what is the current load average?</summary><br><b> One can use `uptime` or `top` </b></details> <details> <summary>You know how to see the load average, great. but what each part of it means? for example 1.43, 2.34, 2.78</summary><br><b> [This article](http://www.brendangregg.com/blog/2017-08-08/linux-load-averages.html) summarizes the load average topic in a great way </b></details> <details> <summary>How to check process usage?</summary><br><b> pidstat </b></details> <details> <summary>How to check disk I/O?</summary><br><b> `iostat -xz 1` </b></details> <details> <summary>How to check how much free memory a system has? How to check memory consumption by each process?</summary><br><b> You can use the commands <code>top</code> and <code>free</code> </b></details> <details> <summary>How to check TCP stats?</summary><br><b> sar -n TCP,ETCP 1 </b></details> <a name=\"questions-linux-processes\"></a>", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "Performance Analysis", "language": "en", "created_at": "2025-07-19T19:22:02.240181"}}
{"text": "<details> <summary>how to list all the processes running in your system?</summary><br><b> The \"ps\" command can be used to list all the processes running in a system. The \"ps aux\" command provides a detailed list of all the processes, including the ones running in the background. </b></details> <details> <summary>How to run a process in the background and why to do that in the first place?</summary><br><b> You can achieve that by specifying & at the end of the command. As to why, since some commands/processes can take a lot of time to finish execution or run forever, you may want to run them in the background instead of waiting for them to finish before gaining control again in current session. </b></details> <details> <summary>How can you find how much memory a specific process consumes?</summary><br><b> <code> mem() { ps -eo rss,pid,euser,args:100 --sort %mem | grep -v grep | grep -i $@ | awk '{printf $1/1024 \"MB\"; $1=\"\"; print }' } </code>", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "Processes", "language": "en", "created_at": "2025-07-19T19:22:02.242196"}}
{"text": "[Source](https://stackoverflow.com/questions/3853655/in-linux-how-to-tell-how-much-memory-processes-are-using) </b></details> <details> <summary>What signal is used by default when you run 'kill *process id*'?</summary><br><b> <pre> The default signal is SIGTERM (15). This signal kills process gracefully which means it allows it to save current state configuration. </pre> </b></details> <details> <summary>What signals are you familiar with?</summary><br><b> SIGTERM - default signal for terminating a process SIGHUP - common usage is for reloading configuration SIGKILL - a signal which cannot caught or ignored To view all available signals run `kill -l` </b></details> <details> <summary>What <code>kill 0</code> does?</summary><br><b> \"kill 0\" sends a signal to all processes in the current process group. It is used to check if the processes exist or not </b></details> <details> <summary>What <code>kill -0 <PID></code> does?</summary><br><b> \"kill -0\" checks if a process with a given", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "Processes", "language": "en", "created_at": "2025-07-19T19:22:02.242232"}}
{"text": "process ID exists or not. It does not actually send any signal to the process. </b></details> <details> <summary>What is a trap?</summary><br><b> A trap is a mechanism that allows the shell to intercept signals sent to a process and perform a specific action, such as handling errors or cleaning up resources before terminating the process. </b></details> <details> <summary>Every couple of days, a certain process stops running. How can you look into why it's happening?</summary><br><b> One way to investigate why a process stops running is to check the system logs, such as the messages in /var/log/messages or journalctl. Additionally, checking the process's resource usage and system load may provide clues as to what caused the process to stop </b></details> <details> <summary>What happens when you press ctrl + c?</summary><br><b> When you press \"Ctrl+C,\" it sends the SIGINT signal to the foreground process, asking it to terminate gracefully. </b></details> <details> <summary>What is a", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "Processes", "language": "en", "created_at": "2025-07-19T19:22:02.242253"}}
{"text": "Daemon in Linux?</summary><br><b> A background process. Most of these processes are waiting for requests or set of conditions to be met before actually running anything. Some examples: sshd, crond, rpcbind. </b></details> <details> <summary>What are the possible states of a process in Linux?</summary><br><b> <pre> Running (R) Uninterruptible Sleep (D) - The process is waiting for I/O Interruptible Sleep (S) Stopped (T) Dead (x) Zombie (z) </pre> </b></details> <details> <summary>How do you kill a process in D state?</summary><br><b> A process in D state (also known as \"uninterruptible sleep\") cannot be killed using the \"kill\" command. The only way to terminate it is to reboot the system. </b></details> <details> <summary>What is a zombie process?</summary><br><b> A process which has finished to run but has not exited. One reason it happens is when a parent process is programmed incorrectly. Every parent process should execute wait() to get the exit code from the child process which", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "Processes", "language": "en", "created_at": "2025-07-19T19:22:02.242272"}}
{"text": "finished to run. But when the parent isn't checking for the child exit code, the child process can still exists although it finished to run. </b></details> <details> <summary>How to get rid of zombie processes?</summary><br><b> You can't kill a zombie process the regular way with `kill -9` for example as it's already dead. One way to kill zombie process is by sending SIGCHLD to the parent process telling it to terminate its child processes. This might not work if the parent process wasn't programmed properly. The invocation is `kill -s SIGCHLD [parent_pid]` You can also try closing/terminating the parent process. This will make the zombie process a child of init (1) which does periodic cleanups and will at some point clean up the zombie process. </b></details> <details> <summary>How to find all the * Processes executed/owned by a certain user * Process which are Java processes * Zombie Processes </summary><br><b> If you mention at any point ps command with arguments, be familiar with", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "Processes", "language": "en", "created_at": "2025-07-19T19:22:02.242291"}}
{"text": "what these arguments does exactly. </b></details> <details> <summary>What is the init process?</summary><br><b> It is the first process executed by the kernel during the booting of a system. It is a daemon process which runs till the system is shutdown. That is why, it is the parent of all the processes </b></details> <details> <summary>Can you describe how processes are being created?</summary><br><b> </b></details> <details> <summary>How to change the priority of a process? Why would you want to do that?</summary><br><b> To change the priority of a process, you can use the nice command in Linux. The nice command allows you to specify the priority of a process by assigning a priority value ranging from -20 to 19. A higher value of priority means lower priority for the process, and vice versa. You may want to change the priority of a process to adjust the amount of CPU time it is allocated by the system scheduler. For example, if you have a CPU-intensive process running on your system", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "Processes", "language": "en", "created_at": "2025-07-19T19:22:02.242310"}}
{"text": "that is slowing down other processes, you can lower its priority to give more CPU time to other processes. </b></details> <details> <summary>Can you explain how network process/connection is established and how it's terminated?></summary><br></b> When a client process on one system wants to establish a connection with a server process on another system, it first creates a socket using the socket system call. The client then calls the connect system call, passing the address of the server as an argument. This causes a three-way handshake to occur between the client and server, where the two systems exchange information to establish a connection. Once the connection is established, the client and server can exchange data using the read and write system calls. When the connection is no longer needed, the client or server can terminate the connection by calling the close system call on the socket. </b></details> <details> <summary>What <code>strace</code> does? What about", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "Processes", "language": "en", "created_at": "2025-07-19T19:22:02.242329"}}
{"text": "<code>ltrace</code>?</summary><br><b> Strace is a debugging tool that is used to monitor the system calls made by a process. It allows you to trace the execution of a process and see the system calls it makes, as well as the signals it receives. This can be useful for diagnosing issues with a process, such as identifying why it is hanging or crashing. Ltrace, on the other hand, is a similar tool that is used to trace the library calls made by a process. It allows you to see the function calls made by a process to shared libraries, as well as the arguments passed to those functions. This can be useful for diagnosing issues with a process that involve library calls, such as identifying why a particular library is causing a problem. </b></details> <details> <summary>Find all the files which end with '.yml' and replace the number 1 in 2 in each file</summary><br><b> find /some_dir -iname \\*.yml -print0 | xargs -0 -r sed -i \"s/1/2/g\" </b></details> <details> <summary>You run ls and you get", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "Processes", "language": "en", "created_at": "2025-07-19T19:22:02.242355"}}
{"text": "\"/lib/ld-linux-armhf.so.3 no such file or directory\". What is the problem?</summary><br><b> The ls executable is built for an incompatible architecture. </b></details> <details> <summary>How would you split a 50 lines file into 2 files of 25 lines each?</summary><br><b> You can use the <code>split</code> command this way: <code>split -l 25 some_file</code> </b></details> <details> <summary>What is a file descriptor? What file descriptors are you familiar with?</summary><br><b> Kerberos File descriptor, also known as file handler, is a unique number which identifies an open file in the operating system. In Linux (and Unix) the first three file descriptors are: * 0 - the default data stream for input * 1 - the default data stream for output * 2 - the default data stream for output related to errors This is a great article on the topic: https://www.computerhope.com/jargon/f/file-descriptor.htm </b></details> <details> <summary>What is NTP? What is it used for?</summary><br><b>", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "Processes", "language": "en", "created_at": "2025-07-19T19:22:02.242573"}}
{"text": "</b></details> <details> <summary>Explain Kernel OOM</summary><br><b> </b></details> <a name=\"questions-linux-security\"></a>", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "Processes", "language": "en", "created_at": "2025-07-19T19:22:02.242603"}}
{"text": "<details> <summary>What is chroot? In what scenarios would you consider using it?</summary><br><b> </b></details> <details> <summary>What is SELiunx?</summary><br><b> </b></details> <details> <summary>What is Kerberos?</summary><br><b> </b></details> <details> <summary>What is nftables?</summary><br><b> </b></details> <details> <summary>What firewalld daemon is responsible for?</summary><br><b> </b></details> <details> <summary>Do you have experience with hardening servers? Can you describe the process?</summary><br><b> </b></details> <details> <summary>How do you create a private key for a CA (certificate authority)?</summary><br><b> One way is using openssl this way: `openssl genrsa -aes256 -out ca-private-key.pem 4096` </b></details> <details> <summary>How do you create a public key for a CA (certificate authority)?</summary><br><b> `openssl req -new -x509 -days 730 -key [private key file name] -sha256 -out ca.pem` If using the private key from the previous question then the command", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "Security", "language": "en", "created_at": "2025-07-19T19:22:02.242794"}}
{"text": "would be: `openssl req -new -x509 -days 730 -key ca-private-key.pem -sha256 -out ca.pem` </b></details> <details> <summary>Demonstrate one way to encode and decode data in Linux</summary><br><b> Encode: `echo -n \"some password\" | base64` Decode: `echo -n \"allE19remO91\" | base64` </b></details> <a name=\"questions-linux-networking\"></a>", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "Security", "language": "en", "created_at": "2025-07-19T19:22:02.242820"}}
{"text": "<details> <summary>How to list all the interfaces?</summary><br><b> ``` ip link show ``` </b></details> <details> <summary>What is the loopback (lo) interface?</summary><br><b> The loopback interface is a special, virtual network interface that your computer uses to communicate with itself. It is used mainly for diagnostics and troubleshooting, and to connect to servers running on the local machine. </b></details> <details> <summary>What the following commands are used for? * ip addr * ip route * ip link * ping * netstat * traceroute</summary><br><b> </b></details> <details> <summary>What is a network namespace? What is it used for?</summary><br><b> </b></details> <details> <summary>How to check if a certain port is being used?</summary><br><b> One of the following would work: ``` netstat -tnlp | grep <port_number> lsof -i -n -P | grep <port_number> ``` </b></details> <details> <summary>How can you turn your Linux server into a router?</summary><br><b> </b></details> <details>", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "Networking", "language": "en", "created_at": "2025-07-19T19:22:02.243319"}}
{"text": "<summary>What is a virtual IP? In what situation would you use it?</summary><br><b> </b></details> <details> <summary>True or False? The MAC address of an interface is assigned/set by the OS</summary><br><b> False </b></details> <details> <summary>Can you have more than one default gateway in a given system?</summary><br><b> Technically, yes. </b></details> <details> <summary>What is telnet and why is it a bad idea to use it in production? (or at all)</summary><br><b> Telnet is a type of client-server protocol that can be used to open a command line on a remote computer, typically a server. By default, all the data sent and received via telnet is transmitted in clear/plain text, therefore it should not be used as it does not encrypt any data between the client and the server. </b></details> <details> <summary>What is the routing table? How do you view it?</summary><br><b> </b></details> <details> <summary>How can you send an HTTP request from your shell?</summary><br><b> <br> Using nc", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "Networking", "language": "en", "created_at": "2025-07-19T19:22:02.243341"}}
{"text": "is one way<br> </b></details> <details> <summary>What are packet sniffers? Have you used one in the past? If yes, which packet sniffers have you used and for what purpose?</summary><br><b> It is a network utility that analyses and may inject tasks into the data-stream travelling over the targeted network. </b></details> <details> <summary>How to list active connections?</summary><br><b> </b></details> <details> <summary>How to trigger neighbor discovery in IPv6?</summary><br><b> One way would be `ping6 ff02::1` </b></details> <details> <summary>What is network interface bonding and do you know how it's performed in Linux?</summary><br><b> </b></details> <details> <summary>What network bonding modes are there?</summary><br><b> There a couple of modes: * balance-rr: round robing bonding * active-backup: a fault tolerance mode where only one is active * balance-tlb: Adaptive transmit load balancing * balance-alb: Adaptive load balancing </b></details> <details> <summary>What is a bridge?", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "Networking", "language": "en", "created_at": "2025-07-19T19:22:02.243367"}}
{"text": "How it's added in Linux OS?</summary><br><b> </b></details> <a name=\"questions-linux-dns\"></a>", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "Networking", "language": "en", "created_at": "2025-07-19T19:22:02.243385"}}
{"text": "<details> <summary>How to check what is the hostname of the system?</summary><br><b> `cat /etc/hostname` You can also run `hostnamectl` or `hostname` but that might print only a temporary hostname. The one in the file is the permanent one. </b></details> <details> <summary>What the file <code>/etc/resolv.conf</code> is used for? What does it include?</summary><br><b> </b></details> <details> <summary>What commands are you using for performing DNS queries (or troubleshoot DNS related issues)?</summary><br><b> You can specify one or more of the following: * <code>dig</code> * <code>host</code> * <code>nslookup</code> </b></details> <details> <summary>You run <code>dig codingshell.com</code> and get the following result: ``` ANSWER SECTION: codingshell.com. 3515 IN A 185.199.109.153 ``` What is the meaning of the number 3515? </summary><br><b> This is the TTL. When you lookup for an address using a domain/host name, your OS is performing DNS resolution by contacting DNS name servers to", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "DNS", "language": "en", "created_at": "2025-07-19T19:22:02.243866"}}
{"text": "get the IP address of the host/domain you are looking for.<br> When you get a reply, this reply in cached in your OS for a certain period of time. This is period of time is also known as TTL and this is the meaning of 3515 number - it will be cached for 3515 seconds before removed from the cache and during that period of time, you'll get the value from the cache instead of asking DNS name servers for the address again. </b></details> <details> <summary> How can we modify the network connection via `nmcli` command, to use `8.8.8.8` as a DNS server? </summary><br><b> 1. Find the connection name: ``` # nmcli con show NAME UUID TYPE DEVICE System ens5 8126c120-a964-e959-ff98-ac4973344505 ethernet ens5 System eth0 5fb06bd0-0bb0-7ffb-45f1-d6edd65f3e03 ethernet -- ``` Here the connection name is \"System ens5\". Let's say we want to modify settings for this connection. 2. Modify the connection to use 8.8.8.8 as DNS server: ``` # nmcli con mod \"System ens5\" ipv4.dns \"8.8.8.8\" ``` 3. We need to", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "DNS", "language": "en", "created_at": "2025-07-19T19:22:02.243894"}}
{"text": "reactivate the connection for the change to take effect: ``` nmcli con up \"System ens5\" ``` 4. Verify our settings once more: ``` cat /etc/resolv.conf nmcli -f ipv4.dns con show \"System ens5\" ``` </b> </details> <a name=\"questions-linux-packaging\"></a>", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "DNS", "language": "en", "created_at": "2025-07-19T19:22:02.244104"}}
{"text": "<details> <summary>Do you have experience with packaging? (as in building packages) Can you explain how does it works?</summary><br><b> </b></details> <details> <summary>How packages installation/removal is performed on the distribution you are using?</summary><br><b> The answer depends on the distribution being used. In Fedora/CentOS/RHEL/Rocky it can be done with `rpm` or `dnf` commands. In Ubuntu it can be done with the `apt` command. </b></details> <details> <summary>RPM: explain the spec format (what it should and can include)</summary><br><b> </b></details> <details> <summary>How do you list the content of a package without actually installing it?</summary><br><b> </b></details> <details> <summary>How to know to which package a file on the system belongs to? Is it a problem if it doesn't belongs to any package?</summary><br><b> </b></details> <details> <summary>Where repositories are stored? (based on the distribution you are using)</summary><br><b> </b></details> <details>", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "Packaging", "language": "en", "created_at": "2025-07-19T19:22:02.244367"}}
{"text": "<summary>What is an archive? How do you create one in Linux?</summary><br><b> </b></details> <details> <summary>How to extract the content of an archive?</summary><br><b> </b></details> <details> <summary>Why do we need package managers? Why not simply creating archives and publish them?</summary><br><b> Package managers allow you to manage packages lifecycle as in installing, removing and updating the packages.<br> In addition, you can specify in a spec how a certain package will be installed - where to copy the files, which commands to run prior to the installation, post the installation, etc. </b></details> <a name=\"questions-linux-dnf\"></a>", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "Packaging", "language": "en", "created_at": "2025-07-19T19:22:02.244391"}}
{"text": "<details> <summary>What is DNF?</summary><br><b> From the [repo](https://github.com/rpm-software-management/dnf): \"Dandified YUM (DNF) is the next upcoming major version of YUM. It does package management using RPM, libsolv and hawkey libraries.\" Official [docs](https://dnf.readthedocs.io/en/latest/) </b></details> <details> <summary>How to look for a package that provides the command /usr/bin/git? (the package isn't necessarily installed)</summary><br><b> dnf provides /usr/bin/git </b></details> <a name=\"questions-linux-apps-and-services\"></a>", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "DNF", "language": "en", "created_at": "2025-07-19T19:22:02.244443"}}
{"text": "<details> <summary>What can you find in /etc/services?</summary><br><b> </b></details> <details> <summary>How to make sure a Service starts automatically after a reboot or crash?</summary><br><b> Depends on the init system. Systemd: <code> systemctl enable [service_name] </code> System V: <code> update-rc.d [service_name] </code> and add this line <code> id:5678:respawn:/bin/sh /path/to/app </code> to /etc/inittab Upstart: add Upstart init script at /etc/init/service.conf </b></details> <details> <summary>You run <code>ssh 127.0.0.1</code> but it fails with \"connection refused\". What could be the problem?</summary><br><b> 1. SSH server is not installed 2. SSH server is not running </b></details> <details> <summary>How to print the shared libraries required by a certain program? What is it useful for?</summary><br><b> </b></details> <details> <summary>What is CUPS?</summary><br><b> </b></details> <details> <summary>What types of web servers are you familiar with?</summary><br><b> Nginx,", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "Applications and Services", "language": "en", "created_at": "2025-07-19T19:22:02.244585"}}
{"text": "Apache httpd. </b></details> <a name=\"questions-linux-users-and-groups\"></a>", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "Applications and Services", "language": "en", "created_at": "2025-07-19T19:22:02.244605"}}
{"text": "<details> <summary>What is a \"superuser\" (or root user)? How is it different from regular users?</summary><br><b> </b></details> <details> <summary>How do you create users? Where user information is stored?</summary><br> Command to create users is `useradd` Syntax: `useradd [options] Username` There are 2 configuration files, which stores users information 1. `/etc/passwd` - Users information like, username, shell etc is stored in this file 2. `/etc/shadow` - Users password is stored in encrypted format </details> <details> <summary>Which file stores information about groups?</summary><br> `/etc/groups` file stores the group name, group ID, usernames which are in secondary group. </details> <details> <summary>How do you change/set the password of a user?</summary><br> `passwd <username>` is the command to set/change password of a user. </details> <details> <summary>Which file stores users passwords? Is it visible for everyone?</summary><br> `/etc/shadow` file holds the passwords of the", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "Users and Groups", "language": "en", "created_at": "2025-07-19T19:22:02.245382"}}
{"text": "users in encrypted format. NO, it is only visible to the `root` user </details> <details> <summary>Do you know how to create a new user without using adduser/useradd command?</summary><br> YES, we can create new user by manually adding an entry in the `/etc/passwd` file. For example, if we need to create a user called `john`. Step 1: Add an entry to `/etc/passwd` file, so user gets created. `echo \"john:x:2001:2001::/home/john:/bin/bash\" >> /etc/passwd` Step 2: Add an entry to `/etc/group` file, because every user belong to the primary group that has same name as the username. `echo \"john:x:2001:\" >> /etc/group` Step 3: Verify if the user got created `id john` </details> <details> <summary>What information is stored in /etc/passwd? explain each field</summary><br> `/etc/passwd` is a configuration file, which contains users information. Each entry in this file has, 7 fields, `username:password:UID:GID:Comment:home directory:shell` `username` - The name of the user. `password` - This", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "Users and Groups", "language": "en", "created_at": "2025-07-19T19:22:02.245411"}}
{"text": "field is actually a placeholder of the password field. Due to security concerns, this field does not contain the password, just a placeholder (x) to the encrypted password stored in `/etc/shadow` file. `UID` - User ID of the user. `GID` - Group ID `Comment` - This field is to provide description about the user. `home directory` - Abousulte path of the user's home directory. This directory gets created once the user is added. `shell` - This field contains the absolute path of the shell that will be used by the respective user. </details> <details> <summary>How to add a new user to the system without providing him the ability to log-in into the system?</summary><br><b> `adduser user_name --shell=/bin/false --no-create-home` You can also add a user and then edit /etc/passwd. </b></details> <details> <summary>How to switch to another user? How to switch to the root user?</summary><br><b> su command. Use su - to switch to root </b></details> <details> <summary>What is the UID the root user?", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "Users and Groups", "language": "en", "created_at": "2025-07-19T19:22:02.245431"}}
{"text": "What about a regular user?</summary><br> UID of root user is 0 Default values of UID_MIN and UID_MAX in `/etc/login.defs` `UID_MIN` is `1000` `UID_MAX` is `60000` Actually, we can change this value. But UID < 1000 are reserved for system accounts. Therefore, as per the default configuration, for regular user UID starts from `1000`. </details> <details> <summary>What can you do if you lost/forogt the root password?</summary><br><b> Re-install the OS IS NOT the right answer :) </b></details> <details> <summary>What is /etc/skel?</summary><br> `/etc/skel` is a directory, that contains files or directories, so when a new user is created, these files/directories created under `/etc/skel` will be copied to user's home directory. </details> <details> <summary>How to see a list of who logged-in to the system?</summary><br><b> Using the `last` command. </b></details> <details> <summary>Explain what each of the following commands does: * useradd * usermod * whoami * id</summary><br><b> `useradd`", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "Users and Groups", "language": "en", "created_at": "2025-07-19T19:22:02.245450"}}
{"text": "- Command for creating new users `usermod` - Modify the users setting `whoami` - Outputs, the username that we are currently logged in `id` - Prints the </b></details> <details> <summary>You run <code>grep $(whoami) /etc/passwd</code> but the output is empty. What might be a possible reason for that?</summary><br><b> The user you are using isn't defined locally but originates from services like LDAP.<br> You can verify with: `getent passwd` </b></details> <a name=\"questions-linux-hardware\"></a>", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "Users and Groups", "language": "en", "created_at": "2025-07-19T19:22:02.245693"}}
{"text": "<details> <summary>Where can you find information on the processor (like number of CPUs)?</summary><br><b> /proc/cpuinfo You can also use `nproc` for number of processors </b></details> <details> <summary>How can you print information on the BIOS, motherboard, processor and RAM?</summary><br><b> dmidecoode </b></details> <details> <summary>How can you print all the information on connected block devices in your system?</summary><br><b> lsblk </b></details> <details> <summary>True or False? In user space, applications don't have full access to hardware resources</summary><br><b> True. Only in kernel space they have full access to hardware resources. </b></details> <a name=\"questions-linux-namespaces\"></a>", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "Hardware", "language": "en", "created_at": "2025-07-19T19:22:02.245828"}}
{"text": "<details> <summary>What types of namespaces are there in Linux?</summary><br><b> - Process ID namespaces: these namespaces include independent set of process IDs - Mount namespaces: Isolation and control of mountpoints - Network namespaces: Isolates system networking resources such as routing table, interfaces, ARP table, etc. - UTS namespaces: Isolate host and domains - IPC namespaces: Isolates interprocess communications - User namespaces: Isolate user and group IDs - Time namespaces: Isolates time machine </b></details> <details> <summary>True or False? In every PID (Process ID) namespace the first process assigned with the process id number 1</summary><br><b> True. Inside the namespace it's PID 1 while to the parent namespace the PID is a different one. </b></details> <details> <summary>True or False? In a child PID namespace all processes are aware of parent PID namespace and processes and the parent PID namespace has no visibility of child PID namespace processes</summary><br><b>", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "Namespaces", "language": "en", "created_at": "2025-07-19T19:22:02.246220"}}
{"text": "False. The opposite is true. Parent PID namespace is aware and has visibility of processes in child PID namespace and child PID namespace has no visibility as to what is going on in the parent PID namespace. </b></details> <details> <summary>True or False? By default, when creating two separate network namespaces, a ping from one namespace to another will work fine</summary><br><b> False. Network namespace has its own interfaces and routing table. There is no way (without creating a bridge for example) for one network namespace to reach another. </b></details> <details> <summary>True or False? With UTS namespaces, processes may appear as if they run on different hosts and domains while running on the same host</summary><br><b> True </b></details> <details> <summary>True or False? It's not possible to have a root user with ID 0 in child user namespaces</summary><br><b> False. In every child user namespace, it's possible to have a separate root user with uid of 0. </b></details>", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "Namespaces", "language": "en", "created_at": "2025-07-19T19:22:02.246242"}}
{"text": "<details> <summary>What time namespaces are used for?</summary><br><b> In time namespaces processes can use different system time. </b></details> <a name=\"questions-linux-virtualization\"></a>", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "Namespaces", "language": "en", "created_at": "2025-07-19T19:22:02.246261"}}
{"text": "<details> <summary>What virtualization solutions are available for Linux?</summary><br><b> * [KVM](https://www.linux-kvm.org/page/Main_Page) * [XEN](http://www.xen.org/) * [VirtualBox](https://www.virtualbox.org/) * [Linux-VServer](http://linux-vserver.org/Welcome_to_Linux-VServer.org) * [User-mode Linux](http://user-mode-linux.sourceforge.net/) * ... </b></details> <details> <summary>What is KVM?</summary><br><b> Is an open source virtualization technology used to operate on x86 hardware. From the official [docs](https://www.linux-kvm.org/page/Main_Page) Recommended read: * [Red Hat Article - What is KVM?](https://www.redhat.com/en/topics/virtualization/what-is-KVM) </b></details> <details> <summary>What is Libvirt?</summary><br><b> It's an open source collection of software used to manage virtual machines. It can be used with: KVM, Xen, LXC and others. It's also called Libvirt Virtualization API. From the official [docs](https://libvirt.org/) Hypervisor supported", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "Virtualization", "language": "en", "created_at": "2025-07-19T19:22:02.246362"}}
{"text": "[docs](https://libvirt.org/drivers.html) </b></details> <a name=\"questions-linux-awk\"></a>", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "Virtualization", "language": "en", "created_at": "2025-07-19T19:22:02.246381"}}
{"text": "<details> <summary>What the <code>awk</code> command does? Have you used it? What for?</summary><br><b> From Wikipedia: \"AWK is domain-specific language designed for text processing and typically used as a data extraction and reporting tool\" </b></details> <details> <summary>How to print the 4th column in a file?</summary><br><b> `awk '{print $4}' file` </b></details> <details> <summary>How to print every line that is longer than 79 characters?</summary><br><b> `awk 'length($0) > 79' file` </b></details> <details> <summary>What the <code>lsof</code> command does? Have you used it? What for?</summary><br><b> </b></details> <details> <summary>What is the difference between find and locate?</summary><br><b> </b></details> <details> <summary>How a user process performs a privileged operation, such as reading from the disk?</summary><br><b> Using system calls </b></details> <a name=\"questions-linux-system-calls\"></a>", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "AWK", "language": "en", "created_at": "2025-07-19T19:22:02.246497"}}
{"text": "<details> <summary>What is a system call? What system calls are you familiar with?</summary><br><b> </b></details> <details> <summary>How a program executes a system call?</summary><br><b> - A program executes a trap instruction. The instruction jump into the kernel while raising the privileged level to kernel space. - Once in kernel space, it can perform any privileged operation - Once it's finished, it calls a \"return-from-trap\" instruction which returns to user space while reducing back the privilege level to user space. </b></details> <details> <summary>Explain the fork() system call</summary><br><b> fork() is used for creating a new process. It does so by cloning the calling process but the child process has its own PID and any memory locks, I/O operations and semaphores are not inherited. </b></details> <details> <summary>What is the return value of fork()?</summary><br><b> - On success, the PID of the child process in parent and 0 in child process - On error, -1 in the parent", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "System Calls", "language": "en", "created_at": "2025-07-19T19:22:02.247637"}}
{"text": "</b></details> <details> <summary>Name one reason for fork() to fail</summary><br><b> Not enough memory to create a new process </b></details> <details> <summary>Why do we need the wait() system call?</summary><br><b> wait() is used by a parent process to wait for the child process to finish execution. If wait is not used by a parent process then a child process might become a zombie process. </b></details> <details> <summary>How the kernel notifies the parent process about child process termination?</summary><br><b> The kernel notifies the parent by sending the SIGCHLD to the parent. </b></details> <details> <summary>How the waitpid() is different from wait()?</summary><br><b> The waitpid() is a non-blocking version of the wait() function.<br> It also supports using library routine (e.g. system()) to wait a child process without messing up with other children processes for which the process has not waited. </b></details> <details> <summary>True or False? The wait() system call won't", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "System Calls", "language": "en", "created_at": "2025-07-19T19:22:02.247689"}}
{"text": "return until the child process has run and exited</summary><br><b> True in most cases though there are cases where wait() returns before the child exits. </b></details> <details> <summary>Explain the exec() system call</summary><br><b> It transforms the current running program into another program.<br> Given the name of an executable and some arguments, it loads the code and static data from the specified executable and overwrites its current code segment and current static code data. After initializing its memory space (like stack and heap) the OS runs the program passing any arguments as the argv of that process. </b></details> <details> <summary>True or False? A successful call to exec() never returns</summary><br><b> True<br> Since a successful exec replace the current process, it can't return anything to the process that made the call. </b></details> <details> <summary>What system call is used for listing files?</summary><br><b> </b></details> <details> <summary>What system calls", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "System Calls", "language": "en", "created_at": "2025-07-19T19:22:02.248053"}}
{"text": "are used for creating a new process?</summary><br><b> fork(), exec() and the wait() system call is also included in this workflow. </b></details> <details> <summary>What execve() does?</summary><br><b> Executes a program. The program is passed as a filename (or path) and must be a binary executable or a script. </b></details> <details> <summary>What is the return value of malloc?</summary><br><b> </b></details> <details> <summary>Explain the pipe() system call. What does it used for?</summary><br><b> [Unix pipe implementation](https://toroid.org/unix-pipe-implementation) \"Pipes provide a unidirectional interprocess communication channel. A pipe has a read end and a write end. Data written to the write end of a pipe can be read from the read end of the pipe. A pipe is created using pipe(2), which returns two file descriptors, one referring to the read end of the pipe, the other referring to the write end.\" </b></details> <details> <summary>What happens when you execute <code>ls", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "System Calls", "language": "en", "created_at": "2025-07-19T19:22:02.248088"}}
{"text": "-l</code>?</summary><br><b> * Shell reads the input using getline() which reads the input file stream and stores into a buffer as a string * The buffer is broken down into tokens and stored in an array this way: {\"ls\", \"-l\", \"NULL\"} * Shell checks if an expansion is required (in case of ls *.c) * Once the program in memory, its execution starts. First by calling readdir() Notes: * getline() originates in GNU C library and used to read lines from input stream and stores those lines in the buffer </b></details> <details> <summary>What happens when you execute <code>ls -l *.log</code>?</summary><br><b> </b></details> <details> <summary>What readdir() system call does?</summary><br><b> </b></details> <details> <summary>What exactly the command <code>alias x=y</code> does?</summary><br><b> </b></details> <details> <summary>Why running a new program is done using the fork() and exec() system calls? why a different API wasn't developed where there is one call to run a new", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "System Calls", "language": "en", "created_at": "2025-07-19T19:22:02.248109"}}
{"text": "program?</summary><br><b> This way provides a lot of flexibility. It allows the shell for example, to run code after the call to fork() but before the call to exec(). Such code can be used to alter the environment of the program it about to run. </b></details> <details> <summary>Describe shortly what happens when you execute a command in the shell</summary><br><b> The shell figures out, using the PATH variable, where the executable of the command resides in the filesystem. It then calls fork() to create a new child process for running the command. Once the fork was executed successfully, it calls a variant of exec() to execute the command and finally, waits the command to finish using wait(). When the child completes, the shell returns from wait() and prints out the prompt again. </b></details> <a name=\"questions-linux-fs-files\"></a>", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "System Calls", "language": "en", "created_at": "2025-07-19T19:22:02.248129"}}
{"text": "<details> <summary>How to create a file of a certain size?</summary><br><b> There are a couple of ways to do that: * dd if=/dev/urandom of=new_file.txt bs=2MB count=1 * truncate -s 2M new_file.txt * fallocate -l 2097152 new_file.txt </b></details> <details> <summary>What does the following block do?: ``` open(\"/my/file\") = 5 read(5, \"file content\") ``` </summary><br><b> These system calls are reading the file <code>/my/file</code> and 5 is the file descriptor number. </b></details> <details> <summary>Describe three different ways to remove a file (or its content)</summary><br><b> </b></details> <details> <summary>What is the difference between a process and a thread?</summary><br><b> </b></details> <details> <summary>What is context switch?</summary><br><b> From [wikipedia](https://en.wikipedia.org/wiki/Context_switch): a context switch is the process of storing the state of a process or thread, so that it can be restored and resume execution at a later point </b></details> <details>", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "Filesystem & Files", "language": "en", "created_at": "2025-07-19T19:22:02.248308"}}
{"text": "<summary>You found there is a server with high CPU load but you didn't find a process with high CPU. How is that possible?</summary><br><b> </b></details> <a name=\"questions-linux-advanced-networking\"></a>", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "Filesystem & Files", "language": "en", "created_at": "2025-07-19T19:22:02.248329"}}
{"text": "<details> <summary>When you run <code>ip a</code> you see there is a device called 'lo'. What is it and why do we need it?</summary><br><b> </b></details> <details> <summary>What the <code>traceroute</code> command does? How does it works?</summary><br><b> Another common way to task this questions is \"what part of the tcp header does traceroute modify?\" </b></details> <details> <summary>What is network bonding? What types are you familiar with?</summary><br><b> </b></details> <details> <summary>How to link two separate network namespaces so you can ping an interface on one namespace from the second one?</summary><br><b> </b></details> <details> <summary>What are cgroups?</summary><br><b> </b></details> <details> <summary>Explain Process Descriptor and Task Structure</summary><br><b> </b></details> <details> <summary>What are the differences between threads and processes?</summary><br><b> </b></details> <details> <summary>Explain Kernel Threads</summary><br><b> </b></details> <details>", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "Advanced Networking", "language": "en", "created_at": "2025-07-19T19:22:02.248669"}}
{"text": "<summary>What happens when socket system call is used?</summary><br><b> This is a good article about the topic: https://ops.tips/blog/how-linux-creates-sockets </b></details> <details> <summary>You executed a script and while still running, it got accidentally removed. Is it possible to restore the script while it's still running?</summary><br><b> It is possible to restore a script while it's still running if it has been accidentally removed. The running script process still has the code in memory. You can use the /proc filesystem to retrieve the content of the running script. 1.Find the Process ID by running ``` ps aux | grep yourscriptname.sh ``` Replace yourscriptname.sh with your script name. 2.Once you have the PID, you can access the script's memory through the /proc filesystem. The script will be available at /proc/<PID>/fd/, where <PID> is the process ID of the running script. Typically, the script's file descriptor is 0 or 1. You can copy the script content to a new file using", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "Advanced Networking", "language": "en", "created_at": "2025-07-19T19:22:02.248689"}}
{"text": "the cp command: ``` cp /proc/<PID>/fd/0 /path_to_restore_your_file/yourscriptname.sh ``` Replace <PID> with the actual PID of the script and /path_to_restore_your_file/yourscriptname.sh with the path where you want to restore the script. </b></details> <a name=\"questions-linux-memory\"></a>", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "Advanced Networking", "language": "en", "created_at": "2025-07-19T19:22:02.248851"}}
{"text": "<details> <summary>What is the difference between MemFree and MemAvailable in /proc/meminfo?</summary><br><b> MemFree - The amount of unused physical RAM in your system MemAvailable - The amount of available memory for new workloads (without pushing system to use swap) based on MemFree, Active(file), Inactive(file), and SReclaimable. </b></details> <details> <summary>What is the difference between paging and swapping?</summary><br><b> </b></details> <details> <summary>Explain what is OOM killer</summary><br><b> </b></details> <a name=\"questions-linux-distributions\"></a>", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "Memory", "language": "en", "created_at": "2025-07-19T19:22:02.248923"}}
{"text": "<details> <summary>What is a Linux distribution?</summary><br><b> </b></details> <details> <summary>What Linux distributions are you familiar with?</summary><br><b> </b></details> <details> <summary>What are the components of a Linux distribution?</summary><br><b> * Kernel * Utilities * Services * Software/Packages Management </b></details> <a name=\"questions-linux-sed\"></a>", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "Distributions", "language": "en", "created_at": "2025-07-19T19:22:02.248962"}}
{"text": "<details> <summary>Using sed, extract the date from the following line: <code>201.7.19.90 - - [05/Jun/1985:13:42:99 +0000] \"GET /site HTTP/1.1\" 200 32421</code></summary><br><b> `echo $line | sed 's/.*\\[//g;s/].*//g;s/:.*//g'` </b></details> <a name=\"questions-linux-misc\"></a>", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "Sed", "language": "en", "created_at": "2025-07-19T19:22:02.248990"}}
{"text": "<details> <summary>What is a Linux distribution?</summary><br><b> * A collection of packages - kernel, GNU, third party apps, ... * Sometimes distributions store some information on the distribution in `/etc/*-release` file * For example for Red Hat distribution it will be `/etc/redhat-release` and for Amazon it will be `/etc/os-release` * `lsb_release` is a common command you can use in multiple different distributions </b></details> <details> <summary>Name 5 commands which are two letters long</summary><br><b> ls, wc, dd, df, du, ps, ip, cp, cd ... </b></details> <details> <summary>What ways are there for creating a new empty file?</summary><br><b> * touch new_file * echo \"\" > new_file </b></details> <details> <summary>How `cd -` works? How does it knows the previous location?</summary><br><b> $OLDPWD </b></details> <details> <summary>List three ways to print all the files in the current directory</summary><br><b> * ls * find . * echo * </b></details> <details> <summary>How to count", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "Misc", "language": "en", "created_at": "2025-07-19T19:22:02.249578"}}
{"text": "the number of lines in a file? What about words?</summary><br><b> For these we can use `wc` command. 1. To count the number of lines in file ```wc -l``` 2. To count the number of words in file ```wc -w``` </b></details> <details> <summary>You define x=2 in /etc/bashrc and x=6 ~/.bashrc you then login to the system. What would be the value of x?</summary><br><b> </b></details> <details> <summary>What is the difference between man and info?</summary><br><b> A good answer can be found [here](https://askubuntu.com/questions/9325/what-is-the-difference-between-man-and-info-documentation) </b></details> <details> <summary>Explain \"environment variables\". How do you list all environment variables?</summary><br><b> </b></details> <details> <summary>What is a TTY device?</summary><br><b> </b></details> <details> <summary>How to create your own environment variables?</summary><br><b> `X=2` for example. But this will persist to new shells. To have it in new shells as well, use `export X=2`", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "Misc", "language": "en", "created_at": "2025-07-19T19:22:02.249600"}}
{"text": "</b></details> <details> <summary>What a double dash (--) mean?</summary><br><b> It's used in commands to mark the end of commands options. One common example is when used with git to discard local changes: `git checkout -- some_file` </b></details> <details> <summary>Wildcards are implemented on user or kernel space?</summary><br><b> </b></details> <details> <summary>If I plug a new device into a Linux machine, where on the system, a new device entry/file will be created?</summary><br><b> /dev </b></details> <details> <summary>Why there are different sections in man? What is the difference between the sections?</summary><br><b> </b></details> <details> <summary>What is User-mode Linux?</summary><br><b> In Linux, user mode is a restricted operating mode in which a user's application or process runs. User mode is a non-privileged mode that prevents user-level processes from accessing sensitive system resources directly. In user mode, an application can only access hardware resources", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "Misc", "language": "en", "created_at": "2025-07-19T19:22:02.249619"}}
{"text": "indirectly, by calling system services or functions provided by the operating system. This ensures that the system's security and stability are maintained by preventing user processes from interfering with or damaging system resources. Additionally, user mode also provides memory protection to prevent applications from accessing unauthorized memory locations. This is done by assigning each process its own virtual memory space, which is isolated from other processes. In contrast to user mode, kernel mode is a privileged operating mode in which the operating system's kernel has full access to system resources, and can perform low-level operations, such as accessing hardware devices and managing system resources directly. </b></details> <details> <summary>Under which license Linux is distributed? </summary><br><b> GPL v2 </b></details>", "metadata": {"source_file": "learning-materials/topics/linux/README.md", "section": "Misc", "language": "en", "created_at": "2025-07-19T19:22:02.249637"}}
{"text": "In this directory you have a file with list of IP addresses called `ip_list`. Using the file, determine which IP address is the most recurring (listed the most times).", "metadata": {"source_file": "learning-materials/topics/linux/exercises/uniqe_count/README.md", "section": "Objectives", "language": "en", "created_at": "2025-07-19T19:22:02.249962"}}
{"text": "`sort ip_list | cut -d' ' -f1 | uniq -c | sort -n | tail -1`", "metadata": {"source_file": "learning-materials/topics/linux/exercises/uniqe_count/solution.md", "section": "Solution", "language": "en", "created_at": "2025-07-19T19:22:02.250065"}}
{"text": "1. Create a file called `x` 2. Create a directory called `content` 3. Move `x` file to the `content` directory 4. Create a file inside the `content` directory called `y` 5. Create the following directory structure in `content` directory: `dir1/dir2/dir3` 6. Remove the content directory", "metadata": {"source_file": "learning-materials/topics/linux/exercises/create_remove/README.md", "section": "Objectives", "language": "en", "created_at": "2025-07-19T19:22:02.250184"}}
{"text": "``` touch x mkdir content mv x content touch content/y mkdir -p content/dir1/dir2/dir3 rm -rf content ```", "metadata": {"source_file": "learning-materials/topics/linux/exercises/create_remove/solution.md", "section": "Solution", "language": "en", "created_at": "2025-07-19T19:22:02.250288"}}
{"text": "1. Create an empty file called `x` in `/tmp` 2. Copy the `x` file you created to your home directory 3. Create a copy of `x` file called `y` 4. Create a directory called `files` and move `x` and `y` there 5. Copy the directory \"files\" and name the copy `copy_of_files` 6. Rename `copy_of_files` directory to `files2` 7. Remove `files` and `files2` directories", "metadata": {"source_file": "learning-materials/topics/linux/exercises/copy/README.md", "section": "Objectives", "language": "en", "created_at": "2025-07-19T19:22:02.250428"}}
{"text": "``` touch /tmp/x cp x ~/ cp x y mkdir files mv x files | mv y files cp -r files copy_of_files mv copy_of_files files2 rm -rf files files2 ```", "metadata": {"source_file": "learning-materials/topics/linux/exercises/copy/solution.md", "section": "Solution", "language": "en", "created_at": "2025-07-19T19:22:02.250749"}}
{"text": "1. Linux :)", "metadata": {"source_file": "learning-materials/topics/linux/exercises/navigation/README.md", "section": "Requirements", "language": "en", "created_at": "2025-07-19T19:22:02.250874"}}
{"text": "1. Change directory to `/tmp` 2. Move to parent directory 3. Change directory to home directory 4. Move to parent directory 5. Move again to parent directory 1. Where are you at? Verify with a command 6. Change to last visited directory", "metadata": {"source_file": "learning-materials/topics/linux/exercises/navigation/README.md", "section": "Objectives", "language": "en", "created_at": "2025-07-19T19:22:02.250915"}}
{"text": "``` cd /tmp cd .. cd ~ cd .. cd ..", "metadata": {"source_file": "learning-materials/topics/linux/exercises/navigation/solution.md", "section": "Solution", "language": "en", "created_at": "2025-07-19T19:22:02.251019"}}
{"text": "pwd cd - ```", "metadata": {"source_file": "learning-materials/topics/linux/exercises/navigation/solution.md", "section": "root (/)", "language": "en", "created_at": "2025-07-19T19:22:02.251036"}}
{"text": "<details> <summary>What is an SLI (Service-Level Indicator)?</summary> <b> An SLI is a measurement used to assess the actual performance or reliability of a service. It serves as the basis for defining SLOs. Examples: - Request latency - Processing throughput - Request failures per unit of time Read more: [Google SRE Handbook](https://sre.google/sre-book/table-of-contents/) </b> </details></br> <details> <summary>What is an SLO (Service-Level Objective)?</summary> <b> An SLO is a target value or range of values for a service level that is measured by an SLI Example: 99% across 30 days for a specific collection of SLIs. It's also worthy to note that the SLO also serves as a lower bound, indicating that there is no requirement to be more reliable than necessary because doing so can delay the rollout of new features. Read more: [Google SRE Handbook](https://sre.google/sre-book/table-of-contents/) </b> </details><br> <details> <summary>What is an SLA (Service-Level Agreement)?</summary>", "metadata": {"source_file": "learning-materials/topics/sre/README.md", "section": "SRE Questions", "language": "en", "created_at": "2025-07-19T19:22:02.251608"}}
{"text": "<b> AN SLA is a formal agreement between a service provider and customers, specifying the expected service quality and consequences for not meeting it. SRE doesn't typically get involved in constructing SLAs, because SLAs are closely tied to business and product decisions Read more: [Google SRE Handbook](https://sre.google/sre-book/table-of-contents/) </b> </details><br> <details> <summary>What is an Error Budget?</summary> <b> An Error Budget represents the acceptable amount of downtime or errors a service can experience while still meeting its SLO. An error budget is 1 minus the SLO of the service. A 99.9% SLO service has a 0.1% error budget. If our service receives 1,000,000 requests in four weeks, a 99.9% availability SLO gives us a budget of 1,000 errors over that period. The error budget is a mechanism for balancing innovation and stability. If the SRE cannot enforce the error budget, the whole system breaks down. Read more: [Google SRE", "metadata": {"source_file": "learning-materials/topics/sre/README.md", "section": "SRE Questions", "language": "en", "created_at": "2025-07-19T19:22:02.251642"}}
{"text": "Handbook](https://sre.google/sre-book/table-of-contents/) </b> </details></br> <details> <summary>What is Toil?</summary> <b> Toil is the kind of work that tends to be manual, repetitive, automatable, tactical, devoid of enduring value, and that scales linearly as a service grows. If you can be automate a task, you should probably automate the task. Automation significantly reduces Toil. Investing in automation results in valuable work with lasting impact, offering scalability potential with minimal adjustments as your system expands. Read more: [Google SRE Handbook](https://sre.google/sre-book/table-of-contents/) </b> </details>", "metadata": {"source_file": "learning-materials/topics/sre/README.md", "section": "SRE Questions", "language": "en", "created_at": "2025-07-19T19:22:02.251662"}}
{"text": "|Name|Topic|Objective & Instructions|Solution|Comments| |--------|--------|------|----|----| | Projects 101 | Projects | [Exercise](projects_101.md) | [Solution](solutions/projects_101.md) | My First Application | Applications | [Exercise](my_first_app.md) | [Solution](solutions/my_first_app.md)", "metadata": {"source_file": "learning-materials/topics/openshift/README.md", "section": "OpenShift Exercises", "language": "en", "created_at": "2025-07-19T19:22:02.251966"}}
{"text": "* [OpenShift 101](#Openshift-101) * [OpenShift Architecture](#Openshift-architecture) * [OpenShift Hands-On Basics](#Openshift-hands-on-basics) * [OpenShift Projects](#Openshift-projects) <a name=\"openshift-101\"></a>", "metadata": {"source_file": "learning-materials/topics/openshift/README.md", "section": "OpenShift Self Assessment", "language": "en", "created_at": "2025-07-19T19:22:02.251996"}}
{"text": "<details> <summary>What is OpenShift?</summary><br><b> OpenShift is a container orchestration platform based on Kubernetes.<br> It can be used for deploying applications while having minimal management overhead. </b></details> <details> <summary>How OpenShift is related to Kubernetes?</summary><br><b> OpenShift is build on top of Kubernetes while defining its own custom resources in addition to the built-in resources. </b></details> <details> <summary>True or False? OpenShift is a IaaS (infrastructure as a service) solution</summary><br><b> False. OpenShift is a PaaS (platform as a service) solution. </b></details> <details> <summary>True or False? OpenShift CLI supports everything kubectl supports, along with additional functionality</summary><br><b> True </b></details> <details> <summary>What are some of OpenShift added features on top of Kubernetes?</summary><br><b> - UI: OpenShift provides unified UI out-of-the-box - Routes: Simple procedure for exposing services - Developer", "metadata": {"source_file": "learning-materials/topics/openshift/README.md", "section": "OpenShift 101", "language": "en", "created_at": "2025-07-19T19:22:02.252164"}}
{"text": "Workflow Support: built-in CI/CD (openshift pipelines), built-in container registry and tooling for building artifacts from source to container images </b></details> <details> <summary>True or False? To run containers on OpenShift, you have to own root privileges</summary><br><b> False. OpenShift supports rootless containers by default. </b></details> <a name=\"openshift-architecture\"></a>", "metadata": {"source_file": "learning-materials/topics/openshift/README.md", "section": "OpenShift 101", "language": "en", "created_at": "2025-07-19T19:22:02.252184"}}
{"text": "<details> <summary>What types of nodes OpenShift has?</summary><br><b> - Workers: Where the end-user applications are running - Masters: Responsible for managing the cluster </b></details> <details> <summary>Which component responsible for determining pod placement?</summary><br><b> The Scheduler. </b></details> <details> <summary>What else the scheduler responsible for except pod placement?</summary><br><b> Application high availability by spreading pod replicas between worker nodes </b></details> <a name=\"openshift-hands-on-basics\"></a>", "metadata": {"source_file": "learning-materials/topics/openshift/README.md", "section": "OpenShift - Architecture", "language": "en", "created_at": "2025-07-19T19:22:02.252235"}}
{"text": "<details> <summary>OpenShift supports many resources. How to get a list of all these resources?</summary><br><b> `oc api-resources` </b></details> <details> <summary>Explain OpenShift CLIs like <code>oc</code> and <code>odo</code></summary><br><b> oc is used for creating applications, but also for administrating OpenShift cluster<br> odo is used solely for managing applications on OpenShift (mainly from developers' perspective) and has nothing to do with administrating the cluster </b></details> <a name=\"openshift-projects\"></a>", "metadata": {"source_file": "learning-materials/topics/openshift/README.md", "section": "OpenShift - Hands-On Basics", "language": "en", "created_at": "2025-07-19T19:22:02.252293"}}
{"text": "<details> <summary>What is a project in OpenShift?</summary><br><b> A project in OpenShift is a Kubernetes namespace with annotations.<br> In simpler words, think about it as an isolated environment for users to manage and organize their resources (like Pods, Deployments, Service, etc.). </b></details> <details> <summary>How to list all projects? What the \"STATUS\" column means in projects list output?</summary><br><b> `oc get projects` will list all projects. The \"STATUS\" column can be used to see which projects are currently active. </b></details> <details> <summary>You have a new team member and you would like to assign to him the \"admin\" role on your project in OpenShift. How to achieve that?</summary><br><b> `oc adm policy add-role-to-user <role> <user> -n <project>` </b></details>", "metadata": {"source_file": "learning-materials/topics/openshift/README.md", "section": "OpenShift - Projects", "language": "en", "created_at": "2025-07-19T19:22:02.252562"}}
{"text": "<details> <summary>How to create a MySQL application using an image from Docker Hub?</summary><br><b> `oc new-app mysql` </b></details>", "metadata": {"source_file": "learning-materials/topics/openshift/README.md", "section": "OpenShift - Applications", "language": "en", "created_at": "2025-07-19T19:22:02.252600"}}
{"text": "<details> <summary>What is an image stream?</summary><br><b> </b></details> <details> <summary>What would be the best way to run and manage multiple OpenShift environments?</summary><br><b> Federation </b></details>", "metadata": {"source_file": "learning-materials/topics/openshift/README.md", "section": "OpenShift - Images", "language": "en", "created_at": "2025-07-19T19:22:02.252623"}}
{"text": "<details> <summary>What is OpenShift Federation?</summary><br><b> Management and deployment of services and workloads across multiple independent clusters from a single API </b></details> <details> <summary>Explain the following in regards to Federation: * Multi Cluster * Federated Cluster * Host Cluster * Member Cluster </summary><br><b> * Multi Cluster - Multiple clusters deployed independently, not being aware of each other * Federated Cluster - Multiple clusters managed by the OpenShift Federation Control Plane * Host Cluster - The cluster that runs the Federation Control Plane * Member Cluster - Cluster that is part of the Federated Cluster and connected to Federation Control Plane </b></details>", "metadata": {"source_file": "learning-materials/topics/openshift/README.md", "section": "OpenShift - Federation", "language": "en", "created_at": "2025-07-19T19:22:02.252753"}}
{"text": "<details> <summary>What is a storage device? What storage devices are there?</summary><br><b> * Hard Disks * SSD * USB * Magnetic Tape </b></details> <details> <summary>What is Random Seek Time?</summary><br><b> The time it takes for a disk to reach the place where the data is located and read a single block/sector. Bones question: What is the random seek time in SSD and Magnetic Disk? Answer: Magnetic is about 10ms and SSD is somewhere between 0.08 and 0.16ms </b></details>", "metadata": {"source_file": "learning-materials/topics/openshift/README.md", "section": "OpenShift - Storage", "language": "en", "created_at": "2025-07-19T19:22:02.252836"}}
{"text": "<details> <summary>What happens when a pod fails or exit due to container crash</summary><br><b> Master node automatically restarts the pod unless it fails too often. </b></details> <details> <summary>What happens when a pod fails too often?</summary><br><b> It's marked as bad by the master node and temporary not restarted anymore. </b></details> <details> <summary>How to find out on which node a certain pod is running?</summary><br><b> `oc get po -o wide` </b></details>", "metadata": {"source_file": "learning-materials/topics/openshift/README.md", "section": "OpenShift - Pods", "language": "en", "created_at": "2025-07-19T19:22:02.252899"}}
{"text": "<details> <summary>Explain Services and their benefits</summary><br><b> - Services in OpenShift define access policy to one or more set of pods.<br> - They are connecting applications together by enabling communication between them - They provide permanent internal IP addresses and hostnames for applications - They are able to provide basic internal load balancing </b></details>", "metadata": {"source_file": "learning-materials/topics/openshift/README.md", "section": "OpenShift - Services", "language": "en", "created_at": "2025-07-19T19:22:02.252946"}}
{"text": "<details> <summary>Explain labels. What are they? When do you use them?</summary><br><b> - Labels are used to group or select API objects - They are simple key-value pairs and can be included in metadata of some objects - A common use case: group pods, services, deployments, ... all related to a certain application </b></details>", "metadata": {"source_file": "learning-materials/topics/openshift/README.md", "section": "OpenShift - Labels", "language": "en", "created_at": "2025-07-19T19:22:02.252992"}}
{"text": "<details> <summary>How to list Service Accounts?</summary><br><b> `oc get serviceaccounts` </b></details>", "metadata": {"source_file": "learning-materials/topics/openshift/README.md", "section": "OpenShift - Service Accounts", "language": "en", "created_at": "2025-07-19T19:22:02.253010"}}
{"text": "<details> <summary>What is a Route?</summary><br><b> A route is exposing a service by giving it hostname which is externally reachable </b></details> <details> <summary>What Route is consists of?</summary><br><b> - name - service selector - (optional) security configuration </b></details> <details> <summary>True or False? Router container can run only on the Master node</summary><br><b> False. It can run on any node. </b></details> <details> <summary>Given an example of how a router is used</summary><br><b> 1. Client is using an address of application running on OpenShift 2. DNS resolves to host running the router 3. Router checks whether route exists 4. Router proxies the request to the internal pod </b></details>", "metadata": {"source_file": "learning-materials/topics/openshift/README.md", "section": "OpenShift - Networking", "language": "en", "created_at": "2025-07-19T19:22:02.253112"}}
{"text": "<details> <summary>What are \"Security Context Constraints\"?</summary><br><b> From [OpenShift Docs](https://docs.openshift.com/container-platform/4.7/authentication/managing-security-context-constraints.html): \"Similar to the way that RBAC resources control user access, administrators can use security context constraints (SCCs) to control permissions for pods\". </b></details> <details> <summary>How to add the ability for the user `user1` to view the project `wonderland` assuming you are authorized to do so</summary><br><b> oc adm policy add-role-to-user view user1 -n wonderland </b></details> <details> <summary>How to check what is the current context?</summary><br><b> `oc whoami --show-context` </b></details>", "metadata": {"source_file": "learning-materials/topics/openshift/README.md", "section": "OpenShift - Security", "language": "en", "created_at": "2025-07-19T19:22:02.253188"}}
{"text": "<details> <summary>What is OpenShift Serverless?</summary><br><b> - In general 'serverless' is a cloud computing model where scaling and provisioning is taken care for application developers, so they can focus on the development aspect rather infrastructure related tasks - OpenShift Serverless allows you to dynamically scale your applications and provides the ability to build event-driven applications, whether the sources are on Kubernetes, the cloud or on-premise solutions - OpenShift Serverless is based on the Knative project. </b></details> <details> <summary>What are some of the event sources you can use with OpenShift Serverless?</summary><br><b> * Kafka * Kubernetes APIs * AWS Kinesis * AWS SQS * JIRA * Slack More are supported and provided with OpenShift. </b></details> <details> <summary>Explain serverless functions</summary><br><b> </b></details> <details> <summary>What is the difference between Serverless Containers and Serverless functions?</summary><br><b> </b></details>", "metadata": {"source_file": "learning-materials/topics/openshift/README.md", "section": "OpenShift - Serverless", "language": "en", "created_at": "2025-07-19T19:22:02.253367"}}
{"text": "<details> <summary>What is Replication Controller?</summary><br><b> Replication Controller responsible for ensuring the specified number of pods is running at all times.<br> If more pods are running than needed -> it deletes some of them<br> If not enough pods are running -> it creates more </b></details>", "metadata": {"source_file": "learning-materials/topics/openshift/README.md", "section": "OpenShift - Misc", "language": "en", "created_at": "2025-07-19T19:22:02.253532"}}
{"text": "In a newly deployed cluster (preferably) perform the following: 1. Log in to the OpenShift cluster 2. List all the projects 3. Create a new project called 'neverland' 4. Check the overview status of the current project", "metadata": {"source_file": "learning-materials/topics/openshift/projects_101.md", "section": "Objectives", "language": "en", "created_at": "2025-07-19T19:22:02.253655"}}
{"text": "In a newly deployed cluster (preferably) perform the following: 1. Login to the OpenShift cluster 2. List all the projects 3. Create a new project called 'neverland' 4. Check the overview status of the current project", "metadata": {"source_file": "learning-materials/topics/openshift/solutions/projects_101.md", "section": "Objectives", "language": "en", "created_at": "2025-07-19T19:22:02.253831"}}
{"text": "``` oc login -u YOUR_USER -p YOUR_PASSWORD_OR_TOKEN oc get projects # Empty output in new cluster oc new-project neverland oc status ```", "metadata": {"source_file": "learning-materials/topics/openshift/solutions/projects_101.md", "section": "Solution", "language": "en", "created_at": "2025-07-19T19:22:02.253863"}}
{"text": "1. Create a MySQL application 2. Describe which OpenShift objects were created", "metadata": {"source_file": "learning-materials/topics/openshift/solutions/my_first_app.md", "section": "Objectives", "language": "en", "created_at": "2025-07-19T19:22:02.253930"}}
{"text": "1. `oc new-app mysql` 2. The following objects were created: * ImageStream:", "metadata": {"source_file": "learning-materials/topics/openshift/solutions/my_first_app.md", "section": "Solution", "language": "en", "created_at": "2025-07-19T19:22:02.253950"}}
{"text": "<details> <summary>Describe shortly what is Zuul</summary><br><b> From [Zuul's docs](https://zuul-ci.org/docs/zuul/about.html): \"Zuul is a Project Gating System. That’s like a CI or CD system, but the focus is on testing the future state of code repositories... Zuul itself is a service which listens to events from various code review systems, executes jobs based on those events, and reports the results back to the code review system.\" </b></details> <details> <summary>What is Nodepool and how is it related to Zuul?</summary><br><b> \"Nodepool is a system for managing test node resources. It supports launching single-use test nodes from cloud providers as well as managing access to pre-defined pre-existing nodes.\" \"Zuul uses a separate component called Nodepool to provide the resources to run jobs. Nodepool works with several cloud providers as well as statically defined nodes (again, simultaneously).\" </b></details> <details> <summary>What is a Pipeline in Zuul?</summary><br><b> A", "metadata": {"source_file": "learning-materials/topics/zuul/README.md", "section": "Basics", "language": "en", "created_at": "2025-07-19T19:22:02.254457"}}
{"text": "pipeline in Zuul is a workflow. This workflow can be executed based on different events - when a change is submitted to a project, when it's merged, etc.<br> The pipeline itself can be applied on one or more different projects (= repositories in hosted or private source control) </b></details> <details> <summary>What is a project in Zuul?</summary><br><b> </b></details>", "metadata": {"source_file": "learning-materials/topics/zuul/README.md", "section": "Basics", "language": "en", "created_at": "2025-07-19T19:22:02.254488"}}
{"text": "- [Google Cloud Platform](#google-cloud-platform) - [Exercises](#exercises) - [Account Setup](#account-setup) - [Compute Engine](#compute-engine) - [Questions](#questions) - [Global Infrastructure](#global-infrastructure) - [gcloud](#gcloud) - [Resource Hierarchy](#resource-hierarchy) - [IAM and Roles](#iam-and-roles) - [Labels and Tags](#labels-and-tags) - [gcloud](#gcloud-1) - [Compute Engine](#compute-engine-1) - [gcloud](#gcloud-2) - [Other](#other) - [Google Kubernetes Engine (GKE)](#google-kubernetes-engine-gke) - [Anthos](#anthos)", "metadata": {"source_file": "learning-materials/topics/gcp/README.md", "section": "Google Cloud Platform", "language": "en", "created_at": "2025-07-19T19:22:02.254904"}}
{"text": "|Name|Topic|Objective & Instructions|Solution|Comments| |--------|--------|------|----|----| | Create a project | Organization | [Exercise](exercises/create_project/exercise.md) | [Solution](exercises/create_project/solution.md) | | | Assign roles | IAM | [Exercise](exercises/assign_roles/exercise.md) | [Solution](exercises/assign_roles/solution.md) | |", "metadata": {"source_file": "learning-materials/topics/gcp/README.md", "section": "Account Setup", "language": "en", "created_at": "2025-07-19T19:22:02.254949"}}
{"text": "|Name|Topic|Objective & Instructions|Solution|Comments| |--------|--------|------|----|----| | Create an instance | Compute, Labels | [Exercise](exercises/instance_101/exercise.md) | [Solution](exercises/instance_101/solution.md) | |", "metadata": {"source_file": "learning-materials/topics/gcp/README.md", "section": "Compute Engine", "language": "en", "created_at": "2025-07-19T19:22:02.254972"}}
{"text": "<details> <summary>Explain each of the following * Zone * Region </summary><br><b> GCP regions are data centers hosted across different geographical locations worldwide.<br> Within each region, there are multiple isolated locations known as Zones. Each zone is one or more data-centers with redundant network and connectivity and power supply. Multiple zones ensure high availability in case one of them goes down </b></details> <details> <summary>True or False? Each GCP region is designed to be completely isolated from the other GCP regions </summary><br><b> True. </b></details> <details> <summary>What considerations to take when choosing an GCP region for running a new application?</summary><br><b> * Services Availability: not all service (and all their features) are available in every region * Reduced latency: deploy application in a region that is close to customers * Compliance: some countries have more strict rules and requirements such as making sure the data stays within the", "metadata": {"source_file": "learning-materials/topics/gcp/README.md", "section": "Global Infrastructure", "language": "en", "created_at": "2025-07-19T19:22:02.255209"}}
{"text": "borders of the country or the region. In that case, only specific region can be used for running the application * Pricing: the pricing might not be consistent across regions so, the price for the same service in different regions might be different. </b></details> <details> <summary>True or False? All GCP services are available in all regions zones</summary><br><b> False. You can see [here](https://cloud.google.com/about/locations) which products/services available in each region. </b></details>", "metadata": {"source_file": "learning-materials/topics/gcp/README.md", "section": "Global Infrastructure", "language": "en", "created_at": "2025-07-19T19:22:02.255230"}}
{"text": "<details> <summary>How to list all regions?</summary><br><b> `gcloud compute regions list` </b></details>", "metadata": {"source_file": "learning-materials/topics/gcp/README.md", "section": "gcloud", "language": "en", "created_at": "2025-07-19T19:22:02.255250"}}
{"text": "<details> <summary>Explain resources hierarchy in GCP</summary><br><b> Organization Folder Project Resources * Organizations - Company * Folder - usually for departments, teams, products, etc. * Project - can be different projects or same project but different environments (dev, staging, production) * Resources - actual GCP services (Compute, App engine, Storage, etc.) </b></details> <details> <summary>True or False? In a project, you can have one or more organizations</summary><br><b> False. It's quite the opposite. First there is an organization and under organization you can have one or more folder with one or more projects. </b></details> <details> <summary>True or False? A resource has to be associated with at least one project</summary><br><b> True. You can't have resources associate with no projects. </b></details> <details> <summary>True or False? Project name has to be globally unique</summary><br><b> True. </b></details>", "metadata": {"source_file": "learning-materials/topics/gcp/README.md", "section": "Resource Hierarchy", "language": "en", "created_at": "2025-07-19T19:22:02.255406"}}
{"text": "<details> <summary>Explain roles and permissions</summary><br><b> Role is an encapsulation of set of permissions. For example an \"owner\" role has more than 3000 assigned permissions to the different components and services of GCP. </b></details> <details> <summary>True or False? Permissive parent policy will always overrule restrictive child policy</summary><br><b> True </b></details>", "metadata": {"source_file": "learning-materials/topics/gcp/README.md", "section": "IAM and Roles", "language": "en", "created_at": "2025-07-19T19:22:02.255688"}}
{"text": "<details> <summary>What are labels?</summary><br><b> You can think about labels in GCP as sticky notes that you attach to different GCP resources. That makes it easier for example, to search for specific resources (like applying the label called \"web-app\" and search for all the resources that are related somehow to \"web-app\") </b></details> <details> <summary>Can you provide some examples to labels usage in GCP?</summary><br><b> * Location (cost center) * Project (or environment, folder, etc.) * Service type * Service owner * Application type * Application owner </b></details> <details> <summary>What are network tags and how are they different from labels? </summary><br><b> As the name suggests, network tags can be applied only to network resources. While labels don't affect the resources on which they are applied, network tags do affect resources (e.g. firewall access and networking routes) </b></details>", "metadata": {"source_file": "learning-materials/topics/gcp/README.md", "section": "Labels and Tags", "language": "en", "created_at": "2025-07-19T19:22:02.255909"}}
{"text": "<details> <summary>List the labels of an instance called \"instance-1\"</summary><br><b> `gcloud compute instances describe instance-1 --format \"yaml(labels)\"` </b></details> <details> <summary>Update a label to \"app=db\" for the instance called \"instance-1\"</summary><br><b> `gcloud compute instances update instance-1 --update-labels app=db` </b></details> <details> <summary>Remove the label \"env\" from an instance called \"instance-1\"</summary><br><b> `gcloud compute instances update instance-1 --remove-labels env` </b></details>", "metadata": {"source_file": "learning-materials/topics/gcp/README.md", "section": "gcloud", "language": "en", "created_at": "2025-07-19T19:22:02.255978"}}
{"text": "<details> <summary>Create an instance with the following properties: * name: instance-1 * machine type: e2-micro * labels: app=web, env=dev </summary><br><b> `gcloud compute instances create instance-1 --labels app=web,env=dev --machine-type=e2-micro` </b></details>", "metadata": {"source_file": "learning-materials/topics/gcp/README.md", "section": "gcloud", "language": "en", "created_at": "2025-07-19T19:22:02.256011"}}
{"text": "<details> <summary>Tell me what do you know about GCP networking</summary><br><b> Virtual Private Cloud(VPC) network is a virtual version of physical network, implemented in Google's internal Network. VPC is a global resource in GCP. Subnetworks(subnets) are regional resources, ie., subnets can be created withinin regions. VPC are created in 2 modes, 1. Auto mode VPC - One subnet in each region is created automatically by GCP while creating VPC 2. Custom mode VPC - No subnets are automatically created. This type of network provides complete control over the subnets creation to the users. </b></details> <details> <summary>Explain Cloud Functions</summary><br><b> Google Cloud Functions is a serverless execution environment for building and connecting cloud services. With Cloud Functions you write simple, single-purpose functions that are attached to events emitted from your cloud infrastructure and services. Your function is triggered when an event being watched is fired. </b></details>", "metadata": {"source_file": "learning-materials/topics/gcp/README.md", "section": "Other", "language": "en", "created_at": "2025-07-19T19:22:02.256483"}}
{"text": "<details> <summary>What is Cloud Datastore?</summary><br><b> Cloud Datastore is a schemaless NoSQL datastore in Google's cloud. Applications can use Datastore to query your data with SQL-like queries that support filtering and sorting. Datastore replicates data across multiple datacenters, which provides a high level of read/write availability. </b></details> <details> <summary>What network tags are used for?</summary><br><b> Network tags allow you to apply firewall rules and routes to a specific instance or set of instances: You make a firewall rule applicable to specific instances by using target tags and source tags. </b></details> <details> <summary>What are flow logs? Where are they enabled?</summary><br><b> VPC Flow Logs records a sample of network flows sent from and received by VM instances, including instances used as Google Kubernetes Engine nodes. These logs can be used for network monitoring, forensics, real-time security analysis, and expense optimization. Enable Flow Logs", "metadata": {"source_file": "learning-materials/topics/gcp/README.md", "section": "Other", "language": "en", "created_at": "2025-07-19T19:22:02.256505"}}
{"text": "1. Open VPC Network in GCP Console 2. Click the name of the subnet 3. Click EDIT button 4. Set Flow Logs to On 5. Click Save </b></details> <details> <summary>How do you list buckets?</summary><br><b> Two ways to do that: $ gsutil ls $ gcloud alpha storage ls </b></details> <details> <summary>What Compute metadata key allows you to run code at startup?</summary><br><b> startap-script </b></details> <details> <summary>What the following commands does? `gcloud deployment-manager deployments create`</summary><br><b> Deployment Manager creates a new deployment. </b></details> <details> <summary>What is Cloud Code?</summary><br><b> It is a set of tools to help developers write, run and debug GCP kubernetes based applications. It provides built-in support for rapid iteration, debugging and running applications in development and production K8s environments. </b></details>", "metadata": {"source_file": "learning-materials/topics/gcp/README.md", "section": "Other", "language": "en", "created_at": "2025-07-19T19:22:02.256524"}}
{"text": "<details> <summary>What is GKE</summary><br><b> * It is the managed kubernetes service on GCP for deploying, managing and scaling containerised applications using Google infrastructure. </b></details>", "metadata": {"source_file": "learning-materials/topics/gcp/README.md", "section": "Google Kubernetes Engine (GKE)", "language": "en", "created_at": "2025-07-19T19:22:02.256553"}}
{"text": "<details> <summary>What is Anthos</summary><br><b> It is a managed application platform for organisations like enterprises that require quick modernisation and certain levels of consistency for their legacy applications in a hybrid or multicloud world. From this explanation the core ideas can be drawn from these statements; * Managed -> the customer does not need to worry about the underlying software integrations, they just enable the API. * application platform -> It consists of open source tools like K8s, Knative, Istio and Tekton * Enterprises -> these are usually organisations with complex needs * Consistency -> to have the same policies declaratively initiated to be run anywhere securely e.g on-prem, GCP or other-clouds (AWS or Azure) fun fact: Anthos is flower in greek, they grow in the ground (earth) but need rain from the clouds to flourish. </b></details> <details> <summary>List the technical components that make up Anthos</summary><br><b> * Infrastructure management - Google", "metadata": {"source_file": "learning-materials/topics/gcp/README.md", "section": "Anthos", "language": "en", "created_at": "2025-07-19T19:22:02.258490"}}
{"text": "Kubernetes Engine (GKE) * Cluster management - GKE, Ingress for Anthos * Service management - Anthos Service Mesh * Policy enforcement - Anthos Config Management, Anthos Enterprise Data Protection, Policy Controller * Application deployment - CI/CD tools like Cloud Build, GitLab * Application development - Cloud Code </b></details> <details> <summary>What is the primary computing environment for Anthos to easily manage workload deployment?</summary><br><b> * Google Kubernetes Engine (GKE) </b></details> <details> <summary>How does Anthos handle the control plane and node components for GKE?</summary><br><b> On GCP the kubernetes api-server is the only control plane component exposed to customers whilst compute engine manages instances in the project. </b></details> <details> <summary>Which load balancing options are available?</summary><br><b> * Networking load balancing for L4 and HTTP(S) Load Balancing for L7 which are both managed services that do not require additional", "metadata": {"source_file": "learning-materials/topics/gcp/README.md", "section": "Anthos", "language": "en", "created_at": "2025-07-19T19:22:02.258521"}}
{"text": "configuration. * Ingress for Anthos which allows the ability to deploy a load balancer that serves an application across multiple clusters on GKE </b></details> <details> <summary>Can you deploy Anthos on AWS?</summary><br><b> * Yes, Anthos on AWS is now GA. For more read [here](https://cloud.google.com/anthos/gke/docs/aws) </b></details> <details> <summary>List and explain the enterprise security capabilities provided by Anthos</summary><br><b> * Control plane security - GCP manages and maintains the K8s control plane out of the box. The user can secure the api-server by using master authorized networks and private clusters. These allow the user to disable access on the public IP address by assigning a private IP address to the master. * Node security - By default workloads are provisioned on Compute engine instances that use Google's Container Optimised OS. This operating system implements a locked-down firewall, limited user accounts with root disabled and a read-only filesystem.", "metadata": {"source_file": "learning-materials/topics/gcp/README.md", "section": "Anthos", "language": "en", "created_at": "2025-07-19T19:22:02.258679"}}
{"text": "There is a further option to enable GKE Sandbox for stronger isolation in multi-tenant deployment scenarios. * Network security - Within a created cluster VPC, Anthos GKE leverages a powerful software-defined network that enables simple Pod-to-Pod communications. Network policies allow locking down ingress and egress connections in a given namespace. Filtering can also be implemented to incoming load-balanced traffic for services that require external access, by supplying whitelisted CIDR IP ranges. * Workload security - Running workloads run with limited privileges, default Docker AppArmor security policies are applied to all Kubernetes Pods. Workload identity for Anthos GKE aligns with the open source kubernetes service accounts with GCP service account permissions. * Audit logging - Administrators are given a way to retain, query, process and alert on events of the deployed environments. </b></details> <details> <summary>How can workloads deployed on Anthos GKE on-prem clusters", "metadata": {"source_file": "learning-materials/topics/gcp/README.md", "section": "Anthos", "language": "en", "created_at": "2025-07-19T19:22:02.258738"}}
{"text": "securely connect to Google Cloud services?</summary><br><b> * Google Cloud Virtual Private Network (Cloud VPN) - this is for secure networking * Google Cloud Key Management Service (Cloud KMS) - for key management </b></details> <details> <summary>What is Island Mode configuration with regards to networking in Anthos GKE deployed on-prem?</summary><br><b> * This is when pods can directly talk to each other within a cluster, but cannot be reached from outside the cluster thus forming an \"island\" within the network that is not connected to the external network. </b></details> <details> <summary>Explain Anthos Config Management</summary><br><b> It is a core component of the Anthos stack which provides platform, service and security operators with a single, unified approach to multi-cluster management that spans both on-premises and cloud environments. It closely follows K8s best practices, favoring declarative approaches over imperative operations, and actively monitors cluster state and", "metadata": {"source_file": "learning-materials/topics/gcp/README.md", "section": "Anthos", "language": "en", "created_at": "2025-07-19T19:22:02.258764"}}
{"text": "applies the desired state as defined in Git. It includes three key components as follows: 1. An importer that reads from a central Git repository 2. A component that synchronises stored configuration data into K8s objects 3. A component that monitors drift between desired and actual cluster configurations with a capability of reconciliation when need rises. </b></details> <details> <summary>How does Anthos Config Management help?</summary><br><b> It follows common modern software development practices which makes cluster configuration, management and policy changes auditable, revertable, and versionable easily enforcing IT governance and unifying resource management in an organisation. </b></details> <details> <summary>What is Anthos Service Mesh?</summary><br><b> * It is a suite of tools that assist in monitoring and managing deployed services on Anthos of all shapes and sizes whether running in cloud, hybrid or multi-cloud environments. It leverages the APIs and core components from", "metadata": {"source_file": "learning-materials/topics/gcp/README.md", "section": "Anthos", "language": "en", "created_at": "2025-07-19T19:22:02.258783"}}
{"text": "Istio, a highly configurable and open-source service mesh platform. </b></details> <details> <summary>Describe the two main components of Anthos Service Mesh</summary><br><b> 1. Data plane - it consists of a set of distributed proxies that mediate all inbound and outbound network traffic between individual services which are configured using a centralised control plane and an open API 2. Control plane - is a fully managed offering outside of Anthos GKE clusters to simplify management overhead and ensure highest possible availability. </b></details> <details> <summary>What are the components of the managed control plane of Anthos Service Mesh?</summary><br><b> 1. Traffic Director - it is GCP's fully managed service mesh traffic control plane, responsible for translating Istio API objects into configuration information for the distributed proxies, as well as directing service mesh ingress and egress traffic 2. Managed CA - is a centralised certificate authority responsible for providing", "metadata": {"source_file": "learning-materials/topics/gcp/README.md", "section": "Anthos", "language": "en", "created_at": "2025-07-19T19:22:02.258802"}}
{"text": "SSL certificates to each of the distributed proxies, authentication information and distributing secrets 3. Operations tooling - formerly stackdriver, provides a managed ingestion point for observability and telemetry, specifically monitoring, tracing and logging data generated by each of the proxies. This powers the observability dashboard for operators to visually inspect their services and service dependencies assisting in the implementation of SRE best practices for monitoring SLIs and establishing SLOs. </b></details> <details> <summary>How does Anthos Service Mesh help?</summary><br><b> Tool and technology integration that makes up Anthos service mesh delivers significant operational benefits to Anthos environments, with minimal additional overhead such as follows: * Uniform observability - the data plane reports service to service communication back to the control plane generating a service dependency graph. Traffic inspection by the proxy inserts headers to facilitate", "metadata": {"source_file": "learning-materials/topics/gcp/README.md", "section": "Anthos", "language": "en", "created_at": "2025-07-19T19:22:02.258821"}}
{"text": "distributed tracing, capturing and reporting service logs together with service-level metrics (i.e latency, errors, availability). * Operational agility - fine-grained controls for managing the flow of inter-mesh (north-south) and intra-mesh (east-west) traffic are provided. * Policy-driven security - policies can be enforced consistently across diverse protocols and runtimes as service communications are secured by default. </b></details> <details> <summary>List possible use cases of traffic controls that can be implemented within Anthos Service Mesh</summary><br><b> * Traffic splitting across differing service versions for canary or A/B testing * Circuit breaking to prevent cascading failures * Fault injection to help build resilient and fault-tolerant deployments * HTTP header-based traffic steering between individual services or versions </b></details> <details> <summary>What is Cloud Run for Anthos?</summary><br><b> It is part of the Anthos stack that brings a serverless container", "metadata": {"source_file": "learning-materials/topics/gcp/README.md", "section": "Anthos", "language": "en", "created_at": "2025-07-19T19:22:02.258839"}}
{"text": "experience to Anthos, offering a high-level platform experience on top of K8s clusters. It is built with Knative, an open-source operator for K8s that brings serverless application serving and eventing capabilities. </b></details> <details> <summary>How does Cloud Run for Anthos simplify operations?</summary><br><b> Platform teams in organisations that wish to offer developers additional tools to test, deploy and run applications can use Knative to enhance this experience on Anthos as Cloud Run. Below are some of the benefits; * Easy migration from K8s deployments - Without Cloud Run, platform engineers have to configure deployment, service, and HorizontalPodAutoscalers(HPA) objects to a loadbalancer and autoscaling. If application is already serving traffic it becomes hard to change configurations or roll back efficiently. Using Cloud Run all this is managed thus the Knative service manifest describes the application to be autoscaled and loadbalanced * Autoscaling - a sudden traffic", "metadata": {"source_file": "learning-materials/topics/gcp/README.md", "section": "Anthos", "language": "en", "created_at": "2025-07-19T19:22:02.258857"}}
{"text": "spike may cause application containers in K8s to crash due to overload thus an efficient automated autoscaling is executed to serve the high volume of traffic * Networking - it has built-in load balancing capabilities and policies for traffic splitting between multiple versions of an application. * Releases and rollouts - supports the notion of the Knatibe API's revisions which describe new versions or different configurations of your application and canary deployments by splitting traffic. * Monitoring - observing and recording metrics such as latency, error rate and requests per second. </b></details> <details> <summary>List and explain three high-level out of the box autoscaling primitives offered by Cloud Run for Anthos that do not exist in K8s natively</summary><br><b> * Rapid, request-based autoscaling - default autoscalers monitor request metrics which allows Cloud Run for Anthos to handle spiky traffic patterns smoothly * Concurrency controls - limits such as max in-flight", "metadata": {"source_file": "learning-materials/topics/gcp/README.md", "section": "Anthos", "language": "en", "created_at": "2025-07-19T19:22:02.259023"}}
{"text": "requests per container are enforced to ensure the container does not become overloaded and crash. More containers are added to handle the spiky traffic, buffering the requests. * Scale to zero - if an application is inactive for a while Cloud Run scales it down to zero to reduce its footprint. Alternatively one can turn off scale-to-zero to prevent cold starts. </b></details> <details> <summary>List some Cloud Run for Anthos use cases</summary><br><b> As it does not support stateful applications or sticky sessions, it is suitable for running stateless applications such as: * Machine learning model predictions e.g Tensorflow serving containers * API gateways, API middleware, web front ends and Microservices * Event handlers, ETL </b></details>", "metadata": {"source_file": "learning-materials/topics/gcp/README.md", "section": "Anthos", "language": "en", "created_at": "2025-07-19T19:22:02.259054"}}
{"text": "1. Create a project with a unique name", "metadata": {"source_file": "learning-materials/topics/gcp/exercises/create_project/solution.md", "section": "Objectives", "language": "en", "created_at": "2025-07-19T19:22:02.259256"}}
{"text": "1. Click in the top bar on \"New Project\" (if you already have a project then, click on the project name and then \"New Project\") or in the search bar insert \"Create Project\". 2. Insert a globally unique project name 3. Optionally choose an organization 4. Optionally put it under a specific folder 5. Click on \"Create\" :)", "metadata": {"source_file": "learning-materials/topics/gcp/exercises/create_project/solution.md", "section": "Console", "language": "en", "created_at": "2025-07-19T19:22:02.259315"}}
{"text": "Click [here](main.tf) to view the solution", "metadata": {"source_file": "learning-materials/topics/gcp/exercises/create_project/solution.md", "section": "Terraform", "language": "en", "created_at": "2025-07-19T19:22:02.259334"}}
{"text": "1. Create a VM instance with the following properties 1. name: instance-1 2. type: e2-micro 3. labels: 1. app: web 2. env: dev 2. Using the CLI (gcloud) perform the following operations: 1. Update \"app\" label to \"db\" 2. Remove \"env\" label", "metadata": {"source_file": "learning-materials/topics/gcp/exercises/instance_101/solution.md", "section": "Objectives", "language": "en", "created_at": "2025-07-19T19:22:02.259644"}}
{"text": "1. Go to Compute Engine -> VM instances 2. Click on \"Create Instance\" 1. Insert the name \"instance-1\" 2. Click on \"Add label\" and add the following labels: 1. app: web 2. env: dev 3. Choose machine type: e2-micro 3. Click on \"Create\" 4. Selected the created instance and click on \"show info panel\" 1. Click on \"labels\" tab and change the value of \"app\" label to \"db\" 2. Remove the \"env\" label", "metadata": {"source_file": "learning-materials/topics/gcp/exercises/instance_101/solution.md", "section": "Console", "language": "en", "created_at": "2025-07-19T19:22:02.259788"}}
{"text": "``` gcloud config set project <PROJECT_ID> gcloud config set compute/region <REGION NAME> gcloud config set compute/zone <ZONE NAME> gcloud compute instances create instance-1 --labels app=web,env=dev --machine-type=e2-micro gcloud compute instances update instance-1 --update-labels app=db gcloud compute instances update instance-1 --remove-labels env ```", "metadata": {"source_file": "learning-materials/topics/gcp/exercises/instance_101/solution.md", "section": "Shell", "language": "en", "created_at": "2025-07-19T19:22:02.259839"}}
{"text": "Click [here](main.tf) to view the main.tf file", "metadata": {"source_file": "learning-materials/topics/gcp/exercises/instance_101/solution.md", "section": "Terraform", "language": "en", "created_at": "2025-07-19T19:22:02.259857"}}
{"text": "1. Assign the following roles to a member in your organization 1. Compute Storage Admin 2. Compute Network Admin 3. Compute Security Admin 2. Verify roles were assigned", "metadata": {"source_file": "learning-materials/topics/gcp/exercises/assign_roles/solution.md", "section": "Objectives", "language": "en", "created_at": "2025-07-19T19:22:02.260083"}}
{"text": "1. Go to IAM & Admin 2. Click on IAM and then on the \"Add\" button 1. Choose the member account to whom the roles will be added 2. Under select role, search for the specified roles under \"Objectives\" and click on \"Save\" 2. The member should now be able to go to the compute engine API and see the resources there.", "metadata": {"source_file": "learning-materials/topics/gcp/exercises/assign_roles/solution.md", "section": "Console", "language": "en", "created_at": "2025-07-19T19:22:02.260146"}}
{"text": "Click [here](main.tf) to view the Terraform main.tf file", "metadata": {"source_file": "learning-materials/topics/gcp/exercises/assign_roles/solution.md", "section": "Terraform", "language": "en", "created_at": "2025-07-19T19:22:02.260166"}}
{"text": "|Name|Topic|Objective & Instructions|Solution|Comments| |--------|--------|------|----|----|", "metadata": {"source_file": "learning-materials/topics/kafka/README.md", "section": "Kafka Exercises", "language": "en", "created_at": "2025-07-19T19:22:02.260341"}}
{"text": "* [Kafka 101](#questions-kafka-101) <a name=\"questions-kafka-101\"></a>", "metadata": {"source_file": "learning-materials/topics/kafka/README.md", "section": "Kafka Self Assessment", "language": "en", "created_at": "2025-07-19T19:22:02.260370"}}
{"text": "<details> <summary>What is Kafka?</summary><br><b> [kafka.apache.org](https://kafka.apache.org): \"Apache Kafka is an open-source distributed event streaming platform used by thousands of companies for high-performance data pipelines, streaming analytics, data integration, and mission-critical applications.\" In other words, Kafka is a sort of distributed log where you can store events, read them and distribute them to different services and do it in high-scale and real-time. </b></details> <details> <summary>What Kafka is used for?</summary><br><b> - Real-time e-commerce - Banking - Health Care - Automotive (traffic alerts, hazard alerts, ...) - Real-time Fraud Detection </b></details> <details> <summary>What is a \"Producer\" in regards to Kafka?</summary><br><b> An application that publishes data to the Kafka cluster. </b></details> <a name=\"questions-kafka-architecture\"></a>", "metadata": {"source_file": "learning-materials/topics/kafka/README.md", "section": "Kafka 101", "language": "en", "created_at": "2025-07-19T19:22:02.260489"}}
{"text": "<details> <summary>What's in a Kafka cluster?</summary><br><b> - Broker: a server with kafka process running on it. Such server has local storage. In a single Kafka clusters there are usually multiple brokers. </b></details> <details> <summary>What is the role of ZooKeeper is Kafka?</summary><br/><b> In Kafka, Zookeeper is a centralized controller that manages metadata for producers, brokers, and consumers. Zookeeper also: <ul> <li>Tracks which brokers are part of the Kafka cluster</li> <li> Determines which broker is the leader of a given partition and topic </li> <li> Performs leader elections </li> <li> Manages cluster membership of brokers </li> </ul> </b> </details>", "metadata": {"source_file": "learning-materials/topics/kafka/README.md", "section": "Kafka Architecture", "language": "en", "created_at": "2025-07-19T19:22:02.260595"}}
{"text": "<details> <summary>What is DevSecOps? What its core principals?</summary><br><b> A couple of quotations from chosen companies: [Snyk](https://snyk.io/series/devsecops): \"DevSecOps refers to the integration of security practices into a DevOps software delivery model. Its foundation is a culture where development and operations are enabled through process and tooling to take part in a shared responsibility for delivering secure software.\" [Red Hat](https://www.redhat.com/en/topics/devops/what-is-devsecops): \"DevSecOps stands for development, security, and operations. It's an approach to culture, automation, and platform design that integrates security as a shared responsibility throughout the entire IT lifecycle.\" [Jfrog](https://jfrog.com/devops-tools/what-is-devsecops): \"DevSecOps principles and practices parallel those of traditional DevOps with integrated and multidisciplinary teams, working together to enable secure continuous software delivery. The DevSecOps development lifecycle", "metadata": {"source_file": "learning-materials/topics/security/README.md", "section": "Security", "language": "en", "created_at": "2025-07-19T19:22:02.262215"}}
{"text": "is a repetitive process that starts with a developer writing code, a build being triggered, the software package deployed to a production environment and monitored for issues identified in the runtime but includes security at each of these stages.\" </b></details> <details> <summary>What the \"Zero Trust\" concept means? How Organizations deal with it?</summary><br><b> [Codefresh definition](https://codefresh.io/security-testing/codefresh-runner-overview): \"Zero trust is a security concept that is centered around the idea that organizations should never trust anyone or anything that does not originate from their domains. Organizations seeking zero trust automatically assume that any external services it commissions have security breaches and may leak sensitive information\" </b></details> <details> <summary>Explain the principle of least privilege</summary><br><b> The principle of least privilege refers to the practice of providing minimal permissions to users, roles, and service accounts", "metadata": {"source_file": "learning-materials/topics/security/README.md", "section": "Security", "language": "en", "created_at": "2025-07-19T19:22:02.262254"}}
{"text": "that allow them to perform their functions. If an entity does not require an access right then it should not have that right. </b></details> <details> <summary>What it means to be \"FIPS compliant\"?</summary><br><b> </b></details> <details> <summary>What is a Certificate Authority?</summary><br><b> [wikipedia](https://en.wikipedia.org/wiki/Certificate_authority) : A certificate Authority that stores, singns and issues certificates. A certificate certifies the authenticity of the public key delivered by the website. It prevents [man-in-the-middle](https://en.wikipedia.org/wiki/Man-in-the-middle_attack) attacks by providing a lot of information which identifie the public key. Importante information provided inside a [X.509](https://www.ssl.com/faqs/what-is-an-x-509-certificate/) certificate are like : * Version Number * Serial Number * Signature Algorithm ID * Issuer Name * Validity period * Subject name * Subject Public Key info Every certificates must be signed by a trusted authority, a", "metadata": {"source_file": "learning-materials/topics/security/README.md", "section": "Security", "language": "en", "created_at": "2025-07-19T19:22:02.262276"}}
{"text": "certificate chain is a concatenation of multiple certificates signed by a more trusted authority from the one delivered by the website to the root Certificate Authority (CA). The root Certificate Authority is the top most trusted authority and every browsers embark their certificate natively. </b></details> <details> <summary>Explain RBAC (Role-based Access Control)</summary><br><b> Access control based on user roles (i.e., a collection of access authorizations a user receives based on an explicit or implicit assumption of a given role). Role permissions may be inherited through a role hierarchy and typically reflect the permissions needed to perform defined functions within an organization. A given role may apply to a single individual or to several individuals. - RBAC mapped to job function, assumes that a person will take on different roles, overtime, within an organization and different responsibilities in relation to IT systems. </b></details>", "metadata": {"source_file": "learning-materials/topics/security/README.md", "section": "Security", "language": "en", "created_at": "2025-07-19T19:22:02.262296"}}
{"text": "<details> <summary>Explain Authentication and Authorization</summary><br><b> Authentication is the process of identifying whether a service or a person is who they claim to be. Authorization is the process of identifying what level of access the service or the person have (after authentication was done) </b></details> <details> <summary>What authentication methods are there?</summary><br><b> </b></details> <details> <summary>Give an example of basic authentication process</summary><br><b> A user uses the browser to authenticate to some server. It does so by using the authorization field which is constructed from the username and the password combined with a single colon. The result string is encoded using a certain character set which is compatible with US-ASCII. The authorization method + a space is prepended to the encoded string. </b></details> <details> <summary>What are the three primary factors of authentication? Give three examples of each</summary><br><b> Something you have -", "metadata": {"source_file": "learning-materials/topics/security/README.md", "section": "Security - Authentication and Authorization", "language": "en", "created_at": "2025-07-19T19:22:02.262830"}}
{"text": "Smart card - Physical authentication device - Software token Something you know - Password - PIN - Passphrase Something you are - Fingerprint - Iris or retina scan - Gait analysis </b></details> <details> <summary>Explain Token-based authentication</summary><br><b> </b></details> <details> <summary>Explain Risk-based authentication</summary><br><b> </b></details> <details> <summary>Explain what is Single Sign-On</summary><br><b> SSO (Single Sign-on), is a method of access control that enables a user to log in once and gain access to the resources of multiple software systems without being prompted to log in again. </b></details> <details> <summary>Explain how the Kerberos authentication protocol works as a SSO solution</summary><br><b> Kerberos works as a SSO solution by only requiring the user to sign in using their credentials once within a specific validity time window. Kerberos authentication grants the user a Ticket Granting Ticket (TGT) from a trusted authentication server which", "metadata": {"source_file": "learning-materials/topics/security/README.md", "section": "Security - Authentication and Authorization", "language": "en", "created_at": "2025-07-19T19:22:02.262859"}}
{"text": "can then be used to request service tickets for accessing various services and resources. By passing around this encrypted TGT instead of credentials, the user does not need to sign-in multiple times for each resource that has been integrated with Kerberos. </b></details> <details> <summary>Does Kerberos make use of symmetric encryption, asymmetric encryption, both, or neither?</summary><br><b> Symmetric Encryption - Kerberos uses exclusively symmetric encryption with pre-shared keys for transmitting encrypted information and authorizing users. </b></details> <details> <summary>Explain MFA (Multi-Factor Authentication)</summary><br><b> Multi-Factor Authentication (Also known as 2FA). Allows the user to present two pieces of evidence, credentials, when logging into an account. - The credentials fall into any of these three categories: something you know (like a password or PIN), something you have (like a smart card), or something you are (like your fingerprint). Credentials must come", "metadata": {"source_file": "learning-materials/topics/security/README.md", "section": "Security - Authentication and Authorization", "language": "en", "created_at": "2025-07-19T19:22:02.262880"}}
{"text": "from two different categories to enhance security. </b></details> <details> <summary>Explain OAuth</summary><br><b> </b></details>", "metadata": {"source_file": "learning-materials/topics/security/README.md", "section": "Security - Authentication and Authorization", "language": "en", "created_at": "2025-07-19T19:22:02.263021"}}
{"text": "<details> <summary>How do you manage sensitive information (like passwords) in different tools and platforms?</summary><br><b> </b></details> <details> <summary>What password attacks are you familiar with?</summary><br><b> * Dictionary * Brute force * Password Spraying * Social Engineering * Whaling * Vishing * Phising * Whaling </b></details> <details> <summary>How to mitigate password attacks?</summary><br><b> * Strong password policy * Do not reuse passwords * ReCaptcha * Training personnel against Social Engineering * Risk Based Authentication * Rate limiting * MFA </b></details> <details> <summary>What is password salting? What attack does it help to deter?</summary><br><b> Password salting is the processing of prepending or appending a series of characters to a user's password before hashing this new combined value. This value should be different for every single user but the same salt should be applied to the same user password every time it is validated. This ensures that users", "metadata": {"source_file": "learning-materials/topics/security/README.md", "section": "Security - Passwords", "language": "en", "created_at": "2025-07-19T19:22:02.263238"}}
{"text": "that have the same password will still have very different hash values stored in the password database. This process specifically helps deter rainbow table attacks since a new rainbow table would need to be computed for every single user in the database. </b></details>", "metadata": {"source_file": "learning-materials/topics/security/README.md", "section": "Security - Passwords", "language": "en", "created_at": "2025-07-19T19:22:02.263260"}}
{"text": "<details> <summary>What are cookies? Explain cookie-based authentication</summary><br><b> </b></details> <details> <summary>True or False? Cookie-based authentication is stateful</summary><br><b> True. Cookie-based authentication session must be kept on both server and client-side. </b></details> <details> <summary>Explain the flow of using cookies</summary><br><b> 1. User enters credentials 2. The server verifies the credentials -> a sessions is created and stored in the database 3. A cookie with the session ID is set in the browser of that user 4. On every request, the session ID is verified against the database 5. The session is destroyed (both on client-side and server-side) when the user logs out </b></details>", "metadata": {"source_file": "learning-materials/topics/security/README.md", "section": "Security - Cookies", "language": "en", "created_at": "2025-07-19T19:22:02.263371"}}
{"text": "<details> <summary>What is SSH how does it work?</summary><br><b> [Wikipedia Definition](https://en.wikipedia.org/wiki/SSH_(Secure_Shell)): \"SSH or Secure Shell is a cryptographic network protocol for operating network services securely over an unsecured network.\" [Hostinger.com Definition](https://www.hostinger.com/tutorials/ssh-tutorial-how-does-ssh-work): \"SSH, or Secure Shell, is a remote administration protocol that allows users to control and modify their remote servers over the Internet.\" [This site](https://www.hostinger.com/tutorials/ssh-tutorial-how-does-ssh-work) explains it in a good way. </b></details> <details> <summary>What is the role of an SSH key?</summary><br><b> [Wikipedia definition](https://en.wikipedia.org/wiki/Secure_Shell) : SSH uses public-key cryptography to authenticate the remote computer and allow it to authenticate the user. Two keys are created, private is stored inside user's computer to decrypt the communication then the public key is stored inside the", "metadata": {"source_file": "learning-materials/topics/security/README.md", "section": "Security - SSH", "language": "en", "created_at": "2025-07-19T19:22:02.263555"}}
{"text": "remoted computer where user want to connect with and it is used to encrypt the communication. </b></details>", "metadata": {"source_file": "learning-materials/topics/security/README.md", "section": "Security - SSH", "language": "en", "created_at": "2025-07-19T19:22:02.263583"}}
{"text": "<details> <summary>Explain Symmetrical encryption</summary><br><b> A symmetric encryption is any technique where a key is used to both encrypt and decrypt the data/entire communication. </b></details> <details> <summary>Explain Asymmetrical encryption</summary><br><b> A asymmetric encryption is any technique where the there is two different keys that are used for encryption and decryption, these keys are known as public key and private key. </b></details> <details> <summary>What is \"Key Exchange\" (or \"key establishment\") in cryptography?</summary><br><b> [Wikipedia](https://en.wikipedia.org/wiki/Key_exchange): \"Key exchange (also key establishment) is a method in cryptography by which cryptographic keys are exchanged between two parties, allowing use of a cryptographic algorithm.\" </b></details> <details> <summary>True or False? The symmetrical encryption is making use of public and private keys where the private key is used to decrypt the data encrypted with a public", "metadata": {"source_file": "learning-materials/topics/security/README.md", "section": "Security - Cryptography", "language": "en", "created_at": "2025-07-19T19:22:02.263931"}}
{"text": "key</summary><br><b> False. This description fits the asymmetrical encryption. </b></details> <details> <summary>True or False? The private key can be mathematically computed from a public key</summary><br><b> False. </b></details> <details> <summary>True or False? In the case of SSH, asymmetrical encryption is not used to the entire SSH session</summary><br><b> True. It is only used during the key exchange algorithm of symmetric encryption. </b></details> <details> <summary>What is Hashing?</summary><br><b> Hashing is a mathematical function for mapping data of arbitrary sizes to fixed-size values. This function produces a \"digest\" of the data that can be used for verifying that the data has not been modified (amongst other uses) </b></details> <details> <summary>How is hashing different from encryption?</summary><br><b> Encrypted data can be decrypted to its original value. Hashed data cannot be reversed to view the original data - hashing is a one-way function. </b></details>", "metadata": {"source_file": "learning-materials/topics/security/README.md", "section": "Security - Cryptography", "language": "en", "created_at": "2025-07-19T19:22:02.263959"}}
{"text": "<details> <summary>How hashes are part of SSH?</summary><br><b> Hashes used in SSH to verify the authenticity of messages and to verify that nothing tampered with the data received. </b></details>", "metadata": {"source_file": "learning-materials/topics/security/README.md", "section": "Security - Cryptography", "language": "en", "created_at": "2025-07-19T19:22:02.263978"}}
{"text": "<details> <summary>Explain the following: * Vulnerability * Exploits * Risk * Threat</summary><br><b> </b></details> <details> <summary>Are you familiar with \"OWASP top 10\"?</summary><br><b> Read about it [here](https://owasp.org/www-project-top-ten) </b></details> <details> <summary>What is XSS?</summary><br><b> Cross Site Scripting (XSS) is an type of a attack when the attacker inserts browser executable code within a HTTP response. Now the injected attack is not stored in the web application, it will only affect the users who open the maliciously crafted link or third-party web page. A successful attack allows the attacker to access any cookies, session tokens, or other sensitive information retained by the browser and used with that site You can test by detecting user-defined variables and how to input them. This includes hidden or non-obvious inputs such as HTTP parameters, POST data, hidden form field values, and predefined radio or selection values. You then analyze each found", "metadata": {"source_file": "learning-materials/topics/security/README.md", "section": "Security - Attacks, Threats, and Vulnerabilities", "language": "en", "created_at": "2025-07-19T19:22:02.266461"}}
{"text": "vector to see if their are potential vulnerabilities, then when found you craft input data with each input vector. Then you test the crafted input and see if it works. </b></details> <details> <summary>What is an SQL injection? How to manage it?</summary><br><b> SQL injection is an attack consists of inserts either a partial or full SQL query through data input from the browser to the web application. When a successful SQL injection happens it will allow the attacker to read sensitive information stored on the database for the web application. You can test by using a stored procedure, so the application must be sanitize the user input to get rid of the risk of code injection. If not then the user could enter bad SQL, that will then be executed within the procedure </b></details> <details> <summary>What is Certification Authority?</summary><br><b> </b></details> <details> <summary>How do you identify and manage vulnerabilities?</summary><br><b> </b></details> <details> <summary>Explain", "metadata": {"source_file": "learning-materials/topics/security/README.md", "section": "Security - Attacks, Threats, and Vulnerabilities", "language": "en", "created_at": "2025-07-19T19:22:02.266618"}}
{"text": "\"Privilege Restriction\"</summary><br><b> </b></details> <details> <summary>How HTTPS is different from HTTP?</summary><br><b> The 'S' in HTTPS stands for 'secure'. HTTPS uses TLS to provide encryption of HTTP requests and responses, as well as providing verifaction by digitally signing requests and responses. As a result, HTTPS is far more secure than HTTP and is used by default for most modern websites. </b></details> <details> <summary>What types of firewalls are there?</summary><br><b> </b></details> <details> <summary>What is DDoS attack? How do you deal with it?</summary><br><b> </b></details> <details> <summary>What is port scanning? When is it used?</summary><br><b> </b></details> <details> <summary>What is the difference between asynchronous and synchronous encryption?</summary><br><b> </b></details> <details> <summary>Explain Man-in-the-middle attack</summary><br><b> </b></details> <details> <summary>Explain CVE and CVSS</summary><br><b> [Red", "metadata": {"source_file": "learning-materials/topics/security/README.md", "section": "Security - Attacks, Threats, and Vulnerabilities", "language": "en", "created_at": "2025-07-19T19:22:02.266652"}}
{"text": "Hat](https://www.redhat.com/en/topics/security/what-is-cve#how-does-it-work) : \"When someone refers to a CVE (Common Vulnerabilities and Exposures), they mean a security flaw that's been assigned a CVE ID number. They don’t include technical data, or information about risks, impacts, and fixes.\" So CVE is just identified by an ID written with 8 digits. The CVE ID have the following format: CVE prefix + Year + Arbitrary Digits. Anyone can submit a vulnerability, [Exploit Database](https://www.exploit-db.com/submit) explains how it works to submit. Then CVSS stands for Common Vulnerability Scoring System, it attempts to assign severity scores to vulnerabilities, allowing to ordonnance and prioritize responses and resources according to threat. </b></details> <details> <summary>What is ARP Poisoning?</summary><br><b> </b></details> <details> <summary>Describe how do you secure public repositories</summary><br><b> </b></details> <details> <summary>What is DNS Spoofing? How to prevent", "metadata": {"source_file": "learning-materials/topics/security/README.md", "section": "Security - Attacks, Threats, and Vulnerabilities", "language": "en", "created_at": "2025-07-19T19:22:02.266672"}}
{"text": "it?</summary><br><b> DNS spoofing occurs when a particular DNS server’s records of “spoofed” or altered maliciously to redirect traffic to the attacker. This redirection of traffic allows the attacker to spread malware, steal data, etc. **Prevention** - Use encrypted data transfer protocols - Using end-to-end encryption vian SSL/TLS will help decrease the chance that a website / its visitors are compromised by DNS spoofing. - Use DNSSEC - DNSSEC, or Domain Name System Security Extensions, uses digitally signed DNS records to help determine data authenticity. - Implement DNS spoofing detection mechanisms - it’s important to implement DNS spoofing detection software. Products such as XArp help product against ARP cache poisoning by inspecting the data that comes through before transmitting it. </b></details> <details> <summary>What can you tell me about Stuxnet?</summary><br><b> Stuxnet is a computer worm that was originally aimed at Iran’s nuclear facilities and has since mutated and", "metadata": {"source_file": "learning-materials/topics/security/README.md", "section": "Security - Attacks, Threats, and Vulnerabilities", "language": "en", "created_at": "2025-07-19T19:22:02.266697"}}
{"text": "spread to other industrial and energy-producing facilities. The original Stuxnet malware attack targeted the programmable logic controllers (PLCs) used to automate machine processes. It generated a flurry of media attention after it was discovered in 2010 because it was the first known virus to be capable of crippling hardware and because it appeared to have been created by the U.S. National Security Agency, the CIA, and Israeli intelligence. </b></details> <details> <summary>What can you tell me about the BootHole vulnerability?</summary><br><b> </b></details> <details> <summary>What can you tell me about Spectre?</summary><br><b> Spectre is an attack method which allows a hacker to “read over the shoulder” of a program it does not have access to. Using code, the hacker forces the program to pull up its encryption key allowing full access to the program </b></details> <details> <summary>Explain \"Format String Vulnerability\"</summary><br><b> </b></details> <details> <summary>Explain", "metadata": {"source_file": "learning-materials/topics/security/README.md", "section": "Security - Attacks, Threats, and Vulnerabilities", "language": "en", "created_at": "2025-07-19T19:22:02.266748"}}
{"text": "DMZ</summary><br><b> </b></details> <details> <summary>Explain TLS</summary><br><b> </b></details> <details> <summary>What is CSRF? How to handle CSRF?</summary><br><b> Cross-Site Request Forgery (CSRF) is an attack that makes the end user to initiate a unwanted action on the web application in which the user has a authenticated session, the attacker may user an email and force the end user to click on the link and that then execute malicious actions. When an CSRF attack is successful it will compromise the end user data You can use OWASP ZAP to analyze a \"request\", and if it appears that there no protection against cross-site request forgery when the Security Level is set to 0 (the value of csrf-token is SecurityIsDisabled.) One can use data from this request to prepare a CSRF attack by using OWASP ZAP </b></details> <details> <summary>Explain HTTP Header Injection vulnerability</summary><br><b> HTTP Header Injection vulnerabilities occur when user input is insecurely included within", "metadata": {"source_file": "learning-materials/topics/security/README.md", "section": "Security - Attacks, Threats, and Vulnerabilities", "language": "en", "created_at": "2025-07-19T19:22:02.266776"}}
{"text": "server responses headers. If an attacker can inject newline characters into the header, then they can inject new HTTP headers and also, by injecting an empty line, break out of the headers into the message body and write arbitrary content into the application's response. </b></details> <details> <summary>What security sources are you using to keep updated on latest news?</summary><br><b> </b></details> <details> <summary>What TCP and UDP vulnerabilities are you familiar with?</summary><br><b> </b></details> <details> <summary>Do using VLANs contribute to network security?</summary><br><b> </b></details> <details> <summary>What are some examples of security architecture requirements?</summary><br><b> </b></details> <details> <summary>What is air-gapped network (or air-gapped environment)? What its advantages and disadvantages?</summary><br><b> </b></details> <details> <summary>Explain what is Buffer Overflow</summary><br><b> A buffer overflow (or buffer overrun) occurs when the volume", "metadata": {"source_file": "learning-materials/topics/security/README.md", "section": "Security - Attacks, Threats, and Vulnerabilities", "language": "en", "created_at": "2025-07-19T19:22:02.266797"}}
{"text": "of data exceeds the storage capacity of the memory buffer. As a result, the program attempting to write the data to the buffer overwrites adjacent memory locations. </b></details> <details> <summary>What is Nonce?</summary><br><b> </b></details> <details> <summary>What is SSRF?</summary><br><b> SSRF (Server-side request forgery) it's a vulnerability where you can make a server make arbitrary requests to anywhere you want. Read more about it at [portswigger.net](https://portswigger.net/web-security/ssrf) </b></details> <details> <summary>Explain MAC flooding attack</summary><br><b> MAC address flooding attack (CAM table flooding attack) is a type of network attack where an attacker connected to a switch port floods the switch interface with very large number of Ethernet frames with different fake source MAC address. </b></details> <details> <summary>What is port flooding?</summary><br><b> </b></details> <details> <summary>What is \"Diffie-Hellman key exchange\" and how does it", "metadata": {"source_file": "learning-materials/topics/security/README.md", "section": "Security - Attacks, Threats, and Vulnerabilities", "language": "en", "created_at": "2025-07-19T19:22:02.267012"}}
{"text": "work?</summary><br><b> Have you heard of [The Two General's Problem](https://en.wikipedia.org/wiki/Two_Generals%27_Problem)? The Diffie-Hellman key exchange is a solution to this problem to allow for the secure exchange of cryptographic keys over an encrypted channel. It works using public/private key pairs (asymmetric encryption). Two parties that wish to communicate securely over a public channel will each generate a public/private key pair and distribute the public key to the other party (note that public keys are free to be exchanged over a public channel). From here, each party can derive a shared key using a combination of their personal private key and the public key of the other party. This combined key can now be used as a symmetric encryption key for communications. </b></details> <details> <summary>Explain \"Forward Secrecy\"</summary><br><b> </b></details> <details> <summary>What is Cache Poisoned Denial of Service?</summary><br><b> CPDoS or Cache Poisoned Denial of Service.", "metadata": {"source_file": "learning-materials/topics/security/README.md", "section": "Security - Attacks, Threats, and Vulnerabilities", "language": "en", "created_at": "2025-07-19T19:22:02.267042"}}
{"text": "It poisons the CDN cache. By manipulating certain header requests, the attacker forces the origin server to return a Bad Request error which is stored in the CDN’s cache. Thus, every request that comes after the attack will get an error page. </b></details> <details> <summary>What is the difference if any between SSL and TLS?</summary><br><b> </b></details> <details> <summary>What's SSL termination (or SSL offloading)?</summary><br><b> SSL termination is the process of decrypting encrypted traffic. The advantage in SSL termination is that the server doesn't have to perform it, we can use SSL termination to reduce the load on the server, speed up some processes, and allow the server to focus on its core functionality (e.g. deliver content) </b></details> <details> <summary>What is SNI (Server Name Indication)?</summary><br><b> [Wikipedia](https://en.wikipedia.org/wiki/Server_Name_Indication): \"an extension to the Transport Layer Security (TLS) computer networking protocol by which a", "metadata": {"source_file": "learning-materials/topics/security/README.md", "section": "Security - Attacks, Threats, and Vulnerabilities", "language": "en", "created_at": "2025-07-19T19:22:02.267063"}}
{"text": "client indicates which hostname it is attempting to connect to at the start of the handshaking process\" </b></details> <details> <summary>What benefits SNI introduces?</summary><br><b> SNI allows a single server to serve multiple certificates using the same IP and port.<br> Practically this means that a single IP can server multiple web services/pages, each using a different certificate. </b></details> <details> <summary>Explain \"Web Cache Deception Attach\"</summary><br><b> [This blog post](https://omergil.blogspot.com/2017/02/web-cache-deception-attack.html) explains it in detail. </b></details>", "metadata": {"source_file": "learning-materials/topics/security/README.md", "section": "Security - Attacks, Threats, and Vulnerabilities", "language": "en", "created_at": "2025-07-19T19:22:02.267085"}}
{"text": "<details> <summary>Explain \"Advanced persistent threat (APT)\"</summary><br><b> </b></details> <details> <summary>What is a \"Backdoor\" in information security?</summary><br><b> </b></details>", "metadata": {"source_file": "learning-materials/topics/security/README.md", "section": "Security - Threats", "language": "en", "created_at": "2025-07-19T19:22:02.267112"}}
{"text": "<details> <summary>Briefly describe what a software supply chain is. </summary><br><b> A company’s software supply chain consists of any third party or open source component which could be used to compromise the final product. Such component is usually an API provided by an actor. For instance Twilio who offers mobile communication APIs to their customers. [WhiteSource](https://www.whitesourcesoftware.com/resources/blog/software-supply-chain-security-the-basics-and-four-critical-best-practices/): \"Enterprise software projects increasingly depend on third-party and open source components. These components are created and maintained by individuals who are not employed by the organization developing the primary software, and who do not necessarily use the same security policies as the organization. This poses a security risk, because differences or inconsistencies between these policies can create overlooked areas of vulnerability that attackers seek to exploit.\" </b></details> <details>", "metadata": {"source_file": "learning-materials/topics/security/README.md", "section": "Software Supply Chain & Security", "language": "en", "created_at": "2025-07-19T19:22:02.267614"}}
{"text": "<summary>What're some benefits of a software supply chain? </summary><br><b> [Increment](https://increment.com/apis/apis-supply-chain-software/): Resource-saving. Using and paying for existing solutions to resource-heavy problems saves time as well as money. Hence resulting in efficient, cheap and greater opportunities to develop and deploy software products for consumers. </b></details> <details> <summary> Give three examples of three potential security threats related to the software supply chain and describe them.</summary><br><b> [IEEE](https://ieeexplore.ieee.org/abstract/document/9203862): * Sensitive data being exposed or lost. * In a software supply chain, sensitive data may be passed throughout the chain. Security threats involve loss or exposure of this data, such as customer credit card details. * Cloud technology. * Data sharing in the cloud might jeopardize the privacy of the data within the chain. * Third-party vendors. * Third-party vendors’ code solutions might not", "metadata": {"source_file": "learning-materials/topics/security/README.md", "section": "Software Supply Chain & Security", "language": "en", "created_at": "2025-07-19T19:22:02.267647"}}
{"text": "provide sufficient cybersecurity and risk being a potential subject to data breaches. </b></details>", "metadata": {"source_file": "learning-materials/topics/security/README.md", "section": "Software Supply Chain & Security", "language": "en", "created_at": "2025-07-19T19:22:02.267669"}}
{"text": "<details> <summary> What is a package manager? </summary><br><b> [Baudry et al.](https://arxiv.org/pdf/2001.07808.pdf): \"A tool that allows you to easily download, add and thus reuse programming libraries in your project.\" E.g. npm or yarn. </b></details> <details> <summary> What is a build tool? </summary><br><b> [Baudry et al.](https://arxiv.org/pdf/2001.07808.pdf): \"A tool that fetches the packages (dependencies) that are required to compile, test and deploy your application.\" </b></details> <details> <summary> Describe bloated dependencies. </summary><br><b> [Baudry et al.](https://arxiv.org/pdf/2001.07808.pdf): An application usually has different dependencies. Typically, not all of them are required for building and running the application. Bloated dependencies is the concept of including the unnecessary dependencies for building and running your application. </b></details> <details> <summary> Explain a few cons of bloated dependencies. </summary><br><b> [Baudry et", "metadata": {"source_file": "learning-materials/topics/security/README.md", "section": "Package management & Security", "language": "en", "created_at": "2025-07-19T19:22:02.268421"}}
{"text": "al.](https://arxiv.org/pdf/2001.07808.pdf): * Challenging to manage. * Decreases performance of the application. * Risk for malicious code that a threathening actor can take advantage of. </b></details> <details> <summary> What solutions are there for managing project dependencies? </summary><br><b> [Npm.js documentation](https://docs.npmjs.com/cli/v8/commands/npm-prune): Use clean-up commands that are usually provided by the package manager authors. For instance, npm prune will remove any extraneous package. Another command is npm audit which will scan your repository and report any vulnerable dependencies found. </b></details> <details> <summary> What is a threatening actor and how can this actor take advantage of open source or third party vendor's packages/libraries? </summary><br><b> [Wikipedia](https://en.wikipedia.org/wiki/Threat_actor): A threatening actor is one or more people who target technical artifacts such as software, networks and/or devices with the purpose of harming", "metadata": {"source_file": "learning-materials/topics/security/README.md", "section": "Package management & Security", "language": "en", "created_at": "2025-07-19T19:22:02.268613"}}
{"text": "it. [Aquasec](https://www.aquasec.com/cloud-native-academy/devsecops/supply-chain-security/): An attacking actor may identify, target and inject malicious software in a vulnerable part of an open source package or a third party vendor’s code. The consumer of this code may consequently and unknowingly deploy the malicious code throughout their pipelines, thus infecting their own projects. An example of this happening is the hack of [SolarWinds](https://www.npr.org/2021/04/16/985439655/a-worst-nightmare-cyberattack-the-untold-story-of-the-solarwinds-hack). </b></details> <details> <summary> How can you make sure that you use trustworthy packages for your project? </summary><br><b> You can’t. You will always be exposed to security risk once you start using open source or vendor packages. The goal is to minimize the risk in order to avoid security breaches. This could be done by: * Regularly update the project's dependencies to apply latest bug fixes and vulnerability clean-ups. * However,", "metadata": {"source_file": "learning-materials/topics/security/README.md", "section": "Package management & Security", "language": "en", "created_at": "2025-07-19T19:22:02.268645"}}
{"text": "unless you trust the author, do not update your dependencies instantly, since package updates recently have been a common target by hackers. * Check for changes of the file content in previous versions. </b></details> <details> <summary> Explain checksum. </summary><br><b> [Fred Cohen (permission needed)](https://reader.elsevier.com/reader/sd/pii/0167404887900319?token=D5339ABC064AD9A2B50B74D8CE890B0E22A302A0BC461A50078D407BEA01052737DC6AAEF95A854E72A73B6D0C67E260&originRegion=eu-west-1&originCreation=20220502180611): Checksum is a way to verify the integrity of information in systems with no built-in protection. In other words, it provides a way of validating that the content of a file or a package / library is intact. This is useful since attacks or errors may occur during transmission of files. However, it requires that the package author has run a checksum function for the file / package which creates a specific hash for that version of the file. A minor change of the file content", "metadata": {"source_file": "learning-materials/topics/security/README.md", "section": "Package management & Security", "language": "en", "created_at": "2025-07-19T19:22:02.268669"}}
{"text": "will result in a different checksum. If you have access to the original checksum of the file, you may run checksum on your own. In case the resulting checksum matches the original one, no changes have been made in the file. You can now conclude that no error or malicious injection was done during transmission of the file. </b></details>", "metadata": {"source_file": "learning-materials/topics/security/README.md", "section": "Package management & Security", "language": "en", "created_at": "2025-07-19T19:22:02.268688"}}
{"text": "<details> <summary>What is Microsegmentation?</summary><br><b> - Security method - Managing network access between endpoints (processes, devices, instances) - A method in which security policies are applied to limit traffic - based on concepts such as \"Zero Trust\" and \"Least Privileged\" - The result of Microsegmentation should be: - Reduced attack ability - Better breach containment </b></details> <details> <summary>Why do we need Microsegmentation solutions? Why using something such as firewalls isn't enough?</summary><br><b> - Firewalls focused on north-south traffic. Basically traffic that is outside of the company perimeter - Traffic that is considered west-east, internal workflows and communication, is usually left untreated </b></details> <details> <summary>How Microsegmentation is applied?</summary><br><b> There are different ways to apply Microsegmentation: - Cloud Native: Using cloud embedded capabilities such as security groups, firewalls, etc. - Agent: Agents running on the", "metadata": {"source_file": "learning-materials/topics/security/README.md", "section": "Microsegmentation", "language": "en", "created_at": "2025-07-19T19:22:02.269225"}}
{"text": "different endpoints (instances, services, etc.) - Network: Modify network devices and their configuration to create microsegmentation </b></details> <details> <summary>What are ephemeral environments in the context of Microsegmentation?</summary><br><b> - These are short-lived resources like containers or serverless functions that start and stop quickly. - Because they don’t last long, they need security rules that can change just as fast. - Microsegmentation helps by giving each one exactly the network access it needs — nothing more. </b></details> <details> <summary>How does Microsegmentation help prevent lateral movement?</summary><br><b> - It sets tight rules for how services or systems can talk to each other. - If one system gets hacked, the attacker can’t easily move to others. - By dividing systems into smaller zones, it makes the whole network harder to break into. </b></details> <details> <summary>What challenges arise when scaling Microsegmentation?</summary><br><b> - As more", "metadata": {"source_file": "learning-materials/topics/security/README.md", "section": "Microsegmentation", "language": "en", "created_at": "2025-07-19T19:22:02.269255"}}
{"text": "systems get added, managing all the rules becomes harder. - It’s tough to keep security rules consistent when everything’s changing all the time. - You also have to be careful not to slow things down while keeping everything secure. </b></details>", "metadata": {"source_file": "learning-materials/topics/security/README.md", "section": "Microsegmentation", "language": "en", "created_at": "2025-07-19T19:22:02.269278"}}
{"text": "For each of the following, identify what is the data type of the result variable 1. a = {'a', 'b', 'c'} 2. b = {'1': '2'} 4. c = ([1, 2, 3]) 4. d = (1, 2, 3) 4. e = True+True", "metadata": {"source_file": "learning-materials/topics/python/advanced_data_types.md", "section": "(Advanced) Identify the data type", "language": "en", "created_at": "2025-07-19T19:22:02.269508"}}
{"text": "For each of the following, identify what is the data type of the result variable 1. a = [1, 2, 3, 4, 5] 2. b = \"Hello, is it me you looking for?\" 3. e = 100 4. f = '100' 5. i = 0.100 6. i = True Bonus question: how to find out in Python what is the data type of certain variable?", "metadata": {"source_file": "learning-materials/topics/python/data_types.md", "section": "Data Types", "language": "en", "created_at": "2025-07-19T19:22:02.269629"}}
{"text": "write a simple class that has two attributes of which one has a default value and has two methods", "metadata": {"source_file": "learning-materials/topics/python/class_0x00.md", "section": "Class", "language": "en", "created_at": "2025-07-19T19:22:02.269700"}}
{"text": "Write a code that reverses a string", "metadata": {"source_file": "learning-materials/topics/python/reverse_string.md", "section": "Reverse a String", "language": "en", "created_at": "2025-07-19T19:22:02.269804"}}
{"text": "1. write a function that sorts the following list of list without using the `sorted()` and `.sort()` function in descending order - list = [[1, 2, 3], [2, 4, 4], [5, 5, 5]] -> [[5, 5, 5], [2, 4, 4], [1, 2, 3]]", "metadata": {"source_file": "learning-materials/topics/python/sort.md", "section": "Sort Descending", "language": "en", "created_at": "2025-07-19T19:22:02.270026"}}
{"text": "1. Write a function that gets a string and compresses it - 'aaaabbccc' -> 'a4b2c3' - 'abbbc' -> 'a1b3c1' 2. Write a function that decompresses a given string - 'a4b2c3' -> 'aaaabbccc' - 'a1b3c1' -> 'abbbc'", "metadata": {"source_file": "learning-materials/topics/python/compress_string.md", "section": "Compress String", "language": "en", "created_at": "2025-07-19T19:22:02.270112"}}
{"text": "1. a = [1, 2, 3, 4, 5] -> list 2. b = \"Hello, is it me you looking for?\" -> string 3. e = 100 -> int 4. f = '100' -> string 5. i = 0.100 -> float 6. i = True -> bool", "metadata": {"source_file": "learning-materials/topics/python/solutions/data_types_solution.md", "section": "Data Types - Solution", "language": "en", "created_at": "2025-07-19T19:22:02.270320"}}
{"text": "`type(...)`", "metadata": {"source_file": "learning-materials/topics/python/solutions/data_types_solution.md", "section": "Bonus question - Answer", "language": "en", "created_at": "2025-07-19T19:22:02.270344"}}
{"text": "1. write a simple class that has two attributes of which one has a default value and has two methods ```python from typing import Optional \"\"\" Student Module \"\"\" class Student: def __init__(self, name: str, department: Optional[str] = None) -> None: \"\"\" Instance Initialization function Args: name (str): Name of student department (Optional[str], optional): Department. Defaults to None. \"\"\" self.name = name self.department = department def getdetails(self) -> str: \"\"\" Gets the students details Returns: str: A formatted string \"\"\" return f\"Name is {self.name}, I'm in department {self.department}\" def change_department(self, new_deparment: str) -> None: \"\"\"Changes the department of the student object Args: new_deparment (str): Assigns the new department value to dept attr \"\"\" self.department = new_deparment", "metadata": {"source_file": "learning-materials/topics/python/solutions/class_0x00_solution.md", "section": "Class 0x00 - Solution", "language": "en", "created_at": "2025-07-19T19:22:02.270565"}}
{"text": "student1 = Student(\"Ayobami\", \"Statistics\") print(student1.getdetails())", "metadata": {"source_file": "learning-materials/topics/python/solutions/class_0x00_solution.md", "section": "student1 instantiation", "language": "en", "created_at": "2025-07-19T19:22:02.270593"}}
{"text": "student1.change_department(\"CS\") print(student1.department) ``` Output ``` Name is Ayobami, I'm in department Statistics CS ```", "metadata": {"source_file": "learning-materials/topics/python/solutions/class_0x00_solution.md", "section": "Calling the change_department function to change the department of student", "language": "en", "created_at": "2025-07-19T19:22:02.270612"}}
{"text": "1. Write a function that gets a string and compresses it - 'aaaabbccc' -> 'a4b2c3' - 'abbbc' -> 'a1b3c1' ``` def compress_str(mystr: str) -> str: result = '' if mystr: prevchar = mystr[0] else: return result count = 1 for nextchar in mystr[1:]: if nextchar == prevchar: count += 1 else: result += prevchar + str(count) count = 1 prevchar = nextchar result += prevchar + str(count) return result ``` 2. Write a function that decompresses a given string - 'a4b2c3' -> 'aaaabbccc' - 'a1b3c1' -> 'abbbc' ``` def decompress_str(mystr: str) -> str: result = '' for index in range(0, len(mystr), 2): result += mystr[index] * int(mystr[index + 1]) return result ```", "metadata": {"source_file": "learning-materials/topics/python/solutions/compress_string_solution.md", "section": "Compress String Solution", "language": "en", "created_at": "2025-07-19T19:22:02.270833"}}
{"text": "For each of the following, identify what is the data type of the result variable 1. a = {'a', 'b', 'c'} -> set 2. b = {'1': '2'} -> dict 4. c = ([1, 2, 3]) -> list 4. d = (1, 2, 3) -> tuple 4. e = True+True -> int", "metadata": {"source_file": "learning-materials/topics/python/solutions/advanced_data_types_solution.md", "section": "(Advanced) Identify the data type", "language": "en", "created_at": "2025-07-19T19:22:02.270949"}}
{"text": "1. write a function that sorts the following list of list without using the `sorted()` and `.sort()` function in descending order - mat_list = [[1, 2, 3], [2, 4, 4], [5, 5, 5]] -> [[5, 5, 5], [2, 4, 4], [1, 2, 3]] ```python def sort_desc(mat: list) -> list: \"\"\" Sorts a list in descending order Args: mat (list): paresd list Returns: list: A new list \"\"\" new_list = [] while mat != []: maxx = max(mat) new_list.append(maxx) mat.remove(maxx) return new_list print(sort_func(mat_list)) ```", "metadata": {"source_file": "learning-materials/topics/python/solutions/sort_solution.md", "section": "Sort Descending - Solution", "language": "en", "created_at": "2025-07-19T19:22:02.271088"}}
{"text": "``` my_string[::-1] ``` A more visual way is:<br> <i>Careful: this is very slow</i> ``` def reverse_string(string): temp = \"\" for char in string: temp = char + temp return temp ```", "metadata": {"source_file": "learning-materials/topics/python/solutions/reverse_string.md", "section": "Reverse a String - Solution", "language": "en", "created_at": "2025-07-19T19:22:02.271170"}}
{"text": "- [Terraform](#terraform) - [Exercises](#exercises) - [Terraform 101](#terraform-101) - [AWS](#aws) - [Questions](#questions) - [Terraform 101](#terraform-101-1) - [Terraform Hands-On Basics](#terraform-hands-on-basics) - [Dependencies](#dependencies) - [Providers](#providers) - [Variables](#variables) - [Input Variables](#input-variables) - [Output Variables](#output-variables) - [Locals](#locals) - [Variables Hands-On](#variables-hands-on) - [Data Sources](#data-sources) - [Lifecycle](#lifecycle) - [Provisioners](#provisioners) - [State](#state) - [Terraform Backend](#terraform-backend) - [Workspaces](#workspaces) - [State Hands-On](#state-hands-on) - [Terraform Structures and Syntax](#terraform-structures-and-syntax) - [Lists](#lists) - [Loops](#loops) - [Maps](#maps) - [Conditionals](#conditionals) - [Misc](#misc) - [Modules](#modules) - [Modules Hands-On](#modules-hands-on) - [Import](#import) - [Version Control](#version-control) - [AWS](#aws-1) - [Validations](#validations) -", "metadata": {"source_file": "learning-materials/topics/terraform/README.md", "section": "Terraform", "language": "en", "created_at": "2025-07-19T19:22:02.272751"}}
{"text": "[Secrets](#secrets) - [Production](#production)", "metadata": {"source_file": "learning-materials/topics/terraform/README.md", "section": "Terraform", "language": "en", "created_at": "2025-07-19T19:22:02.272790"}}
{"text": "<a name=\"exercises-terraform-101\"></a>", "metadata": {"source_file": "learning-materials/topics/terraform/README.md", "section": "Exercises", "language": "en", "created_at": "2025-07-19T19:22:02.272807"}}
{"text": "|Name|Topic|Objective & Instructions|Solution|Comments| |--------|--------|------|----|----| | Local Provider | Basics | [Exercise](exercises/terraform_local_provider/exercise.md) | [Solution](exercises/terraform_local_provider/solution.md) | |", "metadata": {"source_file": "learning-materials/topics/terraform/README.md", "section": "Terraform 101", "language": "en", "created_at": "2025-07-19T19:22:02.272826"}}
{"text": "The following exercises require account in AWS and might cost you $$$ |Name|Topic|Objective & Instructions|Solution|Comments| |--------|--------|------|----|----| | Launch EC2 instance | EC2 | [Exercise](exercises/launch_ec2_instance/exercise.md) | [Solution](exercises/launch_ec2_instance/solution.md) | | | Rename S3 bucket | S3 | [Exercise](exercises/s3_bucket_rename/exercise.md) | [Solution](exercises/s3_bucket_rename/solution.md) | |", "metadata": {"source_file": "learning-materials/topics/terraform/README.md", "section": "AWS", "language": "en", "created_at": "2025-07-19T19:22:02.272862"}}
{"text": "<details> <summary>What is Terraform?</summary><br><b> [Terraform](https://www.terraform.io/intro): \"HashiCorp Terraform is an infrastructure as code tool that lets you define both cloud and on-prem resources in human-readable configuration files that you can version, reuse, and share. You can then use a consistent workflow to provision and manage all of your infrastructure throughout its lifecycle. Terraform can manage low-level components like compute, storage, and networking resources, as well as high-level components like DNS entries and SaaS features.\" </b></details> <details> <summary>What are the advantages in using Terraform or IaC in general?</summary><br><b> - Full automation: In the past, resource creation, modification and removal were handled manually or by using a set of tooling. With Terraform or other IaC technologies, you manage the full lifecycle in an automated fashion.<br> - Modular and Reusable: Code that you write for certain purposes can be used and assembled in", "metadata": {"source_file": "learning-materials/topics/terraform/README.md", "section": "Terraform 101", "language": "en", "created_at": "2025-07-19T19:22:02.273734"}}
{"text": "different ways. You can write code to create resources on a public cloud and it can be shared with other teams who can also use it in their account on the same (or different) cloud><br> - Improved testing: Concepts like CI can be easily applied on IaC based projects and code snippets. This allow you to test and verify operations beforehand - </b></details> <details> <summary>What are some of Terraform features?</summary><br><b> - Declarative: Terraform uses the declarative approach (rather than the procedural one) in order to define end-status of the resources - No agents: as opposed to other technologies (e.g. Puppet) where you use a model of agent and server, with Terraform you use the different APIs (of clouds, services, etc.) to perform the operations - Community: Terraform has strong community who constantly publishes modules and fixes when needed. This ensures there is good modules maintenance and users can get support quite quickly at any point - </b></details> <details>", "metadata": {"source_file": "learning-materials/topics/terraform/README.md", "section": "Terraform 101", "language": "en", "created_at": "2025-07-19T19:22:02.273889"}}
{"text": "<summary>What language does Terraform uses?</summary><br><b> A DSL called \"HCL\" (Hashicorp Configuration Language). A declarative language for defining infrastructure. </b></details> <details> <summary>What's a typical Terraform workflow?</summary><br><b> 1. Write Terraform definitions: `.tf` files written in HCL that described the desired infrastructure state (and run `terraform init` at the very beginning) 2. Review: With command such as `terraform plan` you can get a glance at what Terraform will perform with the written definitions 3. Apply definitions: With the command `terraform apply` Terraform will apply the given definitions, by adding, modifying or removing the resources This is a manual process. Most of the time this is automated so user submits a PR/MR to propose terraform changes, there is a process to test these changes and once merged they are applied (`terraform apply`). </b></details> <details> <summary>What are some use cases for using Terraform?</summary><br><b> -", "metadata": {"source_file": "learning-materials/topics/terraform/README.md", "section": "Terraform 101", "language": "en", "created_at": "2025-07-19T19:22:02.273921"}}
{"text": "Infra provisioning and management: You need to automated or code your infra so you are able to test it easily, apply it and make any changes necessary. - Multi-cloud environment: You manage infrastructure on different clouds, but looking for a consistent way to do it across the clouds - Consistent environments: You manage environments such as test, production, staging, ... and looking for a way to have them consistent so any modification in one of them, applies to other environments as well </b></details> <details> <summary>What's the difference between Terraform and technologies such as Ansible, Puppet, Chef, etc.</summary><br><b> Terraform is considered to be an IaC technology. It's used for provisioning resources, for managing infrastructure on different platforms. Ansible, Puppet and Chef are Configuration Management technologies. They are used once there is an instance running and you would like to apply some configuration on it like installing an application, applying security", "metadata": {"source_file": "learning-materials/topics/terraform/README.md", "section": "Terraform 101", "language": "en", "created_at": "2025-07-19T19:22:02.273942"}}
{"text": "policy, etc. To be clear, CM tools can be used to provision resources so in the end goal of having infrastructure, both Terraform and something like Ansible, can achieve the same result. The difference is in the how. Ansible doesn't saves the state of resources, it doesn't know how many instances there are in your environment as opposed to Terraform. At the same time while Terraform can perform configuration management tasks, it has less modules support for that specific goal and it doesn't track the task execution state as Ansible. The differences are there and it's most of the time recommended to mix the technologies, so Terraform used for managing infrastructure and CM technologies used for configuration on top of that infrastructure. </b></details>", "metadata": {"source_file": "learning-materials/topics/terraform/README.md", "section": "Terraform 101", "language": "en", "created_at": "2025-07-19T19:22:02.273962"}}
{"text": "<details> <summary>Explain the following block of Terraform code ``` resource \"aws_instance\" \"some-instance\" { ami = \"ami-201720221991yay\" instance_type = \"t2.micro } ``` </summary><br><b> It's a resource of type \"aws_instance\" used to provision an instance. The name of the resource (NOT INSTANCE) is \"some-instance\". The instance itself will be provisioned with type \"t2.micro\" and using an image of the AMI \"ami-201720221991yay\". </b></details> <details> <summary>What do you do next after writing the following in main.tf file? ``` resource \"aws_instance\" \"some-instance\" { ami = \"ami-201720221991yay\" instance_type = \"t2.micro } ``` </summary><br><b> Run `terraform init`. This will scan the code in the directory to figure out which providers are used (in this case AWS provider) and will download them. </b></details> <details> <summary>You've executed <code>terraform init</code> and now you would like to move forward to creating the resources but you have concerns and would like to make be", "metadata": {"source_file": "learning-materials/topics/terraform/README.md", "section": "Terraform Hands-On Basics", "language": "en", "created_at": "2025-07-19T19:22:02.274386"}}
{"text": "100% sure on what you are going to execute. What should you be doing?</summary><br><b> Execute `terraform plan`. That will provide a detailed information on what Terraform will do once you apply the changes. </b></details> <details> <summary>You've downloaded the providers, seen the what Terraform will do (with terraform plan) and you are ready to actually apply the changes. What should you do next?</summary><br><b> Run `terraform apply`. That will apply the changes described in your .tf files. </b></details> <details> <summary>Explain the meaning of the following strings that seen at the beginning of each line When you run <code>terraform apply</code> * '+' * '-' * '-/+' </summary><br><b> * '+' - The resource or attribute is going to be added * '-' - the resource or attribute is going to be removed * '-/+' - the resource or attribute is going to be replaced </b></details> <details> <summary>How to cleanup Terraform resources? Why the user should be careful doing so?</summary><br><b>", "metadata": {"source_file": "learning-materials/topics/terraform/README.md", "section": "Terraform Hands-On Basics", "language": "en", "created_at": "2025-07-19T19:22:02.274410"}}
{"text": "`terraform destroy` will cleanup all the resources tracked by Terraform. A user should be careful with this command because there is no way to revert it. Sure, you can always run again \"apply\" but that can take time, generates completely new resources, etc. </b></details>", "metadata": {"source_file": "learning-materials/topics/terraform/README.md", "section": "Terraform Hands-On Basics", "language": "en", "created_at": "2025-07-19T19:22:02.274430"}}
{"text": "<details> <summary>Sometimes you need to reference some resources in the same or separate .tf file. Why and how it's done?</summary><br><b> Why: because resources are sometimes connected or need to be connected. For example, you create an AWS instance with \"aws_instance\" resource but, at the same time you would like also to allow some traffic to it (because by default traffic is not allowed). For that you'll create a \"aws_security_group\" resource and then, in your aws_instance resource, you'll reference it. How: Using the syntax <PROVIDER TYPE>.<NAME>.<ATTRIBUTE> In your AWS instance it would like that: ``` resource \"aws_instance\" \"some-instance\" { ami = \"some-ami\" instance_type = \"t2.micro\" vpc_security_group_ids = [aws_security_group.instance.id] } ``` </b></details> <details> <summary>Does it matter in which order Terraform creates resources?</summary><br><b> Yes, when there is a dependency between different Terraform resources, you want the resources to be created in the right", "metadata": {"source_file": "learning-materials/topics/terraform/README.md", "section": "Dependencies", "language": "en", "created_at": "2025-07-19T19:22:02.274667"}}
{"text": "order and this is exactly what Terraform does. To make it ever more clear, if you have a resource X that references the ID of resource Y, it doesn't makes sense to create first resource X because it won't have any ID to get from a resource that wasn't created yet. </b></details> <details> <summary>Is there a way to print/see the dependencies between the different resources?</summary><br><b> Yes, with `terraform graph` The output is in DOT - A graph description language. </b></details>", "metadata": {"source_file": "learning-materials/topics/terraform/README.md", "section": "Dependencies", "language": "en", "created_at": "2025-07-19T19:22:02.274914"}}
{"text": "<details> <summary>Explain what is a \"provider\"</summary><br><b> [terraform.io](https://www.terraform.io/docs/language/providers/index.html): \"Terraform relies on plugins called \"providers\" to interact with cloud providers, SaaS providers, and other APIs...Each provider adds a set of resource types and/or data sources that Terraform can manage. Every resource type is implemented by a provider; without providers, Terraform can't manage any kind of infrastructure.\" </b></details> <details> <summary>Where can you find publicly available providers?</summary><br><b> In the [Terraform Registry](https://registry.terraform.io/browse/providers) </b></details> <details> <summary>What are the names of the providers in this case? ``` terraform { required_providers { aws = { source = \"hashicorp/aws\" } azurerm = { source = \"hashicorp/azurerm\" version = \"~> 3.0.2\" } } } ``` </summary><br><b> azurerm and aws </b></details> <details> <summary>How to install a provider? </summary><br><b> You write a", "metadata": {"source_file": "learning-materials/topics/terraform/README.md", "section": "Providers", "language": "en", "created_at": "2025-07-19T19:22:02.275221"}}
{"text": "provider block like the following one and run `terraform init` ``` provider \"aws\" { region = \"us-west-1\" } ``` </b></details> <details> <summary>True or False? Applying the following Terraform configuration will fail since no source or version specific for 'aws' provider ``` terraform { required_providers { aws = {} } } ``` </summary><br><b> False. It will look for \"aws\" provider in the public Terraform registry and will take the latest version. </b></details> <details> <summary>Write a configuration of a Terraform provider (any type you would like)</summary><br><b> AWS is one of the most popular providers in Terraform. Here is an example of how to configure it to use one specific region and specifying a specific version of the provider ``` terraform { required_providers { aws = { source = \"hashicorp/aws\" version = \"~> 3.0\" } } }", "metadata": {"source_file": "learning-materials/topics/terraform/README.md", "section": "Providers", "language": "en", "created_at": "2025-07-19T19:22:02.275244"}}
{"text": "provider \"aws\" { region = \"us-west-2\" } ``` </b></details> <details> <summary>Where Terraform installs providers from by default? </summary><br><b> By default Terraform providers are installed from Terraform Registry </b></details> <details> <summary>What is the Terraform Registry?</summary><br><b> The Terraform Registry provides a centralized location for official and community-managed providers and modules. </b></details> <details> <summary>Where providers are downloaded to? (when for example you run <code>terraform init</code>)</summary><br><b> `.terraform` directory. </b></details> <details> <summary>Describe in high level what happens behind the scenes when you run terraform init on on the following Terraform configuration ``` terraform { required_providers { aws = { source = \"hashicorp/aws\" version = \"~> 3.0\" } } } ``` </summary><br><b> 1. Terraform checks if there is an aws provider in this address: `registry.terraform.io/hashicorp/aws` 2. Installs latest version of aws provider", "metadata": {"source_file": "learning-materials/topics/terraform/README.md", "section": "Configure the AWS Provider", "language": "en", "created_at": "2025-07-19T19:22:02.275425"}}
{"text": "(assuming the URL exists and valid) </b></details> <details> <summary>True or False? You can install providers only from hashicorp</summary><br><b> False. You can specify any provider from any URL, not only those from hashicorp. </b></details>", "metadata": {"source_file": "learning-materials/topics/terraform/README.md", "section": "Configure the AWS Provider", "language": "en", "created_at": "2025-07-19T19:22:02.275446"}}
{"text": "<details> <summary>What are input variables good for in Terraform?</summary><br><b> Variables allow you define piece of data in one location instead of repeating the hardcoded value of it in multiple different locations. Then when you need to modify the variable's value, you do it in one location instead of changing each one of the hardcoded values. </b></details> <details> <summary>What type of input variables are supported in Terraform?</summary><br><b> ``` string number bool list(<TYPE>) set(<TYPE>) map(<TYPE>) object({<ATTR_NAME> = <TYPE>, ... }) tuple([<TYPE>, ...]) ``` </b></details> <details> <summary>What's the default input variable type in Terraform?</summary><br><b> `any` </b></details> <details> <summary>What ways are there to pass values for input variables?</summary><br><b> * Using `-var` option in the CLI * Using a file by using the `-var-file` option in the CLI * Environment variable that starts with `TF_VAR_<VAR_NAME>` If no value given, user will be prompted to", "metadata": {"source_file": "learning-materials/topics/terraform/README.md", "section": "Input Variables", "language": "en", "created_at": "2025-07-19T19:22:02.275882"}}
{"text": "provide one. </b></details> <details> <summary>How to reference a variable?</summary><br><b> Using the syntax `var.<VAR_NAME>` </b></details> <details> <summary>What is the effect of setting variable as \"sensitive\"?</summary><br><b> It doesn't show its value when you run `terraform apply` or `terraform plan` but eventually it's still recorded in the state file. </b></details> <details> <summary>True or False? If an expression's result depends on a sensitive variable, it will be treated as sensitive as well</summary><br><b> True </b></details> <details> <summary>The same variable is defined in the following places: - The file `terraform.tfvars` - Environment variable - Using `-var` or `-var-file` According to variable precedence, which source will be used first?</summary><br><b> Terraform loads variables in the following order, with later sources taking precedence over earlier ones: - Environment variable - The file `terraform.tfvars` - Using `-var` or `-var-file` </b></details>", "metadata": {"source_file": "learning-materials/topics/terraform/README.md", "section": "Input Variables", "language": "en", "created_at": "2025-07-19T19:22:02.275911"}}
{"text": "<details> <summary>Whenever you run terraform apply, it prompts to enter a value for a given variable. How to avoid being prompted?</summary><br><b> While removing the variable is theoretically a correct answer, it will probably fail the execution. You can use something like the `-var` option to provide the value and avoid being prompted to insert a value. Another option is to run `export TF_VAR_<VAR_NAME>=<VALUE>`. </b></details>", "metadata": {"source_file": "learning-materials/topics/terraform/README.md", "section": "Input Variables", "language": "en", "created_at": "2025-07-19T19:22:02.275931"}}
{"text": "<details> <summary>What are output variables? Why do we need them?</summary><br><b> Output variable allow you to display/print certain piece of data as part of Terraform execution. The most common use case for it is probably to print the IP address of an instance. Imagine you provision an instance and you would like to know what the IP address to connect to it. Instead of looking for it for the console/OS, you can use the output variable and print that piece of information to the screen </b></details> <details> <summary>Explain the \"sensitive\" parameter of output variable</summary><br><b> When set to \"true\", Terraform will avoid logging output variable's data. The use case for it is sensitive data such as password or private keys. </b></details> <details> <summary>Explain the \"depends\" parameter of output variable</summary><br><b> It is used to set explicitly dependency between the output variable and any other resource. Use case: some piece of information is available only once", "metadata": {"source_file": "learning-materials/topics/terraform/README.md", "section": "Output Variables", "language": "en", "created_at": "2025-07-19T19:22:02.276134"}}
{"text": "another resource is ready. </b></details>", "metadata": {"source_file": "learning-materials/topics/terraform/README.md", "section": "Output Variables", "language": "en", "created_at": "2025-07-19T19:22:02.276271"}}
{"text": "<details> <summary>What are locals?</summary><br><b> Similarly to variables they serve as placeholders for data and values. Differently from variables, users can't override them by passing different values. </b></details> <details> <summary>What's the use case for using locals?</summary><br><b> You have multiple hardcoded values that repeat themselves in different sections, but at the same time you don't want to expose them as in, allow users to override values. </b></details>", "metadata": {"source_file": "learning-materials/topics/terraform/README.md", "section": "Locals", "language": "en", "created_at": "2025-07-19T19:22:02.276342"}}
{"text": "<details> <summary>Demonstrate input variable definition with type, description and default parameters</summary><br><b> ``` variable \"app_id\" { type = string description = \"The id of application\" default = \"some_value\" } ``` Unrelated note: variables are usually defined in their own file (vars.tf for example). </b></details> <details> <summary>How to define an input variable which is an object with attributes \"model\" (string), \"color\" (string), year (number)?</summary><br><b> ``` variable \"car_model\" { description = \"Car model object\" type = object({ model = string color = string year = number }) } ``` Note: you can also define a default for it. </b></details> <details> <summary>How to reference variables?</summary><br><b> Variable are referenced with `var.VARIABLE_NAME` syntax. Let's have a look at an example: vars.tf: ``` variable \"memory\" { type = string default \"8192\" } variable \"cpu\" { type = string default = \"4\" } ``` main.tf: ``` resource \"libvirt_domain\" \"vm1\" { name = \"vm1\"", "metadata": {"source_file": "learning-materials/topics/terraform/README.md", "section": "Variables Hands-On", "language": "en", "created_at": "2025-07-19T19:22:02.276733"}}
{"text": "memory = var.memory cpu = var.cpu } ``` </b></details> <details> <summary>How to reference variable from inside of string literal? (bonus question: how that type of expression is called?)</summary><br><b> Using the syntax: `\"${var.VAR_NAME}\"`. It's called \"interpolation\". Very common to see it used in user_data attribute related to instances. ``` user_data = <<-EOF This is some fabulos string It demonstrates how to use interpolation Yes, it's truly ${var.awesome_or_meh} EOF ``` </b></details> <details> <summary>How can list all outputs without applying Terraform changes?</summary><br><b> `terraform output` will list all outputs without applying any changes </b></details> <details> <summary>Can you see the output of specific variable without applying terrafom changes?</summary><br><b> Yes, with `terraform output <OUTPUT_VAR>`. Very useful for scripts :) </b></details> <details> <summary>Demonstrate how to define locals</summary><br><b> ``` locals { x = 2 y = \"o\" z = 2.2 } ```", "metadata": {"source_file": "learning-materials/topics/terraform/README.md", "section": "Variables Hands-On", "language": "en", "created_at": "2025-07-19T19:22:02.276764"}}
{"text": "</b></details> <details> <summary>Demonstrate how to use a local</summary><br><b> if we defined something like this ``` locals { x = 2 } ``` then to use it, you have to use something like this: `local.x` </b></details>", "metadata": {"source_file": "learning-materials/topics/terraform/README.md", "section": "Variables Hands-On", "language": "en", "created_at": "2025-07-19T19:22:02.276784"}}
{"text": "<details> <summary>Explain data sources in Terraform</summary><br><b> * Data sources used to get data from providers or in general from external resources to Terraform (e.g. public clouds like AWS, GCP, Azure). * Data sources used for reading. They are not modifying or creating anything * Many providers expose multiple data sources </b></details> <details> <summary>Demonstrate how to use data sources</summary><br><b> ``` data \"aws_vpc\" \"default { default = true } ``` </b></details> <details> <summary>How to get data out of a data source?</summary><br><b> The general syntax is `data.<PROVIDER_AND_TYPE>.<NAME>.<ATTRIBUTE>` So if you defined the following data source ``` data \"aws_vpc\" \"default { default = true } ``` You can retrieve the ID attribute this way: `data.aws_vpc.default.id` </b></details> <details> <summary>Is there such a thing as combining data sources? What would be the use case?</summary><br><b> Yes, you can define a data source while using another data source as a filter", "metadata": {"source_file": "learning-materials/topics/terraform/README.md", "section": "Data Sources", "language": "en", "created_at": "2025-07-19T19:22:02.276985"}}
{"text": "for example. Let's say we want to get AWS subnets but only from our default VPC: ``` data \"aws_subnets\" \"default\" { filter { name = \"vpc-id\" values = [data.aws_vpc.default.id] } } ``` </b></details>", "metadata": {"source_file": "learning-materials/topics/terraform/README.md", "section": "Data Sources", "language": "en", "created_at": "2025-07-19T19:22:02.277005"}}
{"text": "<details> <summary>When you update a resource, how it works?</summary><br><b> By default the current resource is deleted, a new one is created and any references pointing the old resource are updated to point the new resource </b></details> <details> <summary>Is it possible to modify the default lifecycle? How? Why?</summary><br><b> Yes, it's possible. There are different lifecycles one can choose from. For example \"create_before_destroy\" which inverts the order and first creates the new resource, updates all the references from old resource to the new resource and then removes the old resource. How to use it: ``` lifecycle { create_before_destroy = true } ``` Why to use it in the first place: you might have resources that have dependency where they dependency itself is immutable (= you can't modify it hence you have to create a new one), in such case the default lifecycle won't work because you won't be able to remove the resource that has the dependency as it still references an old", "metadata": {"source_file": "learning-materials/topics/terraform/README.md", "section": "Lifecycle", "language": "en", "created_at": "2025-07-19T19:22:02.277246"}}
{"text": "resource. AWS ASG + launch configurations is a good example of such use case. </b></details> <details> <summary>You've deployed a virtual machine with Terraform and you would like to pass data to it (or execute some commands). Which concept of Terraform would you use?</summary><br><b> [Provisioners](https://www.terraform.io/docs/language/resources/provisioners) </b></details>", "metadata": {"source_file": "learning-materials/topics/terraform/README.md", "section": "Lifecycle", "language": "en", "created_at": "2025-07-19T19:22:02.277267"}}
{"text": "<details> <summary>What are \"Provisioners\"? What they are used for?</summary><br><b> Provisioners can be described as plugin to use with Terraform, usually focusing on the aspect of service configuration and make it operational. Few example of provisioners: * Run configuration management on a provisioned instance using technology like Ansible, Chef or Puppet. * Copying files * Executing remote scripts </b></details> <details> <summary>Why is it often recommended to use provisioners as last resort?</summary><br><b> Since a provisioner can run a variety of actions, it's not always feasible to plan and understand what will happen when running a certain provisioner. For this reason, it's usually recommended to use Terraform built-in option, whenever's possible. </b></details> <details> <summary>What is <code>local-exec</code> and <code>remote-exec</code> in the context of provisioners?</summary><br><b> <code>local-exec</code> provisioners run commands on the machine where Terraform is", "metadata": {"source_file": "learning-materials/topics/terraform/README.md", "section": "Provisioners", "language": "en", "created_at": "2025-07-19T19:22:02.277966"}}
{"text": "executed, while <code>remote-exec</code> provisioners run commands on the remote resource. </b></details> <details> <summary>What is a \"tainted resource\"?</summary><br><b> It's a resource which was successfully created but failed during provisioning. Terraform will fail and mark this resource as \"tainted\". </b></details> <details> <summary>What <code>terraform taint</code> does?</summary><br><b> <code>terraform taint resource.id</code> manually marks the resource as tainted in the state file. So when you run <code>terraform apply</code> the next time, the resource will be destroyed and recreated. </b></details> <details> <summary>What is a data source? In what scenarios for example would need to use it?</summary><br><b> Data sources lookup or compute values that can be used elsewhere in terraform configuration. There are quite a few cases you might need to use them: * you want to reference resources not managed through terraform * you want to reference resources managed by a different", "metadata": {"source_file": "learning-materials/topics/terraform/README.md", "section": "Provisioners", "language": "en", "created_at": "2025-07-19T19:22:02.277996"}}
{"text": "terraform module * you want to cleanly compute a value with typechecking, such as with <code>aws_iam_policy_document</code> </b></details> <details> <summary>What are output variables and what <code>terraform output</code> does?</summary><br><b> Output variables are named values that are sourced from the attributes of a module. They are stored in terraform state, and can be used by other modules through <code>remote_state</code> </b></details> <details> <summary>Explain \"Remote State\". When would you use it and how?</summary><br><b> Terraform generates a `terraform.tfstate` json file that describes components/service provisioned on the specified provider. Remote State stores this file in a remote storage media to enable collaboration amongst team. </b></details> <details> <summary>Explain \"State Locking\"</summary><br><b> State locking is a mechanism that blocks an operations against a specific state file from multiple callers so as to avoid conflicting operations from different team", "metadata": {"source_file": "learning-materials/topics/terraform/README.md", "section": "Provisioners", "language": "en", "created_at": "2025-07-19T19:22:02.278290"}}
{"text": "members. Once the first caller's operation's lock is released the other team member may go ahead to carryout his own operation. Nevertheless Terraform will first check the state file to see if the desired resource already exist and if not it goes ahead to create it. </b></details> <details> <summary>Aside from <code>.tfvars</code> files or CLI arguments, how can you inject dependencies from other modules?</summary><br><b> The built-in terraform way would be to use <code>remote-state</code> to lookup the outputs from other modules. It is also common in the community to use a tool called <code>terragrunt</code> to explicitly inject variables between modules. </b></details> <details> <summary>How do you import existing resource using Terraform import?</summary><br><b> 1. Identify which resource you want to import. 2. Write terraform code matching configuration of that resource. 3. Run terraform command <code>terraform import RESOURCE ID</code><br> eg. Let's say you want to import an aws", "metadata": {"source_file": "learning-materials/topics/terraform/README.md", "section": "Provisioners", "language": "en", "created_at": "2025-07-19T19:22:02.278323"}}
{"text": "instance. Then you'll perform following: 1. Identify that aws instance in console 2. Refer to it's configuration and write Terraform code which will look something like: ``` resource \"aws_instance\" \"tf_aws_instance\" { ami = data.aws_ami.ubuntu.id instance_type = \"t3.micro\" tags = { Name = \"import-me\" } } ``` 3. Run terraform command <code>terraform import aws_instance.tf_aws_instance i-12345678</code> </b></details>", "metadata": {"source_file": "learning-materials/topics/terraform/README.md", "section": "Provisioners", "language": "en", "created_at": "2025-07-19T19:22:02.278344"}}
{"text": "<details> <summary>What's Terraform State?</summary><br><b> [terraform.io](https://www.terraform.io/language/state): \"Terraform must store state about your managed infrastructure and configuration. This state is used by Terraform to map real world resources to your configuration, keep track of metadata, and to improve performance for large infrastructures.\" In other words, it's a mechanism in Terraform to track resources you've created or cleaned up. This is how terraform knows what to update/create/delete when you run `terraform apply` and also other commands like `terraform destroy`. </b></details> <details> <summary>Where Terraform state is stored?</summary><br><b> There is more than one answer to this question. It's very much depends on whether you share it with others or it's only local in your Terraform directory, but taking a beginner's case, when you run terraform in a directory, the state will be stored in that directory in `terraform.tfstate` file. </b></details> <details>", "metadata": {"source_file": "learning-materials/topics/terraform/README.md", "section": "State", "language": "en", "created_at": "2025-07-19T19:22:02.279318"}}
{"text": "<summary>Can you name three different things included in the state file?</summary><br><b> * The representation of resources - JSON format of the resources, their attributes, IDs, ... everything that required to identify the resource and also anything that was included in the .tf files on these resources * Terraform version * Outputs </b></details> <details> <summary>Why does it matter where you store the tfstate file? In your answer make sure to address the following: * Public vs. Private * Git repository vs. Other locations </summary><br><b> - tfstate contains credentials in plain text. You don't want to put it in publicly shared location - tfstate shouldn't be modified concurrently so putting it in a shared location available for everyone with \"write\" permissions might lead to issues. (Terraform remote state doesn't has this problem). - tfstate is an important file. As such, it might be better to put it in a location that has regular backups and good security. As such, tfstate", "metadata": {"source_file": "learning-materials/topics/terraform/README.md", "section": "State", "language": "en", "created_at": "2025-07-19T19:22:02.279353"}}
{"text": "shouldn't be stored in git repositories. secured storage such as secured buckets, is a better option. </b></details> <details> <summary>True or False? it's common to edit terraform state file directly by hand and even recommended for many different use cases</summary><br><b> False. You should avoid as much possible to edit Terraform state files directly by hand. </b></details> <details> <summary>Why storing state file locally on your computer may be problematic?</summary><br><b> In general, storing state file on your computer isn't a problem. It starts to be a problem when you are part of a team that uses Terraform and then you would like to make sure it's shared. In addition to being shared, you want to be able to handle the fact that different teams members can edit the file and can do it at the same time, so locking is quite an important aspect as well. </b></details> <details> <summary>Mention some best practices related to tfstate</summary><br><b> - Don't edit it manually. tfstate", "metadata": {"source_file": "learning-materials/topics/terraform/README.md", "section": "State", "language": "en", "created_at": "2025-07-19T19:22:02.279375"}}
{"text": "was designed to be manipulated by terraform and not by users directly. - Store it in secured location (since it can include credentials and sensitive data in general) - Backup it regularly so you can roll-back easily when needed - Store it in remote shared storage. This is especially needed when working in a team and the state can be updated by any of the team members - Enabled versioning if the storage where you store the state file, supports it. Versioning is great for backups and roll-backs in case of an issue. </b></details> <details> <summary>How and why concurrent edits of the state file should be avoided?</summary><br><b> If there are two users or processes concurrently editing the state file it can result in invalid state file that doesn't actually represents the state of resources.<br> To avoid that, Terraform can apply state locking if the backend supports that. For example, AWS s3 supports state locking and consistency via DynamoDB. Often, if the backend supports it,", "metadata": {"source_file": "learning-materials/topics/terraform/README.md", "section": "State", "language": "en", "created_at": "2025-07-19T19:22:02.279394"}}
{"text": "Terraform will make use of state locking automatically so nothing is required from the user to activate it. </b></details> <details> <summary>Describe how you manage state file(s) when you have multiple environments (e.g. development, staging and production)</summary><br><b> Probably no right or wrong answer here, but it seems, based on different source, that the overall preferred way is to have a dedicated state file per environment. </b></details> <details> <summary>Why storing the state in versioned control repo is not a good idea?</summary><br><b> * Sensitive data: some resources may specify sensitive data (like passwords and tokens) and everything in a state file is stored in plain text * Prone to errors: when working with Git repos, you mayo often find yourself switch branches, checkout specific commits, perform rebases, ... all these operations may end up in you eventually performing `terraform apply` on non-latest version of your Terraform code </b></details>", "metadata": {"source_file": "learning-materials/topics/terraform/README.md", "section": "State", "language": "en", "created_at": "2025-07-19T19:22:02.279414"}}
{"text": "<details> <summary>What's a Terraform backend? What is the default backend?</summary><br><b> Terraform backend determines how the Terraform state is stored and loaded. By default the state is local, but it's possible to set a remote backend </b></details> <details> <summary>Describe how to set a remote backend of any type you choose</summary><br><b> Let's say we chose use Amazon s3 as a remote Terraform backend where we can store Terraform's state. 1. Write Terraform code for creating an s3 bucket 1. It would be a good idea to add lifecycle of \"prevent_destroy\" to it so it's not accidentally deleted 2. Enable versioning (add a resource of \"aws_s3_bucket_versioning\") 3. Encrypt the bucket (\"aws_s3_bucket_server_side_encryption_configuration\") 4. Block public access 5. Handle locking. One way is to add DB for it 6. Add the point you'll want to run init and apply commands to avoid an issue where you at the same time create the resources for remote backend and also switch to a remote", "metadata": {"source_file": "learning-materials/topics/terraform/README.md", "section": "Terraform Backend", "language": "en", "created_at": "2025-07-19T19:22:02.280222"}}
{"text": "backend 7. Once resources were created, add Terraform backend code ``` terraform { backend \"s3\" { bucket ... } } ``` 7. Run `terraform init` as it will configure the backend </b></details> <details> <summary>How <code>terraform apply</code> workflow is different when a remote backend is used?</summary><br><b> It starts with acquiring a state lock so others can't modify the state at the same time. </b></details> <details> <summary>What would be the process of switching back from remote backend to local?</summary><br><b> 1. You remove the backend code and perform `terraform init` to switch back to `local` backend 2. You remove the resources that are the remote backend itself </b></details> <details> <summary>True or False? it's NOT possible to use variable in a backend configuration</summary><br><b> That's true and quite a limitation as it means you'll have to go to the resources of the remote backend and copy some values to the backend configuration. One way to deal with it is using", "metadata": {"source_file": "learning-materials/topics/terraform/README.md", "section": "Terraform Backend", "language": "en", "created_at": "2025-07-19T19:22:02.280261"}}
{"text": "partial configurations in a completely separate file from the backend itself and then load them with `terraform init -backend-config=some_backend_partial_conf.hcl` </b></details> <details> <summary>Is there a way to obtain information from a remote backend/state using Terraform?</summary><br><b> Yes, using the concept of data sources. There is a data source for a remote state called \"terraform_remote_state\". You can use it the following syntax `data.terraform_remote_state.<NAME>.outputs.<ATTRIBUTE>` </b></details>", "metadata": {"source_file": "learning-materials/topics/terraform/README.md", "section": "Terraform Backend", "language": "en", "created_at": "2025-07-19T19:22:02.280282"}}
{"text": "<details> <summary>Explain what is a Terraform workspace</summary><br><b> [developer.hashicorp.com](https://developer.hashicorp.com/terraform/language/state/workspaces): \"The persistent data stored in the backend belongs to a workspace. The backend initially has only one workspace containing one Terraform state associated with that configuration. Some backends support multiple named workspaces, allowing multiple states to be associated with a single configuration.\" </b></details> <details> <summary>True or False? Each workspace has its own state file</summary><br><b> True </b></details> <details> <summary>Why workspaces might not be the best solution for managing states for different environments? like staging and production</summary><br><b> One reason is that all the workspaces are stored in one location (as in one backend) and usually you don't want to use the same access control and authentication for both staging and production for obvious reasons. Also working in workspaces is", "metadata": {"source_file": "learning-materials/topics/terraform/README.md", "section": "Workspaces", "language": "en", "created_at": "2025-07-19T19:22:02.280454"}}
{"text": "quite prone to human errors as you might accidentally think you are in one workspace, while you are working a completely different one. </b></details>", "metadata": {"source_file": "learning-materials/topics/terraform/README.md", "section": "Workspaces", "language": "en", "created_at": "2025-07-19T19:22:02.280475"}}
{"text": "<details> <summary>Which command will produce a state file?</summary><br><b> `terraform apply` </b></details> <details> <summary>How to inspect current state?</summary><br><b> `terraform show` </b></details> <details> <summary>How to list resources created with Terraform?</summary><br><b> `terraform state list` </b></details> <details> <summary>How do you rename an existing resource?</summary><br><b> `terraform state mv` </b></details> <details> <summary>How to create a new workspace?</summary><br><b> `terraform workspace new <WORKSPACE_NAME>` </b></details> <details> <summary>How to identify which workspace are you using?</summary><br><b> `terraform workspace show` </b></details>", "metadata": {"source_file": "learning-materials/topics/terraform/README.md", "section": "State Hands-On", "language": "en", "created_at": "2025-07-19T19:22:02.280540"}}
{"text": "<details> <summary>How to define an input variable which is a list of numbers?</summary><br><b> ``` variable \"list_of_nums\" { type = list(number) description = \"An example of list of numbers\" default = [2, 0, 1, 7] } ``` </b></details> <details> <summary>How to create a number of resources based on the length of a list?</summary><br><b> ``` resource \"some_resource\" \"some_name\" { count = length(var.some_list) } ``` </b></details> <details> <summary>You have a list variable called \"users\" with an object containing a name attribute like this:<br> ``` variable \"users\" { type = list(object({ name = string age = number })) } ``` How to access the name attribute of the second item in that list?</summary><br><b> `users[1].name` </b></details> <details> <summary>Given the same list, how to access attribute \"name\" of all items?</summary><br><b> `users[*].name` </b></details>", "metadata": {"source_file": "learning-materials/topics/terraform/README.md", "section": "Lists", "language": "en", "created_at": "2025-07-19T19:22:02.280689"}}
{"text": "<details> <summary>What loops are useful for in Terraform?</summary><br><b> The most common use case is when you need to create multiple resources with only a slight difference (like different name). Instead of defining multiple separate resources, you can define it once and create multiple instances of it using loops. </b></details> <details> <summary>Demonstrate how to define a simple Terraform loop</summary><br><b> ``` resource \"aws_instance\" \"server\" { count = 15 } ``` The above configuration will create 15 aws instances. </b></details> <details> <summary>How to create multiple AWS instances but each with a different name?</summary><br><b> ``` resource \"aws_instance\" \"server\" { count = 6 tags = { Name = \"instance-${count.index}\" } } ``` The above configuration will create 6 instances, each with a different name. </b></details> <details> <summary>You have the following variable defined in Terraform ``` variable \"users\" { type = list(string) default = [\"mario\", \"luigi\", \"peach\"] }", "metadata": {"source_file": "learning-materials/topics/terraform/README.md", "section": "Loops", "language": "en", "created_at": "2025-07-19T19:22:02.281768"}}
{"text": "``` How to use it to create users on one of the public clouds (or any other platform, infra)? </summary><br><b> ``` resource \"aws_iam_user\" \"user\" { count = length(var.users) name = var.users[count.index] } ``` </b></details> <details> <summary>Is there any limitation to \"count\" meta-argument?</summary><br><b> * `count` isn't supported within an inline block * It's quite limited when it comes to lists.You'll notice that modifying items in lists or even operations like removal sometimes interpreted in a way you didn't expect. For example, removing an item from a list, may shift other items to a new position and since each position represents a resource with count, that may lead to a result where wrong resources are being modified and removed. There are ways to do deal it, but still using count with lists is not always straightforward </b></details> <details> <summary>What's a for_each loop? How is it different from \"count\"?</summary><br><b> * for_each can applied only on collections", "metadata": {"source_file": "learning-materials/topics/terraform/README.md", "section": "Loops", "language": "en", "created_at": "2025-07-19T19:22:02.281803"}}
{"text": "like maps or sets (as opposed to count that can be applied on lists) * for_each helps to deal with the limitation of `count` which isn't optimal for use cases of modifying lists * for_each supports inline blocks as opposed to `count` </b></details> <details> <summary>Demonstrate how to use the for_each loop</summary><br><b> ``` resource “google_compute_instance” “instances” { for_each = var.names_map name = each.value } ``` </b></details> <details> <summary>The following resource tries to use for_each loop on a list of strings but it fails, why? ``` resource “google_compute_instance” “instances” { for_each = var.names name = each.value } ``` </summary><br><b> for_each can applied only on collections like maps or sets so the list should be converted to a set like this: `for_each = toset(var.names)` </b></details> <details> <summary>How to use for_each loop for inline blocks?</summary><br><b> ``` resource \"some_instance\" \"instance\" { dynamic \"tag\" { for_each = var.tags content { key =", "metadata": {"source_file": "learning-materials/topics/terraform/README.md", "section": "Loops", "language": "en", "created_at": "2025-07-19T19:22:02.282022"}}
{"text": "tag.key value = tag.value } } } ``` </b></details> <details> <summary>There is a list variable called \"users\". You would like to define an output variable with a value of all users in uppercase. How to achieve that?</summary><br><b> ``` output \"users\" { value = [for name in var.user_names : upper(name)] } ``` </b></details> <details> <summary>What's the result of the following code? ``` resource \"random_integer\" \"num\" { min = 20 max = 17 } resource \"aws_instance\" \"instances\" { count = random_integer.num.results } ``` </summary><br><b> The above code will fail as it's not possible to reference resource outputs with count, because Terraform has to compute count before any resources are created (or modified). </b></details> <details> <summary>There is a variable called \"values\" with the following value: [\"mario\", \"luigi\", \"peach\"]. How to create an output variable with the string value of the items in the list: \"mario, luigi, peach,\" ?</summary><br><b> ``` output \"users\" { value = \"%{ for", "metadata": {"source_file": "learning-materials/topics/terraform/README.md", "section": "Loops", "language": "en", "created_at": "2025-07-19T19:22:02.282059"}}
{"text": "name in var.values }${name}, %{ endfor }\" } ``` </b></details> <details> <summary>There is a list variable called \"users\". You would like to define an output variable with a value of all users in uppercase but only if the name is longer than 3 characters. How to achieve that?</summary><br><b> ``` output \"users\" { value = [for name in var.user_names : upper(name) if length(name) > 3] } ``` </b></details>", "metadata": {"source_file": "learning-materials/topics/terraform/README.md", "section": "Loops", "language": "en", "created_at": "2025-07-19T19:22:02.282081"}}
{"text": "<details> <summary>There is a map called \"instances\" * How to extract only the values of that map? * How to extract only the attribute \"name\" from each value? </summary><br><b> * Using the values built-in function: `values(instances)` * `values(instances)[*].name` </b></details> <details> <summary>You have a map variable, called \"users\", with the keys \"name\" and \"age\". Define an output list variable with the following \"my name is {name} and my age is {age}\"</summary><br><b> ``` output \"name_and_age\" { value = [for name, age in var.users : \"my name is ${name} and my age is ${age}\"] } ``` </b></details> <details> <summary>You have a map variable, called \"users\", with the keys \"name\" (string) and \"age\" (number). Define an output map variable with the key being name in uppercase and value being age in the closest whole number </summary><br><b> ``` output \"name_and_age\" { value = {for name, age in var.users : upper(name) => floor(age) } ``` </b></details>", "metadata": {"source_file": "learning-materials/topics/terraform/README.md", "section": "Maps", "language": "en", "created_at": "2025-07-19T19:22:02.282282"}}
{"text": "<details> <summary>How to use conditional expressions in Terraform?</summary><br><b> `some_condition ? \"value_if_true\" : \"value_if_false\"` </b></details> <details> <summary>Explain the following condition: <code>var.x ? 1 : 0</code></summary><br><b> If `x` evaluated to true, the result is 1, otherwise (if false) the result is 0. </b></details> <details> <summary>Explain the following condition: <code>var.x != \"\" ? var.x : \"yay\"</code></summary><br><b> If `x` is an empty string the result is \"yay\", otherwise it's the value of `x` variable </b></details> <details> <summary>Can conditionals be used with meta-arguments?</code></summary><br><b> Yes, for example the \"count\" meta-argument: ``` resource \"aws_instance\" \"server\" { count = var.amount ? 1 : 0 ... } ``` </b></details> <details> <summary>Is it possible to combine conditionals and loop?</code></summary><br><b> Yes, for example: ``` dynamic \"tag\" { for_each = { for key, value in var.tags: key => value if key != \"\" } } ```", "metadata": {"source_file": "learning-materials/topics/terraform/README.md", "section": "Conditionals", "language": "en", "created_at": "2025-07-19T19:22:02.282467"}}
{"text": "<details> <summary>What are meta-arguments in Terraform?</summary><br><b> Arguments that affect the lifecycle of a resources (its creation, modification, ...) and supported by Terraform regardless to the type of resource in which they are used. Some examples: * count: how many resources to create out of one definition of a resource * lifecycle: how to treat resource creation or removal </b></details> <details> <summary>What meta-arguments are you familiar with?</summary><br><b> * count: how many resources to create out of one definition of a resource * lifecycle: how to treat resource creation or removal * depends_on: create a dependency between resources </b></details> <details> <summary>What <code>templatefile</code> function does?</summary><br><b> Renders a template file and returns the result as string. </b></details> <details> <summary>You are trying to use templatefile as part of a module and you use a relative path to load a file but sometimes it fails, especially when others", "metadata": {"source_file": "learning-materials/topics/terraform/README.md", "section": "Misc", "language": "en", "created_at": "2025-07-19T19:22:02.282890"}}
{"text": "try to reuse the module. How can you deal with that?</summary><br><b> Switch relative paths with what is known as path references. These are fixes: paths like module root path, module expression file path, etc. </b></details> <details> <summary>How do you test terraform syntax?</summary><br><b> A valid answer could be \"I write Terraform configuration and try to execute it\" but this makes testing cumbersome and quite complex in general. There is `terraform console` command which allows you to easily execute terraform functions and experiment with general syntax. </b></details> <details> <summary>True or False? Terraform console should be used carefully as it may modify your resources</summary><br><b> False. terraform console is ready-only. </b></details> <details> <summary>You need to render a template and get it as string. Which function would you use?</summary><br><b> `templatefile` function. </b></details> <details> <summary>Explain what <code>depends_on</code> used for and given an", "metadata": {"source_file": "learning-materials/topics/terraform/README.md", "section": "Misc", "language": "en", "created_at": "2025-07-19T19:22:02.282919"}}
{"text": "example</summary><br><b> `depends_on` used to create an explicit dependency between resources in Terraform. For example, there is an application you would like to deploy in a cluster. If the cluster isn't ready (and also managed by Terraform of course) then you can't deploy the app. In this case, you will define \"depends_on\" in the app configuration and its value will be the cluster resource. </b></details>", "metadata": {"source_file": "learning-materials/topics/terraform/README.md", "section": "Misc", "language": "en", "created_at": "2025-07-19T19:22:02.282939"}}
{"text": "<details> <summary>Explain Modules</summary> [Terraform.io](https://www.terraform.io/language/modules/develop): \"A module is a container for multiple resources that are used together. Modules can be used to create lightweight abstractions, so that you can describe your infrastructure in terms of its architecture, rather than directly in terms of physical objects.\" In addition, modules are great for creating reusable Terraform code that can be shared and used not only between different repositories but even within the same repo, between different environments (like staging and production). </b></details> <details> <summary>What makes a Terraform code module? In other words, what a module is from practical perspective?</summary> Basically any file or files in a directory is a module in Terraform. There is no special syntax to use in order to define a module. </b></details> <details> <summary>How do you test a Terraform module?</summary><br><b> There are multiple answers, but the most", "metadata": {"source_file": "learning-materials/topics/terraform/README.md", "section": "Modules", "language": "en", "created_at": "2025-07-19T19:22:02.283559"}}
{"text": "common answer would likely to be using the tool <code>terratest</code>, and to test that a module can be initialized, can create resources, and can destroy those resources cleanly. </b></details> <details> <summary>When creating a module, do you prefer to use inline blocks, separate resources or both? why?</summary> No right or wrong here. Personally, I prefer to use only separate resources in modules as it makes modules more flexible. So if a resource includes inline blocks, that may limit you at some point. </b></details> <details> <summary>True or False? Module source can be only local path</summary> False. It can be a Git URL, HTTP URL, ... for example: ``` module \"some_module\" { source = \"github.com/foo/modules/bar?ref=v0.1\" } ``` </b></details> <details> <summary>Where can you obtain Terraform modules?</summary><br><b> Terraform modules can be found at the [Terrafrom registry](https://registry.terraform.io/browse/modules) </b></details> <details> <summary>You noticed there are", "metadata": {"source_file": "learning-materials/topics/terraform/README.md", "section": "Modules", "language": "en", "created_at": "2025-07-19T19:22:02.283593"}}
{"text": "relative paths in some of your modules and you would like to change that. What can you do and why is that a problem in the first place?</summary><br><b> Relative paths usually work fine in your own environment as you are familiar with the layout and paths used, but when sharing a module and making it reusable, you may bump into issues as it runs on different environments where the relative paths may no longer be relevant. A better approach would be to use `path reference` like one of the following: * `path.module`: the path of the module where the expression is used * `path.cwd`: the path of the current working directory * `path.root`: the path of the root module </b></details>", "metadata": {"source_file": "learning-materials/topics/terraform/README.md", "section": "Modules", "language": "en", "created_at": "2025-07-19T19:22:02.283613"}}
{"text": "<details> <summary>How to use a module?</summary><br><b> The general syntax is: ``` module \"<MODULE_NAME>\" { source = \"<MODULE_SOURCE>\" ... } ``` The critical part is the source which you use to tell Terraform where the module can be found. </b></details> <details> <summary>Demonstrate using a module called \"amazing_modle\" in the path \"../modules/amazing-module\"</summary><br><b> ``` module \"amazing_module\" { source = \"../modules/amazing-module\" } ``` </b></details> <details> <summary>What should be done every time you modify the source parameter of a module?</summary><br><b> `terraform get -update` should be executed as it takes care of downloading and installing the module from the new path. </b></details> <details> <summary>How to access module output variables?</summary><br><b> the general syntax is `module.<MODULE_NAME>.<OUTPUT_VAR_NAME>` </b></details> <details> <summary>You would like to load and render a file from module directory. How to do that?</summary><br><b> script =", "metadata": {"source_file": "learning-materials/topics/terraform/README.md", "section": "Modules Hands-On", "language": "en", "created_at": "2025-07-19T19:22:02.283851"}}
{"text": "templatesfile(\"${path.module}/user-data.sh\", { ... }) </b></details> <details> <summary>There is a module to create a compute instance. How would you use the module to create three separate instances?</summary><br><b> starting with Terraform 0.13, the `count` meta-argument can be used with modules. So you could use something like this: ``` module \"instances\" { source = \"/some/module/path\" count = 3 } ``` You can also use it in outputs vars: `value = module.instances[*]` </b></details>", "metadata": {"source_file": "learning-materials/topics/terraform/README.md", "section": "Modules Hands-On", "language": "en", "created_at": "2025-07-19T19:22:02.283879"}}
{"text": "<details> <summary>Explain Terraform's import functionality</summary><br><b> `terraform import` is a CLI command used for importing an existing infrastructure into Terraform's state. It's does NOT create the definitions/configuration for creating such infrastructure. </b></details> <details> <summary>State two use cases where you would use <code>terraform import</code></summary><br><b> 1. You have existing resources in one of the providers and they are not managed by Terraform (as in not included in the state) 2. You lost your tfstate file and need to rebuild it </b></details>", "metadata": {"source_file": "learning-materials/topics/terraform/README.md", "section": "Import", "language": "en", "created_at": "2025-07-19T19:22:02.283958"}}
{"text": "<details> <summary>You have a Git repository with Terraform files but no .gitignore. What would you add to a .gitignore file in Terraform repository?</summary><br><b> ``` **/.terraform/* *.tfstate *.tfstate.* *.tfvars *.tfvars.json ``` You don't want to store state file nor any downloaded providers in .terraform directory. It also doesn't makes sense to share/store the state backup files. </b></details>", "metadata": {"source_file": "learning-materials/topics/terraform/README.md", "section": "Version Control", "language": "en", "created_at": "2025-07-19T19:22:02.284011"}}
{"text": "<details> <summary>What happens if you update user_data in the following case and apply the changes? ``` resource \"aws_instance\" \"example\" { ami = \"...\" instance_type = \"t2.micro\" user_data = <<-EOF #!/bin/bash echo \"Hello, World\" > index.xhtml EOF } ``` </summary><br><b> Nothing, because user_data is executed on boot so if an instance is already running, it won't change anything. To make it effective you'll have to use `user_data_replace_on_change = true`. </b></details> <details> <summary>You manage ASG with Terraform which means you also have \"aws_launch_configuration\" resources. The problem is that launch configurations are immutable and sometimes you need to change them. This creates a problem where Terraform isn't able to delete ASG because they reference old launch configuration. How to do deal with it?</summary><br><b> Add the following to \"aws_launch_configuration\" resource ``` lifecycle { create_before_destroy = true } ``` This will change the order of how Terraform works.", "metadata": {"source_file": "learning-materials/topics/terraform/README.md", "section": "AWS", "language": "en", "created_at": "2025-07-19T19:22:02.284384"}}
{"text": "First it will create the new resource (launch configuration). then it will update other resources to reference the new launch configuration and finally, it will remove old resources </b></details> <details> <summary>How to manage multiple regions in AWS provider configuration?</summary><br><b> ``` provider \"aws\" { region = \"us-west-1\" alias = \"west_region\" } provider \"aws\" { region = \"us-east-1\" alias = \"east_region\" } data \"aws_region\" \"west_region\" { provider = aws.west_region } data \"aws_region\" \"east_region\" { provider = aws.east_region } ``` To use it: ``` resource \"aws_instance\" \"west_region_instance\" { provider = aws.west_region instance_type = \"t2.micro\" ... } ``` </b></details> <details> <summary>Assuming you have multiple regions configured and you would like to use a module in one of them. How to achieve that?</summary><br><b> ``` module \"some_module\" { source = \"...\" providers = { aws = aws.some_region } ... } ``` </b></details> <details> <summary>How to manage multiple AWS", "metadata": {"source_file": "learning-materials/topics/terraform/README.md", "section": "AWS", "language": "en", "created_at": "2025-07-19T19:22:02.284406"}}
{"text": "accounts?</summary><br><b> One way is to define multiple different provider blocks, each with its own \"assume_role\" ``` provider \"aws\" { region = \"us-west-1\" alias = \"some-region\" assume_role { role_arn = \"arn:aws:iam::<SOME_ACCOUNT_ID>:role/<SOME_ROLE_NAME>\" } } ``` </b></details>", "metadata": {"source_file": "learning-materials/topics/terraform/README.md", "section": "AWS", "language": "en", "created_at": "2025-07-19T19:22:02.284590"}}
{"text": "<details> <summary>How would you enforce users that use your variables to provide values with certain constraints? For example, a number greater than 1</summary><br><b> Using `validation` block ``` variable \"some_var\" { type = number validation { condition = var.some_var > 1 error_message = \"you have to specify a number greater than 1\" } } ``` </b></details>", "metadata": {"source_file": "learning-materials/topics/terraform/README.md", "section": "Validations", "language": "en", "created_at": "2025-07-19T19:22:02.284654"}}
{"text": "<details> <summary>What's the issue with the following provider configuration? ``` provider \"aws\" { region = \"us-west-1\" access_key = \"blipblopblap\" secret_key = \"bipbopbipbop\" } ``` </summary><br><b> It's not secure! you should never store credentials in plain text this way. </b></details> <details> <summary>What can you do to NOT store provider credentials in Terraform configuration files in plain text?</summary><br><b> 1. Use environment variables 2. Use password CLIs (like 1Password which is generic but there also specific provider options like aws-vault) </b></details> <details> <summary>How can you manage secrets/credentials in CI/CD?</summary><br><b> That very much depends on the CI/CD system/platform you are using. - GitHub Actions: Use Open ID Connect (OIDC) to establish connection with your provider. You then can specify in your GitHub Actions workflow the following: ``` - uses: aws-actions/configure-aws-credentials@v1 with: role-to-assume: arn:aws:iam::someIamRole", "metadata": {"source_file": "learning-materials/topics/terraform/README.md", "section": "Secrets", "language": "en", "created_at": "2025-07-19T19:22:02.285147"}}
{"text": "aws-region: ... ``` - Jenkins: If Jenkins runs on the provider, you can use the provider access entities (like roles, policies, ...) to grant the instance, on which Jenkins is running, access control - CircleCI: you can use `CircleCI Context` and then specify it in your CircleCI config file ``` context: - some-context ``` </b></details> <details> <summary>What are the pros and cons of using environment variables for managing secrets in Terraform configurations?</summary><br><b> Pros: * You avoid using secrets directly in configurations in plain text * free (no need to pay for secret management platforms/solutions) * Straightforward to use Cons: * Configurations might not be usable without the environment variables which may make impact the user experience as the user has to know what environment variables he should pass for everything to work properly * Mostly managed outside of Terraform mechanisms which makes it hard to enforce, track, ... anything that is related to secrets when it", "metadata": {"source_file": "learning-materials/topics/terraform/README.md", "section": "Secrets", "language": "en", "created_at": "2025-07-19T19:22:02.285175"}}
{"text": "depends on the user to pass environment variables </b></details> <details> <summary>True or False? If you pass secrets with environment variables, they are not visible in your state file</summary><br><b> False. State files include sensitive data as it is. Which means it's very important that wherever you store your state file, it's encrypted and accessible only to those who should be able to access it. </b></details> <details> <summary>True or False? If you pass secrets from a centralized secrets store (like Hashicorp Vault) they are not visible in plan files (terraform plan)</summary><br><b> False. It doesn't matter where your secrets store (file, environment variables, centralized secrets store), they will be visible in both state file and plan output. </b></details>", "metadata": {"source_file": "learning-materials/topics/terraform/README.md", "section": "Secrets", "language": "en", "created_at": "2025-07-19T19:22:02.285196"}}
{"text": "This section is about how Terraform is actually used in real-life scenarios and organizations. <details> <summary>What structure layout do you use for your projects?</summary><br><b> There is no right or wrong answer, just what you personally adopted or your team, and being able to explain why. One common approach is to have a separate directory for each environment. ``` terraform_project/ staging/ production/ ``` Each environment has its own backend (as you don't want to use the same authentication and access controls for all environments) Going further, under each environment you'll separate between components, applications and services ``` terraform_project/ staging/ applications/ some-app-service-1/ some-app-service-2/ databases/ mongo/ postgres/ networking/ ``` </b></details> <details> <summary>What files do you have you have in your Terraform projects?</summary><br><b> Again, no right or wrong answer. Just your personal experience. main.tf providers.tf outputs.tf variables.tf", "metadata": {"source_file": "learning-materials/topics/terraform/README.md", "section": "Production", "language": "en", "created_at": "2025-07-19T19:22:02.286007"}}
{"text": "dependencies.tf Each one of these files can be divided to smaller parts if needed (no reason to maintain VERY long files) </b></details> <details> <summary>An engineer in your team complains about having to copy-paste quite a lot of code between different folders and files of Terraform. What would you do?</summary><br><b> Suggest to use Terraform modules. </b></details> <details> <summary>When working with nested layout of many directories, it can make it cumbresome to run terraform commands in many different folders. How to deal with it?</summary><br><b> There are multiple ways to deal with it: 1. Write scripts that perform some commands recursively with different conditions 2. Use tools like Terragrunt where you commands like \"run-all\" that can run in parallel on multiple different paths </b></details> <details> <summary>One of the engineers in your team complains the inline shell scripts are quite big and maintaining them in Terraform files seems like a bad idea. What would you", "metadata": {"source_file": "learning-materials/topics/terraform/README.md", "section": "Production", "language": "en", "created_at": "2025-07-19T19:22:02.286037"}}
{"text": "do?</summary><br><b> A good solution for not including shell scripts inline (as in inside terraform configuration files) is to keep them in a separate file and then use the terraform `templatefile` function to render and get them as a string </b></details> <details> <summary>You noticed a lot of your Terraform code/configuration is duplicated, between repositories and also within the same repository between different directories. What one way you may adopt that will help handling with that?</summary><br><b> Using Terraform modules can help greatly with duplicated code and so different environments for example (staging and production) can reuse the same code by using the same modules. </b></details> <details> <summary>You noticed your Terraform code includes quite a lot of hardcoded values (like ports, subnets, ...) and they are duplicated in many locations. How'd you deal with it?</summary><br><b> Using variables might not be a good solution because some things shouldn't be exposed and", "metadata": {"source_file": "learning-materials/topics/terraform/README.md", "section": "Production", "language": "en", "created_at": "2025-07-19T19:22:02.286057"}}
{"text": "accidentally overridden. In such case you might want to use the concept of `locals` </b></details> <details> <summary>Every time there is a change in tags standards (for example your team decided to change one of the tags' name) you find yourself changing tags in multiple files and you find the process quite tedious. What can be done about it?</summary><br><b> Instead of defining tags at resource level, consider using `default_tags` as part of the provider configuration. </b></details> <details> <summary>You would like to change the name of a resource but afraid to cause downtime. What can be done?</summary><br><b> If it's a matter of changing a resource name, you could make use of `terraform state mv <ORIGINAL_RESOURCE_NAME> <NEW_RESOURCE_NAME>` </b></details> <details> <summary>You try to deploy a cluster and an app on that cluster, but the app resource was created before the cluster. How to manage such situation?</summary><br><b> Use the meta-argument `depends_on` in the app", "metadata": {"source_file": "learning-materials/topics/terraform/README.md", "section": "Production", "language": "en", "created_at": "2025-07-19T19:22:02.286077"}}
{"text": "resource definition. This way the app will depend on the cluster resource and order will be maintained in creation of the resources. </b></details>", "metadata": {"source_file": "learning-materials/topics/terraform/README.md", "section": "Production", "language": "en", "created_at": "2025-07-19T19:22:02.286287"}}
{"text": "* An existing S3 bucket tracked by Terraform. If you don't have it, you can use the following block and run `terraform apply`: ```terraform resource \"aws_s3_bucket\" \"some_bucket\" { bucket = \"some-old-bucket\" } ``` Attention: Since S3 buckets are globally unique, you will likely have to rename the bucket as someone else might have named it that way already.", "metadata": {"source_file": "learning-materials/topics/terraform/exercises/s3_bucket_rename/solution.md", "section": "Requirements", "language": "en", "created_at": "2025-07-19T19:22:02.286653"}}
{"text": "1. Rename an existing S3 bucket and make sure it's still tracked by Terraform", "metadata": {"source_file": "learning-materials/topics/terraform/exercises/s3_bucket_rename/solution.md", "section": "Objectives", "language": "en", "created_at": "2025-07-19T19:22:02.286686"}}
{"text": "```sh", "metadata": {"source_file": "learning-materials/topics/terraform/exercises/s3_bucket_rename/solution.md", "section": "Solution", "language": "en", "created_at": "2025-07-19T19:22:02.286700"}}
{"text": "aws s3 mb s3://some-new-bucket-123", "metadata": {"source_file": "learning-materials/topics/terraform/exercises/s3_bucket_rename/solution.md", "section": "A bucket name is immutable in AWS so we'll have to create a new bucket", "language": "en", "created_at": "2025-07-19T19:22:02.286754"}}
{"text": "aws s3 sync s3://some-old-bucket s3://some-new-bucket-123", "metadata": {"source_file": "learning-materials/topics/terraform/exercises/s3_bucket_rename/solution.md", "section": "Sync old bucket to new bucket", "language": "en", "created_at": "2025-07-19T19:22:02.286775"}}
{"text": "terraform state rm aws_s3_bucket.some_bucket", "metadata": {"source_file": "learning-materials/topics/terraform/exercises/s3_bucket_rename/solution.md", "section": "Remove the old bucket from Terraform's state", "language": "en", "created_at": "2025-07-19T19:22:02.286791"}}
{"text": "terraform import aws_s3_bucket.some_bucket some-new-bucket-123 : ' aws_s3_bucket.some_bucket: Refreshing state... [id=some-new-bucket-123] Import successful! The resources that were imported are shown above. These resources are now in your Terraform state and will henceforth be managed by Terraform. '", "metadata": {"source_file": "learning-materials/topics/terraform/exercises/s3_bucket_rename/solution.md", "section": "Import new bucket to Terraform's state", "language": "en", "created_at": "2025-07-19T19:22:02.286822"}}
{"text": "terraform state mv aws_s3_bucket.some_bucket some-new-bucket-123 : ' Move \"aws_s3_bucket.some_bucket\" to \"aws_s3_bucket.some-new-bucket-123\" Successfully moved 1 object(s). '", "metadata": {"source_file": "learning-materials/topics/terraform/exercises/s3_bucket_rename/solution.md", "section": "Move the old bucket from Terraform's state to the new one", "language": "en", "created_at": "2025-07-19T19:22:02.286844"}}
{"text": "aws s3 rm s3://some-old-bucket --recursive aws s3 rb s3://some-old-bucket ```", "metadata": {"source_file": "learning-materials/topics/terraform/exercises/s3_bucket_rename/solution.md", "section": "Remove old bucket", "language": "en", "created_at": "2025-07-19T19:22:02.286862"}}
{"text": "Click [here to view the solution](solution.md)", "metadata": {"source_file": "learning-materials/topics/terraform/exercises/s3_bucket_rename/exercise.md", "section": "Solution", "language": "en", "created_at": "2025-07-19T19:22:02.286997"}}
{"text": "Learn how to use and run Terraform basic commands 1. Create a directory called \"my_first_run\" 2. Inside the directory create a file called \"main.tf\" with the following content ```terraform resource \"local_file\" \"mario_local_file\" { content = \"It's a me, Mario!\" filename = \"/tmp/who_is_it.txt\" } ``` 3. Run `terraform init`. What did it do? 4. Run `terraform plan`. What Terraform is going to perform? 5. Finally, run 'terraform apply' and verify the file was created", "metadata": {"source_file": "learning-materials/topics/terraform/exercises/terraform_local_provider/solution.md", "section": "Objectives", "language": "en", "created_at": "2025-07-19T19:22:02.287225"}}
{"text": "mkdir my_first_run && cd my_first_run", "metadata": {"source_file": "learning-materials/topics/terraform/exercises/terraform_local_provider/solution.md", "section": "Create a directory", "language": "en", "created_at": "2025-07-19T19:22:02.287251"}}
{"text": "cat << EOT >> main.tf resource \"local_file\" \"mario_local_file\" { content = \"It's a me, Mario!\" filename = \"/tmp/who_is_it.txt\" } EOT", "metadata": {"source_file": "learning-materials/topics/terraform/exercises/terraform_local_provider/solution.md", "section": "Create the file 'main.tf'", "language": "en", "created_at": "2025-07-19T19:22:02.287273"}}
{"text": "terraform init", "metadata": {"source_file": "learning-materials/topics/terraform/exercises/terraform_local_provider/solution.md", "section": "Run 'terraform init'", "language": "en", "created_at": "2025-07-19T19:22:02.287286"}}
{"text": "terraform plan", "metadata": {"source_file": "learning-materials/topics/terraform/exercises/terraform_local_provider/solution.md", "section": "Run 'terraform plan'", "language": "en", "created_at": "2025-07-19T19:22:02.287299"}}
{"text": "<< terraform_plan_output Terraform will perform the following actions: # local_file.mario_local_file will be created + resource \"local_file\" \"mario_local_file\" { + content = \"It's a me, Mario!\" + directory_permission = \"0777\" + file_permission = \"0777\" + filename = \"/tmp/who_is_it.txt\" + id = (known after apply) } Plan: 1 to add, 0 to change, 0 to destroy. terraform_plan_output", "metadata": {"source_file": "learning-materials/topics/terraform/exercises/terraform_local_provider/solution.md", "section": "It shows what Terraform is going to perform once you'll run 'terraform apply'", "language": "en", "created_at": "2025-07-19T19:22:02.287345"}}
{"text": "terraform apply -auto-approve ls /tmp/who_is_it.txt", "metadata": {"source_file": "learning-materials/topics/terraform/exercises/terraform_local_provider/solution.md", "section": "Apply main.tf (it's better to run without -auto-approve if you are new to Terraform)", "language": "en", "created_at": "2025-07-19T19:22:02.287374"}}
{"text": "Learn how to use and run Terraform basic commands 1. Create a directory called \"my_first_run\" 2. Inside the directory create a file called \"main.tf\" with the following content ```terraform resource \"local_file\" \"mario_local_file\" { content = \"It's a me, Mario!\" filename = \"/tmp/who_is_it.txt\" } ``` 3. Run `terraform init`. What did it do? 4. Run `terraform plan`. What Terraform is going to perform? 5. Finally, run `terraform apply` and verify the file was created", "metadata": {"source_file": "learning-materials/topics/terraform/exercises/terraform_local_provider/exercise.md", "section": "Objectives", "language": "en", "created_at": "2025-07-19T19:22:02.287576"}}
{"text": "* AWS account", "metadata": {"source_file": "learning-materials/topics/terraform/exercises/launch_ec2_instance/solution.md", "section": "Requirements", "language": "en", "created_at": "2025-07-19T19:22:02.287816"}}
{"text": "1. Write Terraform configuration for launching an EC2 instance 2. Run the commands to apply the configuration and create the EC2 instance 3. What happens if you run again `terraform apply`? 4. Destroy the instance you've created with Terraform", "metadata": {"source_file": "learning-materials/topics/terraform/exercises/launch_ec2_instance/solution.md", "section": "Objectives", "language": "en", "created_at": "2025-07-19T19:22:02.287861"}}
{"text": "``` mkdir exercise cat << EOT >> main.tf terraform { required_providers { aws = { source = \"hashicorp/aws\" version = \"~> 4.16\" } } required_version = \">= 1.2.0\" } provider \"aws\" { region = \"us-west-2\" } resource \"aws_instance\" \"app_server\" { ami = \"ami-830c94e3\" instance_type = \"t2.micro\" tags = { Name = \"ExampleAppServerInstance\" } } EOT terraform init terraform validate terraform plan", "metadata": {"source_file": "learning-materials/topics/terraform/exercises/launch_ec2_instance/solution.md", "section": "Solution", "language": "en", "created_at": "2025-07-19T19:22:02.287918"}}
{"text": "terraform apply -auto-approve", "metadata": {"source_file": "learning-materials/topics/terraform/exercises/launch_ec2_instance/solution.md", "section": "You should see this line at the end: Plan: 1 to add, 0 to change, 0 to destroy", "language": "en", "created_at": "2025-07-19T19:22:02.288063"}}
{"text": "terraform destroy -auto-approve", "metadata": {"source_file": "learning-materials/topics/terraform/exercises/launch_ec2_instance/solution.md", "section": "Remove instance", "language": "en", "created_at": "2025-07-19T19:22:02.288089"}}
{"text": "<details> <summary>Explain the following code: <code>:(){ :|:& };:</code> </summary><br><b> </b></details> <details> <summary>Can you give an example to some Bash best practices?</summary><br><b> </b></details> <details> <summary>What is the ternary operator? How do you use it in bash?</summary><br><b> A short way of using if/else. An example: [[ $a = 1 ]] && b=\"yes, equal\" || b=\"nope\" </b></details> <details> <summary>What does the following code do and when would you use it? <code>diff <(ls /tmp) <(ls /var/tmp)</code> </summary><br> It is called 'process substitution'. It provides a way to pass the output of a command to another command when using a pipe <code>|</code> is not possible. It can be used when a command does not support <code>STDIN</code> or you need the output of multiple commands. https://superuser.com/a/1060002/167769 </details>", "metadata": {"source_file": "learning-materials/tests/testcases/testcase2.md", "section": "Introduction", "language": "en", "created_at": "2025-07-19T19:22:02.288969"}}
{"text": "<details> <summary>What does SQL stand for?</summary><br><b> Structured Query Language </b></details> <details> <summary>How is SQL Different from NoSQL</summary><br><b> The main difference is that SQL databases are structured (data is stored in the form of tables with rows and columns - like an excel spreadsheet table) while NoSQL is unstructured, and the data storage can vary depending on how the NoSQL DB is set up, such as key-value pair, document-oriented, etc. </b></details> <details> <summary>What does it mean when a database is ACID compliant?</summary><br> ACID stands for Atomicity, Consistency, Isolation, Durability. In order to be ACID compliant, the database much meet each of the four criteria **Atomicity** - When a change occurs to the database, it should either succeed or fail as a whole. For example, if you were to update a table, the update should completely execute. If it only partially executes, the update is considered failed as a whole, and will not go through - the", "metadata": {"source_file": "learning-materials/tests/testcases/testcase2.md", "section": ":baby: Beginner", "language": "en", "created_at": "2025-07-19T19:22:02.289868"}}
{"text": "DB will revert back to it's original state before the update occurred. It should also be mentioned that Atomicity ensures that each transaction is completed as it's own stand alone \"unit\" - if any part fails, the whole statement fails. **Consistency** - any change made to the database should bring it from one valid state into the next. For example, if you make a change to the DB, it shouldn't corrupt it. Consistency is upheld by checks and constraints that are pre-defined in the DB. For example, if you tried to change a value from a string to an int when the column should be of datatype string, a consistent DB would not allow this transaction to go through, and the action would not be executed **Isolation** - this ensures that a database will never be seen \"mid-update\" - as multiple transactions are running at the same time, it should still leave the DB in the same state as if the transactions were being run sequentially. For example, let's say that 20 other people were making changes", "metadata": {"source_file": "learning-materials/tests/testcases/testcase2.md", "section": ":baby: Beginner", "language": "en", "created_at": "2025-07-19T19:22:02.289900"}}
{"text": "to the database at the same time. At the time you executed your query, 15 of the 20 changes had gone through, but 5 were still in progress. You should only see the 15 changes that had completed - you wouldn't see the database mid-update as the change goes through. **Durability** - Once a change is committed, it will remain committed regardless of what happens (power failure, system crash, etc.). This means that all completed transactions must be recorded in non-volatile memory. Note that SQL is by nature ACID compliant. Certain NoSQL DB's can be ACID compliant depending on how they operate, but as a general rule of thumb, NoSQL DB's are not considered ACID compliant </details> <details> <summary>When is it best to use SQL? NoSQL?</summary><br><b> SQL - Best used when data integrity is crucial. SQL is typically implemented with many businesses and areas within the finance field due to it's ACID compliance. NoSQL - Great if you need to scale things quickly. NoSQL was designed with web", "metadata": {"source_file": "learning-materials/tests/testcases/testcase2.md", "section": ":baby: Beginner", "language": "en", "created_at": "2025-07-19T19:22:02.289923"}}
{"text": "applications in mind, so it works great if you need to quickly spread the same information around to multiple servers Additionally, since NoSQL does not adhere to the strict table with columns and rows structure that Relational Databases require, you can store different data types together. </b></details> <details> <summary>What is a Cartesian Product?</summary><br> A Cartesian product is when all rows from the first table are joined to all rows in the second table. This can be done implicitly by not defining a key to join, or explicitly by calling a CROSS JOIN on two tables, such as below: Select * from customers **CROSS JOIN** orders; Note that a Cartesian product can also be a bad thing - when performing a join on two tables in which both do not have unique keys, this could cause the returned information to be incorrect. </details>", "metadata": {"source_file": "learning-materials/tests/testcases/testcase2.md", "section": ":baby: Beginner", "language": "en", "created_at": "2025-07-19T19:22:02.289944"}}
{"text": "</b></details> <details> <summary>How many items are in John's cart?</summary><br><b> Select Items_in_cart <br> From Customers <br> Where Customer_Name = \"John Smith\"; </b></details> <details> <summary>What is the sum of all the cash spent across all customers?</summary><br><b> Select SUM(Cash_spent_to_Date) as SUM_CASH <br> From Customers; </b></details> <details> <summary>Tell me about your last big project/task you worked on</summary><br><b> </b></details> <details> <summary>What was most challenging part in the project you worked on?</summary><br><b> </b></details> <details> <summary>Why do you want to work here?</summary><br><b> </b></details> <details> <summary>How did you hear about us?</summary><br><b> Tell them how did you hear about them :D Relax, there is no wrong or right answer here...I think. </b></details>", "metadata": {"source_file": "learning-materials/tests/testcases/testcase2.md", "section": "SQL Specific Questions", "language": "en", "created_at": "2025-07-19T19:22:02.290299"}}
{"text": "<details> <summary>What is Docker? What are you using it for?</summary><br><b> </b></details> <details> <summary>How containers are different from VMs?</summary><br><b> The primary difference between containers and VMs is that containers allow you to virtualize multiple workloads on the operating system while in the case of VMs the hardware is being virtualized to run multiple machines each with its own OS. </b></details> <details> <summary>In which scenarios would you use containers and in which you would prefer to use VMs?</summary><br><b> You should choose VMs when: * you need run an application which requires all the resources and functionalities of an OS * you need full isolation and security You should choose containers when: * you need a lightweight solution * Running multiple versions or instances of a single application </b></details> <details> <summary>Explain Docker architecture</summary><br><b> </b></details> <details> <summary>Describe in detail what happens when you run", "metadata": {"source_file": "learning-materials/tests/testcases/testcase1.md", "section": "Introduction", "language": "en", "created_at": "2025-07-19T19:22:02.290763"}}
{"text": "`docker run hello-world`?</summary><br><b> Docker CLI passes your request to Docker daemon. Docker daemon downloads the image from Docker Hub Docker daemon creates a new container by using the image it downloaded Docker daemon redirects output from container to Docker CLI which redirects it to the standard output </b></details> <details> <summary>How do you run a container?</summary><br><b> </b></details> <details> <summary>What `docker commit` does?. When will you use it?</summary><br><b> </b></details> <details> <summary>How would you transfer data from one container into another?</summary><br><b> </b></details> <details> <summary>What happens to data of the container when a container exists?</summary><br><b> </b></details> <details> <summary>Explain what each of the following commands do: * docker run * docker rm * docker ps * docker pull * docker build * docker commit</summary><br><b> </b></details> <details> <summary>How do you remove old, non running, containers?</summary><br><b>", "metadata": {"source_file": "learning-materials/tests/testcases/testcase1.md", "section": "Introduction", "language": "en", "created_at": "2025-07-19T19:22:02.290921"}}
{"text": "<summary>You have a colleague you don‘t get along with. Tell us some strategies how you create a good work relationship with them anyway.</summary><br><b> Bad answer: I don't. Better answer: Every person has strengths and weaknesses. This is true also for colleagues I don't have good work relationship with and this is what helps me to create good work relationship with them. If I am able to highlight or recognize their strengths I'm able to focus mainly on that when communicating with them. </b></details> <details> <summary>What do you love about your work?</summary><br><b> You know the best, but some ideas if you find it hard to express yourself: * Diversity * Complexity * Challenging * Communication with several different teams </b></details> <details> <summary>What are your responsibilities in your current position?</summary><br><b> You know the best :) </b></details> <summary>Why should we hire you for the role?</summary><br><b> You can use and elaborate on one or all of the", "metadata": {"source_file": "learning-materials/tests/testcases/testcase3.md", "section": "Introduction", "language": "en", "created_at": "2025-07-19T19:22:02.291469"}}
{"text": "following: * Passion * Motivation * Autodidact * Creativity (be able to support it with some actual examples) </b></details>", "metadata": {"source_file": "learning-materials/tests/testcases/testcase3.md", "section": "Introduction", "language": "en", "created_at": "2025-07-19T19:22:02.291550"}}
{"text": "A list of questions you as a candidate can ask the interviewer during or after the interview. These are only a suggestion, use them carefully. Not every interviewer will be able to answer these (or happy to) which should be perhaps a red flag warning for your regarding working in such place but that's really up to you. <details> <summary>What do you like about working here?</summary><br><b> </b></details> <details> <summary>How does the company promote personal growth?</summary><br><b> </b> <details> <summary>What is the current level of technical debt you are dealing with?</summary><br><b> Be careful when asking this question - all companies, regardless of size, have some level of tech debt. Phrase the question in the light that all companies have the deal with this, but you want to see the current pain points they are dealing with <br> This is a great way to figure how managers deal with unplanned work, and how good they are at setting expectations with projects. </b></details>", "metadata": {"source_file": "learning-materials/tests/testcases/testcase3.md", "section": "Questions you CAN ask", "language": "en", "created_at": "2025-07-19T19:22:02.291807"}}
{"text": "help () { echo \"Usage: compare <filename1> <filename2>\" echo } validate_args() { # Ensure that 2 arguments are passed if [ $# != 2 ] then help exit 1 fi i=1 for dir in \"$@\" do # Validate existence of directories if [ ! -d \"$dir\" ] then echo \"Directory $dir does not exist\" exit 1 fi echo \"Directory $i: $dir\" i=$((i + 1)) done echo } compare() { echo \"Comparing directories...\" echo diff -r \"$1\" \"$2\" if [ $? -eq 0 ] then echo \"No difference\" fi exit 0 } while getopts \":h\" option; do case $option in h) # display Help help exit 0;; \\?) # invalid option echo \"Error: Invalid option\" exit 1;; esac done validate_args \"$@\" compare \"$1\" \"$2\" ```", "metadata": {"source_file": "learning-materials/exercises/shell/solutions/directories_comparison.md", "section": "!/usr/bin/env bash", "language": "en", "created_at": "2025-07-19T19:22:02.292167"}}
